1
00:00:00,000 --> 00:00:02,000
What about more once we start?

2
00:00:30,000 --> 00:00:37,840
All right, we're gonna get started now.

3
00:00:37,840 --> 00:00:41,360
Yeah, it's been two minutes.

4
00:00:41,360 --> 00:00:46,280
Hi everyone, thank you for coming.

5
00:00:46,280 --> 00:00:50,160
This is the tutorial for analyzing census data with pandas.

6
00:00:50,160 --> 00:00:54,000
If it's not the tutorial, you sign up for it, you can leave.

7
00:00:54,000 --> 00:00:57,680
You don't need to install anything beforehand.

8
00:00:57,680 --> 00:00:58,800
We're gonna be using my binder.

9
00:00:59,520 --> 00:01:05,880
So we're gonna I'm gonna show you the link. It's actually that link up top and then it's gonna send you to

10
00:01:07,400 --> 00:01:13,200
My binder which is distribution binder hub distribution. You pointed it over a github repo

11
00:01:13,200 --> 00:01:18,320
They take all the requirements install it and then you're gonna have a jupyter lab interface on your browser

12
00:01:18,320 --> 00:01:20,320
So you don't need to install anything

13
00:01:20,320 --> 00:01:22,320
I'll show you the the

14
00:01:22,560 --> 00:01:28,280
info for the github repo in a bit if you want to download this stuff and follow along in your own computer

15
00:01:29,640 --> 00:01:31,640
Before we start I got

16
00:01:31,680 --> 00:01:36,840
Some housekeeping notes. There's gonna be a survey link email to you afterwards

17
00:01:38,000 --> 00:01:40,000
About this tutorial how I did

18
00:01:41,400 --> 00:01:43,400
And and there's gonna be a

19
00:01:44,160 --> 00:01:50,000
Snack break at 3 and if you see that I'm not slowing down by like 250 or 245

20
00:01:50,520 --> 00:01:54,120
You can let me know that you're gonna have to go on the fact break soon

21
00:01:54,840 --> 00:01:56,840
So

22
00:01:56,840 --> 00:02:00,280
Let's get started. So who am I? I my name is Sergio Sanchez

23
00:02:00,760 --> 00:02:06,640
I'm a research associate at the Public Policy Institute of California in the higher ed center. So I do

24
00:02:08,480 --> 00:02:12,200
Objective independent nonpartisan research

25
00:02:14,200 --> 00:02:20,540
Mostly on higher ed public policy in California and a lot of the work that I that I do

26
00:02:21,840 --> 00:02:23,840
our work is

27
00:02:24,120 --> 00:02:29,160
Covers developmental education reform, which is which is huge in California right now in community colleges

28
00:02:29,800 --> 00:02:31,800
some stuff about economic mobility and

29
00:02:32,560 --> 00:02:37,520
some stuff about immigration and immigration patterns and people coming in and out of California a

30
00:02:37,800 --> 00:02:39,800
lot of this is based on

31
00:02:41,080 --> 00:02:48,320
Census data, especially the e-com mobility stuff the immigration stuff. There's a lot of data the census collects and

32
00:02:50,120 --> 00:02:52,120
So we're gonna learn some of that

33
00:02:52,640 --> 00:02:54,640
I'm also on my free time

34
00:02:55,640 --> 00:02:59,040
Very interested in data visualization. I am one of the facilitators

35
00:02:59,840 --> 00:03:04,640
the newly formed data visualization society if you're in Twitter on

36
00:03:05,720 --> 00:03:09,600
Looking into the hashtag data is you might have heard of them

37
00:03:10,120 --> 00:03:12,640
I also have a personal project

38
00:03:13,560 --> 00:03:18,800
Called tacos de datos or data tacos where I where it's what I'm doing right now

39
00:03:18,800 --> 00:03:25,400
But in Spanish and try to expand this knowledge and best practices to Latin America and Spain

40
00:03:27,840 --> 00:03:34,960
Some housekeeping you don't need to download anything we're gonna use my binder and also this is like the best room

41
00:03:36,320 --> 00:03:39,120
To be to be in if we're gonna need Wi-Fi at all

42
00:03:39,760 --> 00:03:41,400
So we should be good

43
00:03:41,400 --> 00:03:47,760
But the github repo is here. You can clone it and just CD into it. All we need is

44
00:03:48,800 --> 00:03:53,440
Jupiter and pandas and you can I'm gonna be using Jupiter lab and when we go on my binder

45
00:03:53,440 --> 00:03:58,760
We're gonna be using Jupiter lab, but you can follow along on a jupyter notebook if you're more comfortable doing that

46
00:04:02,880 --> 00:04:04,960
What I just said we're gonna be using Jupiter lab

47
00:04:07,400 --> 00:04:09,960
We're gonna be using my binder like I said, this is

48
00:04:11,880 --> 00:04:17,000
Binder allows you to create custom computing environments that can be shared and used by many remote users

49
00:04:17,160 --> 00:04:24,480
So what I did was put together a github repo I added a directory there with our requirements

50
00:04:25,920 --> 00:04:30,320
Txt file or environment that yamo file for a conda environment

51
00:04:30,320 --> 00:04:35,320
And then you go to my binder dot org dot org you point it at your github repo

52
00:04:35,680 --> 00:04:40,680
They're gonna take that grab all the materials there install the dependencies and then you can have a

53
00:04:41,560 --> 00:04:44,320
Jupiter interface a jupyter notebook a jupyter lab

54
00:04:45,040 --> 00:04:47,720
They have our studio now too and even shiny apps

55
00:04:52,560 --> 00:04:57,080
But you can have a lot of stuff and it's free movie free it's a great resource

56
00:04:59,720 --> 00:05:03,480
So census data this is gonna be I'm gonna do a quick

57
00:05:05,240 --> 00:05:11,600
Presentation talking about census data and when we when we what they have and some packages in Python that are

58
00:05:12,000 --> 00:05:17,720
Very popular to to download and access this data and then we're gonna get started on the tutorial

59
00:05:19,160 --> 00:05:24,320
So the US census conducts more than 130 surveys every year

60
00:05:25,200 --> 00:05:29,640
They have household surveys with data on education employment health

61
00:05:30,360 --> 00:05:32,360
migration income levels

62
00:05:35,200 --> 00:05:40,600
Just a lot of stuff facilities how if you have access to the internet if you have access to

63
00:05:41,600 --> 00:05:43,200
running water

64
00:05:43,200 --> 00:05:48,080
Just all types of stuff is it's a lot and I'm just gonna show you real quick

65
00:06:05,960 --> 00:06:11,280
So under census on census gov you can see all the information about their data

66
00:06:11,600 --> 00:06:17,560
These are some of the household surveys and you could spend a lot of time going through every single one of them

67
00:06:17,880 --> 00:06:21,560
They have stuff on education and children's health college graduates

68
00:06:22,240 --> 00:06:24,240
fishing hunting wildlife

69
00:06:24,880 --> 00:06:31,060
So it's just a massive amount of information the census has available for you for all of us for free

70
00:06:33,500 --> 00:06:35,500
They also have

71
00:06:36,460 --> 00:06:42,620
They also have business surveys they go out and survey retail

72
00:06:44,860 --> 00:06:48,620
All types of businesses and they have statistics on entrepreneurship

73
00:06:50,260 --> 00:06:56,680
The important export stuff on the microeconomic level which is kind of like what's more commonly known when you hear about in the news

74
00:06:56,680 --> 00:06:59,800
Oh, we have a trade deficit with X and X country

75
00:07:00,500 --> 00:07:03,220
But they also have like monthly retail sales

76
00:07:03,460 --> 00:07:05,460
So if you want to

77
00:07:05,980 --> 00:07:11,700
If you're thinking about starting a business in certain part of the country and you're like, oh, I don't know if I should have here

78
00:07:11,700 --> 00:07:15,420
I'm cutting next over and you can you can find a lot of information about

79
00:07:16,740 --> 00:07:18,740
the market

80
00:07:18,740 --> 00:07:20,740
Where you might want to start business

81
00:07:22,740 --> 00:07:26,860
One of the most popular household service is the American commuter survey they do it every year since

82
00:07:27,380 --> 00:07:31,260
2006 I believe and it's one of the

83
00:07:31,580 --> 00:07:37,240
And it's one of the the best sources of information when it comes to population

84
00:07:38,100 --> 00:07:40,100
in the US and it's

85
00:07:40,900 --> 00:07:47,500
You can you can look at this data up to all the way down to like census tracts in some areas

86
00:07:48,260 --> 00:07:53,020
Depending on how many people live in some parts of the country, but you can get like very fine

87
00:07:53,620 --> 00:07:57,780
Data out of out of this and this is all completely free completely open

88
00:07:58,780 --> 00:08:05,900
And we're gonna this is actually what we're gonna be studying we're gonna be analyzing today a subset of the American community survey

89
00:08:08,180 --> 00:08:12,420
So, how do you get this information how do you get all this data from the census

90
00:08:13,660 --> 00:08:18,780
I'm gonna show you two or three ways to do it. They have what's called the American fact-finder and

91
00:08:19,780 --> 00:08:24,220
This is a curated website. You can you can shop here

92
00:08:27,820 --> 00:08:32,740
And they have tables already prepared for you and a lot of statistics

93
00:08:34,540 --> 00:08:37,580
Again age business and industry from the business

94
00:08:38,100 --> 00:08:39,220
surveys

95
00:08:39,220 --> 00:08:43,020
Poverty levels there's poverty measuring poverty can get really complicated

96
00:08:43,820 --> 00:08:45,820
So they have a lot of different measures

97
00:08:46,780 --> 00:08:48,300
race ethnicity

98
00:08:48,300 --> 00:08:49,820
veterans and

99
00:08:49,820 --> 00:08:51,820
You can click on any of these

100
00:08:53,940 --> 00:08:55,940
I have a

101
00:08:56,940 --> 00:09:04,020
A lot of information and you can you can modify this table move it around and just like I'm not interested in

102
00:09:05,300 --> 00:09:07,860
In this one variable or this other variable

103
00:09:08,500 --> 00:09:10,620
And you can start curating your own tables if that's

104
00:09:11,580 --> 00:09:14,920
You need it for any type of research that you're looking for

105
00:09:14,920 --> 00:09:16,920
I

106
00:09:23,440 --> 00:09:27,960
Am going to faz you can tell me slow down if you have any questions just raise your hand

107
00:09:29,800 --> 00:09:31,880
They also have tables

108
00:09:31,880 --> 00:09:37,360
Pre-populated pre-computed tables kind of like American fact-finder American fact-finder just puts them all together in one nice

109
00:09:38,160 --> 00:09:40,160
place, but they have

110
00:09:41,040 --> 00:09:46,400
These pre-populated tables that are very common

111
00:09:47,360 --> 00:09:49,200
in

112
00:09:49,200 --> 00:09:51,560
journalism or in certain certain research

113
00:09:52,760 --> 00:09:55,720
areas where you all you need to do is

114
00:09:56,520 --> 00:10:01,120
Find out like the median income of X County or X

115
00:10:02,160 --> 00:10:04,160
Are you going to compare?

116
00:10:04,640 --> 00:10:06,640
historic levels of income and

117
00:10:07,200 --> 00:10:10,640
Every single county of the US and they have

118
00:10:12,400 --> 00:10:18,720
They have them ready to go and because they because these are computer by the census they have access to

119
00:10:19,320 --> 00:10:24,200
The whole census data when we're looking at things like the ACS and stuff that we can get

120
00:10:24,480 --> 00:10:29,920
They obviously for privacy reasons you cannot get access to to the whole data file you can get

121
00:10:30,800 --> 00:10:32,800
What's called a sample a public use?

122
00:10:33,440 --> 00:10:35,440
micro-sample

123
00:10:35,480 --> 00:10:37,000
pumps

124
00:10:37,000 --> 00:10:41,160
And but these are calculated from the total

125
00:10:41,680 --> 00:10:44,520
Data so these if what you're looking for is

126
00:10:45,160 --> 00:10:49,720
Can be found in one of these tables this could this would be more accurate, and I'm just going to show you

127
00:10:51,280 --> 00:10:53,280
One of them so you can see

128
00:10:56,040 --> 00:10:58,040
Detail

129
00:10:58,560 --> 00:11:00,560
I'm not gonna open it

130
00:11:01,480 --> 00:11:07,920
But all these tables are also found in American fact-finder. That's that's what they're doing instead of you downloading an excel file

131
00:11:08,600 --> 00:11:11,760
you can just go to American fact-finder and if you know the

132
00:11:13,480 --> 00:11:16,000
The table that you're looking for the information that you're looking for

133
00:11:18,280 --> 00:11:20,120
You can just

134
00:11:20,120 --> 00:11:25,000
Even just Google it and by that by by that I mean these tables don't are not named

135
00:11:25,840 --> 00:11:26,960
gender

136
00:11:27,000 --> 00:11:31,320
Statistics by state or they're not named educational attainment by

137
00:11:32,360 --> 00:11:34,360
County they're called like B

138
00:11:35,000 --> 00:11:37,000
15003 e

139
00:11:37,040 --> 00:11:38,840
underscore L and

140
00:11:38,840 --> 00:11:43,920
Then underscore underscore 2l and underscore 3l for all the variations

141
00:11:46,640 --> 00:11:52,060
Another place which is actually my favorite place to get this type of information it bumps or I pumps

142
00:11:52,900 --> 00:11:58,440
They provide census and survey data from a world around the world integrated across time and space

143
00:11:59,980 --> 00:12:04,700
Which is huge they they harmonize all these surveys and

144
00:12:05,580 --> 00:12:07,780
They make it so you can just choose

145
00:12:08,420 --> 00:12:13,780
one variable for internet access for example, which is what we're gonna be looking at later on today and

146
00:12:15,020 --> 00:12:20,500
If the census changed the way they ask the question or if they change the answers that you could give

147
00:12:20,700 --> 00:12:22,700
And they started grouping one

148
00:12:24,140 --> 00:12:29,980
All these answers like back in 2005 they didn't ask much about

149
00:12:31,820 --> 00:12:35,180
If you have if you access the internet through your phone a lot

150
00:12:35,340 --> 00:12:40,580
So that eventually changed because the census wants to keep an accurate kind of all this of all this information

151
00:12:41,100 --> 00:12:44,740
And it pumps what they do is harmonize these variables

152
00:12:44,740 --> 00:12:47,480
So you can get one variable and you know, that's the variable that you want

153
00:12:47,680 --> 00:12:52,360
The change the answers might change a little bit but they make it so you can handle the data

154
00:12:56,280 --> 00:12:58,280
This is kind of how it looks and I

155
00:12:59,600 --> 00:13:03,920
Recorded this as a gift just in case I couldn't show you but I see that the wife is doing great

156
00:13:03,920 --> 00:13:07,400
So I'm going to show you a little quick what this looks like

157
00:13:08,480 --> 00:13:10,480
on their website

158
00:13:10,680 --> 00:13:12,680
which is

159
00:13:13,200 --> 00:13:15,200
I pumps that org

160
00:13:18,480 --> 00:13:20,480
And they have I

161
00:13:21,000 --> 00:13:27,560
Cannot stop talking how amazing these people are they have a lot of surveys not just about census data

162
00:13:27,800 --> 00:13:29,800
They have health data

163
00:13:29,920 --> 00:13:36,080
This one I have not had I don't have an excuse to use but they have geographical data and

164
00:13:36,880 --> 00:13:40,760
You can make a lot of cool maps and you can find they have all these shape files for

165
00:13:41,160 --> 00:13:47,560
Whatever you might want to look at not just for the US but other countries around the world across time

166
00:13:48,160 --> 00:13:50,160
so I was just they have

167
00:13:51,120 --> 00:13:53,720
another resources and webinars and I was I was just

168
00:13:55,480 --> 00:13:57,480
Watching the webinar they did on

169
00:13:58,280 --> 00:14:02,320
Ipams international and they they they mentioned

170
00:14:03,240 --> 00:14:05,560
Just like oh, yeah, we also have

171
00:14:06,440 --> 00:14:09,000
harmonized shape files for countries

172
00:14:09,880 --> 00:14:16,280
like Kenya and they showed the example how they put it together and then Kenya has run through some political changes and

173
00:14:17,840 --> 00:14:22,760
They'll take care of they'll take care of all that for you so you don't have to

174
00:14:24,160 --> 00:14:27,520
do some data all the data

175
00:14:29,400 --> 00:14:32,080
Magic to do to find out a way

176
00:14:32,160 --> 00:14:35,600
To harmonize all these variables across time

177
00:14:35,720 --> 00:14:40,000
So this is kind of how it looks like and this is how I got the data set that we're gonna be working on

178
00:14:40,840 --> 00:14:42,840
today

179
00:14:43,120 --> 00:14:49,840
You can you can send this section select harmonized variables you can look at person level statistics

180
00:14:53,200 --> 00:14:55,680
Look how many sections we have and each of them have

181
00:14:57,600 --> 00:14:59,600
Like nine at least

182
00:15:00,320 --> 00:15:01,880
Um

183
00:15:01,880 --> 00:15:07,200
Variables there's a lot of information here. I'm always here on the education and you can see that I'm always happy

184
00:15:07,520 --> 00:15:09,520
This is what I do. I work it

185
00:15:10,720 --> 00:15:13,040
But and they'll show you here for example

186
00:15:13,040 --> 00:15:19,200
You can see the years they have it for some of these years though. They just stopped the question and it just there's no way you can

187
00:15:20,880 --> 00:15:22,880
Have something equivalent for today's

188
00:15:23,960 --> 00:15:29,480
Age but this is a great resource if you're interested in and census data and they everything is free

189
00:15:29,720 --> 00:15:33,160
And they're really great if you have a question you can just email them

190
00:15:40,000 --> 00:15:42,000
So, how do you get this data using Python

191
00:15:44,080 --> 00:15:47,200
There's a few packages at PIPI related to census data

192
00:15:47,600 --> 00:15:53,520
There's more than I thought it was just gonna be like one someone like me. I was just like I'm tired

193
00:15:53,520 --> 00:15:55,920
I'm going to have them so all the time. I just want to get it

194
00:15:56,680 --> 00:15:58,680
But no, there's there's many

195
00:15:59,000 --> 00:16:03,060
Packages on PIPI you can just pip install any of these I'm going to show you four

196
00:16:03,800 --> 00:16:08,840
Notable ones and two of these actually got from a newsletter from the census they they

197
00:16:09,400 --> 00:16:11,840
I'm not saying they're promoting them or that they

198
00:16:13,880 --> 00:16:18,240
Then they are pushing those packages, but they they know they exist and they're okay with

199
00:16:19,240 --> 00:16:21,240
sharing them in their newsletter

200
00:16:21,520 --> 00:16:23,520
So the easiest one census

201
00:16:23,520 --> 00:16:25,520
Is

202
00:16:26,120 --> 00:16:29,160
Literally just PIP install census and then you can get access to

203
00:16:30,920 --> 00:16:34,240
The Census Bureau's API and

204
00:16:35,680 --> 00:16:39,680
It has it provides access to ACS, which is what the American Community Survey

205
00:16:41,120 --> 00:16:46,040
SF one is a summary file and summer SF three are so many files for the decennial

206
00:16:46,880 --> 00:16:48,880
censuses and

207
00:16:48,880 --> 00:16:53,040
You can spend a lot of time just talking about the different ways the census

208
00:16:54,520 --> 00:16:59,000
Summarizes their their findings, but most of the time for high-level stuff

209
00:16:59,000 --> 00:17:02,780
You just look at the ACS which is the most accurate. It's every year

210
00:17:03,600 --> 00:17:05,600
And not every ten years

211
00:17:08,000 --> 00:17:13,680
This is as easy as it gets you just pip install census you you get an API key from the census

212
00:17:14,200 --> 00:17:19,040
Website and then this is what I was talking about if you know the name of your table

213
00:17:19,640 --> 00:17:21,640
be 25034

214
00:17:21,760 --> 00:17:27,320
Underscore zero one zero e which is obviously something about education

215
00:17:29,640 --> 00:17:31,640
You can tell her

216
00:17:32,160 --> 00:17:34,920
Then you can just ask for for that table for

217
00:17:36,040 --> 00:17:39,080
The states for example, this this is what's asking for

218
00:17:39,880 --> 00:17:41,880
You can do that by County by congressional

219
00:17:42,880 --> 00:17:44,560
district

220
00:17:44,560 --> 00:17:50,160
They have what what are called Puma's public use micro area. I think

221
00:17:52,360 --> 00:18:00,040
Which are like tiny tiny area geographical areas where you have like 10,000 people I think so in big cities or

222
00:18:00,560 --> 00:18:03,360
States with high populations you can get like really

223
00:18:04,040 --> 00:18:07,520
Granular data and you can see almost like block to block

224
00:18:08,720 --> 00:18:10,200
differences

225
00:18:10,200 --> 00:18:11,400
but

226
00:18:11,400 --> 00:18:13,400
obviously everything

227
00:18:14,520 --> 00:18:19,520
They do a great job at keeping things private and in this answer so they'll you won't be able to find anything

228
00:18:20,080 --> 00:18:24,360
Too crazy hopefully senpai, which is another

229
00:18:25,960 --> 00:18:30,320
Another wrapper which is is based on an R package ACS

230
00:18:31,640 --> 00:18:37,720
And now I'm gonna the github repo has a link has this presentation show. This is a jupyter notebook

231
00:18:38,480 --> 00:18:40,480
so you're gonna have access to all these links and

232
00:18:41,520 --> 00:18:44,040
I'm just gonna show you they have an intro notebook

233
00:18:44,960 --> 00:18:46,960
So

234
00:18:48,080 --> 00:18:50,360
It's as easy as again you

235
00:18:51,520 --> 00:18:56,320
You ask for the decennial census file summary file or the ACS

236
00:18:56,320 --> 00:18:59,920
I think they do ACS and the cool thing about this one is that it returns

237
00:19:00,760 --> 00:19:06,520
Pandas data frames, which is always nice. I think census might return

238
00:19:07,640 --> 00:19:09,640
like a list of dictionaries

239
00:19:09,960 --> 00:19:13,200
which you can still read into pandas fairly easily, but

240
00:19:14,040 --> 00:19:16,040
You can just get a pandas data frame done

241
00:19:17,080 --> 00:19:19,080
That's great

242
00:19:20,720 --> 00:19:23,120
Next and we're almost done with this and then we can get started

243
00:19:23,960 --> 00:19:30,040
Census data downloader. This is actually a CLI a command line interface by the Los Angeles Times

244
00:19:30,600 --> 00:19:35,240
Data disk they they're doing a lot of cool stuff with Python and open source and open data

245
00:19:37,320 --> 00:19:42,480
All you need again is a an API key from the census and they're pretty quick if you go to a website use if you

246
00:19:42,480 --> 00:19:44,840
Request an API key you're gonna get one

247
00:19:45,920 --> 00:19:48,800
Most likely in the next five minutes. They'll just gonna email you one

248
00:19:49,480 --> 00:19:56,840
All you need to do for this one. Just export your census API key as an environment variable and then you can then start using

249
00:19:56,880 --> 00:19:58,880
their CLI

250
00:19:59,320 --> 00:20:06,120
Tool just off your terminal and what this does is download it downloads

251
00:20:07,880 --> 00:20:09,880
That ugly name

252
00:20:10,440 --> 00:20:13,720
Table be three to underscore something something

253
00:20:14,720 --> 00:20:22,400
And then they'll reformat it so the columns actually make sense and the name of the table makes sense and you can actually read it

254
00:20:23,080 --> 00:20:31,600
They it's it's a version like 0.05. I think it's very early and they have but they have a lot of tables already

255
00:20:32,800 --> 00:20:36,400
Put together so you can just ask for median age or educational attainment

256
00:20:37,360 --> 00:20:42,640
Not just in states but in counties in the counties of just one state

257
00:20:44,280 --> 00:20:46,920
Etc and the last one which is

258
00:20:47,720 --> 00:20:50,080
one of my favorites who it's called census data and

259
00:20:50,800 --> 00:20:52,560
This is the most

260
00:20:52,560 --> 00:20:57,240
Pythonic one. I would say out of all of them. They they really make it

261
00:20:58,880 --> 00:21:04,680
So that it's very Python friendly if you don't Python on any other

262
00:21:07,400 --> 00:21:12,120
Much work in any other way before I'm doing data analysis with pandas

263
00:21:13,080 --> 00:21:15,480
They they have a class

264
00:21:16,520 --> 00:21:19,200
for census geographies you can create

265
00:21:20,400 --> 00:21:23,160
These queries and it gets it's really great

266
00:21:23,960 --> 00:21:25,960
if we have time by the end of the

267
00:21:26,520 --> 00:21:28,280
workshop, we're gonna

268
00:21:28,280 --> 00:21:34,120
Do an example of how to use census data to get to recreate what we what we did on

269
00:21:36,480 --> 00:21:38,480
The workshop

270
00:21:38,480 --> 00:21:43,680
This is the last line that was supposed to be the last line. This is laughing. This is very last thing. Let's analyze some sensitive

271
00:21:45,080 --> 00:21:47,080
but I

272
00:21:49,280 --> 00:21:56,280
Okay, so now we're gonna actually start get started with the workshop you all you need to do is go on this link

273
00:21:56,280 --> 00:21:58,200
It's all caps

274
00:21:58,200 --> 00:22:00,200
bidley census

275
00:22:01,680 --> 00:22:04,880
Census-data-pycon 2019

276
00:22:06,400 --> 00:22:12,560
I'm gonna write it over here just to make it easier for everyone to read and that's gonna send you that's gonna take you to

277
00:22:14,920 --> 00:22:16,920
My binder so we can get started

278
00:22:20,000 --> 00:22:22,000
Analyzing some census data

279
00:22:28,280 --> 00:22:30,280
And you have any questions

280
00:22:36,600 --> 00:22:41,040
Is it working for everyone yeah, okay

281
00:23:00,320 --> 00:23:02,700
And this is how this is how my binder looks like you

282
00:23:02,700 --> 00:23:04,700
I

283
00:23:04,780 --> 00:23:09,720
Get this is already pointing at my the the github repo I put together beforehand

284
00:23:10,140 --> 00:23:13,180
But if you go to my binder or it's gonna ask you

285
00:23:13,940 --> 00:23:17,940
the link for the github repo and you can even point it to a

286
00:23:18,580 --> 00:23:23,780
Specific notebook file or any file and you get a repo and it's gonna open it by default

287
00:23:23,780 --> 00:23:27,900
Which is what you should be looking at right now. It's gonna be it should open to the

288
00:23:28,740 --> 00:23:30,300
first notebook

289
00:23:30,300 --> 00:23:32,300
prepping data

290
00:23:32,700 --> 00:23:34,700
to do perfect

291
00:23:38,900 --> 00:23:43,740
Presentation you don't need to do that. I'm doing this because I'm

292
00:23:48,300 --> 00:23:49,860
Okay

293
00:23:49,860 --> 00:23:54,380
So if you're not familiar with the jupyter lab interface over here you have a

294
00:23:55,180 --> 00:24:01,220
Your files and you can go all the way to home. This is what the github repo looks like

295
00:24:01,940 --> 00:24:04,340
These are folders you have the readme file

296
00:24:05,900 --> 00:24:09,860
Maybe double click on the readme file is gonna show you the the raw text

297
00:24:09,860 --> 00:24:15,420
But you can right click on any file and then it's gonna actually give you the options on how to open things

298
00:24:16,700 --> 00:24:18,700
This works

299
00:24:18,740 --> 00:24:22,180
for notebooks to if you want to open a notebook in

300
00:24:23,540 --> 00:24:26,020
Editor way, I'm not gonna do that because I want to set up

301
00:24:27,020 --> 00:24:31,340
But you could see the actual Jason representation of your notebook

302
00:24:32,660 --> 00:24:35,460
Here are your your sessions we have open a

303
00:24:37,260 --> 00:24:39,700
Kernel for our notebook

304
00:24:43,300 --> 00:24:47,660
Over here you have some other stuff that we're not gonna be looking at today

305
00:24:49,180 --> 00:24:51,180
But it's

306
00:24:52,620 --> 00:24:54,720
So let's get started we're gonna be

307
00:24:56,460 --> 00:25:03,740
Trying to recreate this fact sheet some of my colleagues at PPAC put together

308
00:25:05,140 --> 00:25:07,140
specifically this one

309
00:25:07,460 --> 00:25:10,180
There's one chart and these three bars

310
00:25:11,540 --> 00:25:14,140
Because I didn't want to put like a huge data file and

311
00:25:15,420 --> 00:25:19,300
But what we what we're looking at what we're looking to recreate is this

312
00:25:20,300 --> 00:25:21,460
part

313
00:25:21,500 --> 00:25:28,300
How many households in X state and you can choose your own state the data is for all the US but

314
00:25:29,220 --> 00:25:34,100
I've set it up. So you choose whatever one and they should all work the same

315
00:25:35,020 --> 00:25:38,340
how many households in a certain state have access to

316
00:25:39,740 --> 00:25:40,940
broadband

317
00:25:40,940 --> 00:25:43,660
high-speed internet, which seems to be

318
00:25:44,660 --> 00:25:47,460
very important nowadays and how does that

319
00:25:48,100 --> 00:25:50,340
vary across racial ethnic groups

320
00:25:52,460 --> 00:25:54,460
We can even look more into

321
00:25:55,220 --> 00:25:58,540
What we're gonna start you see but we can even divided by

322
00:26:00,020 --> 00:26:01,780
How many

323
00:26:01,780 --> 00:26:07,520
House households with school-aged children have access to the internet they need internet for school now

324
00:26:08,460 --> 00:26:10,460
So as a policy person

325
00:26:10,980 --> 00:26:12,980
To be very useful information

326
00:26:13,260 --> 00:26:15,260
Okay, so let's get started

327
00:26:15,820 --> 00:26:17,820
any questions so far

328
00:26:17,820 --> 00:26:19,820
you want to hear me can anyone see the

329
00:26:20,820 --> 00:26:22,820
Screen

330
00:26:25,420 --> 00:26:29,340
Okay, so this is pretty much how I set up

331
00:26:31,060 --> 00:26:34,340
Notebook where I'm gonna start I usually divide it into a data prep

332
00:26:34,940 --> 00:26:37,340
notebook and an analysis notebook because if I

333
00:26:37,780 --> 00:26:42,580
Start analyzing something and I'm gonna figure I messed up something in the preparation of my file

334
00:26:42,580 --> 00:26:44,940
I would like to just fix that and

335
00:26:45,860 --> 00:26:51,940
Have to rerun everything so modularizing your your workflow is always great

336
00:26:53,060 --> 00:26:55,680
I've set up these research questions

337
00:26:57,540 --> 00:27:03,060
For us to answer what share of a household in X state have access to high-speed internet and

338
00:27:03,780 --> 00:27:09,100
Does this not mean number of area cross demographic groups and in this case, we're gonna be looking at race ethnicity

339
00:27:09,780 --> 00:27:10,860
and

340
00:27:10,900 --> 00:27:14,420
I usually and these are some tips and these are personal tips

341
00:27:14,420 --> 00:27:16,580
You don't have to follow this if you have a better workflow

342
00:27:18,180 --> 00:27:23,860
The it's up to you, but this is something I found to be like the very basic

343
00:27:24,820 --> 00:27:26,160
standard

344
00:27:26,160 --> 00:27:29,780
Good way to organize myself. I'll have a notebook. I'll set up

345
00:27:31,100 --> 00:27:33,340
The project or the project name that I'm working on

346
00:27:34,260 --> 00:27:40,260
What is this notebook doing prepping? What am I doing? If I'm researching anything? These are my research questions

347
00:27:40,860 --> 00:27:42,860
What's the goal of this notebook?

348
00:27:44,620 --> 00:27:46,960
This notebook I want to explore my

349
00:27:48,060 --> 00:27:53,260
Master file the raw data file that I got from the census and I'm gonna create a working data

350
00:27:53,260 --> 00:27:55,700
So I'm gonna grab whatever I think is useful from that

351
00:27:56,260 --> 00:28:00,860
Create another data set. That's the one I'm gonna be working with because I don't want to touch the raw data

352
00:28:00,860 --> 00:28:05,900
I don't want to modify that in case I mess up and I need to go back to it

353
00:28:05,900 --> 00:28:12,380
The context is I this is huge and even if you don't put this in your regular workflow

354
00:28:12,380 --> 00:28:16,180
I would suggest that you do this because in three months or in three even three weeks

355
00:28:16,180 --> 00:28:21,180
I'll go back to a notebook all back to the work that I was doing and I have no idea why I was doing anything

356
00:28:21,180 --> 00:28:23,180
that I have here and I

357
00:28:24,060 --> 00:28:26,060
might look at

358
00:28:26,060 --> 00:28:30,060
like some cool thing that I figure out how to do and then it's just lost forever because I

359
00:28:30,060 --> 00:28:32,540
Remember what I was doing it why I was doing it that way

360
00:28:33,700 --> 00:28:38,420
So just putting a little bit of context at the very top of your notebook is always nice

361
00:28:39,660 --> 00:28:43,060
So again, so we're gonna get started and I've set up a

362
00:28:43,860 --> 00:28:47,500
Lot of the code and as we as we progress through the notebook

363
00:28:47,500 --> 00:28:51,260
There's gonna be less and less code that I have put there for you

364
00:28:51,260 --> 00:28:53,260
and you're just gonna have to

365
00:28:54,660 --> 00:28:56,180
Type it yourself

366
00:28:56,180 --> 00:28:58,780
But I wanted to set this up because

367
00:29:00,300 --> 00:29:02,300
I'm doing

368
00:29:03,180 --> 00:29:05,980
This is something that I do on every notebook

369
00:29:05,980 --> 00:29:12,180
And this is essential to the rest of the notebooks to make sure that we all are working in the same place at the same pace

370
00:29:12,180 --> 00:29:14,180
and we get the same

371
00:29:14,660 --> 00:29:16,660
variables and

372
00:29:17,980 --> 00:29:19,980
The same

373
00:29:21,100 --> 00:29:23,100
Workflow sorry

374
00:29:24,100 --> 00:29:26,100
Workflow sorry

375
00:29:26,300 --> 00:29:31,860
So the very first thing we're gonna do is obviously set up our environment import whatever we're gonna need

376
00:29:34,140 --> 00:29:38,260
We're gonna use obviously pandas that's in the title of the tutorial

377
00:29:38,860 --> 00:29:42,420
We're gonna use path live has anyone worked with path live before

378
00:29:43,060 --> 00:29:48,660
You it's it's a great tool because when and I'm sure there's other ways

379
00:29:48,660 --> 00:29:52,520
But I use it because I have this my personal computers at MacBook

380
00:29:53,780 --> 00:29:56,660
My work computer is a Windows and they use different

381
00:29:58,060 --> 00:30:00,060
slashes

382
00:30:00,220 --> 00:30:06,860
Yeah, so it just if I write if I write something and that has to do with paths in my regular computer

383
00:30:06,860 --> 00:30:10,300
And my personal computer might not work on my work computer or vice versa

384
00:30:11,180 --> 00:30:16,980
And path live what it does is it creates these path file objects and they just work on

385
00:30:17,660 --> 00:30:23,820
On unix I mean on Linux on Mac OS on Windows they just they just work

386
00:30:24,700 --> 00:30:26,620
Which is great

387
00:30:26,620 --> 00:30:30,700
also this daytime tip of importing daytime and

388
00:30:31,300 --> 00:30:36,380
Then creating a today variable so you can tag your your data files

389
00:30:37,020 --> 00:30:44,580
Is great tagging your data files or tagging your your work with just the date is not I mean we have version control

390
00:30:45,140 --> 00:30:47,140
But when you're doing data

391
00:30:48,300 --> 00:30:51,820
There's still version control systems for for big data files

392
00:30:52,300 --> 00:30:56,420
But this is a great first step. Just make sure that that you can differentiate

393
00:30:58,260 --> 00:31:01,700
Between the files that you're you're working with

394
00:31:02,580 --> 00:31:04,580
and

395
00:31:05,100 --> 00:31:11,060
I did not come up with that daytime tip. I found it online and I forgot to save the link

396
00:31:11,460 --> 00:31:13,460
So if you know

397
00:31:13,500 --> 00:31:15,500
Someone posted this

398
00:31:15,540 --> 00:31:21,460
From daytime import daytime as DT. So you're not confused with so many daytimes

399
00:31:22,020 --> 00:31:24,020
Call daytime calls on your code

400
00:31:25,140 --> 00:31:28,500
Find on Twitter. Yeah, I saved me a ton of time and confusion

401
00:31:29,180 --> 00:31:33,260
So if you're that person or if you know that person you can give them a high five for me

402
00:31:34,140 --> 00:31:39,580
So I'm gonna give you two minutes and I want you to try to figure out what's happening here. You're gonna import the

403
00:31:40,500 --> 00:31:42,500
The the libraries

404
00:31:42,820 --> 00:31:45,900
You're gonna create a today variable

405
00:31:46,620 --> 00:31:50,940
Which should change from April 27th to May 1st?

406
00:31:53,340 --> 00:31:55,340
And you don't have I mean I'm using

407
00:31:55,660 --> 00:32:01,580
if you haven't worked with daytime that I just put that together for you, but you if you know what you're doing with the

408
00:32:02,100 --> 00:32:04,100
daytime object with the

409
00:32:04,820 --> 00:32:06,820
string f time

410
00:32:07,820 --> 00:32:15,700
You can change it however you want the first the percent D is giving you the 27 and then I added the dash

411
00:32:16,300 --> 00:32:21,300
Percent B is giving you the month with the abbreviated way with the with the actual name of the month

412
00:32:22,420 --> 00:32:26,580
Dash percent Y is giving you a two two digit year

413
00:32:27,180 --> 00:32:30,460
If you do capital Y you can get four digit year

414
00:32:31,060 --> 00:32:33,060
I

415
00:32:34,140 --> 00:32:40,180
Am gonna give you two minutes to work on that and then I'm gonna stop talking I keep talking

416
00:32:52,660 --> 00:32:58,380
Okay, so if you don't have a kernel you can over here and the refresh

417
00:33:01,420 --> 00:33:05,980
Symbol you can restart a kernel and if you don't have a kernel it might ask you to

418
00:33:08,740 --> 00:33:16,140
To choose a Python 3 was the only one that we have if not you can refresh the page my binder will like

419
00:33:18,140 --> 00:33:24,460
Will stop working for you if you have ten minutes of an activity and I'm sure I've been talking for more than ten minutes

420
00:33:30,580 --> 00:33:35,020
And these are not the actual names of the paths you you you need to change those

421
00:33:35,820 --> 00:33:38,220
You don't want a path name called xxx

422
00:33:43,980 --> 00:33:48,300
You can name is something intuitive like be three zero four underscore, yeah

423
00:34:00,460 --> 00:34:02,460
You

424
00:34:30,460 --> 00:34:32,460
You

425
00:35:00,460 --> 00:35:02,460
You

426
00:35:30,460 --> 00:35:32,460
You

427
00:36:00,460 --> 00:36:02,460
You

428
00:36:30,460 --> 00:36:32,460
You

429
00:36:58,300 --> 00:37:00,300
Yeah

430
00:37:01,020 --> 00:37:04,840
Our is he and if you go on the

431
00:37:16,380 --> 00:37:22,260
Okay, I'm gonna start working on my version over here you don't have to follow exactly what I'm doing

432
00:37:25,940 --> 00:37:28,740
You don't have to name them the same thing

433
00:37:30,500 --> 00:37:35,900
But that's what the rest of the notebook kind of assumes you're gonna be doing so if

434
00:37:37,580 --> 00:37:40,420
You don't want to think too much about it you can just copy these

435
00:37:45,380 --> 00:37:52,480
And I got this structure from again not me I did not come up with this

436
00:37:56,140 --> 00:37:59,060
Cookie cutter reproducible science

437
00:38:01,180 --> 00:38:05,760
Someone put together a basic structure for reproducible science

438
00:38:09,500 --> 00:38:11,420
Projects

439
00:38:11,420 --> 00:38:17,580
MK rap and this is the basic structure which has worked great for me a

440
00:38:18,460 --> 00:38:24,020
Lot of this I don't need because I for example. I'm not using at work. I'm not producing a lot of

441
00:38:24,980 --> 00:38:27,020
Code is really just a few pandas

442
00:38:27,740 --> 00:38:29,740
Notebooks. Oh, yeah, sorry

443
00:38:31,420 --> 00:38:33,420
But this is this is a great resource

444
00:38:35,460 --> 00:38:44,140
This is this is a cookie cutter repo, but this is by MK rap cookie cutter dash reproducible dash science

445
00:38:44,940 --> 00:38:46,940
and

446
00:38:49,340 --> 00:38:53,440
You can just once you pip install cookie cutter you can use it from the command line

447
00:38:54,260 --> 00:39:00,000
Cookie cutter and then you can you don't have to do this if you don't for some reason my computer never works for this

448
00:39:01,020 --> 00:39:03,020
HTTPS that github.com

449
00:39:03,540 --> 00:39:10,340
Slash MK rap slash this is gonna ask you a bunch of questions. Who are you? What license do you want to use?

450
00:39:11,260 --> 00:39:14,380
What's the name of the project all that stuff and then it's gonna create this for you

451
00:39:14,940 --> 00:39:18,620
So this is why I'm using the external interim processed and raw

452
00:39:19,980 --> 00:39:22,900
Structure in this project. It's it's worked out great for me

453
00:39:24,580 --> 00:39:26,580
Especially this part just having

454
00:39:27,580 --> 00:39:33,060
A directory with raw files that I know I should not touch look I got this from I pumps I got this from the census

455
00:39:33,580 --> 00:39:35,580
this is

456
00:39:36,500 --> 00:39:38,500
Original immutable data

457
00:39:39,660 --> 00:39:45,380
What if I do something to it? I'll save it on interim or processed if I'm done processing it and

458
00:39:47,420 --> 00:39:49,420
Yeah, so just another tip

459
00:39:49,820 --> 00:39:51,820
You speak different

460
00:39:52,700 --> 00:39:57,420
Yes, okay, so external what the way I've been using this and this is obviously all

461
00:39:58,100 --> 00:40:04,860
For whatever you think works best for you. This is just a template. But what I've been doing for external is

462
00:40:06,020 --> 00:40:08,020
I use

463
00:40:10,100 --> 00:40:12,100
Now national

464
00:40:12,900 --> 00:40:19,200
Center for education statistics data and then I'll get it from another place that kind of pre-process it

465
00:40:19,700 --> 00:40:21,940
And then puts it in a nice format for me

466
00:40:21,940 --> 00:40:27,340
So I know it's not raw data that I got exactly from them or if a colleague put together

467
00:40:28,260 --> 00:40:30,260
Like an extract of something else

468
00:40:30,300 --> 00:40:37,200
So I know it's not raw data because it's not directly from the source and it's been processed but not by me

469
00:40:37,200 --> 00:40:39,980
So I don't have it. So I don't have a notebook over here that

470
00:40:40,900 --> 00:40:43,360
Created that or over here in source

471
00:40:45,380 --> 00:40:48,860
But then again, this is all whatever you think works best for you

472
00:40:49,940 --> 00:40:51,940
Let's go back to

473
00:40:54,220 --> 00:40:56,220
This

474
00:40:59,140 --> 00:41:04,300
All right, I also included this tree function

475
00:41:05,540 --> 00:41:10,740
From in this script over here. All it does is displays the tree

476
00:41:11,700 --> 00:41:17,180
This place a tree a directories tree so you can see what files you have there

477
00:41:17,180 --> 00:41:22,020
You could do the same going over here going on your exercises you go on data

478
00:41:22,540 --> 00:41:26,780
You want to see what's in the raw file and the raw folder? Oh, here it is

479
00:41:27,460 --> 00:41:30,380
So all I'm gonna do is from tools

480
00:41:30,820 --> 00:41:35,980
Which is that script that I have in my same folder with my notebooks over here exercises

481
00:41:36,740 --> 00:41:38,180
notebooks

482
00:41:38,180 --> 00:41:41,180
I'm gonna import that function tree

483
00:41:41,940 --> 00:41:43,940
so I can then use it and

484
00:41:44,940 --> 00:41:48,140
And then I want to see the raw

485
00:41:50,780 --> 00:41:54,700
Data path and you should be able to tab autocomplete

486
00:41:57,980 --> 00:42:02,700
They're not just run this oh I did not run it

487
00:42:06,220 --> 00:42:09,260
Another way to check that if you're doing the right thing is

488
00:42:11,700 --> 00:42:13,700
Yeah

489
00:42:14,020 --> 00:42:16,020
Oh

490
00:42:16,220 --> 00:42:18,220
Yeah

491
00:42:18,700 --> 00:42:23,940
Tab autocomplete if you spell if you start spelling your variable tab autocomplete should

492
00:42:24,860 --> 00:42:27,140
Know there you go you click tab

493
00:42:27,980 --> 00:42:31,480
It gives you the variable if you have not declared the variable if you declare it a different way

494
00:42:32,100 --> 00:42:35,020
You know, you're doing something wrong if the tab autocomplete is not working

495
00:42:36,660 --> 00:42:43,020
So this is the data we have we have ACS data and I have it in DTA format and CSV format

496
00:42:44,340 --> 00:42:46,340
And they're both compressed

497
00:42:46,460 --> 00:42:48,380
It booms will give you

498
00:42:48,380 --> 00:42:50,380
compressed data files by default

499
00:42:50,980 --> 00:42:53,820
Which is what you want. You don't want to it's just easier

500
00:42:55,780 --> 00:43:00,300
Step two we're gonna load the data into a data frame into a variable

501
00:43:01,220 --> 00:43:04,740
I've provided you here the code, but this is just markdown

502
00:43:05,900 --> 00:43:10,360
So I have this here and blank so you can type that up

503
00:43:10,760 --> 00:43:12,760
The main lesson here is that

504
00:43:13,440 --> 00:43:14,800
CSV

505
00:43:14,800 --> 00:43:18,920
Format pan this is really good with CSV format format files

506
00:43:20,120 --> 00:43:25,800
And you don't need to give it any other parameter if you're working with a compressed CSV file

507
00:43:26,200 --> 00:43:32,320
Just hand you just give it the path. Oh and this oh, this might be confusing the raw data path and

508
00:43:33,240 --> 00:43:35,640
Then slash and then the name of a file

509
00:43:35,800 --> 00:43:37,800
And then the name of a file

510
00:43:38,120 --> 00:43:45,840
Ultimately, I'm gonna give you an example here it joins them. This is how you join. There's one way to join

511
00:43:49,920 --> 00:43:52,560
Yes, this is why it's so awesome

512
00:43:55,880 --> 00:43:59,760
No, no, that's the thing it just it just knows

513
00:44:00,840 --> 00:44:03,000
It just knows it's the magic of path loop

514
00:44:03,760 --> 00:44:06,600
So that's that's a that's that's what's happening there if you

515
00:44:12,360 --> 00:44:15,060
Which is why I was included this tree

516
00:44:16,840 --> 00:44:25,080
Object this tree function because you want to know what what's going on here pandas by default if you want to

517
00:44:26,360 --> 00:44:32,560
If you start giving it a path if you start giving it a path you can start you can tap autocomplete

518
00:44:33,000 --> 00:44:35,000
and

519
00:44:37,280 --> 00:44:42,880
It just knows but if you're like just to make a more reproducible across platforms

520
00:44:46,240 --> 00:44:47,800
It's better if you use path live

521
00:44:47,800 --> 00:44:54,680
But if you're using path live you don't get that tab auto complete and you're in your file path right away for with pandas

522
00:44:55,640 --> 00:44:57,640
So I'm gonna start

523
00:44:58,640 --> 00:45:05,880
So if you over here you couldn't go ACS tab

524
00:45:08,760 --> 00:45:11,480
It just doesn't work with with path live like this

525
00:45:12,680 --> 00:45:14,680
but

526
00:45:15,320 --> 00:45:17,880
They did that head and what data that I mean

527
00:45:17,960 --> 00:45:24,780
What that head is doing to your pandas data frame is asking you for the head of the pandas data frame

528
00:45:24,820 --> 00:45:29,300
Asking you for it's gonna display the top five elements the first five rows

529
00:45:30,780 --> 00:45:33,840
It might take a while because the file is like 20

530
00:45:35,340 --> 00:45:37,340
20 20 gigabytes 20 megabytes

531
00:45:38,900 --> 00:45:40,900
You can put 20 gigabytes

532
00:45:49,500 --> 00:45:52,860
And you can I mean I try to make this as

533
00:45:55,260 --> 00:46:00,380
Independent have this workshop be as usable as it can by itself

534
00:46:00,380 --> 00:46:02,540
so I that's why I have so much text and

535
00:46:02,940 --> 00:46:09,340
Trying to describe what I'm repeating here what I'm saying out loud here and it should match more or less

536
00:46:09,340 --> 00:46:12,620
What's written in the notebook? So if you want to go back later or?

537
00:46:13,700 --> 00:46:18,320
Tomorrow or in a few days, or you want to send it to anyone this should more or less

538
00:46:19,580 --> 00:46:21,580
Stand on its own

539
00:46:21,580 --> 00:46:23,580
Yeah

540
00:46:35,740 --> 00:46:38,240
This one this one

541
00:46:39,340 --> 00:46:47,260
Okay. Yes. So what exactly so has anyone tried the first one loading up CSV? Yes. Oh

542
00:46:48,220 --> 00:46:50,220
You're okay. Sorry

543
00:46:50,980 --> 00:46:52,980
Okay, so we're gonna move on to the next one so

544
00:46:55,300 --> 00:46:57,300
Ipams like I said it gives you

545
00:46:58,100 --> 00:47:01,580
data in G zip format it's

546
00:47:02,180 --> 00:47:06,980
Compressed and it's very easy to to work with special in Python Python has G zip

547
00:47:08,060 --> 00:47:11,220
in the center library so you can just import G zip and

548
00:47:12,100 --> 00:47:15,420
open a G zip file just load it up and

549
00:47:16,140 --> 00:47:19,860
You're completely right this code it might be confusing if you mess up

550
00:47:20,220 --> 00:47:23,500
This very this first one or if you change the name over here

551
00:47:24,380 --> 00:47:29,860
Because for the rest of the notebook I'm gonna assume that your data is saved to a variable data

552
00:47:30,100 --> 00:47:33,260
And what this is doing is overriding the last one

553
00:47:35,100 --> 00:47:37,100
So

554
00:47:37,660 --> 00:47:39,980
Let's make sure that we get we get this one right

555
00:47:42,900 --> 00:47:47,500
And here I have a little explanation on why we're using the the DTA file if you go to a pumps

556
00:47:48,500 --> 00:47:53,060
When you're selecting your data at the very end you can choose what format you want it in

557
00:47:54,460 --> 00:48:00,660
You can get in CSV format you can get in its data DTA you can get it on SAS and

558
00:48:01,620 --> 00:48:03,100
SPSS

559
00:48:03,100 --> 00:48:06,300
Which path pandas has functions to read to?

560
00:48:07,300 --> 00:48:09,900
But I've worked with with data

561
00:48:10,580 --> 00:48:17,100
I started economics and this is what a lot of people use and if you're doing anything with census data or any

562
00:48:17,660 --> 00:48:24,120
Social that anything that people in the social sciences might be working with state. Is is very common

563
00:48:24,540 --> 00:48:30,860
So a DTA file is very common and I mean it's coming up that pandas has a just read it's data

564
00:48:31,740 --> 00:48:33,580
function to

565
00:48:33,580 --> 00:48:35,580
Okay, so I'm gonna get

566
00:48:35,700 --> 00:48:38,940
Going over here. We have 50 minutes to finish this one hopefully

567
00:48:41,220 --> 00:48:44,340
Import GZ this is important because I I

568
00:48:45,220 --> 00:48:49,760
Kept doing this practicing this notebook, and I always forget to import GZ

569
00:48:50,980 --> 00:48:52,260
Yeah

570
00:48:52,260 --> 00:48:58,260
Which is about signal but time because I wrote this and I still forget to do it

571
00:48:59,060 --> 00:49:01,060
But just make sure that you're importing

572
00:49:02,700 --> 00:49:04,700
GZ before

573
00:49:06,580 --> 00:49:10,540
Then data equals PD reads data

574
00:49:11,540 --> 00:49:13,540
And then you're passing it that file

575
00:49:14,180 --> 00:49:16,180
Object that you're creating you're

576
00:49:17,580 --> 00:49:19,580
Starting with this

577
00:49:21,780 --> 00:49:27,580
And then I'm gonna ask you to display the first five rows like we did just a few cells above

578
00:49:43,540 --> 00:49:45,540
I

579
00:50:13,540 --> 00:50:15,540
I

580
00:50:24,060 --> 00:50:32,380
We're gonna be using the DTA file for the rest of the notebook so that it's more important to get the state of file working

581
00:50:35,860 --> 00:50:42,300
Ready to load it and we display it first five rows ahead. Yes

582
00:50:43,540 --> 00:50:46,940
I noticed that there's a PD dot reads data

583
00:50:48,660 --> 00:50:50,660
Yes

584
00:50:52,020 --> 00:50:55,020
For this for this one oh

585
00:50:57,100 --> 00:50:59,500
For the right if you do it directly to

586
00:51:00,780 --> 00:51:05,900
For this one yes this one you cannot direct you can you cannot read a

587
00:51:06,740 --> 00:51:12,540
GZip compressed file directly because the read state of function just cannot do that

588
00:51:13,020 --> 00:51:14,460
but

589
00:51:14,460 --> 00:51:17,940
Yes, they have a reads data and have a to state

590
00:51:17,940 --> 00:51:22,900
So even if you're working with CSV files, and then eventually you want to just save it as a DTA file

591
00:51:23,940 --> 00:51:31,560
You can just instead of saving it that PD to I mean data to CSV you can use data to state

592
00:51:31,560 --> 00:51:33,560
And

593
00:51:39,240 --> 00:51:41,000
This is

594
00:51:41,000 --> 00:51:47,200
One of the main benefits of working with a state of file from a pumps if you go up here

595
00:51:47,720 --> 00:51:49,720
This is what your CSV file looks like

596
00:51:51,560 --> 00:51:53,560
One zeros

597
00:51:54,680 --> 00:52:00,600
Once 20s just numbers for the codes in your data

598
00:52:01,840 --> 00:52:08,660
Which you can then match with the data dictionary you can just grab those and then just rename all your your values

599
00:52:09,160 --> 00:52:11,640
so they match something but a

600
00:52:13,040 --> 00:52:15,040
state of file

601
00:52:15,240 --> 00:52:17,240
has embedded

602
00:52:18,080 --> 00:52:24,620
What they call it value labels, so pandas will read those as categorical variables

603
00:52:25,360 --> 00:52:29,520
categorical columns which under the hood are also are still

604
00:52:30,240 --> 00:52:33,640
Zeros and ones but it will display them as

605
00:52:34,520 --> 00:52:36,400
their actual value

606
00:52:36,400 --> 00:52:39,680
Which saves you a ton of time because if you save this

607
00:52:40,320 --> 00:52:42,320
column state fit as

608
00:52:43,360 --> 00:52:44,600
a

609
00:52:44,600 --> 00:52:46,600
bunch of strings

610
00:52:46,800 --> 00:52:48,200
That would take a lot of memory

611
00:52:48,200 --> 00:52:52,080
I mean maybe not a lot but it will take a lot more memory than

612
00:52:52,320 --> 00:52:57,560
It would if you save it as a categorical in a state of file or as just a bunch of int

613
00:52:58,120 --> 00:53:04,840
Or floats and then just have your data dictionary somewhere else and just working with the state of file and puts it all together

614
00:53:07,360 --> 00:53:10,760
But it is a different type of data so we just need to be aware of

615
00:53:11,480 --> 00:53:18,560
Of that we can still compare it like it's a string and we'll see some other ways we can compare stuff in the category

616
00:53:19,120 --> 00:53:20,560
categorical

617
00:53:20,560 --> 00:53:24,440
variables later on so now we're moving on to step three

618
00:53:28,080 --> 00:53:35,520
There are three functions that I am suggesting here that you use

619
00:53:39,920 --> 00:53:43,480
But you're deducing them yourself, I'm not telling you what the functions are

620
00:53:44,920 --> 00:53:51,200
So I'm just gonna give you two minutes to to fill out these two these three steps, and then we can talk about what?

621
00:53:52,000 --> 00:53:54,000
What each of those is doing?

622
00:53:57,560 --> 00:54:03,280
I'm gonna be walking around if you have any questions you can just

623
00:58:27,560 --> 00:58:29,560
We say

624
00:58:57,560 --> 00:58:59,560
I

625
00:59:12,120 --> 00:59:18,000
Will do this info

626
00:59:22,800 --> 00:59:26,360
Describes over here and shape

627
00:59:28,360 --> 00:59:30,360
All right

628
00:59:33,800 --> 00:59:39,160
As I knew as everyone done these three at least as some people going ahead

629
00:59:40,280 --> 00:59:42,720
So what do you think info?

630
00:59:43,800 --> 00:59:45,800
Is giving you

631
00:59:46,080 --> 00:59:48,920
From from your data frame. What are these?

632
00:59:51,560 --> 00:59:55,920
Yeah, this is giving you the columns the total of your columns and the type

633
00:59:57,600 --> 01:00:00,440
Each of your columns is and you may notice

634
01:00:02,720 --> 01:00:04,720
Things like age here

635
01:00:05,800 --> 01:00:07,800
It's still a category and

636
01:00:08,440 --> 01:00:13,840
Not an int which is a little weird, but we'll see that later

637
01:00:15,720 --> 01:00:19,640
There in the notebook even year two is a category

638
01:00:20,400 --> 01:00:23,360
So sometimes it has its ups and downs and you just got to be aware

639
01:00:23,360 --> 01:00:30,920
It's always good to run info on your data from just so you know, you're aware what each of the types of your columns is

640
01:00:32,960 --> 01:00:34,960
Describe what do you think describe is doing?

641
01:00:45,280 --> 01:00:52,000
Exactly and is and because you cannot run a standard deviation or get the mean of

642
01:00:53,880 --> 01:00:55,560
Not a number

643
01:00:55,560 --> 01:00:59,880
So you only get the the quantitative columns that you have here

644
01:00:59,920 --> 01:01:04,520
So if you go up here, you'll notice serial is in 32

645
01:01:06,800 --> 01:01:10,240
Household weight which is what this represents. It's also an int

646
01:01:11,400 --> 01:01:13,400
county FIP

647
01:01:13,720 --> 01:01:16,480
Per num, which is persons number and persons weight

648
01:01:18,080 --> 01:01:22,000
These are statistical weights not the actual person's weight that'd be weird

649
01:01:23,840 --> 01:01:29,640
And it will run basic statistics for you on all the quantitative variables if you want to

650
01:01:30,160 --> 01:01:34,920
Just grab one. You can just pass one column here and just be like, oh, we'll just give me

651
01:01:36,200 --> 01:01:37,680
HHW

652
01:01:37,680 --> 01:01:40,280
Yes, okay. I wasn't 100% sure

653
01:01:41,600 --> 01:01:43,600
And what do you think shape is giving you?

654
01:01:46,160 --> 01:01:49,760
Yes, it's the shape of your data frame and rows and columns and if you these

655
01:01:50,560 --> 01:01:53,800
Pandas is built on non-py and non-py

656
01:01:54,600 --> 01:01:56,600
You can have like a lot more

657
01:01:57,520 --> 01:02:01,760
Dimensions in your data and it will give you instead of just rows and columns

658
01:02:01,760 --> 01:02:08,400
They can give you if you run shape on a non-py array of like many dimensions. It will give you all those

659
01:02:09,280 --> 01:02:10,560
dimensions

660
01:02:10,560 --> 01:02:13,960
All right. So we now know more or less what our data looks like

661
01:02:15,280 --> 01:02:17,280
What its types are?

662
01:02:17,720 --> 01:02:19,720
we can start moving on to

663
01:02:20,000 --> 01:02:24,400
Cutting off all the extra that we don't want or what we don't need

664
01:02:27,920 --> 01:02:32,520
And that's exactly what what's in the text so you can try this try the following

665
01:02:33,280 --> 01:02:39,960
That's my suggestion. That's what I'm gonna be doing up here, but you can choose whichever state you want and if you choose any other state

666
01:02:40,640 --> 01:02:42,640
You're gonna have different

667
01:02:43,600 --> 01:02:44,600
Oh

668
01:02:44,600 --> 01:02:46,720
Statistics and different numbers for the rest of the notebook

669
01:02:47,000 --> 01:02:52,960
But it should all the mechanics should be the same and you should be able to to run the code the same just

670
01:02:53,360 --> 01:02:55,360
Just choose a different state

671
01:02:55,880 --> 01:02:58,080
Or district of Columbia yet

672
01:02:59,000 --> 01:03:01,440
You spell out a district of Columbia not just DC

673
01:03:12,640 --> 01:03:14,640
I

674
01:03:28,360 --> 01:03:31,600
Shouldn't have run that actually it's gonna be a surprise

675
01:03:42,640 --> 01:03:44,640
You

676
01:03:55,160 --> 01:04:00,680
I'm gonna get going because we have 35 minutes for snack time

677
01:04:02,960 --> 01:04:05,360
When you run this code when you

678
01:04:07,000 --> 01:04:09,000
Compare two values

679
01:04:09,720 --> 01:04:12,760
What on a column in pandas which is

680
01:04:13,920 --> 01:04:17,120
What pandas calls a series which is a long array?

681
01:04:18,040 --> 01:04:23,720
You non dimensional zero dimensions. All right, so when you're comparing it or doing

682
01:04:24,320 --> 01:04:27,560
math on a upper adding a comparing to an operator

683
01:04:29,840 --> 01:04:35,280
Was if you're comparing like this you get you're gonna get

684
01:04:36,280 --> 01:04:42,400
Oh my god, sorry it will return an array of the same size of

685
01:04:43,680 --> 01:04:49,320
Booleans and it's gonna tell you true if it matches the condition that you passed or false if it did not

686
01:04:51,680 --> 01:04:58,280
Like this because I'm using Ohio which is right in the middle, let me see if I try all about

687
01:04:59,280 --> 01:05:00,520
Oh

688
01:05:00,520 --> 01:05:07,000
So if I try Alabama it's gonna grab the because Alabama's all the way to the top isn't on the zero index you can see

689
01:05:08,200 --> 01:05:12,240
This array starts with truth. So each of these

690
01:05:13,360 --> 01:05:15,080
rows

691
01:05:15,080 --> 01:05:20,680
Their state fib column matches the word Alabama and

692
01:05:21,480 --> 01:05:24,200
You can use then this array

693
01:05:25,240 --> 01:05:27,240
as what

694
01:05:27,280 --> 01:05:32,080
I think it's this is in the pandas docs. They use they call it a mask

695
01:05:32,400 --> 01:05:36,840
So you're gonna you can use then this mask of trues and falses

696
01:05:39,120 --> 01:05:41,120
I'm just gonna keep it out

697
01:05:43,160 --> 01:05:49,720
Trues and falses as a mask to grab certain rows of your

698
01:05:51,200 --> 01:05:55,200
Full data frame. So if you if you pass something like this

699
01:05:55,600 --> 01:05:57,600
You can use this array

700
01:05:58,240 --> 01:06:03,280
to get the same size data state fib

701
01:06:06,240 --> 01:06:07,600
Alabama

702
01:06:07,600 --> 01:06:11,600
This I'm just saving what all this array that I got over here

703
01:06:12,640 --> 01:06:18,520
We're saving it to the variable mask state and I like and I like starting on my

704
01:06:19,280 --> 01:06:21,920
variables that I'm gonna use as masks or filters

705
01:06:22,000 --> 01:06:26,440
Or if I'm gonna be filtering out on the household weight

706
01:06:28,520 --> 01:06:31,440
Variable or any other variable like the age or

707
01:06:33,040 --> 01:06:35,040
Gq which are group quarters

708
01:06:35,680 --> 01:06:37,680
I'm going to

709
01:06:38,680 --> 01:06:41,140
Name each of those variables mask underscore

710
01:06:42,200 --> 01:06:44,280
Gq condition or mask underscore age

711
01:06:45,320 --> 01:06:50,620
To keep track of that and because once later on if I want to use it I can just mask underscore

712
01:06:51,100 --> 01:06:55,460
Tab and I'm gonna see all the filters that I've created all the masks that I have created

713
01:07:00,140 --> 01:07:04,660
I'm gonna use head to look at it and this is just an example of what's happening here

714
01:07:05,540 --> 01:07:09,620
Actually, Alabama is not a great example because this is the first five rows anyways

715
01:07:12,100 --> 01:07:15,060
So this is what's happening I'm grabbing that array of

716
01:07:16,300 --> 01:07:19,780
Booleans and I'm passing it to my data frame

717
01:07:20,820 --> 01:07:22,820
with these square brackets and

718
01:07:24,780 --> 01:07:26,780
That tells pandas

719
01:07:27,820 --> 01:07:30,780
Match the in the these the indices

720
01:07:31,420 --> 01:07:35,980
The index here and then just grab me the rows that that are true

721
01:07:36,700 --> 01:07:39,520
I'm using Ohio. So this is slightly different than over here

722
01:07:39,780 --> 01:07:46,980
But all is this is doing is go to the zero index on both the data data frame. I'm not my mask

723
01:07:47,500 --> 01:07:50,280
All right, if it's true, give me the row

724
01:07:50,620 --> 01:07:52,180
It's not

725
01:07:52,180 --> 01:07:54,180
Then don't show it to me

726
01:07:56,300 --> 01:08:03,020
So that's one way to filter out to slice data to they call it Boolean indexing

727
01:08:07,540 --> 01:08:09,540
So we're gonna save it to

728
01:08:12,620 --> 01:08:18,740
I'm gonna save this same thing that I got here mask state data with a mask

729
01:08:19,580 --> 01:08:22,660
I'm gonna save it to another variable

730
01:08:25,380 --> 01:08:32,140
And over here I go into a little bit of a description of why you should add this last part that copy

731
01:08:33,100 --> 01:08:35,100
when you're assigning a

732
01:08:36,340 --> 01:08:42,020
subset of a data frame saving it to another data frame when you're actually

733
01:08:42,020 --> 01:08:44,020
Saving

734
01:08:44,020 --> 01:08:50,620
From the pandas doc documentation is what they call a view of the other data frame

735
01:08:51,220 --> 01:08:58,500
So to save memory is kind of like pointing at the same thing and that can have unintended consequences later on if you're then if you then

736
01:08:59,260 --> 01:09:03,260
Yeah, if you start modifying your data on the second data frame or on the first one

737
01:09:03,780 --> 01:09:06,540
I have not found a bunch of I

738
01:09:07,260 --> 01:09:09,260
have not found a lot of

739
01:09:09,420 --> 01:09:16,540
Trouble doing this when I forget but you will get a warning every single time if you if you try doing this

740
01:09:21,300 --> 01:09:24,380
Just state data give me the data mask

741
01:09:28,300 --> 01:09:33,760
Right now it saves but then if I do state data

742
01:09:39,500 --> 01:09:41,500
Then you you can add a new column

743
01:09:44,260 --> 01:09:47,700
Oh you see this I use data too much

744
01:09:49,380 --> 01:09:52,180
Another column equals five

745
01:09:53,420 --> 01:09:59,660
This is the type of stuff this this is what what I try to do here is create a new column and then assign a 23 to

746
01:09:59,660 --> 01:10:00,860
all of them

747
01:10:00,860 --> 01:10:02,860
But you're gonna get this

748
01:10:03,020 --> 01:10:04,100
warning

749
01:10:04,100 --> 01:10:08,620
You're trying to set it on a copy of a slice of a data frame of another data frame

750
01:10:09,540 --> 01:10:10,740
try

751
01:10:10,740 --> 01:10:12,740
using .loc

752
01:10:12,860 --> 01:10:17,180
Because this is gonna actually assign values to to the data frame

753
01:10:17,780 --> 01:10:19,780
so when you when you got

754
01:10:21,540 --> 01:10:23,540
Something like this when you have a data frame

755
01:10:24,100 --> 01:10:30,380
Called data you pass it a mask and you're gonna you create in a subset from that data frame onto another

756
01:10:31,300 --> 01:10:35,160
Just make sure you use copy and sometimes you don't even need it

757
01:10:36,260 --> 01:10:38,020
We're just talking

758
01:10:38,020 --> 01:10:40,020
Over here you might get

759
01:10:41,300 --> 01:10:47,860
If you're doing state state state data and instead of data you have already something

760
01:10:49,060 --> 01:10:57,040
Set up and you're creating a subset of that's of its own thing and then just overriding the first variable

761
01:10:57,580 --> 01:11:00,500
You don't necessarily need to to add copy

762
01:11:01,780 --> 01:11:04,020
Because you're overriding and you're completely

763
01:11:04,860 --> 01:11:09,420
Forgetting about the first one, but it's always good practice and it doesn't it doesn't hurt it doesn't

764
01:11:11,180 --> 01:11:13,180
Make panda slower or anything

765
01:11:14,460 --> 01:11:16,460
So we're gonna save

766
01:11:19,300 --> 01:11:21,300
That data

767
01:11:23,380 --> 01:11:25,380
With a mask state

768
01:11:27,940 --> 01:11:31,940
Save it as a copy

769
01:11:35,020 --> 01:11:37,020
And then

770
01:11:38,820 --> 01:11:46,060
Now let's see what columns we have in our data frame and how do you think we would do that?

771
01:11:50,140 --> 01:11:54,340
Anyone how would you ask that column?

772
01:12:04,020 --> 01:12:06,020
You could just do

773
01:12:06,140 --> 01:12:07,860
state data that had

774
01:12:07,860 --> 01:12:15,860
But a lot of the times especially I mean, this is a very small subset of the American Community Survey and sometimes I want to have

775
01:12:16,580 --> 01:12:19,980
race ethnicity educational attainment age

776
01:12:19,980 --> 01:12:21,980
What else can have

777
01:12:27,660 --> 01:12:29,340
Merital status

778
01:12:29,340 --> 01:12:31,020
Nativity status

779
01:12:31,020 --> 01:12:35,740
So it's gonna have a lot more columns that you could show in the data frame

780
01:12:36,580 --> 01:12:38,580
Just in a cell here

781
01:12:39,620 --> 01:12:42,180
So you can just do dot columns and it's gonna give you a list

782
01:12:43,180 --> 01:12:45,180
Oh

783
01:12:47,540 --> 01:12:50,740
Perfect this is the type of stuff that you

784
01:12:54,380 --> 01:13:01,980
Just learn working with the data it bombs has this thing called these are the harmonized variables race

785
01:13:03,260 --> 01:13:05,380
hispan for hispanic and

786
01:13:06,420 --> 01:13:11,140
relate as it for the relationship between the person in the observation

787
01:13:12,420 --> 01:13:14,740
to the household to the head of the household and

788
01:13:15,660 --> 01:13:17,660
related race

789
01:13:17,860 --> 01:13:19,860
raised and hispan

790
01:13:20,660 --> 01:13:25,620
Are the detailed variables so you can see let me show you super quick

791
01:13:26,380 --> 01:13:28,380
You could see

792
01:13:30,420 --> 01:13:32,420
State data

793
01:13:36,820 --> 01:13:38,820
Raised

794
01:13:38,820 --> 01:13:40,820
I

795
01:13:41,660 --> 01:13:43,660
Show I'm just gonna show you

796
01:13:44,300 --> 01:13:46,980
Head if you if you pass a parameter to the head

797
01:13:48,420 --> 01:13:50,820
Call it's gonna show you that many

798
01:13:51,820 --> 01:13:55,320
Rows by default. It will show you five if you want to see

799
01:13:56,100 --> 01:14:02,260
15 for example, you just pass it 15 and and it will show you 15 rows and here we can see

800
01:14:02,260 --> 01:14:04,260
Oh

801
01:14:06,900 --> 01:14:11,300
Okay, so this one this one works you can see in the race

802
01:14:12,140 --> 01:14:16,620
column you're gonna have something like three or more major races because

803
01:14:17,660 --> 01:14:24,620
There's a lot of combinations that would satisfy three or more major races. But if you're going to race D you can actually see

804
01:14:25,620 --> 01:14:31,920
The three or more races that that person identifies us a lot of times. We'll just work with race

805
01:14:32,740 --> 01:14:40,260
Hispan because you don't want to I mean this could get really granular for race that you can have a lot of different combinations

806
01:14:41,060 --> 01:14:43,060
same thing with relate

807
01:14:44,220 --> 01:14:46,220
Let me see

808
01:14:46,820 --> 01:14:49,460
Other non relatives 1990 employers

809
01:14:50,460 --> 01:14:53,500
Challenge. Okay, you cannot really see on this one, but you can get

810
01:14:54,300 --> 01:14:57,620
children for child there's many definite there's many

811
01:14:57,620 --> 01:15:03,660
Detailed definitions and they'll they'll get like very specific especially because old censuses

812
01:15:04,380 --> 01:15:09,060
had like very weird ways of asking for stuff and they'll they'll be like

813
01:15:09,740 --> 01:15:16,700
Adopted adopted child from previous marriage or adoptive child from somewhere else and like all different types of of children

814
01:15:16,700 --> 01:15:20,240
But you don't really need that to differentiate their own

815
01:15:20,940 --> 01:15:22,940
children here

816
01:15:22,940 --> 01:15:24,940
Okay

817
01:15:30,140 --> 01:15:35,560
Okay, so here I gave you a space to show you how you can drop columns

818
01:15:36,500 --> 01:15:42,780
It's as easy as just dot drop and then you give it the columns you give it a list of columns

819
01:15:44,340 --> 01:15:46,780
So if I wanted to drop those race the

820
01:15:49,540 --> 01:15:51,540
Hispan D

821
01:15:52,540 --> 01:15:54,540
Related

822
01:15:56,420 --> 01:15:58,420
No, it's columns and

823
01:15:59,740 --> 01:16:01,740
This will return

824
01:16:01,780 --> 01:16:03,620
your whole data frame

825
01:16:03,620 --> 01:16:09,180
But without those columns, so if you want to save this you would have to save it to another variable

826
01:16:09,180 --> 01:16:13,620
But you don't have to do that yet just yet because well unless you're completely sure of the columns that you're dropping

827
01:16:15,820 --> 01:16:18,100
And this is why I

828
01:16:19,260 --> 01:16:21,260
Might be on the next one

829
01:16:22,340 --> 01:16:26,380
Yeah, you can sometimes and some of these you can always do

830
01:16:27,660 --> 01:16:36,580
Not not in everything you can do in place equals true and it's just gonna save it to itself. I saw a talk by I

831
01:16:39,100 --> 01:16:46,180
Don't remember his name, but he's one of the core developers in pandas and he said not to use in place because it just has some

832
01:16:47,460 --> 01:16:49,460
unintended consequences sometimes

833
01:16:49,620 --> 01:16:53,700
So that's why I'm not included here, but yeah, you could just do in place

834
01:16:54,860 --> 01:16:57,020
for some of these especially a drop or

835
01:16:57,780 --> 01:16:59,780
in reset index is kind of

836
01:17:00,580 --> 01:17:02,580
common

837
01:17:07,260 --> 01:17:11,940
What do I have in I mentioned here that you should drop columns only if you're

838
01:17:12,900 --> 01:17:16,700
If you're over like 90% sure you're not gonna need it

839
01:17:17,180 --> 01:17:20,980
Especially because this data set is not particularly huge is not gigabytes and gigabytes of data

840
01:17:21,380 --> 01:17:24,900
So you don't mind just having an extra column that you might ignore just down the road

841
01:17:25,860 --> 01:17:30,280
If you're dealing with gigabytes of data, then you might just be like, okay

842
01:17:30,280 --> 01:17:34,900
Well, there's no way I can have an extra column because it's gonna take an extra hour to run this

843
01:17:36,660 --> 01:17:38,660
But just just in this

844
01:17:39,620 --> 01:17:45,740
I would just drop those if I'm 90% sure and this is the value of having

845
01:17:46,780 --> 01:17:50,220
the raw directory and the process or interim directory

846
01:17:50,900 --> 01:17:52,700
because sorry again

847
01:17:52,700 --> 01:17:59,620
because if you drop those columns and then you save it back to the original file and you just lost those and

848
01:18:00,340 --> 01:18:07,500
The and it's just good practice to have to keep in mind. This is my raw data directory. There's the original files. I'm not

849
01:18:08,340 --> 01:18:09,780
modifying those

850
01:18:09,780 --> 01:18:13,300
Ever I got these and then whatever I do. I'm gonna save it somewhere else

851
01:18:17,180 --> 01:18:19,540
Okay, so now we're gonna explore

852
01:18:21,020 --> 01:18:23,700
GQ which stands for group quarters

853
01:18:25,300 --> 01:18:27,300
Which are

854
01:18:27,340 --> 01:18:33,100
Group living arrangements is what they call so such as rooming houses or military barracks

855
01:18:34,220 --> 01:18:36,220
dorms and colleges

856
01:18:36,740 --> 01:18:39,820
certain institutional living arrangements

857
01:18:39,820 --> 01:18:45,260
We go more in depth over here but essentially if there's like five or more people

858
01:18:46,020 --> 01:18:48,020
unrelated to each other

859
01:18:48,340 --> 01:18:51,580
That live in one place. It's considered a living quarter

860
01:18:52,100 --> 01:18:54,100
by census standards

861
01:18:55,260 --> 01:19:02,260
But we want to see here is the unique values of your GQ variable

862
01:19:03,100 --> 01:19:06,220
This is gonna return a list of the

863
01:19:07,660 --> 01:19:09,660
The unique values in that

864
01:19:10,180 --> 01:19:14,460
Array and some extra information. So now you can see here

865
01:19:15,460 --> 01:19:17,700
We knew that it was a categorical variable

866
01:19:18,580 --> 01:19:20,580
This this tells you

867
01:19:21,100 --> 01:19:23,100
That you can see here

868
01:19:23,100 --> 01:19:29,100
That you can see here that you can see here that you can see here that you can see here that you can see here that you can see here

869
01:19:29,940 --> 01:19:31,940
This this tells you

870
01:19:33,020 --> 01:19:40,420
Yeah, these are the unique values this is these are the categories that GQ has and if you look further over here

871
01:19:41,620 --> 01:19:44,260
You can see this this is particularly interesting

872
01:19:45,140 --> 01:19:47,460
This means these are ordered

873
01:19:50,980 --> 01:19:56,700
Categories so you you you can then ask for comparison as like give me

874
01:19:57,100 --> 01:20:01,220
a state data where GQ is less than

875
01:20:02,220 --> 01:20:06,660
Additional households under 1990 definition and that's gonna that's gonna run

876
01:20:07,500 --> 01:20:10,500
Because this is a categorical variable if it was strings

877
01:20:10,500 --> 01:20:13,540
There's no way you can say oh this string is greater than this other string

878
01:20:13,980 --> 01:20:18,500
but because it's categorical variable a category categorical column and

879
01:20:19,580 --> 01:20:23,260
Because it's an ordered categorical column

880
01:20:24,100 --> 01:20:31,460
Not all categorical columns can our order but this one is you can then have these greater than or less than

881
01:20:31,940 --> 01:20:33,500
comparisons between

882
01:20:33,500 --> 01:20:35,500
strings or values

883
01:20:35,500 --> 01:20:37,500
And they're super

884
01:20:38,380 --> 01:20:43,820
Optimized because under under the hood these are just numbers. These are like one one two, three four five and

885
01:20:44,340 --> 01:20:46,540
It's just you that can see the value labels

886
01:20:47,180 --> 01:20:49,660
So it's like comparing two two integers

887
01:20:53,260 --> 01:21:00,620
We can also use this over here value counts, I'm kind of rushing because we have snack break in 15 minutes

888
01:21:01,860 --> 01:21:05,020
But I can slow down if anyone needs me to

889
01:21:07,980 --> 01:21:14,160
You can also do this value counts on the same on a on a column

890
01:21:15,100 --> 01:21:19,060
state data GQ and what do you think is gonna give you

891
01:21:20,380 --> 01:21:22,520
If you ran it you can just tell us

892
01:21:23,780 --> 01:21:26,180
What do you expect is gonna

893
01:21:29,060 --> 01:21:34,940
Yes, exactly it's gonna give you the counts for each of the values in your series that you're passing

894
01:21:36,820 --> 01:21:38,820
So this

895
01:21:40,780 --> 01:21:42,580
So this

896
01:21:42,580 --> 01:21:48,740
Just counts everything all the about it has we already know they has five values. So it just counts

897
01:21:49,140 --> 01:21:51,840
each instance of each of those values

898
01:21:53,340 --> 01:21:55,340
So now we know we have

899
01:21:56,340 --> 01:22:04,260
112 thousand households under the 1970 definition of household which I included here from the impums documentation

900
01:22:07,060 --> 01:22:11,660
This can get a little confusing sometimes when we're working with weighted data

901
01:22:12,140 --> 01:22:16,020
we're working with a sample of the Americans Community Survey and

902
01:22:16,460 --> 01:22:22,140
Each observation has household weights and person weights. That means each observation

903
01:22:23,020 --> 01:22:26,340
Actually represents more than one person. These are not

904
01:22:27,940 --> 01:22:29,700
112,000 households

905
01:22:29,700 --> 01:22:31,700
These are 112,000

906
01:22:31,900 --> 01:22:36,860
Observations and that's one of the main differences between between weighted working with weighted data

907
01:22:37,300 --> 01:22:41,160
Working with census data because we never really get access to the whole data file

908
01:22:45,740 --> 01:22:51,460
So this because because our data is a sample of the total

909
01:22:52,540 --> 01:22:54,540
You

910
01:22:55,620 --> 01:22:57,620
See this over here

911
01:22:58,740 --> 01:23:00,340
HHWT

912
01:23:00,340 --> 01:23:04,540
That's a household weight that tells me that this line is

913
01:23:05,460 --> 01:23:07,460
representing five fifty five households

914
01:23:07,820 --> 01:23:15,020
So each observation each line in my data file is not one household and it's not one person

915
01:23:15,500 --> 01:23:21,340
This person is a head householder. He's a male. He's 79. He's white and

916
01:23:22,140 --> 01:23:27,020
He has no internet access and his host and his house apartment or blah blah

917
01:23:27,580 --> 01:23:29,580
Which falls under the household on?

918
01:23:30,700 --> 01:23:32,700
definition from 1970

919
01:23:33,020 --> 01:23:37,620
He's in Ohio on a I think zero means like we it's not

920
01:23:38,860 --> 01:23:42,180
It's like no and this person is

921
01:23:43,380 --> 01:23:45,380
representing

922
01:23:45,380 --> 01:23:47,020
55 people

923
01:23:47,020 --> 01:23:50,540
So instead of having 55 the same of fairly similar

924
01:23:50,700 --> 01:23:54,260
Observations they just group them all into one

925
01:23:55,060 --> 01:24:00,260
Because also they don't want to give you like each person's actual answers for the community survey

926
01:24:02,620 --> 01:24:05,780
So this so if we wanted to find out like the share of

927
01:24:07,260 --> 01:24:14,100
Households that have access to internet we could just like and in each of those was already one one observation was one household

928
01:24:14,660 --> 01:24:17,840
This could just give us the numbers and then we can just run

929
01:24:18,420 --> 01:24:20,420
Well, I have this many

930
01:24:20,580 --> 01:24:23,900
Just add them up and then give me the ones that fall under X

931
01:24:25,300 --> 01:24:29,060
Value over here. It's a high speed or sineth

932
01:24:30,140 --> 01:24:31,940
the internet access one

933
01:24:31,940 --> 01:24:35,440
But because we're working with weighted data, we have to do other stuff

934
01:24:37,220 --> 01:24:39,220
And then we're gonna

935
01:24:39,260 --> 01:24:42,580
So anyone with me everyone with me so far

936
01:24:44,100 --> 01:24:47,540
We're gonna have break in like ten minutes and you can

937
01:24:48,500 --> 01:24:50,620
Get snacks and then come and talk to me

938
01:24:52,220 --> 01:24:55,180
You have more questions so what we're gonna create here

939
01:24:56,060 --> 01:25:03,180
Is another mask just like we did earlier with the state. We want to create a mask with two conditions

940
01:25:03,780 --> 01:25:05,540
so

941
01:25:05,540 --> 01:25:08,060
in for our data what we want to do is

942
01:25:08,900 --> 01:25:12,740
grab the households that fall under the 1970 definition and

943
01:25:13,580 --> 01:25:20,380
Those that fall under the 1990 definition and here is the explanation if you're interested

944
01:25:21,020 --> 01:25:26,100
In the 1970s the limit was five or more if you have five or more people living

945
01:25:26,860 --> 01:25:28,180
unrelated people

946
01:25:28,180 --> 01:25:31,640
Living in one place your group quarter your your

947
01:25:32,460 --> 01:25:34,580
building slash house slash

948
01:25:35,220 --> 01:25:40,420
Apartment is a group quarter in 1990. They bumped it up to ten people. So if there's ten people

949
01:25:41,060 --> 01:25:44,780
Living in a housing unit and they're all related to each other

950
01:25:45,420 --> 01:25:47,380
Then it's a group quarter

951
01:25:47,380 --> 01:25:49,380
but starting 2000

952
01:25:49,900 --> 01:25:54,740
They stopped the limit now. They have a list an actual list somewhere in the census

953
01:25:55,780 --> 01:26:00,900
where they where they that's continuously maintained by the Census Bureau and

954
01:26:01,620 --> 01:26:03,620
if you if your

955
01:26:03,860 --> 01:26:09,840
Housing unit which would be considered a group quarter under under these other definitions is not in that list

956
01:26:10,080 --> 01:26:12,080
they're gonna count it as a household and

957
01:26:12,800 --> 01:26:17,520
It's not a lot as you can see here that it's only 20 observations in our data

958
01:26:18,400 --> 01:26:22,640
But if we want to be as accurate as possible, we just wouldn't count those

959
01:26:24,080 --> 01:26:27,160
Under what we what we think of a household

960
01:26:28,280 --> 01:26:30,280
so I'm gonna give you two minutes and

961
01:26:30,920 --> 01:26:32,920
I'm gonna set up the first one

962
01:26:33,600 --> 01:26:35,600
You're doing the same thing state

963
01:26:35,880 --> 01:26:41,680
You you you're creating a comparison that's gonna return something

964
01:26:44,600 --> 01:26:50,920
And you're creating another condition for another value this looks pretty much the same

965
01:27:00,280 --> 01:27:05,520
Yeah, that's the thing about categorical data if it's a long definition you have to type out

966
01:27:05,600 --> 01:27:07,600
the whole thing

967
01:27:15,080 --> 01:27:21,280
Oh and don't read ahead because that's the answer that's a tip to make it

968
01:27:35,600 --> 01:27:37,600
You

969
01:28:05,600 --> 01:28:07,600
You

970
01:28:35,600 --> 01:28:37,600
You

971
01:29:05,600 --> 01:29:07,600
You

972
01:29:35,600 --> 01:29:37,600
You

973
01:29:57,800 --> 01:30:03,440
Okay, and I'm just gonna give you the answer for this one because it's a little confusing what I'm suggesting you do here is

974
01:30:03,920 --> 01:30:06,080
Create a copy like we mentioned earlier

975
01:30:08,160 --> 01:30:10,160
This is that

976
01:30:10,440 --> 01:30:12,440
Example I gave you earlier because we're

977
01:30:13,280 --> 01:30:15,280
grabbing state data

978
01:30:15,560 --> 01:30:21,920
Passing in a mask so it's gonna be a subset of itself and then we're saving that to the variable state data

979
01:30:22,440 --> 01:30:28,040
Which means we're overriding it. We don't need to add the dot copy at the end

980
01:30:28,640 --> 01:30:30,640
Because we're just completely forgetting about the original

981
01:30:31,160 --> 01:30:36,520
But it doesn't affect it and it's just good practice is there something I try to do myself

982
01:30:37,040 --> 01:30:41,080
So I won't forget when I actually have to do the dot copy

983
01:31:01,640 --> 01:31:02,840
Oh

984
01:31:02,840 --> 01:31:09,360
Yeah, I get it sometimes but I didn't this time I when I did it in my my earlier

985
01:31:09,360 --> 01:31:12,000
I got the same thing, but all it's doing is

986
01:31:12,720 --> 01:31:17,240
Just matching the index and the indexes and the C's

987
01:31:18,880 --> 01:31:20,880
These are the index

988
01:31:21,160 --> 01:31:25,600
They're meaningless to us. So it doesn't doesn't really affect it and all it's doing is just

989
01:31:25,600 --> 01:31:33,160
Making sure they match like we're changing one of these I think the mask household because it's gonna be trues and falses

990
01:31:35,160 --> 01:31:37,160
Let me see

991
01:31:45,960 --> 01:31:50,120
On state data, I mean you you

992
01:31:51,920 --> 01:31:53,920
You could let me see

993
01:31:55,600 --> 01:31:57,600
You

994
01:32:01,120 --> 01:32:03,880
You can if you do that index is gonna give you the index

995
01:32:08,320 --> 01:32:10,320
Tweeting

996
01:32:16,480 --> 01:32:21,360
Yeah, no, I'm not a conclusion what's happening I got the same morning earlier

997
01:32:21,600 --> 01:32:29,120
And if I if I find out I'll add it to the repudging just add a note but it's not I'm not getting it. Yeah

998
01:32:33,280 --> 01:32:35,280
Yes, that's exactly right the

999
01:32:36,320 --> 01:32:38,320
this serial

1000
01:32:38,880 --> 01:32:45,280
Means is the serial number of the household under the census standards on the American Community Survey standard

1001
01:32:45,600 --> 01:32:47,600
So if you see here

1002
01:32:48,600 --> 01:32:51,760
These two have the same house they have the same serial

1003
01:32:52,880 --> 01:32:57,880
so there is a head householder male 79 and

1004
01:32:58,720 --> 01:33:01,880
his spouse female 79 in

1005
01:33:02,880 --> 01:33:04,880
This one household

1006
01:33:05,760 --> 01:33:11,360
That's that's all he's doing and the serial number is what we're gonna we keep that we need that

1007
01:33:12,640 --> 01:33:15,840
That's why I mentioned though, if you're not 90% sure don't drop it

1008
01:33:16,680 --> 01:33:18,600
If you dropped it

1009
01:33:18,600 --> 01:33:25,800
We're just gonna have to get it again because the serial number is what keeps that tells you which household is which

1010
01:33:33,000 --> 01:33:35,760
Yes, so these are not like

1011
01:33:36,800 --> 01:33:42,280
It's not an actual house somewhere that has a serial number that the census have saved. This is just the way to track

1012
01:33:42,480 --> 01:33:47,800
Same household throughout like for different observations and a lot of the times when we're working with

1013
01:33:48,320 --> 01:33:52,200
Individual level data we might just drop all that like we don't care the house

1014
01:33:52,200 --> 01:33:54,680
All we care is the person's weight and how many people?

1015
01:33:55,240 --> 01:34:00,600
Count but because what we're looking at today is how many households have

1016
01:34:01,400 --> 01:34:03,720
Access to the internet to high-speed internet

1017
01:34:05,080 --> 01:34:07,080
We keep these

1018
01:34:07,240 --> 01:34:09,240
This household level variables

1019
01:34:09,840 --> 01:34:14,800
We have this that's good the serial number is is what but we're gonna need later on

1020
01:34:18,360 --> 01:34:24,400
Okay, so we created this we created a copy and now we have

1021
01:34:26,640 --> 01:34:28,400
A

1022
01:34:28,400 --> 01:34:31,120
Working data set our state underscore data

1023
01:34:32,040 --> 01:34:36,000
You might you might have saved it as something else now. We only have one state

1024
01:34:36,240 --> 01:34:42,440
We dropped all the other 49 and or the district of Columbia, Columbia and

1025
01:34:43,520 --> 01:34:46,960
We kept we dropped all the group quarters. We kept only households

1026
01:34:47,480 --> 01:34:49,880
So now we can actually if we look at our equation

1027
01:34:49,880 --> 01:34:56,480
We want to see how many households have access to high-speed internet over the total households in the state. We got

1028
01:34:58,480 --> 01:35:02,400
The denominator easily now we have all the households in the state that we care

1029
01:35:02,400 --> 01:35:04,400
That we consider households

1030
01:35:04,400 --> 01:35:06,400
And I think this is subjective though

1031
01:35:06,880 --> 01:35:12,800
You could just not include the 1990 definition and say just like if there's six people living in one house

1032
01:35:12,800 --> 01:35:15,760
It's not a house. It's a group quarter and just drop it

1033
01:35:15,760 --> 01:35:20,320
All you got to do is make sure you defend it or put a note in your analysis

1034
01:35:20,880 --> 01:35:23,920
So when people try to recreate it, they know that that's what you did

1035
01:35:25,200 --> 01:35:29,840
But now we have only the households that we care about and only the state that we care about

1036
01:35:29,840 --> 01:35:32,800
So now we have a working data set now we can just save it

1037
01:35:35,520 --> 01:35:38,560
We're gonna use data again, so we're gonna use the to state up

1038
01:35:40,880 --> 01:35:42,880
function

1039
01:35:44,400 --> 01:35:51,840
And what I'm doing here what I'm suggesting you do is use an f-string which started in 3.6

1040
01:35:53,360 --> 01:35:55,360
Python 3.6

1041
01:35:55,440 --> 01:35:57,440
To include this

1042
01:35:58,000 --> 01:36:02,960
Tag to include this tag this today tag that we created at the very beginning

1043
01:36:04,240 --> 01:36:06,240
Over here zero one

1044
01:36:06,320 --> 01:36:08,320
dash may dash 19

1045
01:36:09,120 --> 01:36:11,520
You don't have to do it this way. You could use

1046
01:36:12,480 --> 01:36:13,760
any other

1047
01:36:13,760 --> 01:36:15,760
way of string interpolation

1048
01:36:17,280 --> 01:36:21,120
This is just suggested and because we're in my vendor if we're you're following through

1049
01:36:22,480 --> 01:36:24,800
On my vendor, I think we're using three seven

1050
01:36:27,440 --> 01:36:32,640
So you just let's save that to our interim data path, whatever you name that

1051
01:36:33,920 --> 01:36:38,240
And I would suggest I highly encourage to pass this

1052
01:36:38,960 --> 01:36:40,320
write index

1053
01:36:40,320 --> 01:36:41,760
Equals false

1054
01:36:41,760 --> 01:36:47,440
Because by default is going to write the index and then you're going to have an extra column. That's meaningless to you. It's just going to be like

1055
01:36:48,560 --> 01:36:50,560
What is it?

1056
01:36:51,660 --> 01:36:53,660
3,000 218 5099

1057
01:36:54,800 --> 01:36:56,800
Which means nothing to you

1058
01:36:57,440 --> 01:36:59,440
And two two two two two state

1059
01:37:01,200 --> 01:37:03,200
Save that

1060
01:37:05,680 --> 01:37:07,680
To stay down

1061
01:37:07,680 --> 01:37:12,400
And if you want to see what other things you can pass to stay down on jupyter if you

1062
01:37:13,440 --> 01:37:15,520
Use the question mark at the end of the function

1063
01:37:17,280 --> 01:37:19,280
It's going to give you the

1064
01:37:19,600 --> 01:37:21,600
the doc string

1065
01:37:21,680 --> 01:37:23,680
So the file name is what we're passing here

1066
01:37:25,120 --> 01:37:27,120
Is interim data path

1067
01:37:27,520 --> 01:37:29,520
And it's going to expand it to include

1068
01:37:30,720 --> 01:37:32,720
interim data path slash

1069
01:37:32,800 --> 01:37:37,440
state underscore data dash zero one dash may dash 19

1070
01:37:38,480 --> 01:37:40,480
dot dta

1071
01:37:41,600 --> 01:37:46,160
Do we need to convert the dates we don't have any dates we don't care write the index it's set to true

1072
01:37:48,080 --> 01:37:51,360
But we don't want the index because it's meaningless to us in this analysis

1073
01:37:52,880 --> 01:37:56,960
Encoding by default is latin one. That's what's data uses still

1074
01:37:58,000 --> 01:38:01,200
Um by order all these other things we don't really care about

1075
01:38:02,400 --> 01:38:08,080
Uh, but well here are some examples. This is just to show you what you can do if you in case you didn't know

1076
01:38:08,800 --> 01:38:10,240
question mark

1077
01:38:10,240 --> 01:38:12,320
It will give you the doc string

1078
01:38:13,600 --> 01:38:16,480
Which is very useful if you do two question marks

1079
01:38:17,360 --> 01:38:19,840
It'll give you the actual code

1080
01:38:21,600 --> 01:38:23,280
Do do do do

1081
01:38:23,280 --> 01:38:25,200
Which is a lot

1082
01:38:25,200 --> 01:38:28,480
If you know what you're doing then you want to make sure you like

1083
01:38:29,840 --> 01:38:33,920
Um, this is exactly what you want. This is another way to look at it

1084
01:38:36,080 --> 01:38:38,880
Okay, so we're doing this interim

1085
01:38:41,120 --> 01:38:47,040
Data path f string state data dash

1086
01:38:49,200 --> 01:38:51,200
Today

1087
01:38:52,320 --> 01:38:54,320
And you could just write the date yourself

1088
01:38:55,200 --> 01:38:56,720
If you wanted to

1089
01:38:56,720 --> 01:38:58,400
but this

1090
01:38:58,400 --> 01:39:01,440
Workflow has worked pretty great for me. I'll just

1091
01:39:02,320 --> 01:39:05,920
If i'm running something and I changed just a little bit of the code earlier on

1092
01:39:06,880 --> 01:39:08,880
And i'm running it the next day

1093
01:39:10,800 --> 01:39:16,400
I don't overwrite the data that I that this subset of the data that I created the day before

1094
01:39:18,480 --> 01:39:23,760
So it's just you might you could end up with a lot of copies of the same data if you're running it every day

1095
01:39:25,920 --> 01:39:32,160
But you just gotta it's it's still a nice nice tool and you could get more more

1096
01:39:35,520 --> 01:39:37,520
Detailed here, this is just the day

1097
01:39:38,080 --> 01:39:41,280
The month and the year, but I think it goes to like nanosecond

1098
01:39:42,160 --> 01:39:46,720
So if you really care about the time like I I I ran this at 359

1099
01:39:47,840 --> 01:39:49,840
0101

1100
01:39:50,000 --> 01:39:52,000
You should save it like that

1101
01:39:52,160 --> 01:39:54,160
saves

1102
01:39:55,040 --> 01:39:56,320
And

1103
01:39:56,320 --> 01:40:00,960
Let me see if we still have time. No, it's 302. Sorry. You were two minutes late for snack

1104
01:40:01,680 --> 01:40:04,640
But this is a good a good time to stop

1105
01:40:05,280 --> 01:40:10,340
Uh, we have saved the data and when we come back, we'll talk more about what what we've done so far. Thank you

1106
01:40:24,160 --> 01:40:26,240
You

1107
01:40:54,160 --> 01:40:56,240
You

1108
01:41:24,160 --> 01:41:26,240
You

1109
01:41:54,160 --> 01:41:56,240
You

1110
01:42:24,160 --> 01:42:26,240
You

1111
01:42:54,160 --> 01:42:56,240
You

1112
01:43:24,160 --> 01:43:26,240
You

1113
01:43:54,160 --> 01:43:56,240
You

1114
01:44:24,160 --> 01:44:26,240
You

1115
01:44:54,160 --> 01:44:56,240
You

1116
01:45:24,160 --> 01:45:26,240
You

1117
01:45:54,160 --> 01:45:56,240
You

1118
01:46:24,160 --> 01:46:26,240
You

1119
01:46:54,160 --> 01:46:56,240
You

1120
01:47:24,160 --> 01:47:26,240
You

1121
01:47:54,160 --> 01:47:56,240
You

1122
01:48:24,160 --> 01:48:26,240
You

1123
01:48:54,160 --> 01:48:56,240
You

1124
01:49:24,160 --> 01:49:26,240
You

1125
01:49:54,160 --> 01:49:56,240
You

1126
01:50:24,160 --> 01:50:26,240
You

1127
01:50:54,160 --> 01:50:56,240
You

1128
01:51:24,160 --> 01:51:26,240
You

1129
01:51:54,160 --> 01:51:56,240
You

1130
01:52:24,160 --> 01:52:26,240
You

1131
01:52:54,160 --> 01:52:56,240
You

1132
01:53:24,160 --> 01:53:26,240
You

1133
01:53:54,160 --> 01:53:56,240
You

1134
01:54:24,160 --> 01:54:26,240
You

1135
01:54:54,160 --> 01:54:56,240
You

1136
01:55:24,160 --> 01:55:26,240
You

1137
01:55:54,160 --> 01:55:56,240
You

1138
01:56:24,160 --> 01:56:26,240
You

1139
01:56:54,160 --> 01:56:56,240
You

1140
01:57:24,160 --> 01:57:26,240
You

1141
01:57:54,160 --> 01:57:56,240
You

1142
01:58:24,160 --> 01:58:26,240
You

1143
01:58:54,160 --> 01:58:56,240
You

1144
01:59:24,160 --> 01:59:26,240
You

1145
01:59:54,160 --> 01:59:56,240
You

1146
02:00:24,160 --> 02:00:26,240
You

1147
02:00:54,160 --> 02:00:56,240
You

1148
02:01:24,160 --> 02:01:26,240
You

1149
02:01:54,160 --> 02:01:56,240
You

1150
02:02:24,160 --> 02:02:26,240
You

1151
02:02:54,160 --> 02:02:56,240
You

1152
02:03:24,160 --> 02:03:26,240
You

1153
02:03:54,160 --> 02:03:56,240
You

1154
02:04:24,160 --> 02:04:26,240
You

1155
02:04:54,160 --> 02:04:56,160
You

1156
02:05:10,960 --> 02:05:12,960
Yes

1157
02:05:14,720 --> 02:05:17,620
Okay, yes, sorry, um I forgot about that

1158
02:05:24,160 --> 02:05:26,160
You

1159
02:05:27,200 --> 02:05:29,200
Yeah, I lost mine too

1160
02:05:31,840 --> 02:05:33,840
Rookie mistake

1161
02:05:54,160 --> 02:05:56,160
You

1162
02:06:24,160 --> 02:06:26,160
You

1163
02:06:52,000 --> 02:06:54,000
Oh

1164
02:06:54,160 --> 02:06:56,160
Yeah

1165
02:07:24,160 --> 02:07:26,160
You

1166
02:07:31,840 --> 02:07:33,840
One more minute

1167
02:07:54,160 --> 02:07:56,640
You

1168
02:08:24,160 --> 02:08:26,320
You

1169
02:08:54,160 --> 02:08:56,160
You

1170
02:09:06,160 --> 02:09:09,200
Yeah, that's that's why it doesn't work

1171
02:09:17,760 --> 02:09:19,760
Yeah

1172
02:09:24,160 --> 02:09:26,160
You

1173
02:09:41,920 --> 02:09:43,920
Okay

1174
02:09:49,360 --> 02:09:51,360
As a time

1175
02:09:52,320 --> 02:09:54,320
Okay, we're gonna go ahead and get started

1176
02:09:55,840 --> 02:09:57,840
The second part of

1177
02:09:58,400 --> 02:10:02,400
Our analysis we're gonna so when we left off before the break

1178
02:10:03,200 --> 02:10:05,440
We had just saved our data if you're using

1179
02:10:06,240 --> 02:10:08,240
my binder

1180
02:10:08,320 --> 02:10:10,320
to follow along your

1181
02:10:10,960 --> 02:10:12,960
session might have

1182
02:10:13,280 --> 02:10:17,840
Finished if you don't do anything for 10 minutes, they'll kind of like shut it down

1183
02:10:18,800 --> 02:10:22,720
What I just did right now was go all the way to the top here

1184
02:10:22,800 --> 02:10:24,800
I just refresh to restart my kernel

1185
02:10:25,680 --> 02:10:27,760
And I just run every single cell again

1186
02:10:28,720 --> 02:10:30,720
If that happened to you, you can

1187
02:10:33,360 --> 02:10:35,600
Just refresh the page and then we'll start

1188
02:10:36,560 --> 02:10:38,560
You're not gonna have that saved

1189
02:10:38,800 --> 02:10:44,080
but it's great that we finished all the way to save it saving the data file because you could just

1190
02:10:44,880 --> 02:10:46,880
Go here to the solutions

1191
02:10:48,000 --> 02:10:51,840
And just run this code until all the way to saving a file

1192
02:10:53,120 --> 02:10:54,800
all the way to

1193
02:10:54,800 --> 02:11:01,600
Step five when we save the file does that make sense if if if you're running my binder and your session didn't

1194
02:11:02,400 --> 02:11:03,600
expire

1195
02:11:03,600 --> 02:11:09,200
Uh, if you're not seeing any errors, then you're good to go. This is just in case you try running something and it tells you

1196
02:11:09,760 --> 02:11:12,240
Kernels that or your session has expired or something

1197
02:11:12,400 --> 02:11:14,400
So

1198
02:11:16,720 --> 02:11:18,720
Okay, so I just wanna

1199
02:11:22,560 --> 02:11:27,040
We can just go over what we have done so far we have

1200
02:11:28,640 --> 02:11:30,240
At the very beginning

1201
02:11:30,240 --> 02:11:36,560
We have put together our environment all the libraries that we're going to need we set up the path file the file

1202
02:11:37,200 --> 02:11:38,400
paths

1203
02:11:38,400 --> 02:11:43,840
So this can be run on any platform because of pathlib it will figure out

1204
02:11:44,640 --> 02:11:47,680
Yeah, go up one directory find the data one find the raw one

1205
02:11:48,400 --> 02:11:49,920
the raw directory

1206
02:11:49,920 --> 02:11:53,040
Um, this is going to work even with the forward slashes

1207
02:11:53,600 --> 02:11:56,420
and windows or macOS or linux

1208
02:11:57,040 --> 02:11:59,840
We have this little function that's going to give us the

1209
02:12:01,040 --> 02:12:04,400
Tree of a directory tree so we can explore

1210
02:12:05,360 --> 02:12:11,360
The same path it doesn't have to be raw data path you can pass interim if you if you're working on something else

1211
02:12:12,400 --> 02:12:13,680
later on

1212
02:12:13,680 --> 02:12:14,960
We have

1213
02:12:14,960 --> 02:12:16,320
loaded up

1214
02:12:16,320 --> 02:12:19,600
um a compressed file so we know how to load up

1215
02:12:20,560 --> 02:12:24,400
either csv compressed file gz gzip csv files

1216
02:12:25,040 --> 02:12:28,640
gzip compressed csv files directly onto pandas

1217
02:12:29,920 --> 02:12:31,200
or

1218
02:12:31,200 --> 02:12:33,200
using gzip

1219
02:12:34,400 --> 02:12:36,320
loading up more

1220
02:12:36,320 --> 02:12:39,120
different formats like .dta

1221
02:12:40,960 --> 02:12:42,960
We saw the

1222
02:12:42,960 --> 02:12:48,320
Quote-unquote value of having a dta file it comes with value labels

1223
02:12:49,600 --> 02:12:55,680
Which can then be interpreted as categorical columns in pandas which allows us to do

1224
02:12:56,860 --> 02:13:00,800
comparisons even between strings on the categories if those

1225
02:13:01,520 --> 02:13:03,520
Categories are ordered

1226
02:13:04,560 --> 02:13:06,560
We know how to do some basic

1227
02:13:09,020 --> 02:13:11,020
Statistics and

1228
02:13:11,200 --> 02:13:13,680
basic descriptives of your data frame

1229
02:13:14,640 --> 02:13:17,520
We know how to find what type of data you're working with

1230
02:13:18,560 --> 02:13:23,460
We know how to find like the basic statistical descriptive analytics

1231
02:13:24,480 --> 02:13:26,740
of your quantitative variables

1232
02:13:28,000 --> 02:13:31,760
We know how to find the shape of our our data frame and this is particularly useful in

1233
02:13:32,720 --> 02:13:36,480
If you think you have like 300 million observations and then you happen

1234
02:13:37,440 --> 02:13:44,560
And looking at the data it looks like you have three million and if you look at the memory you might or might not notice

1235
02:13:45,280 --> 02:13:48,080
Those size difference you see a big file and it's a big file

1236
02:13:48,480 --> 02:13:51,760
But if you expect it a lot more this is you're going to see it right away

1237
02:13:52,480 --> 02:13:56,400
Over here or if you have a lot less columns that you thought than you thought

1238
02:13:56,880 --> 02:13:58,880
So we started out

1239
02:13:59,920 --> 02:14:06,000
From then we we moved on and we started modifying our our data set

1240
02:14:06,480 --> 02:14:11,440
We we loaded up the master file and then we started looking at slicing and

1241
02:14:12,640 --> 02:14:14,960
grabbing just subsets of that data set

1242
02:14:15,760 --> 02:14:17,760
We we dropped

1243
02:14:19,200 --> 02:14:20,560
The rows

1244
02:14:20,560 --> 02:14:23,680
That we did not want in this case. We looked at the states

1245
02:14:23,840 --> 02:14:28,080
And then we chose one state that's what that's the one we're interested in drop everything else

1246
02:14:29,920 --> 02:14:32,800
Is what they call boolean indexing

1247
02:14:33,840 --> 02:14:39,200
We created we create we learned more about this little work

1248
02:14:41,360 --> 02:14:44,160
Two step process of creating a mask

1249
02:14:45,120 --> 02:14:47,760
Passing that onto a data frame to create a subset

1250
02:14:48,560 --> 02:14:51,600
And then saving that onto another

1251
02:14:51,680 --> 02:14:53,680
Data frame

1252
02:14:56,240 --> 02:14:58,800
We'll learn about copy if you're if you're

1253
02:15:00,480 --> 02:15:04,000
Pointing it if you have a master file and you grab a subset

1254
02:15:04,720 --> 02:15:08,720
And then you're going to save that to another variable. You want to create a copy of that

1255
02:15:10,080 --> 02:15:13,280
Otherwise, you're you're creating a view

1256
02:15:13,840 --> 02:15:18,240
Or a copy of a slice of another data frame. That's what another way pandas documentation

1257
02:15:19,040 --> 02:15:21,040
Looks at it

1258
02:15:21,440 --> 02:15:26,160
This is then this is good practice because you don't it might have unintended consequences down the road

1259
02:15:26,960 --> 02:15:29,040
Uh, we learned how to look at the columns

1260
02:15:30,080 --> 02:15:31,600
Of a data frame

1261
02:15:31,600 --> 02:15:33,600
We learned how to drop columns

1262
02:15:39,840 --> 02:15:44,160
We learned how to find the unique values of a specific series or a column

1263
02:15:45,120 --> 02:15:47,600
It's a pen pandas data frame column

1264
02:15:48,240 --> 02:15:54,320
Uh each of these a column is also a series a pandas series, which is another object

1265
02:15:55,280 --> 02:15:57,360
We learned how to find the unique values of that

1266
02:15:58,960 --> 02:16:00,400
We

1267
02:16:00,400 --> 02:16:04,240
Learned how to find the count of those unique values

1268
02:16:06,160 --> 02:16:11,520
And then we learned that that does not help us a lot when we're working with weighted data, which is most of

1269
02:16:12,720 --> 02:16:16,800
The the way you're going to be dealing with census data for privacy reasons

1270
02:16:18,400 --> 02:16:24,000
Um, we created another mask in this this time we use two conditions. So now

1271
02:16:25,120 --> 02:16:26,400
uh

1272
02:16:26,400 --> 02:16:28,400
This is going to return a true

1273
02:16:29,280 --> 02:16:31,040
or false

1274
02:16:31,040 --> 02:16:34,000
a series of booleans truths or falses

1275
02:16:35,120 --> 02:16:36,720
and

1276
02:16:36,720 --> 02:16:40,480
It's going to compare it to another series of truths and falses

1277
02:16:42,080 --> 02:16:44,080
Another boolean series and then

1278
02:16:44,320 --> 02:16:50,560
We're gonna from those we're gonna compare them to each other and if if either or oh no

1279
02:16:52,320 --> 02:16:54,320
I lost everything

1280
02:16:54,720 --> 02:16:57,600
It's fine. We have the solutions. Uh if we if

1281
02:16:59,040 --> 02:17:01,520
Once we have that two two condition

1282
02:17:02,400 --> 02:17:04,000
those two conditions

1283
02:17:04,000 --> 02:17:06,000
we will get um

1284
02:17:06,000 --> 02:17:08,000
one series of

1285
02:17:08,640 --> 02:17:16,880
Booliants that satisfy either or I mean they satisfy or one of those conditions

1286
02:17:17,600 --> 02:17:22,800
Uh, just as a note. I added here that because we know gq is categorical

1287
02:17:24,000 --> 02:17:26,960
Because we looked at it over here in our info

1288
02:17:28,080 --> 02:17:30,080
gq

1289
02:17:30,400 --> 02:17:32,720
Is of type category

1290
02:17:33,840 --> 02:17:34,880
uh

1291
02:17:34,880 --> 02:17:36,960
and because we saw also in

1292
02:17:38,960 --> 02:17:40,960
Here that it is

1293
02:17:42,080 --> 02:17:44,080
An ordered categorical

1294
02:17:44,720 --> 02:17:49,840
Which means some of these even though their strengths are greater than another value

1295
02:17:50,720 --> 02:17:52,720
we could uh

1296
02:17:53,200 --> 02:17:56,160
Do this we can just look at gq

1297
02:17:56,800 --> 02:17:58,800
and grab me all the

1298
02:17:58,800 --> 02:18:03,600
the and just tell me if each of the values in gq is less than or equal to

1299
02:18:04,480 --> 02:18:06,880
additional households under the 1990 definition

1300
02:18:08,080 --> 02:18:15,280
This doesn't work with strings because you cannot compare strings like this, but because it's a category and we know cat this category comes

1301
02:18:16,240 --> 02:18:18,080
is greater than

1302
02:18:18,080 --> 02:18:20,080
the 1970 definition

1303
02:18:21,680 --> 02:18:25,440
That's what we can do that type of analysis that that type of comparison there

1304
02:18:26,960 --> 02:18:33,280
We learned about the uh, we created a subset and from there we got a working data set. So now we have

1305
02:18:34,080 --> 02:18:35,600
a

1306
02:18:35,600 --> 02:18:37,600
state underscore data dash

1307
02:18:38,000 --> 02:18:40,240
Today whatever today's date is that dta

1308
02:18:40,960 --> 02:18:42,960
in our interim data path

1309
02:18:44,320 --> 02:18:47,040
Um, i'm not going to go over this extra credit part

1310
02:18:48,160 --> 02:18:50,160
But what I wanted to show you here is that

1311
02:18:51,600 --> 02:18:54,800
Because age is of categorical variable

1312
02:18:55,360 --> 02:18:58,880
It's a categorical variable which we saw over here again from info

1313
02:18:59,520 --> 02:19:01,520
With the age is category

1314
02:19:01,840 --> 02:19:03,360
even though

1315
02:19:03,360 --> 02:19:06,260
When you look at age, it looks like their numbers

1316
02:19:08,960 --> 02:19:11,840
Oh, this is not going to work because I didn't run it

1317
02:19:14,480 --> 02:19:16,820
Even though let me see if this works

1318
02:19:25,440 --> 02:19:29,140
Even though you see here age 79 79 28

1319
02:19:30,800 --> 02:19:37,120
78 and 52 those look like numbers, but those are actually the labels of

1320
02:19:38,080 --> 02:19:40,080
the age categories

1321
02:19:40,320 --> 02:19:42,160
and

1322
02:19:42,160 --> 02:19:44,160
You can you can run into some

1323
02:19:45,360 --> 02:19:48,480
Issues if you try to compare them as numbers

1324
02:19:49,520 --> 02:19:51,680
So what i'm trying to do here, so this could be

1325
02:19:52,640 --> 02:19:55,680
I just want to give me the households that have children of over five

1326
02:19:56,880 --> 02:19:58,480
I run this

1327
02:19:58,480 --> 02:20:01,120
It's going to tell me can I compare categorical?

1328
02:20:02,080 --> 02:20:06,160
With a scalar which is not a category because this five is not a category

1329
02:20:06,160 --> 02:20:07,440
category

1330
02:20:07,440 --> 02:20:12,480
category the category for five years old is the label five

1331
02:20:13,840 --> 02:20:15,440
This is going to work

1332
02:20:15,440 --> 02:20:19,760
and this is because age again, it's it's a category and it would try to

1333
02:20:21,600 --> 02:20:23,600
Um, let me show you something

1334
02:20:24,720 --> 02:20:30,320
Age uh in pandas. There's this other thing that would it's not super common

1335
02:20:30,720 --> 02:20:32,720
Uh

1336
02:20:33,200 --> 02:20:38,000
In in general, but you can just try to read a

1337
02:20:39,280 --> 02:20:41,600
Column as type something else

1338
02:20:42,640 --> 02:20:45,280
For example, this fib codes over here

1339
02:20:47,120 --> 02:20:52,560
There we saw that those are integers in our data frame, but they actually

1340
02:20:55,200 --> 02:20:57,840
Mean something else in geographic

1341
02:20:58,560 --> 02:21:05,860
Analysis and the county fib codes have to have three digits. So this zero actually should be 000

1342
02:21:06,960 --> 02:21:10,080
Or this 49 should be 049 because that means something

1343
02:21:10,640 --> 02:21:12,400
and and geo

1344
02:21:12,400 --> 02:21:13,840
gis

1345
02:21:13,840 --> 02:21:15,440
analysis

1346
02:21:15,440 --> 02:21:18,880
So you could uh, we could just grab this county fib

1347
02:21:19,840 --> 02:21:26,080
Here and then as type string and then just change that whole thing string, but I just want to show you what happens when you try

1348
02:21:27,040 --> 02:21:32,080
To read this as int the age which we would think I was like, oh if the label is five

1349
02:21:32,400 --> 02:21:34,480
Just make it an actual five, right?

1350
02:21:35,760 --> 02:21:38,720
It's not gonna work and the reason why it doesn't work

1351
02:21:40,480 --> 02:21:46,000
Is because this you have one value in in your series

1352
02:21:47,840 --> 02:21:50,880
For kids less than one year old and instead of

1353
02:21:51,680 --> 02:21:56,240
Using the the number zero to to denote that it's less than one

1354
02:21:57,040 --> 02:22:01,920
They use the label less than one year old and they don't and you don't know how to change that

1355
02:22:02,480 --> 02:22:06,080
Look, uh pandas doesn't know how to um doesn't want to assume what you want to change that to

1356
02:22:07,520 --> 02:22:10,720
What you would like to change that to um, there's other ways

1357
02:22:11,520 --> 02:22:13,520
uh, I wonder

1358
02:22:14,720 --> 02:22:17,680
Errors it raises errors you could just say errors

1359
02:22:17,680 --> 02:22:19,680
Errors

1360
02:22:20,400 --> 02:22:25,520
Ignore and then it works but then all the errors are just going to be

1361
02:22:28,000 --> 02:22:31,760
Are still going to be there or you could try to coerce

1362
02:22:33,520 --> 02:22:37,920
Is it course oh, it's just raising Chris and another uh another pandas

1363
02:22:39,440 --> 02:22:41,520
Further in the in the second notebook we might look at that

1364
02:22:42,640 --> 02:22:46,720
But there's other ways right now because we just know that it's a category we can just use

1365
02:22:47,520 --> 02:22:51,360
Quotations around the number and we'll that will denote that it's the number five

1366
02:22:52,880 --> 02:22:54,800
Um

1367
02:22:54,800 --> 02:22:56,800
This extra credit part was just to

1368
02:22:57,200 --> 02:23:02,320
To trim our data set even more if you wanted to look at households with school-aged children

1369
02:23:02,960 --> 02:23:05,520
Which could be something of interest to to people?

1370
02:23:07,120 --> 02:23:09,120
Making policy decisions

1371
02:23:09,520 --> 02:23:11,520
Maybe you find that

1372
02:23:11,920 --> 02:23:13,520
kids in

1373
02:23:14,160 --> 02:23:19,920
Latino households are less likely to have access to broadband internet and you want to do something or your politicians want to so

1374
02:23:19,920 --> 02:23:22,240
Well, you find out that you go talk to your politician and then

1375
02:23:22,960 --> 02:23:24,960
Tell them to do something

1376
02:23:26,640 --> 02:23:29,760
So now we're going to move on to zero one the next notebook

1377
02:23:31,600 --> 02:23:35,760
By this time you should have on your on your data

1378
02:23:37,920 --> 02:23:40,720
Folder on interim a data set

1379
02:23:41,840 --> 02:23:43,200
state data

1380
02:23:43,280 --> 02:23:44,960
underscore

1381
02:23:44,960 --> 02:23:46,960
and today's date

1382
02:23:47,280 --> 02:23:50,640
I have this other one because I ran the solutions notebook

1383
02:23:51,600 --> 02:23:54,240
All the way through so working data is actually

1384
02:23:56,000 --> 02:23:58,800
Saved there if you run the solutions

1385
02:23:59,920 --> 02:24:04,000
I think it's only if you run solutions and that is actually the one with

1386
02:24:04,960 --> 02:24:06,880
Just school age

1387
02:24:06,880 --> 02:24:08,880
Households with school-aged children

1388
02:24:09,520 --> 02:24:14,320
Okay, and then we have so we're going to move on to zero one digital divide

1389
02:24:15,040 --> 02:24:20,640
The analysis and I have set up pretty much the same thing. I have the project name

1390
02:24:21,440 --> 02:24:24,640
What i'm doing now instead of data prep i'm doing data analysis

1391
02:24:25,040 --> 02:24:29,120
Which is just a reminder for me. I have this to remind me that this is what i'm trying to recreate

1392
02:24:29,760 --> 02:24:31,760
I still have my research questions

1393
02:24:32,000 --> 02:24:36,400
I updated my goal because the goal of this notebook is now that we have the working data set

1394
02:24:37,200 --> 02:24:39,280
And I have a link to the to the notebook

1395
02:24:40,880 --> 02:24:42,880
So I can just look at that if I want

1396
02:24:45,200 --> 02:24:48,880
I want to answer the research questions now that I have the data that I want to look at

1397
02:24:49,760 --> 02:24:51,760
From the from the master file

1398
02:24:51,920 --> 02:24:57,920
Now i'm going to run the analysis and a lot of the time you're going to spend you you're going to spend most of your time

1399
02:24:58,480 --> 02:25:00,160
prepping the data

1400
02:25:00,160 --> 02:25:02,160
and then the analysis is going to be

1401
02:25:02,320 --> 02:25:04,320
slightly faster, but the

1402
02:25:04,720 --> 02:25:09,120
Data preparation is not only what takes the longest it's usually the most important part, too

1403
02:25:10,000 --> 02:25:14,080
because when if you want to publish this if you want to write something up

1404
02:25:14,960 --> 02:25:18,960
You need to be able to defend what you did and you you need to be able to say

1405
02:25:19,760 --> 02:25:23,680
actually, I did not include households under the 2000 definition because

1406
02:25:24,480 --> 02:25:30,400
x y and z or I am only looking at this definition of households or i'm only looking at school-aged children

1407
02:25:31,120 --> 02:25:34,400
And i'm and i'm defining school age as five to eighteen

1408
02:25:35,200 --> 02:25:38,640
or fourteen to eighteen if I want to look at high school or

1409
02:25:39,440 --> 02:25:46,240
Not eighteen because then turn then technically they're technically adults. So only at seventeen, but you need to be able to defend that and you need to

1410
02:25:48,000 --> 02:25:53,600
Um, so that's one that's why the data preparation part is essential in every project

1411
02:25:54,240 --> 02:25:56,240
Um, I have left this

1412
02:25:57,040 --> 02:25:59,040
Up to you to write the context

1413
02:25:59,760 --> 02:26:01,600
um

1414
02:26:01,600 --> 02:26:05,280
You you I suggest that you write the context of this notebook

1415
02:26:05,840 --> 02:26:07,840
uh, you can say something like

1416
02:26:08,480 --> 02:26:10,480
run data prep notebook

1417
02:26:10,560 --> 02:26:13,280
Uh, now I have a data set with only households in

1418
02:26:13,920 --> 02:26:15,920
these two definitions

1419
02:26:16,480 --> 02:26:19,200
It's for this state is using this year's data

1420
02:26:19,840 --> 02:26:23,440
Blah blah blah. It's just something to remind yourself of what you have done

1421
02:26:24,640 --> 02:26:26,880
in case you come back

1422
02:26:27,040 --> 02:26:29,920
In six months and you know like hey

1423
02:26:30,880 --> 02:26:33,680
What is untitled 34 supposed to do?

1424
02:26:34,800 --> 02:26:37,920
Oh also, please name your notebooks every time you start

1425
02:26:38,400 --> 02:26:40,400
You start a new one

1426
02:26:41,440 --> 02:26:43,440
So i'm gonna give you a minute

1427
02:26:56,880 --> 02:26:58,880
So

1428
02:27:15,040 --> 02:27:21,920
Yeah, so when I left it gave me this i'm not sure how to get back on the page from here, oh if you

1429
02:27:22,480 --> 02:27:25,200
Uh, yeah this session left so you need to go back

1430
02:27:26,960 --> 02:27:28,960
And then it's gonna

1431
02:27:32,080 --> 02:27:34,800
Oh, yeah, it's not so you're gonna have to start a new session

1432
02:27:35,680 --> 02:27:37,680
Um, I would just run the solutions

1433
02:27:38,240 --> 02:27:40,820
Okay notebook all the way to saving the file

1434
02:27:56,880 --> 02:27:58,880
So

1435
02:28:26,880 --> 02:28:28,880
Okay

1436
02:28:42,800 --> 02:28:45,300
And once you're done with the context

1437
02:28:47,040 --> 02:28:48,880
You can then

1438
02:28:48,880 --> 02:28:52,080
Start working on setting up your environment. This is pretty much the same thing

1439
02:28:52,720 --> 02:28:55,460
Within the first notebook you import the libraries

1440
02:28:57,440 --> 02:29:00,960
You put together this today tag that you might use later

1441
02:29:02,480 --> 02:29:06,160
And you set these path files up

1442
02:29:07,520 --> 02:29:09,520
the file paths

1443
02:29:26,880 --> 02:29:28,880
So

1444
02:29:56,880 --> 02:29:58,880
So

1445
02:30:26,880 --> 02:30:28,960
So

1446
02:30:56,880 --> 02:30:58,960
So

1447
02:31:27,680 --> 02:31:29,680
So

1448
02:31:29,680 --> 02:31:31,680
So

1449
02:31:32,640 --> 02:31:34,640
So

1450
02:31:56,400 --> 02:32:00,800
And I just want to take a quick detour

1451
02:32:01,840 --> 02:32:03,360
Um

1452
02:32:03,360 --> 02:32:05,360
If you

1453
02:32:05,920 --> 02:32:07,360
Think this is annoying

1454
02:32:07,360 --> 02:32:11,600
Um putting writing all your imports every single time, especially if they're the same

1455
02:32:12,160 --> 02:32:14,720
Setting up your data paths every single time

1456
02:32:16,640 --> 02:32:20,400
This is pretty much the same to me every time I do it so I'll have like

1457
02:32:20,960 --> 02:32:22,320
project the

1458
02:32:22,320 --> 02:32:26,800
Data analysis part maybe our cues goal context and these same

1459
02:32:27,760 --> 02:32:29,920
First like the first two three cells

1460
02:32:30,880 --> 02:32:33,200
There's this great tool called jupyter lab templates

1461
02:32:35,280 --> 02:32:40,880
If you're using jupyter lab, you can create a template and it will

1462
02:32:43,440 --> 02:32:45,680
You you have your template saved somewhere

1463
02:32:47,520 --> 02:32:51,600
And it will create a copy of that so you can get started from that and you can have

1464
02:32:52,000 --> 02:32:54,400
It's not going to run the code automatically, but if you have

1465
02:32:55,360 --> 02:32:59,840
Um, you see this part it has inserted inserted code already there

1466
02:33:02,080 --> 02:33:07,600
So if you're using the same if you're typing the same three or four cells every single time

1467
02:33:08,240 --> 02:33:10,560
You can just create a template save it somewhere

1468
02:33:11,360 --> 02:33:12,480
um

1469
02:33:12,480 --> 02:33:14,720
And then set up jupyter lab templates

1470
02:33:16,000 --> 02:33:18,000
Let's just pip install it

1471
02:33:18,080 --> 02:33:20,080
installing the extension

1472
02:33:20,160 --> 02:33:22,000
enable it

1473
02:33:22,000 --> 02:33:23,840
and then in your

1474
02:33:23,840 --> 02:33:25,840
dot jupyter

1475
02:33:26,620 --> 02:33:28,880
Directory in the config file

1476
02:33:30,560 --> 02:33:35,440
You just add the path to your template and instead of just sample here, it's going to show you this

1477
02:33:36,080 --> 02:33:37,520
And you can then

1478
02:33:37,520 --> 02:33:39,600
Have something with all your your path file

1479
02:33:40,240 --> 02:33:42,240
your file paths

1480
02:33:42,560 --> 02:33:44,560
Set up every single time. This is what I do

1481
02:33:44,960 --> 02:33:46,960
in my work computer just

1482
02:33:47,360 --> 02:33:53,200
Create that and then I have the first cell says something like project name and then just in big

1483
02:33:53,440 --> 02:33:55,440
I mean, it's already in h. I mean just one

1484
02:33:56,240 --> 02:33:58,720
Pound sign and says I have the name of the project

1485
02:33:59,360 --> 02:34:01,360
And just add the date here

1486
02:34:01,440 --> 02:34:03,040
and then I have

1487
02:34:03,040 --> 02:34:05,040
These already set up

1488
02:34:05,200 --> 02:34:07,200
So I can just run them and not type

1489
02:34:08,240 --> 02:34:10,240
every single time

1490
02:34:11,040 --> 02:34:13,040
These 10 lines of code

1491
02:34:13,600 --> 02:34:15,600
Which is not even a lot, but it's kind of annoying

1492
02:34:17,600 --> 02:34:20,240
So jupyter lab templates, it's a great tool

1493
02:34:23,200 --> 02:34:30,880
Okay, so we have we loaded up the libraries we

1494
02:34:32,720 --> 02:34:34,960
Declared the file paths

1495
02:34:36,960 --> 02:34:43,600
Here I just put three interim data paths so I want to see what what's in my interim data folder

1496
02:34:44,880 --> 02:34:46,480
This might be different

1497
02:34:46,480 --> 02:34:49,680
in your hand I have the working data file

1498
02:34:50,640 --> 02:34:56,960
Because I run the solutions and all the way through at the very last one when we're looking at school age children is just

1499
02:34:57,920 --> 02:34:59,920
I saved as working data instead of state data

1500
02:35:01,520 --> 02:35:03,520
I loaded up the the data

1501
02:35:04,240 --> 02:35:08,480
And then this one I haven't ran. It looks like I ran it because I copied it

1502
02:35:09,520 --> 02:35:16,160
I ran it first and then I deleted the code. What do you think goes here? What do you think? What gives you this type of output?

1503
02:35:16,160 --> 02:35:22,560
Yeah, this is the shape of your of your data frame

1504
02:35:25,360 --> 02:35:28,560
And this and this changed because the first the first time I ran it was with

1505
02:35:29,600 --> 02:35:33,360
School age children data set which is a subset of this

1506
02:35:34,480 --> 02:35:36,480
um

1507
02:35:36,640 --> 02:35:39,520
This one what do you think is going to give you this output?

1508
02:35:41,440 --> 02:35:44,240
What goes here head yes

1509
02:35:46,240 --> 02:35:49,760
And again, it might change a little bit if you you if you chose a different state

1510
02:35:51,440 --> 02:35:56,560
If you didn't do the extra credit part where it's not extra credit, but the bonus part with

1511
02:35:57,040 --> 02:36:01,280
School age children only so this might look a little different, but the mechanics

1512
02:36:01,920 --> 02:36:03,920
Should be the same

1513
02:36:03,920 --> 02:36:05,920
Um, what does this look like?

1514
02:36:12,240 --> 02:36:14,240
Do you remember

1515
02:36:15,200 --> 02:36:21,120
Oh, yeah, sorry, yes it is info. Thank you. I'm gonna close this one too

1516
02:36:23,280 --> 02:36:25,280
Can you see plus

1517
02:36:32,080 --> 02:36:34,660
Yeah, this is info info

1518
02:36:34,660 --> 02:36:36,660
Info

1519
02:36:40,580 --> 02:36:44,340
It's really nice about info too is it will give you the memory usage

1520
02:36:45,380 --> 02:36:46,340
um

1521
02:36:46,340 --> 02:36:51,620
For for your data frame, maybe you're working with a lot of data. You don't know what as long as your computer

1522
02:36:52,820 --> 02:36:57,460
Info is going to tell you that you're using like 500 megabytes or something

1523
02:37:00,100 --> 02:37:01,780
So

1524
02:37:01,780 --> 02:37:05,460
We have looked at the descriptors we loaded up the data we kind of know how it looks like

1525
02:37:06,260 --> 02:37:07,380
um

1526
02:37:07,380 --> 02:37:10,820
Now for the real enough for the analytical part of this

1527
02:37:11,700 --> 02:37:13,460
um

1528
02:37:13,460 --> 02:37:20,980
We want to look at households that have access to broadband internet to broadband to high-speed internet

1529
02:37:22,500 --> 02:37:26,980
But our data right now is one observation per person

1530
02:37:28,020 --> 02:37:30,580
um, and like we mentioned earlier you can

1531
02:37:31,540 --> 02:37:37,780
You have the serial number for each of the households so you can track how many households you have by just looking at

1532
02:37:38,500 --> 02:37:42,820
the unique number of of cereals of serial values

1533
02:37:43,700 --> 02:37:47,460
But we still need to change our data a little bit because we don't

1534
02:37:48,580 --> 02:37:50,020
need

1535
02:37:50,020 --> 02:37:51,780
person level

1536
02:37:51,780 --> 02:37:54,340
data we need a household level and

1537
02:37:55,300 --> 02:37:58,420
This this information comes from from ipums

1538
02:37:58,820 --> 02:38:05,940
It is generally a good idea to use household weight when conducting a household level analysis of any ipum sample

1539
02:38:06,660 --> 02:38:13,940
To use household weight is optional when analyzing one of the flat or unweighted ipum samples, which we're not using we're using a weighted sample

1540
02:38:15,460 --> 02:38:16,740
um

1541
02:38:16,740 --> 02:38:18,020
uses

1542
02:38:18,020 --> 02:38:22,340
Users should also be sure to select one person from the household

1543
02:38:22,740 --> 02:38:28,740
From the household person number equals one to represent the entire household. So if we look at our data over here

1544
02:38:30,820 --> 02:38:32,820
Um

1545
02:38:32,900 --> 02:38:35,540
Do you remember how these two are in the same household?

1546
02:38:37,140 --> 02:38:39,140
You see that this

1547
02:38:40,260 --> 02:38:45,300
Household weight is the same for both of them because you could just grab one of them

1548
02:38:46,020 --> 02:38:48,020
Uh, it's better

1549
02:38:48,420 --> 02:38:55,700
It's good practice I mean you should from ipum's suggestion you should grab the first

1550
02:38:57,060 --> 02:39:03,060
The pernum equals one which is usually the first it is the first person that shows up in the household

1551
02:39:03,780 --> 02:39:05,300
and it is

1552
02:39:05,300 --> 02:39:07,140
the head or

1553
02:39:07,140 --> 02:39:09,140
the head of the household

1554
02:39:11,860 --> 02:39:14,980
So that's as simple as it gets all we got to do is

1555
02:39:15,780 --> 02:39:22,180
Is drop every observation in our data every person

1556
02:39:23,140 --> 02:39:25,460
Um that is not the household head

1557
02:39:27,220 --> 02:39:30,920
So I set up the code here and then i'm going to give you a minute to run this

1558
02:39:44,980 --> 02:39:46,980
So

1559
02:40:15,460 --> 02:40:17,460
So

1560
02:40:42,420 --> 02:40:44,840
Yes represents more than one person

1561
02:40:44,980 --> 02:40:46,660
And that's a confusing part.

1562
02:40:46,660 --> 02:40:48,060
Well, that's a confusing part.

1563
02:40:48,060 --> 02:40:49,540
Yes, sorry.

1564
02:40:49,540 --> 02:40:53,780
Yeah, the one I was thinking of is, OK, so I see we have three

1565
02:40:53,780 --> 02:40:55,140
records all with the same serial.

1566
02:40:55,140 --> 02:40:58,580
So this is our sample household.

1567
02:40:58,580 --> 02:40:59,100
Yes.

1568
02:40:59,100 --> 02:41:03,060
So the household setup of these people.

1569
02:41:03,060 --> 02:41:07,700
And then the household weight is how many of those?

1570
02:41:07,700 --> 02:41:08,940
Yeah.

1571
02:41:08,940 --> 02:41:10,540
And the household weight will be the same

1572
02:41:10,540 --> 02:41:11,900
for each of those observations.

1573
02:41:11,940 --> 02:41:16,580
That's why you don't want to run your analysis,

1574
02:41:16,580 --> 02:41:19,820
like add up all the household weights on everything,

1575
02:41:19,820 --> 02:41:23,500
because you're counting one household multiple times,

1576
02:41:23,500 --> 02:41:24,380
if that makes sense.

1577
02:41:24,380 --> 02:41:26,980
If you did want to do that, if you

1578
02:41:26,980 --> 02:41:29,660
didn't want to calculate the total number of households,

1579
02:41:29,660 --> 02:41:32,340
you select all the records where per number is one,

1580
02:41:32,340 --> 02:41:33,940
take the sum of the household.

1581
02:41:33,940 --> 02:41:35,420
Yes, exactly.

1582
02:41:35,420 --> 02:41:37,060
Now it's going to be that.

1583
02:41:37,060 --> 02:41:41,100
So I'm grabbing the first two of my data set,

1584
02:41:41,100 --> 02:41:43,340
which is for Ohio.

1585
02:41:43,340 --> 02:41:47,820
And this is one household.

1586
02:41:47,820 --> 02:41:49,260
Each person has a different weight,

1587
02:41:49,260 --> 02:41:54,260
because they're representing a different amount of people.

1588
02:41:54,260 --> 02:41:57,220
But the household weight is the same.

1589
02:41:57,220 --> 02:42:02,340
And so if you were to add up the household weights

1590
02:42:02,340 --> 02:42:06,100
of all your whole data frame right now,

1591
02:42:06,100 --> 02:42:08,340
of your whole data set, you would

1592
02:42:08,340 --> 02:42:12,220
be counting multiple times the same household for however

1593
02:42:12,220 --> 02:42:14,140
many people there are in that one.

1594
02:42:14,140 --> 02:42:17,980
And if every single household was just two people,

1595
02:42:17,980 --> 02:42:20,140
you could easily just, OK, we'll add it up,

1596
02:42:20,140 --> 02:42:21,980
and then just divide it by half, because I'm

1597
02:42:21,980 --> 02:42:23,620
counting every household twice.

1598
02:42:23,620 --> 02:42:25,540
But that's not the case, because some households

1599
02:42:25,540 --> 02:42:28,700
have more than one person, I mean, more than two people.

1600
02:42:28,700 --> 02:42:32,060
So this is why we do it this way.

1601
02:42:32,100 --> 02:42:42,100
We grab all the observations that

1602
02:42:42,100 --> 02:42:50,500
were per num, the person's number per num, is equal to 1.

1603
02:42:50,500 --> 02:42:53,500
And we're going to keep those.

1604
02:42:53,500 --> 02:42:55,100
And then I'm just running this.

1605
02:42:55,100 --> 02:42:56,460
This is not saving to anything.

1606
02:42:56,460 --> 02:42:58,980
I'm just going to look at the shape of that.

1607
02:42:58,980 --> 02:43:06,540
So now we only have 48,564 observations in my data set

1608
02:43:06,540 --> 02:43:07,060
in Ohio.

1609
02:43:07,060 --> 02:43:10,180
It might be different if you're doing a different state.

1610
02:43:10,180 --> 02:43:19,140
But this is 48,000, 49,000 out of 112,000.

1611
02:43:19,140 --> 02:43:22,220
So we dropped quite a few observations, which makes sense.

1612
02:43:22,220 --> 02:43:24,260
If most households have more than one person,

1613
02:43:24,260 --> 02:43:26,540
you're going to be counting by more than half, usually.

1614
02:43:29,980 --> 02:43:34,660
So we're going to save this subset of data

1615
02:43:34,660 --> 02:43:37,420
to an appropriately named variable.

1616
02:43:37,420 --> 02:43:41,300
It's good practice to name your variables something

1617
02:43:41,300 --> 02:43:45,580
that is very descriptive.

1618
02:43:45,580 --> 02:43:51,540
Data, mask per num.

1619
02:43:51,540 --> 02:43:55,940
And you could approach this in at least two different ways.

1620
02:43:56,020 --> 02:44:02,660
You can just keep all the instances

1621
02:44:02,660 --> 02:44:04,660
where per num equals 1.

1622
02:44:04,660 --> 02:44:07,700
So you're keeping the household heads.

1623
02:44:07,700 --> 02:44:14,900
Or you could look at those that don't match the household head

1624
02:44:14,900 --> 02:44:15,860
and drop those.

1625
02:44:15,860 --> 02:44:17,580
But that's a little bit more complicated.

1626
02:44:17,580 --> 02:44:20,260
And it's just easier to think about it like,

1627
02:44:20,260 --> 02:44:22,820
look for the ones that I want and grab those,

1628
02:44:22,820 --> 02:44:25,780
and then just drop everything else.

1629
02:44:25,780 --> 02:44:27,340
But there's definitely another way.

1630
02:44:27,340 --> 02:44:29,860
You could just do the opposite.

1631
02:44:33,620 --> 02:44:34,140
You save it.

1632
02:44:34,140 --> 02:44:38,140
And I did not add it here in the template of the notebook.

1633
02:44:38,140 --> 02:44:43,620
But remember to use copy, because we're

1634
02:44:43,620 --> 02:44:46,140
going to be using state households later on.

1635
02:44:46,140 --> 02:44:47,720
And we're going to be modifying it.

1636
02:44:47,720 --> 02:44:50,100
And you're going to keep getting those warnings.

1637
02:44:50,100 --> 02:44:55,060
The analysis and the code might run just fine.

1638
02:44:55,060 --> 02:45:00,540
Sometimes it might have some unintended consequences.

1639
02:45:00,540 --> 02:45:03,380
But I think warnings are annoying.

1640
02:45:03,380 --> 02:45:07,020
So if I could just not get any warnings, that's great.

1641
02:45:07,020 --> 02:45:10,340
So I keep doing this.

1642
02:45:10,340 --> 02:45:12,100
All right, so step three.

1643
02:45:12,100 --> 02:45:14,860
You'll notice that I have a lot less code already written here

1644
02:45:14,860 --> 02:45:16,740
for you.

1645
02:45:16,740 --> 02:45:20,500
I think the better way to learn is you typing out the code.

1646
02:45:20,500 --> 02:45:22,580
But I wanted to set up as much as I could

1647
02:45:22,580 --> 02:45:24,940
without giving you too much.

1648
02:45:24,980 --> 02:45:30,900
But now what we're doing here is just running value counts

1649
02:45:30,900 --> 02:45:41,540
on, in this case, CNF, CNF, HHH, right?

1650
02:45:41,540 --> 02:45:43,900
Yeah, series.

1651
02:45:43,900 --> 02:45:45,740
And say high speed.

1652
02:45:48,700 --> 02:45:54,820
So grab from your newly made data frame, state households,

1653
02:45:54,820 --> 02:46:01,940
grab this variable, CNF, and run value counts on them.

1654
02:46:01,940 --> 02:46:05,020
So let's find out what we can about it.

1655
02:46:18,700 --> 02:46:21,220
State households.

1656
02:46:25,020 --> 02:46:27,020
CNF.

1657
02:46:32,780 --> 02:46:34,780
And there you go.

1658
02:46:54,820 --> 02:47:13,900
Another way of grabbing a data frames column

1659
02:47:13,900 --> 02:47:17,900
is by using dot notation.

1660
02:47:17,900 --> 02:47:19,660
It's not recommended.

1661
02:47:19,700 --> 02:47:24,500
But you could just write like dot psi high speed,

1662
02:47:24,500 --> 02:47:28,500
dot value counts, and it will run just fine.

1663
02:47:28,500 --> 02:47:31,020
But when you're dealing with administrative data,

1664
02:47:31,020 --> 02:47:34,300
you just don't know what they're going to name their columns.

1665
02:47:34,300 --> 02:47:36,220
And it might start with a number,

1666
02:47:36,220 --> 02:47:37,780
or it might have spaces in between,

1667
02:47:37,780 --> 02:47:40,660
and then it's just not going to work.

1668
02:47:40,660 --> 02:47:49,540
So this is why we're using this way.

1669
02:47:49,540 --> 02:47:51,980
Psi high speed.

1670
02:47:51,980 --> 02:47:53,460
And it runs just the same.

1671
02:48:06,500 --> 02:48:10,180
I'm going to keep going, but if you need me to slow down,

1672
02:48:10,180 --> 02:48:11,020
just let me know.

1673
02:48:13,700 --> 02:48:16,340
Value counts has a parameter normalized,

1674
02:48:16,340 --> 02:48:19,420
set to false by default. This is what

1675
02:48:19,420 --> 02:48:22,180
we did last time, which is question mark at the end,

1676
02:48:22,180 --> 02:48:25,340
and I will show you the doc string.

1677
02:48:25,340 --> 02:48:29,220
This is very useful if you're dealing

1678
02:48:29,220 --> 02:48:33,300
with what IPM's called over here,

1679
02:48:33,300 --> 02:48:36,740
flat, quote unquote, or unweighted data.

1680
02:48:36,740 --> 02:48:44,460
If each of your observations represents one person,

1681
02:48:44,460 --> 02:48:48,260
in this case, or one household, then you

1682
02:48:48,260 --> 02:48:59,860
could run this code, state households, tab, sine.

1683
02:49:02,180 --> 02:49:04,020
Value counts, just like we did earlier,

1684
02:49:04,020 --> 02:49:08,980
but then you set normalized to true.

1685
02:49:08,980 --> 02:49:12,780
And you're going to get the share of each of those values.

1686
02:49:12,780 --> 02:49:16,340
Which, if this was an unweighted file,

1687
02:49:16,340 --> 02:49:19,340
again, if each observation was a household,

1688
02:49:19,340 --> 02:49:22,540
represented only one household, then we won't have our answer.

1689
02:49:22,540 --> 02:49:25,940
We'll just be like, oh, well, yeah.

1690
02:49:25,940 --> 02:49:29,260
Out of, sine represents people that have access.

1691
02:49:29,260 --> 02:49:30,380
I'm going to leave this here.

1692
02:49:30,380 --> 02:49:31,260
I'm not going to go.

1693
02:49:31,260 --> 02:49:35,540
But it's people having access to internet services, right?

1694
02:49:35,540 --> 02:49:41,180
So we can see if this was not a weighted sample,

1695
02:49:41,180 --> 02:49:45,860
if this was an unweighted data set,

1696
02:49:45,860 --> 02:49:47,700
you could just get your answer.

1697
02:49:47,700 --> 02:49:49,620
This is not the variable we're looking at.

1698
02:49:49,620 --> 02:49:53,620
But you could just run value counts,

1699
02:49:53,620 --> 02:49:55,740
normalized equals true.

1700
02:49:55,740 --> 02:50:01,100
Now we know that 83% of, in this case,

1701
02:50:01,100 --> 02:50:06,460
the observations in our sample, in our data set,

1702
02:50:06,460 --> 02:50:09,220
yes, have access to internet service with a subscription.

1703
02:50:09,220 --> 02:50:17,860
And we can do the same with state, households,

1704
02:50:17,860 --> 02:50:24,340
side head speed, value counts, normalized equals true.

1705
02:50:24,340 --> 02:50:26,180
And this is the variable we're interested in.

1706
02:50:26,180 --> 02:50:30,220
And you can see it up, just up in the notebook,

1707
02:50:30,220 --> 02:50:32,780
you see from IMPLM's their descriptions.

1708
02:50:35,260 --> 02:50:37,060
This is the variable we're interested in.

1709
02:50:37,060 --> 02:50:38,740
We want to see how many people have access

1710
02:50:38,740 --> 02:50:40,660
to high speed internet.

1711
02:50:40,660 --> 02:50:43,660
And that's what this variable is calling.

1712
02:50:43,660 --> 02:50:48,460
High speed internet, anything that falls under cable modem,

1713
02:50:48,460 --> 02:50:51,780
fiber optic, or DSL service.

1714
02:50:51,780 --> 02:50:55,540
And this is the power of IMPLM's.

1715
02:50:55,540 --> 02:50:59,900
In 2016, the census started asking,

1716
02:50:59,900 --> 02:51:02,460
do you have access to high speed internet, yes or no?

1717
02:51:02,460 --> 02:51:08,700
And by yes, I mean cable modem, fiber optic, or DSL service.

1718
02:51:08,700 --> 02:51:11,540
In 2015 and before, they would ask you,

1719
02:51:11,540 --> 02:51:12,580
do you have access to internet?

1720
02:51:12,580 --> 02:51:13,060
Yes.

1721
02:51:13,060 --> 02:51:16,180
Do you have access to it through DSL?

1722
02:51:16,180 --> 02:51:16,860
Yes, no.

1723
02:51:16,860 --> 02:51:17,660
Through fiber optic?

1724
02:51:17,660 --> 02:51:18,500
Yes, no.

1725
02:51:18,500 --> 02:51:19,300
Through cable modem?

1726
02:51:19,300 --> 02:51:19,780
Yes, no.

1727
02:51:19,780 --> 02:51:29,660
So if you look at the variable, IMPLM's high speed codes,

1728
02:51:29,660 --> 02:51:31,300
you can't see it because it's in full.

1729
02:51:34,100 --> 02:51:38,420
If you look at the description, this size.

1730
02:51:38,420 --> 02:51:39,060
Oh, look.

1731
02:51:39,060 --> 02:51:41,820
I looked it up already.

1732
02:51:41,820 --> 02:51:42,500
This is great.

1733
02:51:42,500 --> 02:51:44,140
The documentation from IMPLM's is great.

1734
02:51:44,140 --> 02:51:46,500
So if you're working with census data,

1735
02:51:46,500 --> 02:51:48,140
and especially if it's across years,

1736
02:51:48,140 --> 02:51:50,420
this is a great resource.

1737
02:51:50,420 --> 02:51:53,300
I'm going to put it in full screen.

1738
02:51:53,340 --> 02:52:00,460
So this will describe what the variable is actually reading.

1739
02:52:00,460 --> 02:52:03,020
And in the notebook, I have the description.

1740
02:52:03,020 --> 02:52:04,020
It's copied from here.

1741
02:52:06,660 --> 02:52:11,780
If you look at the codes, you're going to see this.

1742
02:52:11,780 --> 02:52:18,500
So starting 2016, you only get NA, GQ, yes,

1743
02:52:18,500 --> 02:52:22,700
through cable modem, fiber optic, or DSL, or no.

1744
02:52:22,700 --> 02:52:27,540
But before 2016, so 2015, 14, 13,

1745
02:52:27,540 --> 02:52:32,220
you could find how many people have access to the internet

1746
02:52:32,220 --> 02:52:36,660
through DSL service only, or fiber optic only.

1747
02:52:40,700 --> 02:52:42,420
It's useful if that's something that's

1748
02:52:42,420 --> 02:52:44,420
what you're interested in.

1749
02:52:44,420 --> 02:52:51,220
But if you wanted to look at trends, 2013 to 2017,

1750
02:52:51,220 --> 02:52:57,900
IMPLM harmonizes this into 0, 1, or 2

1751
02:52:57,900 --> 02:53:02,500
in the regular, in the not detailed version.

1752
02:53:02,500 --> 02:53:04,540
And then you could go deeper into it

1753
02:53:04,540 --> 02:53:10,740
and then find out these other categories for the answers.

1754
02:53:10,740 --> 02:53:13,580
If you want to learn more about this, the comparability tab

1755
02:53:13,580 --> 02:53:17,860
is great, because it will explain to you,

1756
02:53:17,860 --> 02:53:18,700
this is what happened.

1757
02:53:18,700 --> 02:53:23,460
In 2016, you could distinguish whether site high speed values

1758
02:53:23,460 --> 02:53:25,940
were allocated due to server responses for blah, blah,

1759
02:53:25,940 --> 02:53:26,740
blah, blah.

1760
02:53:26,740 --> 02:53:27,820
And it will explain to you.

1761
02:53:27,820 --> 02:53:31,500
And then site speed is only five years

1762
02:53:31,500 --> 02:53:33,140
that they've been doing that.

1763
02:53:33,140 --> 02:53:40,580
But when you look at, for example, birthplace data,

1764
02:53:40,580 --> 02:53:44,180
some countries just don't exist anymore.

1765
02:53:44,180 --> 02:53:46,980
So it will explain this here.

1766
02:53:46,980 --> 02:53:49,580
We used to call it this, and now it's this.

1767
02:53:49,580 --> 02:53:54,340
We used to call it USSR, or we used to call it India.

1768
02:53:54,340 --> 02:53:58,740
But now it's all these other countries.

1769
02:53:58,740 --> 02:54:05,020
So it's always good to look at the description, and the codes,

1770
02:54:05,020 --> 02:54:06,100
and the comparability.

1771
02:54:06,100 --> 02:54:09,820
And for example, the birthplace variable,

1772
02:54:09,820 --> 02:54:11,620
that would be one that you would want

1773
02:54:11,620 --> 02:54:15,340
to keep the D version of it, just

1774
02:54:15,340 --> 02:54:21,100
like we had raised his band and relate, at the end,

1775
02:54:21,100 --> 02:54:23,540
the detailed version.

1776
02:54:23,540 --> 02:54:28,140
If you're doing anything with immigration, like I have,

1777
02:54:28,140 --> 02:54:31,860
you want to keep the detailed version.

1778
02:54:31,860 --> 02:54:40,820
Because sometimes you'll have something like Pakistan labeled

1779
02:54:40,820 --> 02:54:41,940
as India, still.

1780
02:54:41,940 --> 02:54:46,340
Because at some point, it used to be just coded as India,

1781
02:54:46,340 --> 02:54:46,940
not anymore.

1782
02:54:46,940 --> 02:54:48,460
So you want the detailed version.

1783
02:54:58,740 --> 02:55:00,580
So again, here I'm just describing to you

1784
02:55:00,580 --> 02:55:03,340
that if this was unweighted data,

1785
02:55:03,340 --> 02:55:04,780
this would be the end of our analysis.

1786
02:55:04,780 --> 02:55:08,180
We find out, oh, look, 69% of people

1787
02:55:08,180 --> 02:55:10,940
have access to the internet.

1788
02:55:10,940 --> 02:55:12,900
But it's not, because it's weighted data.

1789
02:55:12,900 --> 02:55:17,460
So 69% of our observations have access to the internet.

1790
02:55:17,460 --> 02:55:23,060
So to find out how to get the actual number of households,

1791
02:55:23,060 --> 02:55:26,540
is what you were saying, is you find the households that

1792
02:55:26,540 --> 02:55:30,660
match this condition, that yes, they have access

1793
02:55:30,660 --> 02:55:33,740
to the internet data, to high speed internet.

1794
02:55:33,740 --> 02:55:38,100
And then you add up the household weight variable.

1795
02:55:38,100 --> 02:55:42,060
Because that's the actual count of households.

1796
02:55:42,060 --> 02:55:45,220
And to do that in Pandas, you do what

1797
02:55:45,220 --> 02:55:49,460
it's called grouping and aggregating data.

1798
02:55:49,460 --> 02:55:55,500
So to group data, you have to just use the group by method.

1799
02:55:55,500 --> 02:56:00,860
And to group by, you're going to pass it a column,

1800
02:56:00,860 --> 02:56:01,940
or a list of columns.

1801
02:56:02,460 --> 02:56:06,940
And Pandas is going to create this group by object

1802
02:56:06,940 --> 02:56:10,380
that you need to apply a function to.

1803
02:56:10,380 --> 02:56:11,540
And this is all from the docs.

1804
02:56:11,540 --> 02:56:13,100
And I'm speeding up a little bit,

1805
02:56:13,100 --> 02:56:18,100
because we don't have a lot of time left.

1806
02:56:18,100 --> 02:56:20,140
Or battery.

1807
02:56:20,140 --> 02:56:21,580
I'll fix that right now.

1808
02:56:21,580 --> 02:56:26,420
But if you wanted to group by, let's say,

1809
02:56:26,420 --> 02:56:28,780
psi high speed, because that's the variable we're interested

1810
02:56:28,780 --> 02:56:31,020
in, and you want to group by, let's say,

1811
02:56:31,020 --> 02:56:32,620
that's the variable we're interested in.

1812
02:56:32,620 --> 02:56:35,500
We want to group it by the values of this column.

1813
02:56:38,620 --> 02:56:45,740
And then you get this object, which is not very helpful.

1814
02:56:45,740 --> 02:56:47,540
What you got to do is apply a function.

1815
02:56:47,540 --> 02:56:50,140
So try this.

1816
02:56:50,140 --> 02:56:54,300
And instead of sum, try it with mean, max, min.

1817
02:56:54,300 --> 02:56:58,300
And this is standard deviation.

1818
02:56:58,300 --> 02:57:00,420
And you'll see what it looks like,

1819
02:57:00,420 --> 02:57:02,340
what a group by object looks like once you

1820
02:57:02,340 --> 02:57:04,580
have applied a function to it.

1821
02:57:04,580 --> 02:57:06,060
I'm going to charge my computer.

1822
02:57:15,060 --> 02:57:16,540
Or not.

1823
02:57:16,540 --> 02:57:18,020
Didn't I bring a charger?

1824
02:57:22,020 --> 02:57:27,020
Do you have the USB-C?

1825
02:57:27,020 --> 02:57:30,020
Not sure.

1826
02:57:30,020 --> 02:57:34,020
No, it's a Mac.

1827
02:57:34,020 --> 02:57:35,260
No.

1828
02:57:35,260 --> 02:57:36,420
Yeah, it's a Mac.

1829
02:57:36,420 --> 02:57:37,460
Sorry.

1830
02:57:37,460 --> 02:57:39,540
Thank you.

1831
02:57:39,540 --> 02:57:42,900
What is this?

1832
02:57:42,900 --> 02:57:44,700
Yes, could I have my Mac?

1833
02:57:44,700 --> 02:57:45,220
Thank you.

1834
02:57:57,020 --> 02:58:17,740
Oh, no.

1835
02:58:17,740 --> 02:58:18,740
My computer's too old.

1836
02:58:22,740 --> 02:58:23,620
Yeah, this is not.

1837
02:58:23,620 --> 02:58:25,140
Sorry.

1838
02:58:25,140 --> 02:58:25,620
Yeah.

1839
02:58:27,020 --> 02:58:29,420
I'll be forever.

1840
02:58:29,420 --> 02:58:30,420
Oh, shoot.

1841
02:58:30,420 --> 02:58:31,420
I'm going to do that.

1842
02:58:31,420 --> 02:58:32,420
I just want to see.

1843
02:58:32,420 --> 02:58:33,420
OK, we'll see.

1844
02:58:44,420 --> 02:58:45,420
OK.

1845
02:58:45,420 --> 02:58:52,420
I actually have another computer.

1846
02:58:52,420 --> 02:58:55,420
We'll just switch midway through.

1847
02:58:57,020 --> 02:58:57,520
OK.

1848
02:59:06,520 --> 02:59:08,020
How much do I have?

1849
02:59:08,020 --> 02:59:08,520
4%?

1850
02:59:08,520 --> 02:59:11,020
OK.

1851
02:59:11,020 --> 02:59:15,420
So this is, I'm going to show you some households.

1852
02:59:15,420 --> 02:59:17,820
Group by.

1853
02:59:17,820 --> 02:59:24,380
I'm doing county fit, but you'll see later on.

1854
02:59:24,380 --> 02:59:26,300
So this is what it looks like.

1855
02:59:26,300 --> 02:59:29,420
It's creating another data frame.

1856
02:59:29,420 --> 02:59:33,660
Its index is going to be the column that you passed,

1857
02:59:33,660 --> 02:59:35,500
or the list of columns.

1858
02:59:35,500 --> 02:59:41,420
You could pass county fit, side high speed.

1859
02:59:45,420 --> 02:59:48,700
And it's going to create what is called a multi-level index,

1860
02:59:48,700 --> 02:59:51,780
multi-index, I think.

1861
02:59:51,780 --> 02:59:54,420
You can tell that these are the indices,

1862
02:59:54,420 --> 03:00:01,220
because they're just lower than the rest of the columns.

1863
03:00:01,220 --> 03:00:06,540
And when you apply something like sum,

1864
03:00:06,540 --> 03:00:09,580
which is a mathematical function,

1865
03:00:09,580 --> 03:00:15,420
it's going to apply to only the quantitative columns.

1866
03:00:15,420 --> 03:00:18,740
Same with mean.

1867
03:00:19,740 --> 03:00:24,540
So some of these might not mean anything, like per num.

1868
03:00:24,540 --> 03:00:27,460
The average per num is one, because that's the only one

1869
03:00:27,460 --> 03:00:28,780
we kept, right?

1870
03:00:28,780 --> 03:00:31,180
The average person's weight is going to be this.

1871
03:00:31,180 --> 03:00:32,140
Doesn't mean anything.

1872
03:00:32,140 --> 03:00:35,060
We're not looking for the averages.

1873
03:00:35,060 --> 03:00:39,180
If you pass something like count, does that work?

1874
03:00:39,180 --> 03:00:39,780
OK.

1875
03:00:39,780 --> 03:00:42,660
Count, because it's not a mathematical function,

1876
03:00:42,660 --> 03:00:44,740
it can apply to every single row,

1877
03:00:44,740 --> 03:00:47,580
every single column in your data frame.

1878
03:00:47,700 --> 03:00:51,780
And all it's going to do is count the number of instances

1879
03:00:51,780 --> 03:00:55,100
any value is in it, I believe.

1880
03:00:55,100 --> 03:00:56,340
So yeah.

1881
03:00:56,340 --> 03:00:59,420
So you'll see this.

1882
03:00:59,420 --> 03:01:05,420
Everything is the same, because every row in your data frame

1883
03:01:05,420 --> 03:01:08,260
right now in your data set has a value,

1884
03:01:08,260 --> 03:01:09,860
because we got it from the census.

1885
03:01:09,860 --> 03:01:15,060
They won't give you missing value data.

1886
03:01:15,060 --> 03:01:15,580
OK.

1887
03:01:15,580 --> 03:01:18,540
Let me see if we can get it moved on to.

1888
03:01:18,540 --> 03:01:21,660
OK, so just like you would grab any column

1889
03:01:21,660 --> 03:01:25,940
from any other data frame, you can do the same here.

1890
03:01:25,940 --> 03:01:28,580
Just square brackets, pass me the column.

1891
03:01:28,580 --> 03:01:33,900
So if we want to group by sine high speed,

1892
03:01:33,900 --> 03:01:35,820
and you want to grab the household weight,

1893
03:01:35,820 --> 03:01:40,540
and then I'll give you a minute to play around with that.

1894
03:01:40,540 --> 03:01:43,220
But we have already spoken about what

1895
03:01:43,220 --> 03:01:44,780
we need to do with the household weight.

1896
03:02:13,220 --> 03:02:13,700
Oh, no.

1897
03:02:13,700 --> 03:02:16,580
The cave wasn't supposed to show.

1898
03:02:16,580 --> 03:02:18,060
Ignore that.

1899
03:02:18,060 --> 03:02:18,580
Oh, wait.

1900
03:02:18,580 --> 03:02:21,620
It's probably in your computer.

1901
03:02:21,620 --> 03:02:22,120
OK.

1902
03:02:22,120 --> 03:02:27,340
So we could try the mean of the household weight.

1903
03:02:27,340 --> 03:02:33,500
So the average household weight represents 95.

1904
03:02:33,500 --> 03:02:37,100
So we can try the mean of the household weight.

1905
03:02:37,100 --> 03:02:43,140
So the average household weight represents 95.

1906
03:02:43,140 --> 03:02:45,060
Households.

1907
03:02:45,060 --> 03:02:50,340
But if we run the sum, like we mentioned earlier,

1908
03:02:50,340 --> 03:02:52,660
these are the actual numbers.

1909
03:02:52,660 --> 03:03:03,540
Now you have found there's, in Ohio, 3,223,198 households

1910
03:03:03,540 --> 03:03:07,860
with access to high speed internet.

1911
03:03:07,860 --> 03:03:10,700
And these are households, again, from what

1912
03:03:10,700 --> 03:03:14,500
we define as the household we're interested in,

1913
03:03:14,500 --> 03:03:18,780
under the 1970 definition and the 1990 definition,

1914
03:03:18,780 --> 03:03:20,060
which is up to 10 people.

1915
03:03:20,060 --> 03:03:22,700
If there's 11 people living in a housing unit,

1916
03:03:22,700 --> 03:03:24,420
and they're not related to each other,

1917
03:03:24,420 --> 03:03:27,140
we don't want to know anything about them.

1918
03:03:27,140 --> 03:03:31,980
Drop them in the data prep file.

1919
03:03:31,980 --> 03:03:33,440
And I want to show you something here.

1920
03:03:33,440 --> 03:03:37,900
And I might go later in the notebook,

1921
03:03:37,900 --> 03:03:41,420
you'll see something else, the same here.

1922
03:03:41,420 --> 03:03:47,100
But just like you passed just one column,

1923
03:03:47,100 --> 03:03:48,720
if you wanted to pass multiple columns,

1924
03:03:48,720 --> 03:03:53,260
you pass them as a list, household and per weight,

1925
03:03:53,260 --> 03:03:53,740
for example.

1926
03:03:56,540 --> 03:03:58,100
This should be pretty much the same,

1927
03:03:58,100 --> 03:04:00,140
because you dropped every other person.

1928
03:04:00,140 --> 03:04:03,700
So per weight is meaningless right now.

1929
03:04:03,700 --> 03:04:09,580
But did you notice the change in the look of this?

1930
03:04:09,580 --> 03:04:14,700
When you have just one column, pass it as this,

1931
03:04:14,700 --> 03:04:20,020
household, no, hhwt.

1932
03:04:20,020 --> 03:04:22,580
This is what this looks like.

1933
03:04:22,580 --> 03:04:25,220
This is a series.

1934
03:04:25,220 --> 03:04:30,620
If you pass it, even if you pass it just as a one element list,

1935
03:04:30,620 --> 03:04:34,100
because you're passing it a list,

1936
03:04:34,100 --> 03:04:37,220
it's going to look like a data frame.

1937
03:04:37,220 --> 03:04:40,660
And most of the things that you can do to a series,

1938
03:04:40,660 --> 03:04:43,300
you can do to a data frame.

1939
03:04:43,300 --> 03:04:46,980
But there are some things that you cannot do to a data frame

1940
03:04:46,980 --> 03:04:51,740
that you can only apply to series, like value counts.

1941
03:04:56,860 --> 03:04:59,060
Value count, drop duplicates.

1942
03:04:59,060 --> 03:05:02,580
There are certain methods that only work for series,

1943
03:05:02,580 --> 03:05:05,220
and not data frames.

1944
03:05:05,220 --> 03:05:06,560
So you need to be aware of that.

1945
03:05:06,560 --> 03:05:10,300
And I usually like this, because it's more readable.

1946
03:05:10,300 --> 03:05:11,740
It looks nicer.

1947
03:05:11,740 --> 03:05:15,060
And a lot of the time, I'm not doing anything with the data

1948
03:05:15,060 --> 03:05:18,140
that it has to be a series.

1949
03:05:18,140 --> 03:05:20,380
I need it as a series, and there's no other way.

1950
03:05:20,380 --> 03:05:22,420
So if I can, I'll just have it as a data frame.

1951
03:05:22,420 --> 03:05:25,580
It's easier to read.

1952
03:05:25,620 --> 03:05:29,380
So now we have the actual numbers.

1953
03:05:29,380 --> 03:05:33,100
This code I put together just so it will match

1954
03:05:33,100 --> 03:05:34,300
what you have in your.

1955
03:05:34,300 --> 03:05:37,820
If you're using a different state, this is going to change.

1956
03:05:37,820 --> 03:05:44,340
But this is the same thing as this over here.

1957
03:05:44,340 --> 03:05:45,620
We have, wait.

1958
03:05:48,940 --> 03:05:49,860
Oh, this is no.

1959
03:05:49,860 --> 03:05:52,740
No.

1960
03:05:52,740 --> 03:05:54,100
Python is zero indexed.

1961
03:05:57,140 --> 03:06:01,740
So now we see three million households in Ohio

1962
03:06:01,740 --> 03:06:05,820
have access to high speed internet.

1963
03:06:05,820 --> 03:06:06,940
But how many?

1964
03:06:06,940 --> 03:06:08,780
That's our research question, number one.

1965
03:06:11,380 --> 03:06:13,660
So to make this a zero follow, we

1966
03:06:13,660 --> 03:06:17,420
can save this same thing that we did over here.

1967
03:06:17,420 --> 03:06:21,060
Save it to a variable.

1968
03:06:21,060 --> 03:06:26,380
So all we're doing is typing this onto this.

1969
03:06:26,380 --> 03:06:31,340
And while you do that, I'm going to switch computers.

1970
03:06:31,340 --> 03:06:32,740
Hopefully it works.

1971
03:06:47,420 --> 03:06:50,180
Moment of truth.

1972
03:07:17,420 --> 03:07:24,180
every day.

1973
03:07:47,420 --> 03:07:54,420
Actually, I'm going to...

1974
03:07:54,420 --> 03:08:07,420
Yeah, I'm just going to go to the solutions.

1975
03:08:07,420 --> 03:08:22,420
I'm not going to do this whole thing.

1976
03:08:22,420 --> 03:08:41,420
Okay, so we are...

1977
03:08:41,420 --> 03:08:45,420
Here.

1978
03:08:45,420 --> 03:08:48,420
So we have this.

1979
03:08:48,420 --> 03:09:12,420
I saved it with just one, so I didn't pass the list, so this is a series.

1980
03:09:12,420 --> 03:09:16,420
So we have already a series, and we already have the number of households.

1981
03:09:16,420 --> 03:09:20,420
So how do we find the sum of the elements of a series?

1982
03:09:20,420 --> 03:09:24,420
How do you think?

1983
03:09:24,420 --> 03:09:28,420
That's a cool thing about pandas is most...

1984
03:09:28,420 --> 03:09:34,420
A lot of the times it's very intuitive.

1985
03:09:34,420 --> 03:09:39,420
You could do CUM sum, and that's going to keep adding more and more,

1986
03:09:39,420 --> 03:09:46,420
because we want just the sum of every element like this, we just pass that sum at the end of it.

1987
03:09:46,420 --> 03:09:50,420
And then you... There you go.

1988
03:09:50,420 --> 03:09:53,420
It's like this.

1989
03:09:53,420 --> 03:09:57,420
Because I'm in the solutions notebook, I'm going to try to keep the answers below,

1990
03:09:57,420 --> 03:10:08,420
so you don't see the rest of the answers.

1991
03:10:08,420 --> 03:10:13,420
Does this tutorial...

1992
03:10:13,420 --> 03:10:25,420
We have eight minutes left.

1993
03:10:25,420 --> 03:10:32,420
So this was the revelation.

1994
03:10:32,420 --> 03:10:38,420
So now we have households with high speed Internet here.

1995
03:10:38,420 --> 03:10:40,420
We have each of the valleys.

1996
03:10:40,420 --> 03:10:43,420
We have households that answer N, A.

1997
03:10:43,420 --> 03:10:47,420
We have households that said yes, I have access to Internet, to high speed Internet.

1998
03:10:47,420 --> 03:10:50,420
And we have households that said no.

1999
03:10:50,420 --> 03:10:54,420
And now we also have the total households.

2000
03:10:54,420 --> 03:11:01,420
So we now have the denominator of our equation.

2001
03:11:01,420 --> 03:11:08,420
When you apply a function just like we did dot sum to a series,

2002
03:11:08,420 --> 03:11:12,420
all it did was add up every single element of it.

2003
03:11:12,420 --> 03:11:18,420
Series are NumPy arrays, which are not like list.

2004
03:11:18,420 --> 03:11:24,420
So if you apply comparison like we did earlier,

2005
03:11:24,420 --> 03:11:28,420
or if you apply... If you add something or multiply it by something,

2006
03:11:28,420 --> 03:11:35,420
it's not going to do... It's going to apply that operation to every single one of its elements.

2007
03:11:35,420 --> 03:11:40,420
So if you have your households with high speed Internet, with high speed access,

2008
03:11:40,420 --> 03:11:44,420
which is this series of three elements,

2009
03:11:44,420 --> 03:11:48,420
and you were to multiply it times a million,

2010
03:11:48,420 --> 03:11:52,420
it's going to grab each of those elements and multiply it times a million.

2011
03:11:52,420 --> 03:11:55,420
It's not going to add it up and then multiply it that times a million.

2012
03:11:55,420 --> 03:12:00,420
Or it's not going to do it to just the first element or something.

2013
03:12:00,420 --> 03:12:06,420
It applies to every single element. It's vectorized, I think is the term.

2014
03:12:06,420 --> 03:12:13,420
So now that you have this, now that you know you can do some math to a series,

2015
03:12:13,420 --> 03:12:16,420
and it's going to apply that to every single one of its elements,

2016
03:12:16,420 --> 03:12:21,420
how do you think we can find the share of each of these?

2017
03:12:21,420 --> 03:12:27,420
How much each of these make of the total?

2018
03:12:27,420 --> 03:12:30,420
Over here.

2019
03:12:30,420 --> 03:12:36,420
Yeah, as simple as that. Just like over here we divide by a million,

2020
03:12:36,420 --> 03:12:39,420
and it divided each of those elements by a million.

2021
03:12:39,420 --> 03:12:44,420
Instead of a million, we can just pass the total of our households.

2022
03:12:44,420 --> 03:12:50,420
And then this is the actual number.

2023
03:12:50,420 --> 03:12:53,420
This is different from what we had over here.

2024
03:12:53,420 --> 03:12:57,420
Sometimes it might be very similar, but not usually.

2025
03:12:57,420 --> 03:13:05,420
When we did the normalize, we saw 80% of our observations said yes,

2026
03:13:05,420 --> 03:13:11,420
had a value of yes, I have access to the Internet.

2027
03:13:11,420 --> 03:13:15,420
When you actually account for the weight and you weight it correctly,

2028
03:13:15,420 --> 03:13:23,420
it's 78%, 78.7. So 79 versus 80%.

2029
03:13:23,420 --> 03:13:29,420
Sometimes they get very lacking, but this is the actual accurate number.

2030
03:13:29,420 --> 03:13:37,420
And this is for Ohio, it's higher actually than, not that I'm surprised, Ohio's great.

2031
03:13:37,420 --> 03:13:39,420
But this is, oh, I don't have it.

2032
03:13:39,420 --> 03:13:41,420
The California one, the one that we're trying to recreate,

2033
03:13:41,420 --> 03:13:43,420
for California the number is 74%.

2034
03:13:43,420 --> 03:13:53,420
Only 74% of households in California have access to high speed Internet.

2035
03:13:53,420 --> 03:13:57,420
And I completely overshot.

2036
03:13:57,420 --> 03:14:00,420
I thought we were going to have time for part two,

2037
03:14:00,420 --> 03:14:04,420
but I think we're finishing in four minutes.

2038
03:14:04,420 --> 03:14:06,420
The mechanics are the same.

2039
03:14:06,420 --> 03:14:13,420
The only thing that you need to do now is instead of grouping by just side high speed,

2040
03:14:13,420 --> 03:14:21,420
by their value, you want to find these numbers for each of race ethnicity group.

2041
03:14:21,420 --> 03:14:29,420
So to see if certain households, certain demographic groups have less access to high speed Internet.

2042
03:14:29,420 --> 03:14:36,420
And I try to be as thorough as I can in the notebooks and you have the solutions here.

2043
03:14:36,420 --> 03:14:40,420
So if you follow this through, you should be able to do it on your own

2044
03:14:40,420 --> 03:14:45,420
and you can find the answer for whatever state you want.

2045
03:14:45,420 --> 03:14:51,420
And you can learn a little bit more about pandas and deriving variables from another column

2046
03:14:51,420 --> 03:14:58,420
and creating groups out of another column, aggregating data in multiple ways.

2047
03:14:58,420 --> 03:15:01,420
I added as many examples as I could over here.

2048
03:15:01,420 --> 03:15:06,420
You can do other stuff, not just the group by method.

2049
03:15:06,420 --> 03:15:11,420
You can find, you can do a group by and then slice it.

2050
03:15:11,420 --> 03:15:17,420
So it just gives you the answer that you want after you have done the math.

2051
03:15:17,420 --> 03:15:20,420
So you can get this.

2052
03:15:20,420 --> 03:15:23,420
You can do cross sections. You can do cross tabs.

2053
03:15:23,420 --> 03:15:26,420
I really wish we got into this one because this is the fun part.

2054
03:15:26,420 --> 03:15:31,420
The data preparation is not the fun part most of the time.

2055
03:15:31,420 --> 03:15:37,420
But yes, everything is going to be on the link.

2056
03:15:37,420 --> 03:15:40,420
Thank you so much. There's going to be a survey at the end.

2057
03:15:40,420 --> 03:15:45,420
Don't mention that I didn't finish. I'm just kidding.

2058
03:15:45,420 --> 03:15:48,420
They're going to mail you a survey.

2059
03:15:48,420 --> 03:15:52,420
Thank you so much for coming. And it was great. Thank you so much.

2060
03:15:52,420 --> 03:15:54,420
Thank you.

