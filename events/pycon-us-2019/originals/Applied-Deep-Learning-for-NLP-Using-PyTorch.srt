1
00:00:00,000 --> 00:00:04,080
One, I think we can start now.

2
00:00:04,080 --> 00:00:06,240
We still have some people coming in.

3
00:00:06,240 --> 00:00:07,800
I think we can start.

4
00:00:07,800 --> 00:00:11,040
Most people are here already.

5
00:00:11,040 --> 00:00:13,960
So yeah, let's get started.

6
00:00:13,960 --> 00:00:16,920
This talk is, I don't remember the title.

7
00:00:16,920 --> 00:00:18,760
I know it's about NLP.

8
00:00:18,760 --> 00:00:23,240
I choose a title that probably is something related to NLP.

9
00:00:23,240 --> 00:00:25,840
But this will be a talk about NLP and how

10
00:00:25,840 --> 00:00:31,280
to use something like PyTorch for building NLP models.

11
00:00:31,280 --> 00:00:33,440
That's what we're going to do in this tutorial.

12
00:00:33,440 --> 00:00:35,080
So if you're in the wrong tutorial,

13
00:00:35,080 --> 00:00:37,760
you're free to leave now if you're in the right one.

14
00:00:37,760 --> 00:00:39,080
Let's go.

15
00:00:39,080 --> 00:00:43,720
This is the GitHub address.

16
00:00:43,720 --> 00:00:46,240
Here you can access all of the resources.

17
00:00:46,240 --> 00:00:47,200
We have slides.

18
00:00:47,200 --> 00:00:51,480
We have something like this.

19
00:00:51,480 --> 00:00:54,480
We have a slide, which is a website.

20
00:00:54,520 --> 00:00:57,840
You can access it, and you can ask questions,

21
00:00:57,840 --> 00:00:59,200
questions that are not so urgent.

22
00:00:59,200 --> 00:01:00,280
You can just ask them there.

23
00:01:00,280 --> 00:01:02,640
But if you have more urgent questions

24
00:01:02,640 --> 00:01:05,040
that you need some clarification or answer,

25
00:01:05,040 --> 00:01:05,840
you're free to ask.

26
00:01:05,840 --> 00:01:06,720
We have a mic here.

27
00:01:06,720 --> 00:01:10,080
So we'll just pass it to you, and you can ask.

28
00:01:10,080 --> 00:01:14,400
Yeah, and we have a hashtag, please, if you use Twitter.

29
00:01:14,400 --> 00:01:17,240
Just tweet it, and we'll make sure to retweet it

30
00:01:17,240 --> 00:01:21,160
later during our break.

31
00:01:21,160 --> 00:01:24,680
So just a bit about myself.

32
00:01:24,680 --> 00:01:26,280
My origin is Belize.

33
00:01:26,280 --> 00:01:29,760
So Belize is located in Central America.

34
00:01:29,760 --> 00:01:32,440
I was doing a PhD for six years.

35
00:01:32,440 --> 00:01:36,600
So I did a master's and a PhD in Taiwan,

36
00:01:36,600 --> 00:01:39,640
PhD related to data science field.

37
00:01:39,640 --> 00:01:42,080
And I focus a lot on NLP.

38
00:01:42,080 --> 00:01:44,680
So most of the stuff that I did for these six years,

39
00:01:44,680 --> 00:01:47,880
this is what I will use to present today.

40
00:01:47,880 --> 00:01:51,480
And I gave talks in some of these developer conferences.

41
00:01:51,480 --> 00:01:55,520
So I'm a really active speaker at some conferences.

42
00:01:55,520 --> 00:01:59,280
And my job is I do education at Elastic.

43
00:01:59,280 --> 00:02:01,680
Elastic recently went public.

44
00:02:01,680 --> 00:02:06,240
It's a company that deals with distributed databases.

45
00:02:06,240 --> 00:02:10,840
So I help to basically come up with,

46
00:02:10,840 --> 00:02:13,280
help develop curriculums, and teach some of the things

47
00:02:13,280 --> 00:02:16,680
that the great programmers are doing at the company.

48
00:02:16,680 --> 00:02:18,040
And then this is my Twitter account.

49
00:02:18,040 --> 00:02:19,400
You can get access to it.

50
00:02:19,400 --> 00:02:20,560
And this is my son here.

51
00:02:20,560 --> 00:02:22,200
So he's a three-year-old.

52
00:02:22,200 --> 00:02:25,560
And we can see that he's like with the Jabiru.

53
00:02:25,560 --> 00:02:29,200
Jabiru is a very popular bird in Central America and South

54
00:02:29,200 --> 00:02:31,320
America if you are from that region.

55
00:02:31,320 --> 00:02:33,200
And the special thing about the Jabiru

56
00:02:33,200 --> 00:02:36,360
is that we have it on the, we just recently put it

57
00:02:36,360 --> 00:02:36,920
on the coin.

58
00:02:36,920 --> 00:02:40,120
So our official coin, official dollar has the bird.

59
00:02:40,120 --> 00:02:45,240
And we replaced this bird with the Kill Bill Toucan.

60
00:02:45,240 --> 00:02:48,520
The Kill Bill Toucan is a bird that's,

61
00:02:48,520 --> 00:02:49,840
it's our national bird.

62
00:02:49,840 --> 00:02:54,680
So that's cool that the Jabiru, although not a very famous bird,

63
00:02:54,680 --> 00:02:58,280
but got some recognition.

64
00:02:58,280 --> 00:02:59,360
So some housekeeping.

65
00:02:59,360 --> 00:03:04,560
We have a survey that we need to fill out from the organizers.

66
00:03:04,560 --> 00:03:06,080
And we also have a break-it tree.

67
00:03:06,080 --> 00:03:07,920
So initially, I didn't have the break-it tree.

68
00:03:07,920 --> 00:03:09,380
But we will take the break-it tree.

69
00:03:09,380 --> 00:03:12,480
We'll see how we can modify part of the schedule today.

70
00:03:15,480 --> 00:03:17,720
So I just want to clarify, usually when

71
00:03:17,720 --> 00:03:20,000
I do a presentation, I usually have this slide.

72
00:03:20,000 --> 00:03:22,000
I like to put the slide.

73
00:03:22,000 --> 00:03:24,920
It's important to know what the tutorial is about

74
00:03:24,920 --> 00:03:26,440
and what it's not about.

75
00:03:26,440 --> 00:03:29,000
So this tutorial is a very easy guide

76
00:03:29,000 --> 00:03:32,120
to understand Pytorch and its concepts.

77
00:03:32,120 --> 00:03:36,480
It's a very practical approach to NLP.

78
00:03:36,480 --> 00:03:40,520
We're going to focus on the nitty gritty of applied NLP.

79
00:03:40,520 --> 00:03:43,080
And this tutorial will be student-oriented,

80
00:03:43,080 --> 00:03:46,200
which means you'll be the focus of this tutorial.

81
00:03:46,200 --> 00:03:48,040
We have a lot of exercises.

82
00:03:48,040 --> 00:03:51,680
And we have a really packed tutorial for today.

83
00:03:51,680 --> 00:03:55,240
And we expect exercises, question session, and group work.

84
00:03:55,240 --> 00:03:56,680
So if you have questions that you

85
00:03:56,680 --> 00:03:59,880
would like me to ask during the small breaks,

86
00:03:59,880 --> 00:04:03,040
please do so in the website I shared before.

87
00:04:03,040 --> 00:04:06,640
And this tutorial will be very little theoretical explanation.

88
00:04:06,640 --> 00:04:08,760
So I won't focus on that boring part.

89
00:04:08,760 --> 00:04:11,280
So mostly it will be about application.

90
00:04:11,280 --> 00:04:15,240
This is not a deep learning course, very important.

91
00:04:15,240 --> 00:04:17,040
We will do some deep learning, but it's not

92
00:04:17,040 --> 00:04:19,120
a deep learning course.

93
00:04:19,120 --> 00:04:21,520
There is very little use of external libraries.

94
00:04:21,520 --> 00:04:24,080
And there is a purpose why I designed the tutorial

95
00:04:24,080 --> 00:04:24,840
this way.

96
00:04:24,840 --> 00:04:27,640
I think in order for you to get the idea of how

97
00:04:27,640 --> 00:04:30,440
to build NLP models, you have to know

98
00:04:30,440 --> 00:04:34,520
how to build the components of the model itself from scratch.

99
00:04:34,520 --> 00:04:36,880
So that's going to be the focus of the tutorial.

100
00:04:36,880 --> 00:04:39,640
And from there on, I believe that once you

101
00:04:39,640 --> 00:04:42,000
have this background knowledge and you know how to use it,

102
00:04:42,000 --> 00:04:43,800
too, like PyTorch, it will be easy for you

103
00:04:43,800 --> 00:04:46,680
to go and see a tutorial and adopt things quickly.

104
00:04:46,680 --> 00:04:50,120
So that's kind of the idea behind the tutorial.

105
00:04:50,120 --> 00:04:53,480
So more formally, the objectives of the course of the tutorial

106
00:04:53,480 --> 00:04:57,440
is to provide a quick overview of PyTorch high level APIs.

107
00:04:57,440 --> 00:04:59,240
So I'll be using the documentation on that,

108
00:04:59,240 --> 00:05:01,240
especially during the exercise.

109
00:05:01,240 --> 00:05:03,560
And it is intentionally built like this

110
00:05:03,560 --> 00:05:05,920
so that you have that hands-on practice.

111
00:05:05,920 --> 00:05:08,840
And the idea is to introduce the important foundational

112
00:05:08,840 --> 00:05:12,280
knowledge to apply modern NLP and to inform students

113
00:05:12,280 --> 00:05:15,160
about the latest deep learning techniques for modern NLP.

114
00:05:15,160 --> 00:05:17,560
So we will do cover some deep learning stuff

115
00:05:17,560 --> 00:05:21,800
and to engage students and test knowledge acquisition as well.

116
00:05:21,800 --> 00:05:24,200
Just formally.

117
00:05:24,200 --> 00:05:26,560
So just a quick survey.

118
00:05:26,560 --> 00:05:29,240
I assume that everyone uses Python here.

119
00:05:29,240 --> 00:05:29,840
Is that correct?

120
00:05:29,840 --> 00:05:32,240
Maybe raise of hands.

121
00:05:32,240 --> 00:05:33,720
Perfect.

122
00:05:33,720 --> 00:05:34,960
How about PyTorch?

123
00:05:34,960 --> 00:05:37,960
Have you used PyTorch before?

124
00:05:37,960 --> 00:05:38,560
OK.

125
00:05:38,560 --> 00:05:40,320
That's great.

126
00:05:40,320 --> 00:05:43,320
That's great because the tutorial was designed like this.

127
00:05:43,320 --> 00:05:46,520
I knew that maybe because PyTorch is a recent tool.

128
00:05:46,520 --> 00:05:48,800
So I knew that there's high demand for it.

129
00:05:48,800 --> 00:05:50,440
And that's perfect.

130
00:05:50,440 --> 00:05:51,920
It is designed this way.

131
00:05:51,920 --> 00:05:52,840
How about TensorFlow?

132
00:05:52,840 --> 00:05:55,520
Do you have experience with TensorFlow?

133
00:05:55,520 --> 00:05:56,840
Yeah, we do have some hands.

134
00:05:56,840 --> 00:05:57,560
Yes.

135
00:05:57,560 --> 00:05:58,440
That's good.

136
00:05:58,440 --> 00:06:00,640
That knowledge will come in handy.

137
00:06:00,640 --> 00:06:03,640
And especially when we do form the groups,

138
00:06:03,640 --> 00:06:06,880
I would expect that people that have that experience

139
00:06:06,880 --> 00:06:10,240
could maybe become group leaders and help out the rest.

140
00:06:10,240 --> 00:06:12,120
I do believe that would be a great way

141
00:06:12,120 --> 00:06:15,080
to share your knowledge as well.

142
00:06:15,080 --> 00:06:17,280
So the journey will be something like this.

143
00:06:17,280 --> 00:06:19,720
We have a very brief introduction to NLP.

144
00:06:19,720 --> 00:06:21,520
I'm not going to spend too much time on it.

145
00:06:21,520 --> 00:06:23,960
I'm just going to say what's the idea behind it.

146
00:06:23,960 --> 00:06:26,720
And there is a five-part tutorial.

147
00:06:26,720 --> 00:06:29,360
Feel free to interrupt me if you have any questions, by the way.

148
00:06:29,360 --> 00:06:31,560
I'm just talking here, but you can interrupt me

149
00:06:31,560 --> 00:06:32,960
if you have questions.

150
00:06:32,960 --> 00:06:35,920
So part one is introducing the concept of tensors.

151
00:06:35,920 --> 00:06:38,680
Most of deep learning and LMP now and modern LMP

152
00:06:38,680 --> 00:06:42,120
is about tensors and how to manipulate and operate on them.

153
00:06:42,120 --> 00:06:46,160
And part two will be PyTorch core functionalities,

154
00:06:46,160 --> 00:06:49,440
very important part of the presentation.

155
00:06:49,440 --> 00:06:52,320
So Dask is in charge of background.

156
00:06:52,320 --> 00:06:56,440
And then we have part three, which is NLP basics and things

157
00:06:56,440 --> 00:06:58,760
like preprocessing, tokenization,

158
00:06:58,760 --> 00:07:01,160
creating vocabularies, and this kind of thing.

159
00:07:01,160 --> 00:07:03,840
And part four is apply deep learning to NLP.

160
00:07:03,840 --> 00:07:06,120
So each one of these is a notebook.

161
00:07:06,120 --> 00:07:10,720
And I shared some instructions before we started here.

162
00:07:10,720 --> 00:07:12,680
Like a week ago, I shared some instructions on how

163
00:07:12,680 --> 00:07:16,160
to use something like Google Collab

164
00:07:16,160 --> 00:07:18,240
to be able to do all the data science stuff.

165
00:07:18,240 --> 00:07:21,160
So hopefully, you got that email,

166
00:07:21,160 --> 00:07:24,520
and you have set up an account, and you have

167
00:07:24,520 --> 00:07:25,800
access to Google Collab.

168
00:07:25,800 --> 00:07:27,760
I hope that that's the case.

169
00:07:27,760 --> 00:07:30,320
If not, then you'll have to maybe form groups

170
00:07:30,320 --> 00:07:32,360
and just help each other.

171
00:07:33,240 --> 00:07:36,080
The part five is best practices for deploying and training

172
00:07:36,080 --> 00:07:36,960
NLP models.

173
00:07:36,960 --> 00:07:38,320
So this part is optional.

174
00:07:38,320 --> 00:07:40,440
This will be more like what you do

175
00:07:40,440 --> 00:07:42,400
if you want to do research in NLP,

176
00:07:42,400 --> 00:07:45,080
is like how to properly evaluate models,

177
00:07:45,080 --> 00:07:47,440
things like confusion matrices and this kind of thing.

178
00:07:47,440 --> 00:07:49,640
Not really important if you only want to apply NLP,

179
00:07:49,640 --> 00:07:53,720
but I think it's kind of a critical part of the NLP

180
00:07:53,720 --> 00:07:55,120
as well.

181
00:07:55,120 --> 00:07:57,120
So that's just the core of the tutorial.

182
00:08:00,760 --> 00:08:01,960
So what is NLP?

183
00:08:01,960 --> 00:08:03,360
Very brief introduction.

184
00:08:03,360 --> 00:08:05,920
My definition is there are so many different definitions,

185
00:08:05,920 --> 00:08:08,240
and it's because it's a really new field.

186
00:08:08,240 --> 00:08:09,480
We have so many definitions.

187
00:08:09,480 --> 00:08:11,920
So I just made a very simple definition,

188
00:08:11,920 --> 00:08:13,640
which is a computational approach

189
00:08:13,640 --> 00:08:16,040
to understanding language, teaching machines

190
00:08:16,040 --> 00:08:18,920
how to understand language.

191
00:08:18,920 --> 00:08:20,880
So it includes some ubiquitous tasks

192
00:08:20,880 --> 00:08:22,880
like the following, tokenization,

193
00:08:22,880 --> 00:08:24,200
word sense, system integration.

194
00:08:24,200 --> 00:08:26,400
I will explain more later during the notebooks

195
00:08:26,400 --> 00:08:27,960
what these things mean.

196
00:08:27,960 --> 00:08:30,320
Name entity recognition, part of speech.

197
00:08:30,320 --> 00:08:32,160
You may have heard about these concepts.

198
00:08:32,160 --> 00:08:34,880
Sentence classification, language generation,

199
00:08:34,880 --> 00:08:37,200
question answering, and machine translation.

200
00:08:37,200 --> 00:08:40,960
So all of this is included in NLP.

201
00:08:40,960 --> 00:08:43,400
And there is like a little Venn diagram here.

202
00:08:43,400 --> 00:08:44,960
Maybe not the most accurate Venn diagram,

203
00:08:44,960 --> 00:08:48,160
but I think it gets the idea.

204
00:08:48,160 --> 00:08:54,760
So NLP, you can see that it's part of the whole AI movement.

205
00:08:54,760 --> 00:08:57,600
So traditional NLP versus modern NLP.

206
00:08:57,600 --> 00:09:00,000
So usually traditional NLP is you

207
00:09:00,000 --> 00:09:04,000
learn features from data versus human engineering features.

208
00:09:04,000 --> 00:09:07,040
So a traditional way is just engineering features.

209
00:09:07,040 --> 00:09:09,120
And the modern way is to learn features from the data

210
00:09:09,120 --> 00:09:13,400
automatically using some tool like RNN, LSTM, or CNN.

211
00:09:13,400 --> 00:09:16,400
And each one operates very differently.

212
00:09:16,400 --> 00:09:19,320
So we will learn more about these later on.

213
00:09:19,320 --> 00:09:21,600
And some of the weaknesses and strengths, well,

214
00:09:21,600 --> 00:09:24,240
I only listed two, but there could be a whole list.

215
00:09:24,240 --> 00:09:27,680
So the advantage is that these models,

216
00:09:27,680 --> 00:09:31,080
you can learn efficiently from data, learn features.

217
00:09:31,080 --> 00:09:36,640
And the disadvantage now is that it requires large scale data.

218
00:09:36,640 --> 00:09:37,680
That's a disadvantage.

219
00:09:37,680 --> 00:09:41,200
So me working six years in one field, in one topic,

220
00:09:41,200 --> 00:09:44,480
I realized that large scale data was always the problem.

221
00:09:44,480 --> 00:09:46,240
I was a bottleneck, especially because I

222
00:09:46,240 --> 00:09:47,920
deal with real world data sets.

223
00:09:47,920 --> 00:09:50,540
I could never get access to those large scale data sets.

224
00:09:50,540 --> 00:09:54,240
So using something like CNN or RNN,

225
00:09:54,240 --> 00:09:56,400
I always had that difficulty.

226
00:09:56,400 --> 00:09:58,200
It's quite challenging.

227
00:09:58,200 --> 00:10:00,080
So this is just a high level overview

228
00:10:00,080 --> 00:10:02,400
of what an NLP framework covers.

229
00:10:02,400 --> 00:10:04,520
So we have a database over there.

230
00:10:04,520 --> 00:10:09,240
That can be anything, anything related to language, of course.

231
00:10:09,240 --> 00:10:11,240
Then you have some data preprocessing,

232
00:10:11,240 --> 00:10:14,120
data transformation, modeling, and inferencing.

233
00:10:14,120 --> 00:10:16,840
So at a high level, this is what it looks like.

234
00:10:16,840 --> 00:10:19,840
But each one includes very key components,

235
00:10:19,840 --> 00:10:22,640
very detailed components.

236
00:10:22,640 --> 00:10:26,000
So we will go through each one at all phases.

237
00:10:26,000 --> 00:10:30,720
And that's why I have initially designed four segments,

238
00:10:30,720 --> 00:10:33,800
four notebooks, each covering each one of the components

239
00:10:33,800 --> 00:10:35,360
here.

240
00:10:35,360 --> 00:10:38,900
So let's just jump into the demonstrations.

241
00:10:38,900 --> 00:10:41,080
So some instructions.

242
00:10:41,080 --> 00:10:44,320
If you didn't get the link that I shared,

243
00:10:44,320 --> 00:10:48,480
I have a folder here with a data set that we're going to use.

244
00:10:48,480 --> 00:10:51,920
And you can get access to it now if you

245
00:10:51,920 --> 00:10:54,740
have access to the slides.

246
00:10:54,740 --> 00:10:58,020
So you can get access to the slide using the following link.

247
00:10:58,020 --> 00:11:00,340
I'll just share it here.

248
00:11:00,340 --> 00:11:03,060
So this link here, you can use that link

249
00:11:03,060 --> 00:11:05,100
and you can get access to the slides.

250
00:11:05,100 --> 00:11:07,740
And you can get access to all the notebooks this way.

251
00:11:07,740 --> 00:11:09,460
So first thing, slides.

252
00:11:09,460 --> 00:11:12,480
Then you can get access to all the notebooks

253
00:11:12,480 --> 00:11:13,780
if you're using Google Cloud.

254
00:11:13,780 --> 00:11:15,860
Because that's the main thing we're going to use.

255
00:11:15,860 --> 00:11:18,180
And if you don't have access to Google Cloud,

256
00:11:18,180 --> 00:11:22,140
if you have your own environment set up, that's perfect.

257
00:11:22,140 --> 00:11:23,380
You can use that as well.

258
00:11:23,380 --> 00:11:24,820
But I expect that you would already

259
00:11:24,820 --> 00:11:27,780
have installed something like PyTorch and other tools

260
00:11:27,780 --> 00:11:29,820
that we're going to use today.

261
00:11:29,820 --> 00:11:35,980
So how many of you guys have Google Cloud set up?

262
00:11:35,980 --> 00:11:39,980
OK, yeah, a lot, it looks like.

263
00:11:39,980 --> 00:11:41,540
How about your own environment?

264
00:11:44,260 --> 00:11:45,220
OK.

265
00:11:45,220 --> 00:11:47,900
OK.

266
00:11:47,900 --> 00:11:51,540
So we can just start with the first part.

267
00:11:51,540 --> 00:11:53,300
You have any questions before we proceed?

268
00:11:53,300 --> 00:11:56,300
Because now the more exciting part of the presentation

269
00:11:56,300 --> 00:11:57,100
is going to start.

270
00:11:57,100 --> 00:11:59,300
But you'll feel like at times this

271
00:11:59,300 --> 00:12:02,340
will go super fast because there's so much thing

272
00:12:02,340 --> 00:12:04,740
that I wanted to put into this tutorial.

273
00:12:04,740 --> 00:12:06,380
But we only have three hours.

274
00:12:06,380 --> 00:12:09,460
Three hours is a very small amount of time.

275
00:12:09,460 --> 00:12:12,220
But we will do as much as we could today.

276
00:12:12,220 --> 00:12:15,140
And I hope that each one of you get at least something

277
00:12:15,140 --> 00:12:16,980
that you can take away with you.

278
00:12:16,980 --> 00:12:18,300
That's the main goal.

279
00:12:23,300 --> 00:12:29,740
So let me know at the back if you can see the presentation.

280
00:12:29,740 --> 00:12:32,740
If you can see the notebook, no.

281
00:12:32,740 --> 00:12:33,740
Let's see.

282
00:12:33,740 --> 00:12:36,220
I'll try to change it.

283
00:12:36,220 --> 00:12:39,220
How about that?

284
00:12:39,220 --> 00:12:41,700
It's better.

285
00:12:41,700 --> 00:12:42,180
Let's see.

286
00:12:42,180 --> 00:12:45,180
I think we can go one level.

287
00:12:45,180 --> 00:12:47,700
I think we can do fine with this.

288
00:12:47,700 --> 00:12:50,380
So the first part is mostly like cover

289
00:12:51,340 --> 00:12:56,140
The first part is basically a primer on tensors.

290
00:12:56,140 --> 00:12:57,660
I wasn't going to do this part.

291
00:12:57,660 --> 00:13:00,540
But I realized that most of you guys

292
00:13:00,540 --> 00:13:02,340
are actually starting to use PyTorch

293
00:13:02,340 --> 00:13:03,940
or maybe have not used it before.

294
00:13:03,940 --> 00:13:06,380
So I think it's a perfect time to introduce it

295
00:13:06,380 --> 00:13:09,060
and just get the idea of what kind of features

296
00:13:09,060 --> 00:13:13,100
and what kind of tools you have access to with this ecosystem.

297
00:13:13,100 --> 00:13:15,540
So this is just a bunch of things that we'll do.

298
00:13:15,540 --> 00:13:16,340
What is a tensor?

299
00:13:16,340 --> 00:13:17,100
Building tensors.

300
00:13:17,100 --> 00:13:19,340
Everything is about tensors at the beginning.

301
00:13:19,340 --> 00:13:24,420
But slowly we will transition into the NLP material.

302
00:13:24,420 --> 00:13:25,740
But this part is super important.

303
00:13:25,740 --> 00:13:28,820
And we only have roughly, I planned it 20 minutes.

304
00:13:28,820 --> 00:13:30,900
And I do have some exercises at the end.

305
00:13:30,900 --> 00:13:33,220
So I will time this just to make sure

306
00:13:33,220 --> 00:13:35,660
that we are on schedule with everything.

307
00:13:35,660 --> 00:13:37,860
So let's see.

308
00:13:37,860 --> 00:13:38,740
So what is a tensor?

309
00:13:38,740 --> 00:13:40,660
That's the first question.

310
00:13:40,660 --> 00:13:42,580
A tensor is just define it.

311
00:13:42,580 --> 00:13:46,020
I would define it as just like a data structure, basically,

312
00:13:46,020 --> 00:13:47,660
that you use in deep learning a lot.

313
00:13:47,660 --> 00:13:48,780
That's how I would define it.

314
00:13:48,780 --> 00:13:50,700
Very simple definition of it.

315
00:13:50,700 --> 00:13:52,900
And basically, it's the core data structure

316
00:13:52,900 --> 00:13:56,940
that you use to build models from the beginning to the end.

317
00:13:56,940 --> 00:13:58,380
So that's what you're going to do.

318
00:13:58,380 --> 00:14:01,420
And you do transformations in these tensors.

319
00:14:01,420 --> 00:14:05,580
And that's just basically what you're doing all the time.

320
00:14:05,580 --> 00:14:12,380
So this snippet here is just you see here.

321
00:14:12,380 --> 00:14:14,500
And I will teach you how to use Google Cloud.

322
00:14:14,500 --> 00:14:16,620
Because if you're not familiar with it,

323
00:14:16,620 --> 00:14:17,700
it's pretty simple to use.

324
00:14:17,700 --> 00:14:19,580
If you have used Jupyter Notebooks before,

325
00:14:19,580 --> 00:14:21,700
I think you'll be OK.

326
00:14:21,700 --> 00:14:25,940
So I basically imported Torch here,

327
00:14:25,940 --> 00:14:28,460
which is the main library.

328
00:14:28,460 --> 00:14:30,740
If you have access to the notebook, please,

329
00:14:30,740 --> 00:14:32,580
just follow along.

330
00:14:32,580 --> 00:14:35,100
But you will get an opportunity to do this

331
00:14:35,100 --> 00:14:38,460
once I cover this part.

332
00:14:38,460 --> 00:14:42,500
So building tensors, constructing tensors.

333
00:14:42,500 --> 00:14:44,540
So the first thing is how to build a scalar.

334
00:14:44,540 --> 00:14:47,780
That's kind of the most primitive way

335
00:14:47,780 --> 00:14:48,860
of defining a tensor.

336
00:14:48,860 --> 00:14:50,100
It's just building a scalar.

337
00:14:50,100 --> 00:14:53,020
It's basically like a number or maybe a letter

338
00:14:53,020 --> 00:14:53,940
or something like that.

339
00:14:53,940 --> 00:14:55,900
It's just one element.

340
00:14:55,900 --> 00:14:57,180
So we have one element there.

341
00:14:57,180 --> 00:14:57,980
We print it out.

342
00:14:57,980 --> 00:14:58,780
And that's it.

343
00:15:04,460 --> 00:15:08,980
So after that, we have a 2D tensor, which is a vector.

344
00:15:08,980 --> 00:15:11,340
If you're familiar with these terms, it's OK.

345
00:15:11,340 --> 00:15:12,660
Just bear with me.

346
00:15:12,700 --> 00:15:14,940
We'll move pretty quickly here.

347
00:15:14,940 --> 00:15:16,860
So you'll get a chance to also, if you

348
00:15:16,860 --> 00:15:19,020
are not familiar with it, you'll get a chance to explore.

349
00:15:19,020 --> 00:15:19,520
Yes?

350
00:15:19,520 --> 00:15:21,980
Can you just follow along?

351
00:15:21,980 --> 00:15:23,340
Not really.

352
00:15:23,340 --> 00:15:25,020
The way how I designed it is because we

353
00:15:25,020 --> 00:15:26,100
don't have a lot of time.

354
00:15:26,100 --> 00:15:29,220
So what I will do is I will just go over it pretty quickly.

355
00:15:29,220 --> 00:15:31,020
And then I will give you guys some time

356
00:15:31,020 --> 00:15:34,260
to just go through the exercises and go on it by yourself.

357
00:15:34,260 --> 00:15:36,620
So like I said, most of the tutorial

358
00:15:36,620 --> 00:15:38,620
will be I want it to be student-oriented,

359
00:15:38,620 --> 00:15:40,020
so you've got the hands on.

360
00:15:40,020 --> 00:15:41,660
And you can ask your questions.

361
00:15:41,660 --> 00:15:45,580
You can ask me, and I'll be happy to support.

362
00:15:45,580 --> 00:15:49,060
I think we'll find the right way to create one.

363
00:15:49,060 --> 00:15:50,980
Maybe you can create a copy of it.

364
00:15:50,980 --> 00:15:52,300
Yeah, you'll need to create a copy of it.

365
00:15:52,300 --> 00:15:53,740
But I'll show you how to do all of this.

366
00:15:53,740 --> 00:15:54,540
So just bear with me.

367
00:15:54,540 --> 00:15:56,700
I will just go through the tutorial pretty quickly,

368
00:15:56,700 --> 00:16:01,740
and then we'll do this all together.

369
00:16:01,740 --> 00:16:02,980
So we have a 2D tensor.

370
00:16:02,980 --> 00:16:03,740
We created it.

371
00:16:06,260 --> 00:16:08,780
Actually, that's not a 2D tensor, but anyways.

372
00:16:09,780 --> 00:16:11,260
Yes.

373
00:16:11,260 --> 00:16:13,140
Something happened there.

374
00:16:13,140 --> 00:16:14,620
I'll fix that.

375
00:16:14,620 --> 00:16:17,580
So a 2D tensor will only be, oh, sorry.

376
00:16:17,580 --> 00:16:20,300
So a 1D tensor is a vector.

377
00:16:20,300 --> 00:16:21,980
The 2D tensor is a matrix.

378
00:16:21,980 --> 00:16:25,300
So we have that there.

379
00:16:25,300 --> 00:16:27,460
And then we have a 3D tensor, which

380
00:16:27,460 --> 00:16:30,380
is just another dimension added to the data structure.

381
00:16:33,500 --> 00:16:37,820
And then we can also build a tensor using

382
00:16:37,820 --> 00:16:41,060
some other data structures, using another format.

383
00:16:41,060 --> 00:16:43,180
So usually, you can build tensors using NumPy,

384
00:16:43,180 --> 00:16:44,340
if you're familiar with that.

385
00:16:44,340 --> 00:16:45,880
I think most people that do data science

386
00:16:45,880 --> 00:16:47,180
are familiar with this, too.

387
00:16:47,180 --> 00:16:50,340
So you can also build out of a NumPy array,

388
00:16:50,340 --> 00:16:52,300
also build a tensor.

389
00:16:52,300 --> 00:16:56,860
It's pretty flexible and very dynamic in that way.

390
00:16:56,860 --> 00:16:58,100
So I have an exercise here.

391
00:16:58,100 --> 00:17:00,700
Like I say, I will just give you time to do all of this.

392
00:17:00,700 --> 00:17:02,500
And if you have questions, we'll definitely

393
00:17:02,500 --> 00:17:04,500
do all of this together.

394
00:17:04,500 --> 00:17:06,780
Special tensors.

395
00:17:06,780 --> 00:17:09,420
There are some special tensors offered by PyTorch.

396
00:17:09,420 --> 00:17:11,140
And the community is very heavily involved

397
00:17:11,140 --> 00:17:12,380
with this library.

398
00:17:12,380 --> 00:17:15,480
So you'll find that some people use things

399
00:17:15,480 --> 00:17:18,500
like a matrix of full zeros.

400
00:17:18,500 --> 00:17:21,300
Some would use matrix of full ones.

401
00:17:21,300 --> 00:17:23,340
And then you have all these special cases.

402
00:17:23,340 --> 00:17:25,220
And once it's popular, then you will

403
00:17:25,220 --> 00:17:27,340
see that there's a function for it automatically.

404
00:17:27,340 --> 00:17:29,900
And that's pretty neat.

405
00:17:29,900 --> 00:17:32,220
And yeah, you have very special tensors

406
00:17:32,220 --> 00:17:34,100
like this one, which is a diagonal one.

407
00:17:35,100 --> 00:17:38,780
And then we can also build tensors using

408
00:17:38,780 --> 00:17:40,100
some random information.

409
00:17:43,180 --> 00:17:46,740
And you can also build another tensor.

410
00:17:46,740 --> 00:17:50,340
So what I mean by that is you build one tensor.

411
00:17:50,340 --> 00:17:51,620
It has some properties.

412
00:17:51,620 --> 00:17:53,380
You want to preserve that property.

413
00:17:53,380 --> 00:17:56,020
And you want to build another tensor

414
00:17:56,020 --> 00:17:57,340
with those same properties.

415
00:17:57,340 --> 00:17:59,020
And properties, I mean dimension.

416
00:17:59,020 --> 00:18:03,740
So when you create it, you create it like that.

417
00:18:03,860 --> 00:18:08,780
So B copies the properties of A. And you

418
00:18:08,780 --> 00:18:12,020
have a matrix full of ones.

419
00:18:12,020 --> 00:18:12,540
That's neat.

420
00:18:12,540 --> 00:18:15,380
It's so dynamic and so flexible that you can do all

421
00:18:15,380 --> 00:18:16,340
of these kind of things.

422
00:18:19,740 --> 00:18:21,660
Yeah, the data type is also copied along too.

423
00:18:27,460 --> 00:18:33,260
So we have Linspace, which is a function to create a range.

424
00:18:33,260 --> 00:18:35,100
Usually, sometimes we will need a range,

425
00:18:35,100 --> 00:18:37,900
especially when we are dealing with batches of data.

426
00:18:37,900 --> 00:18:38,740
We will need a range.

427
00:18:38,740 --> 00:18:40,780
So we have a function for that.

428
00:18:40,780 --> 00:18:42,300
We just recently found out this one.

429
00:18:42,300 --> 00:18:44,660
I would usually use NumPy for it.

430
00:18:44,660 --> 00:18:47,140
But the library is growing so fast

431
00:18:47,140 --> 00:18:49,540
that you always have a new function every time building.

432
00:18:49,540 --> 00:18:51,860
So one of the things that you will find out

433
00:18:51,860 --> 00:18:53,540
from this tutorial is that you'll

434
00:18:53,540 --> 00:18:57,780
be referring to the documentation pretty heavily.

435
00:18:57,780 --> 00:18:58,780
That's pretty normal.

436
00:18:58,780 --> 00:19:01,660
Even for someone that is using PyTorch a lot,

437
00:19:01,660 --> 00:19:03,380
you will just reference that tutorial.

438
00:19:03,380 --> 00:19:04,820
And the tutorial is pretty cool.

439
00:19:04,820 --> 00:19:06,260
It's well-built.

440
00:19:06,260 --> 00:19:08,700
And it has a lot of examples, which is neat.

441
00:19:08,700 --> 00:19:12,100
And you'll definitely learn a lot using the documentation.

442
00:19:12,100 --> 00:19:14,300
Even for experts, you always find

443
00:19:14,300 --> 00:19:18,340
that you are referring to that tutorial all the time.

444
00:19:18,340 --> 00:19:21,100
So we have some exercise here again just to construct

445
00:19:21,100 --> 00:19:21,580
a tensor.

446
00:19:21,580 --> 00:19:23,860
I'll give you time for that.

447
00:19:23,860 --> 00:19:26,940
Tensor properties, how to check what type of tensor.

448
00:19:26,940 --> 00:19:28,260
Someone asked what type.

449
00:19:28,260 --> 00:19:31,580
We have what the type of the tensor is.

450
00:19:31,820 --> 00:19:33,060
What's the size of the tensor?

451
00:19:33,060 --> 00:19:34,980
Sometimes it's really good to probe that

452
00:19:34,980 --> 00:19:37,180
before you're going to train a model.

453
00:19:37,180 --> 00:19:41,540
And you have tensor slicing, how to capture a specific row,

454
00:19:41,540 --> 00:19:43,340
a specific column of the data.

455
00:19:43,340 --> 00:19:48,300
That's pretty standard data science stuff here.

456
00:19:48,300 --> 00:19:51,180
And we have tensor operations and transformations.

457
00:19:51,180 --> 00:19:53,740
So how to transform?

458
00:19:53,740 --> 00:19:57,820
We have a tensor of ones.

459
00:19:57,820 --> 00:20:02,220
How do we convert the first two rows into zeros

460
00:20:02,220 --> 00:20:03,100
and now the rest one?

461
00:20:03,100 --> 00:20:06,660
So you can do that pretty easily.

462
00:20:06,660 --> 00:20:10,220
And how to transpose, which is a very important functionality

463
00:20:10,220 --> 00:20:11,140
in deep learning.

464
00:20:11,140 --> 00:20:12,340
How to transpose a matrix.

465
00:20:16,060 --> 00:20:22,300
OK, so we have here we print out the.

466
00:20:22,300 --> 00:20:25,540
So you notice that when we do a tensor here,

467
00:20:25,540 --> 00:20:28,580
the tensor looks something like that.

468
00:20:28,580 --> 00:20:30,780
We have the zeros and ones here.

469
00:20:30,780 --> 00:20:32,740
And when we transpose it, you notice

470
00:20:32,740 --> 00:20:34,220
that it changes the shape.

471
00:20:34,220 --> 00:20:38,660
So not the shape, but you notice the values, they shift.

472
00:20:38,660 --> 00:20:43,460
So the thing is, when you do the transpose, it's not in place.

473
00:20:43,460 --> 00:20:46,340
Which means it's not going to be applied to A here.

474
00:20:46,340 --> 00:20:49,860
Because we're using a notebook, the transformation appears.

475
00:20:49,860 --> 00:20:53,500
But usually A would not be preserved.

476
00:20:53,500 --> 00:20:57,100
So what I mean by that is, let's say I print A again.

477
00:20:57,100 --> 00:20:58,060
It's not preserved.

478
00:20:58,060 --> 00:21:01,220
And that's very important, especially when you are

479
00:21:01,220 --> 00:21:04,340
using that functionality a lot.

480
00:21:04,340 --> 00:21:06,540
I always remember that it's not in place.

481
00:21:06,540 --> 00:21:09,260
And if you want to make it explicit in place,

482
00:21:09,260 --> 00:21:11,500
you'd have to use something like this,

483
00:21:11,500 --> 00:21:13,820
which is a special character.

484
00:21:13,820 --> 00:21:17,620
So in this case, the method will be transpose underscore.

485
00:21:17,620 --> 00:21:20,020
And that preserves the transformation.

486
00:21:20,020 --> 00:21:23,180
So now you see that it's preserved here.

487
00:21:23,180 --> 00:21:27,660
So the transpose of what we wanted is here.

488
00:21:27,660 --> 00:21:29,420
So some simple maths.

489
00:21:29,420 --> 00:21:32,700
We have built two tensors.

490
00:21:32,700 --> 00:21:33,980
Do a plus here.

491
00:21:33,980 --> 00:21:35,260
It's pretty simple.

492
00:21:35,260 --> 00:21:38,140
If you're familiar with that, this

493
00:21:38,140 --> 00:21:41,980
is basically what you do with every deep learning library.

494
00:21:41,980 --> 00:21:44,140
And how to add.

495
00:21:44,140 --> 00:21:46,540
Also, if you want to add in place,

496
00:21:46,540 --> 00:21:48,260
you use that underscore again.

497
00:21:48,260 --> 00:21:50,540
So everything that is in place, you use the underscore.

498
00:21:53,820 --> 00:21:56,740
So power and sum also, you can have a tensor.

499
00:21:56,740 --> 00:22:01,100
You can do lots of different operations in it.

500
00:22:01,100 --> 00:22:02,420
You can do power of 2.

501
00:22:02,420 --> 00:22:04,180
And then you can sum all the elements.

502
00:22:04,180 --> 00:22:07,220
And you have some final output.

503
00:22:07,220 --> 00:22:10,260
That's a very basic math you can do with those tensors.

504
00:22:10,260 --> 00:22:12,300
And you have element-wise multiplication,

505
00:22:12,300 --> 00:22:17,100
which is also a very important operation in deep learning.

506
00:22:17,100 --> 00:22:19,340
So you have that element-wise.

507
00:22:19,340 --> 00:22:21,620
You're going to times the i by the 1s.

508
00:22:21,620 --> 00:22:23,180
And you have that spatial matrix,

509
00:22:23,180 --> 00:22:27,660
which is basically just the i itself back again.

510
00:22:27,660 --> 00:22:29,300
So I have some exercise here.

511
00:22:29,300 --> 00:22:32,740
It's a very tricky exercise.

512
00:22:32,740 --> 00:22:35,900
I would say that if you go through the notebook,

513
00:22:35,900 --> 00:22:38,020
if you form groups, if you don't understand some part

514
00:22:38,020 --> 00:22:41,060
and you want to capture what it is, just ask the question.

515
00:22:41,060 --> 00:22:45,500
I would say that if you do this exercise on your own,

516
00:22:45,500 --> 00:22:47,020
you're in good hands.

517
00:22:47,020 --> 00:22:51,260
For a beginner, you will be good using this tool.

518
00:22:51,260 --> 00:22:53,460
Because it actually will challenge you to think a bit

519
00:22:53,460 --> 00:22:56,260
on how you can use those functions.

520
00:22:56,260 --> 00:22:59,220
So CUDA tensors.

521
00:22:59,220 --> 00:23:01,700
So this one is a little bit more advanced.

522
00:23:01,700 --> 00:23:05,300
So if you're using a lot of GPUs,

523
00:23:05,300 --> 00:23:08,300
this is where all these functionalities come into play.

524
00:23:08,300 --> 00:23:10,900
So you have some functionalities here,

525
00:23:10,900 --> 00:23:15,260
like checking whether a tensor is existing in your GPU

526
00:23:15,260 --> 00:23:17,340
or if it exists in the CPU.

527
00:23:17,340 --> 00:23:18,860
So you can check all of those.

528
00:23:18,860 --> 00:23:20,900
And you have all the different methods to do that.

529
00:23:26,580 --> 00:23:30,540
So here, it's pretty simple to check whether after you

530
00:23:30,540 --> 00:23:34,900
have assigned something to a CPU or a CPU,

531
00:23:34,900 --> 00:23:38,140
you want to know later on where is that stored,

532
00:23:38,140 --> 00:23:40,820
where are you operating on, what kind of device.

533
00:23:40,820 --> 00:23:43,340
So you can actually check like that with this very simple

534
00:23:43,340 --> 00:23:43,820
method.

535
00:23:43,820 --> 00:23:53,820
So we have like, so you can see that if you use GPU,

536
00:23:53,820 --> 00:23:55,740
you know that the difference between CPU and GPU

537
00:23:55,740 --> 00:23:57,580
is pretty significant, especially

538
00:23:57,580 --> 00:24:03,180
when you are multiplying really high dimensional tensors.

539
00:24:03,180 --> 00:24:06,660
So in this case, for example, I have one here.

540
00:24:06,660 --> 00:24:08,260
1000 is not so huge.

541
00:24:08,260 --> 00:24:11,380
But when you're doing 10,000, for example,

542
00:24:11,380 --> 00:24:14,300
that's pretty heavy already on the CPU.

543
00:24:14,300 --> 00:24:17,540
So you can see that this is 103 milliseconds.

544
00:24:17,540 --> 00:24:21,580
But if you use something like a GPU,

545
00:24:21,580 --> 00:24:23,660
and the cool thing about Google Collab

546
00:24:23,660 --> 00:24:25,660
is that you already have access to some GPU

547
00:24:25,660 --> 00:24:26,540
to play around with.

548
00:24:26,540 --> 00:24:28,860
So you will see that the difference here.

549
00:24:28,860 --> 00:24:30,540
Well, here it took a little while.

550
00:24:30,540 --> 00:24:33,900
That's because it has to first connect to the GPU.

551
00:24:33,900 --> 00:24:37,220
But if we do it again, it's pretty fast, right?

552
00:24:37,220 --> 00:24:41,820
It's like two times faster.

553
00:24:41,820 --> 00:24:43,420
So you can see 20.6 milliseconds.

554
00:24:43,420 --> 00:24:45,900
It took a lot shorter time.

555
00:24:45,900 --> 00:24:48,400
And you see that the difference between the two operations

556
00:24:48,400 --> 00:24:49,740
is that we have that .cuda.

557
00:24:49,740 --> 00:24:53,340
.cuda means that we want to operate,

558
00:24:53,340 --> 00:24:57,300
want to assign this tensor into the GPU.

559
00:24:57,300 --> 00:24:59,500
And you don't really need to know all the GPU stuff,

560
00:24:59,500 --> 00:25:00,820
all the low level stuff.

561
00:25:00,820 --> 00:25:02,700
All you need is just that function itself,

562
00:25:02,700 --> 00:25:04,420
and then you assign it.

563
00:25:04,420 --> 00:25:07,620
And it's so flexible for a programmer that's

564
00:25:07,620 --> 00:25:10,340
want to leverage both CPU and GPU

565
00:25:10,340 --> 00:25:12,300
and use some form of parallelization.

566
00:25:12,300 --> 00:25:15,180
This is so easy.

567
00:25:15,180 --> 00:25:17,620
It's pretty easy to use these functions.

568
00:25:17,620 --> 00:25:18,700
And that's, yeah.

569
00:25:24,020 --> 00:25:25,460
Yeah.

570
00:25:25,460 --> 00:25:27,420
So that's a great question.

571
00:25:27,420 --> 00:25:31,700
So the question was if you can use all the different functions

572
00:25:31,700 --> 00:25:32,940
that you have in NumPy, right?

573
00:25:32,940 --> 00:25:35,420
And if you can use them in PyTorch as well, or CUDA,

574
00:25:35,420 --> 00:25:37,260
you say it, right?

575
00:25:37,260 --> 00:25:37,860
Yeah.

576
00:25:37,860 --> 00:25:41,460
So yeah, so PyTorch is basically built on top of NumPy.

577
00:25:41,460 --> 00:25:44,220
So you have access to all the functionalities.

578
00:25:44,220 --> 00:25:47,420
And once you have that tensor, you can just assign it to a CUDA.

579
00:25:47,420 --> 00:25:53,860
So it's a pretty straightforward library in that sense.

580
00:25:53,860 --> 00:25:57,580
So one thing that I do notice from a lot of tutorials

581
00:25:57,580 --> 00:26:00,660
is that they teach you how to have a similar tutorial

582
00:26:00,660 --> 00:26:01,180
like this.

583
00:26:01,180 --> 00:26:02,620
You may have seen it online.

584
00:26:02,620 --> 00:26:06,060
So you have that tutorial where you do element-wise multiplication.

585
00:26:06,060 --> 00:26:07,300
You do a transport.

586
00:26:07,300 --> 00:26:08,020
That's good.

587
00:26:08,020 --> 00:26:11,500
But one of the fundamental things about this library,

588
00:26:11,500 --> 00:26:16,020
and in particular, tensors and deep learning,

589
00:26:16,020 --> 00:26:19,060
is that you will usually have a chain of operations.

590
00:26:19,060 --> 00:26:23,260
You have to think of data like in a chain of operations.

591
00:26:23,260 --> 00:26:26,460
And that concept is called competition graphs.

592
00:26:26,460 --> 00:26:29,780
So what you do is basically have a function

593
00:26:29,780 --> 00:26:32,060
being applied on top of a data structure.

594
00:26:32,420 --> 00:26:34,860
And you keep applying different functions on top of that.

595
00:26:34,860 --> 00:26:38,540
And it transforms a tensor, a data structure, into different values.

596
00:26:38,540 --> 00:26:40,300
And you keep doing this transformation.

597
00:26:40,300 --> 00:26:42,900
So the concept of chaining is super important.

598
00:26:42,900 --> 00:26:49,780
But I don't see that it's pretty like for many people that do tutorials

599
00:26:49,780 --> 00:26:53,020
and do blogs and this kind of thing, they're not focusing on that.

600
00:26:53,020 --> 00:26:58,820
I think it's really fundamental to be focused on that and how to leverage it.

601
00:26:58,820 --> 00:27:00,900
So I have a basic example here.

602
00:27:00,900 --> 00:27:03,660
If you can go through this example, you'll definitely

603
00:27:03,660 --> 00:27:09,900
understand how to chain different operations.

604
00:27:09,900 --> 00:27:12,740
And you will see how that makes sense when you're doing deep learning

605
00:27:12,740 --> 00:27:13,620
for MLP.

606
00:27:13,620 --> 00:27:16,460
Later on, you will see.

607
00:27:16,460 --> 00:27:20,020
So I just have some example here that you can go through on your own.

608
00:27:20,020 --> 00:27:23,460
We'll give you a chance to do that.

609
00:27:23,460 --> 00:27:26,980
These are just more operations, transformations.

610
00:27:26,980 --> 00:27:28,620
Yeah, and that's it for this part.

611
00:27:28,620 --> 00:27:32,820
So what I would do is basically, I don't know how you prefer.

612
00:27:32,820 --> 00:27:38,980
If we can do groups, I think it would be better if we can do groups.

613
00:27:38,980 --> 00:27:42,380
We can go or just stand up behind someone.

614
00:27:42,380 --> 00:27:45,540
And we can just quickly go through these exercises,

615
00:27:45,540 --> 00:27:50,180
just so that everyone gets something from the tutorial itself.

616
00:27:50,180 --> 00:27:53,220
So any volunteers for leaders?

617
00:27:53,220 --> 00:27:56,220
We need at least, I would say, at least seven.

618
00:27:56,220 --> 00:27:57,460
Any volunteers?

619
00:27:57,500 --> 00:28:00,180
We just quickly go through the exercise.

620
00:28:00,180 --> 00:28:00,820
Is that OK?

621
00:28:00,820 --> 00:28:05,900
Or how do you prefer we do the exercise?

622
00:28:05,900 --> 00:28:07,340
By table?

623
00:28:07,340 --> 00:28:07,940
OK.

624
00:28:07,940 --> 00:28:11,900
How much time do you think you need to finish all the exercises?

625
00:28:11,900 --> 00:28:13,380
We have, like, let's see.

626
00:28:18,380 --> 00:28:20,900
Oh, you were working on it already?

627
00:28:20,900 --> 00:28:23,740
Yeah, it'd be cool if at least everyone gets

628
00:28:23,740 --> 00:28:27,020
to do at least part of the exercise.

629
00:28:27,020 --> 00:28:31,780
Anything that you are not comfortable with here, please ask.

630
00:28:31,780 --> 00:28:33,580
And we'll go over it together.

631
00:28:33,580 --> 00:28:35,660
And I will actually provide some.

632
00:28:35,660 --> 00:28:38,500
I will go over some of the solutions also myself here,

633
00:28:38,500 --> 00:28:41,500
so that you can see on the screen.

634
00:28:41,500 --> 00:28:43,420
So maybe five minutes.

635
00:28:43,420 --> 00:28:43,940
Five minutes.

636
00:28:43,940 --> 00:28:45,900
We don't want to spend too much time on this one,

637
00:28:45,900 --> 00:28:48,700
because we have all the other core material that we need to cover.

638
00:28:48,700 --> 00:28:50,220
And we don't have a lot of time.

639
00:28:50,220 --> 00:28:51,380
So yeah, let's just start.

640
00:28:52,380 --> 00:28:54,820
Please, if you have any questions, just raise your hand,

641
00:28:54,820 --> 00:28:56,060
and I will come to you.

642
00:29:21,380 --> 00:29:21,880
OK.

643
00:29:51,380 --> 00:29:51,880
OK.

644
00:30:51,380 --> 00:30:51,880
OK.

645
00:31:21,380 --> 00:31:21,880
OK.

646
00:31:51,380 --> 00:31:51,880
OK.

647
00:32:21,380 --> 00:32:21,880
OK.

648
00:33:21,380 --> 00:33:21,880
OK.

649
00:33:51,380 --> 00:33:51,880
OK.

650
00:34:21,380 --> 00:34:21,880
OK.

651
00:34:51,380 --> 00:34:51,880
OK.

652
00:35:21,380 --> 00:35:21,880
OK.

653
00:35:51,380 --> 00:35:51,880
OK.

654
00:36:21,380 --> 00:36:21,880
OK.

655
00:36:51,380 --> 00:36:51,880
OK.

656
00:37:21,380 --> 00:37:21,880
OK.

657
00:37:51,380 --> 00:37:51,880
OK.

658
00:38:21,380 --> 00:38:21,880
OK.

659
00:38:51,380 --> 00:38:51,880
OK.

660
00:39:21,380 --> 00:39:21,880
OK.

661
00:39:51,380 --> 00:39:51,880
OK.

662
00:39:51,880 --> 00:39:52,380
OK.

663
00:39:52,380 --> 00:39:52,880
OK.

664
00:39:52,880 --> 00:39:53,380
OK.

665
00:39:53,380 --> 00:39:53,880
OK.

666
00:39:53,880 --> 00:39:54,380
OK.

667
00:39:54,380 --> 00:39:54,880
OK.

668
00:39:54,880 --> 00:39:55,380
OK.

669
00:39:55,380 --> 00:39:55,880
OK.

670
00:39:55,880 --> 00:39:56,380
OK.

671
00:39:56,380 --> 00:39:56,880
OK.

672
00:39:56,880 --> 00:39:57,380
OK.

673
00:39:57,380 --> 00:39:57,880
OK.

674
00:39:57,880 --> 00:39:58,380
OK.

675
00:39:58,380 --> 00:39:58,880
OK.

676
00:39:58,880 --> 00:39:59,380
OK.

677
00:39:59,380 --> 00:39:59,880
OK.

678
00:39:59,880 --> 00:40:00,380
OK.

679
00:40:00,380 --> 00:40:00,880
OK.

680
00:40:00,880 --> 00:40:01,380
OK.

681
00:40:01,380 --> 00:40:01,880
OK.

682
00:40:01,880 --> 00:40:02,380
OK.

683
00:40:02,380 --> 00:40:02,880
OK.

684
00:40:02,880 --> 00:40:03,380
OK.

685
00:40:03,380 --> 00:40:03,880
OK.

686
00:40:03,880 --> 00:40:04,380
OK.

687
00:40:04,380 --> 00:40:04,880
OK.

688
00:40:04,880 --> 00:40:05,380
OK.

689
00:40:05,380 --> 00:40:05,880
OK.

690
00:40:05,880 --> 00:40:06,380
OK.

691
00:40:06,380 --> 00:40:06,880
OK.

692
00:40:06,880 --> 00:40:07,380
OK.

693
00:40:07,380 --> 00:40:07,880
OK.

694
00:40:07,880 --> 00:40:08,380
OK.

695
00:40:08,380 --> 00:40:08,880
OK.

696
00:40:08,880 --> 00:40:09,380
OK.

697
00:40:09,380 --> 00:40:09,880
OK.

698
00:40:09,880 --> 00:40:10,380
OK.

699
00:40:10,380 --> 00:40:10,880
OK.

700
00:40:10,880 --> 00:40:11,380
OK.

701
00:40:11,380 --> 00:40:11,880
OK.

702
00:40:11,880 --> 00:40:12,380
OK.

703
00:40:12,380 --> 00:40:12,880
OK.

704
00:40:12,880 --> 00:40:13,380
OK.

705
00:40:13,380 --> 00:40:13,880
OK.

706
00:40:13,880 --> 00:40:14,380
OK.

707
00:40:14,380 --> 00:40:14,880
OK.

708
00:40:14,880 --> 00:40:15,380
OK.

709
00:40:15,380 --> 00:40:15,880
OK.

710
00:40:15,880 --> 00:40:16,380
OK.

711
00:40:16,380 --> 00:40:16,880
OK.

712
00:40:16,880 --> 00:40:17,380
OK.

713
00:40:17,380 --> 00:40:17,880
OK.

714
00:40:17,880 --> 00:40:18,880
OK.

715
00:40:18,880 --> 00:40:19,380
OK.

716
00:40:19,380 --> 00:40:19,880
OK.

717
00:40:19,880 --> 00:40:20,380
OK.

718
00:40:20,380 --> 00:40:20,880
OK.

719
00:40:20,880 --> 00:40:21,380
OK.

720
00:40:21,380 --> 00:40:21,880
OK.

721
00:40:21,880 --> 00:40:22,380
OK.

722
00:40:22,380 --> 00:40:22,880
OK.

723
00:40:22,880 --> 00:40:23,380
OK.

724
00:40:23,380 --> 00:40:23,880
OK.

725
00:40:23,880 --> 00:40:24,380
OK.

726
00:40:24,380 --> 00:40:24,880
OK.

727
00:40:24,880 --> 00:40:25,380
OK.

728
00:40:25,380 --> 00:40:25,880
OK.

729
00:40:25,880 --> 00:40:26,380
OK.

730
00:40:26,380 --> 00:40:26,880
OK.

731
00:40:26,880 --> 00:40:27,380
OK.

732
00:40:27,380 --> 00:40:27,880
OK.

733
00:40:27,880 --> 00:40:28,380
OK.

734
00:40:28,380 --> 00:40:28,880
OK.

735
00:40:28,880 --> 00:40:29,380
OK.

736
00:40:29,380 --> 00:40:29,880
OK.

737
00:40:29,880 --> 00:40:30,380
OK.

738
00:40:30,380 --> 00:40:30,880
OK.

739
00:40:30,880 --> 00:40:31,380
OK.

740
00:40:31,380 --> 00:40:31,880
OK.

741
00:40:31,880 --> 00:40:32,380
OK.

742
00:40:32,380 --> 00:40:32,880
OK.

743
00:40:32,880 --> 00:40:33,380
OK.

744
00:40:33,380 --> 00:40:33,880
OK.

745
00:40:33,880 --> 00:40:34,380
OK.

746
00:40:34,380 --> 00:40:34,880
OK.

747
00:40:34,880 --> 00:40:35,380
OK.

748
00:40:35,380 --> 00:40:35,880
OK.

749
00:40:35,880 --> 00:40:36,380
OK.

750
00:40:36,380 --> 00:40:36,880
OK.

751
00:40:36,880 --> 00:40:37,380
OK.

752
00:40:37,380 --> 00:40:37,880
OK.

753
00:40:37,880 --> 00:40:38,380
OK.

754
00:40:38,380 --> 00:40:38,880
OK.

755
00:40:38,880 --> 00:40:39,380
OK.

756
00:40:39,380 --> 00:40:39,880
OK.

757
00:40:39,880 --> 00:40:40,380
OK.

758
00:40:40,380 --> 00:40:40,880
OK.

759
00:40:40,880 --> 00:40:41,380
OK.

760
00:40:41,380 --> 00:40:41,880
OK.

761
00:40:41,880 --> 00:40:42,380
OK.

762
00:40:42,380 --> 00:40:42,880
OK.

763
00:40:42,880 --> 00:40:43,880
OK.

764
00:40:43,880 --> 00:40:44,380
OK.

765
00:40:44,380 --> 00:40:44,880
OK.

766
00:40:44,880 --> 00:40:45,380
OK.

767
00:40:45,380 --> 00:40:45,880
OK.

768
00:40:45,880 --> 00:40:46,880
OK.

769
00:40:46,880 --> 00:40:47,380
OK.

770
00:40:47,380 --> 00:40:47,880
OK.

771
00:40:47,880 --> 00:40:48,380
OK.

772
00:40:48,380 --> 00:40:48,880
OK.

773
00:40:48,880 --> 00:40:49,380
OK.

774
00:40:49,380 --> 00:40:49,880
OK.

775
00:40:49,880 --> 00:40:50,380
OK.

776
00:40:50,380 --> 00:40:50,880
OK.

777
00:40:50,880 --> 00:40:51,380
OK.

778
00:40:51,380 --> 00:40:51,880
OK.

779
00:40:51,880 --> 00:40:52,380
OK.

780
00:40:52,380 --> 00:40:52,880
OK.

781
00:40:52,880 --> 00:40:53,380
OK.

782
00:40:53,380 --> 00:40:53,880
OK.

783
00:40:53,880 --> 00:40:54,380
OK.

784
00:40:54,380 --> 00:40:54,880
OK.

785
00:40:54,880 --> 00:40:55,380
OK.

786
00:40:55,380 --> 00:40:55,880
OK.

787
00:40:55,880 --> 00:40:56,380
OK.

788
00:40:56,380 --> 00:40:56,880
OK.

789
00:40:56,880 --> 00:40:57,380
OK.

790
00:40:57,380 --> 00:40:57,880
OK.

791
00:40:57,880 --> 00:40:58,380
OK.

792
00:40:58,380 --> 00:40:58,880
OK.

793
00:40:58,880 --> 00:40:59,380
OK.

794
00:40:59,380 --> 00:40:59,880
OK.

795
00:40:59,880 --> 00:41:00,380
OK.

796
00:41:00,380 --> 00:41:00,880
OK.

797
00:41:00,880 --> 00:41:01,380
OK.

798
00:41:01,380 --> 00:41:01,880
OK.

799
00:41:01,880 --> 00:41:02,380
OK.

800
00:41:02,380 --> 00:41:02,880
OK.

801
00:41:02,880 --> 00:41:03,380
OK.

802
00:41:03,380 --> 00:41:03,880
OK.

803
00:41:03,880 --> 00:41:04,380
OK.

804
00:41:04,380 --> 00:41:04,880
OK.

805
00:41:04,880 --> 00:41:05,380
OK.

806
00:41:05,380 --> 00:41:05,880
OK.

807
00:41:05,880 --> 00:41:06,380
OK.

808
00:41:06,380 --> 00:41:06,880
OK.

809
00:41:06,880 --> 00:41:07,380
OK.

810
00:41:07,380 --> 00:41:07,880
OK.

811
00:41:07,880 --> 00:41:08,380
OK.

812
00:41:08,380 --> 00:41:08,880
OK.

813
00:41:08,880 --> 00:41:09,380
OK.

814
00:41:09,380 --> 00:41:09,880
OK.

815
00:41:09,880 --> 00:41:10,380
OK.

816
00:41:10,380 --> 00:41:10,880
OK.

817
00:41:10,880 --> 00:41:11,380
OK.

818
00:41:11,380 --> 00:41:12,380
OK.

819
00:41:12,380 --> 00:41:12,880
OK.

820
00:41:12,880 --> 00:41:13,380
OK.

821
00:41:13,380 --> 00:41:13,880
OK.

822
00:41:13,880 --> 00:41:14,380
OK.

823
00:41:14,380 --> 00:41:15,380
OK.

824
00:41:15,380 --> 00:41:15,880
OK.

825
00:41:15,880 --> 00:41:16,380
OK.

826
00:41:16,380 --> 00:41:16,880
OK.

827
00:41:16,880 --> 00:41:17,380
OK.

828
00:41:17,380 --> 00:41:17,880
OK.

829
00:41:17,880 --> 00:41:18,380
OK.

830
00:41:18,380 --> 00:41:18,880
OK.

831
00:41:18,880 --> 00:41:19,380
OK.

832
00:41:19,380 --> 00:41:19,880
OK.

833
00:41:19,880 --> 00:41:20,380
OK.

834
00:41:20,380 --> 00:41:20,880
OK.

835
00:41:20,880 --> 00:41:21,380
OK.

836
00:41:21,380 --> 00:41:21,880
OK.

837
00:41:21,880 --> 00:41:22,380
OK.

838
00:41:22,380 --> 00:41:22,880
OK.

839
00:41:22,880 --> 00:41:23,380
OK.

840
00:41:23,380 --> 00:41:23,880
OK.

841
00:41:23,880 --> 00:41:24,380
OK.

842
00:41:24,380 --> 00:41:24,880
OK.

843
00:41:24,880 --> 00:41:25,380
OK.

844
00:41:25,380 --> 00:41:25,880
OK.

845
00:41:25,880 --> 00:41:26,380
OK.

846
00:41:26,380 --> 00:41:26,880
OK.

847
00:41:26,880 --> 00:41:27,380
OK.

848
00:41:27,380 --> 00:41:27,880
OK.

849
00:41:27,880 --> 00:41:28,380
OK.

850
00:41:28,380 --> 00:41:28,880
OK.

851
00:41:28,880 --> 00:41:29,380
OK.

852
00:41:29,380 --> 00:41:29,880
OK.

853
00:41:29,880 --> 00:41:30,380
OK.

854
00:41:30,380 --> 00:41:30,880
OK.

855
00:41:30,880 --> 00:41:31,380
OK.

856
00:41:31,380 --> 00:41:31,880
OK.

857
00:41:31,880 --> 00:41:32,380
OK.

858
00:41:32,380 --> 00:41:32,880
OK.

859
00:41:32,880 --> 00:41:33,380
OK.

860
00:41:33,380 --> 00:41:33,880
OK.

861
00:41:33,880 --> 00:41:34,380
OK.

862
00:41:34,380 --> 00:41:34,880
OK.

863
00:41:34,880 --> 00:41:35,380
OK.

864
00:41:35,380 --> 00:41:35,880
OK.

865
00:41:35,880 --> 00:41:36,380
OK.

866
00:41:36,380 --> 00:41:36,880
OK.

867
00:41:36,880 --> 00:41:37,380
OK.

868
00:41:37,380 --> 00:41:37,880
OK.

869
00:41:37,880 --> 00:41:38,380
OK.

870
00:41:38,380 --> 00:41:38,880
OK.

871
00:41:38,880 --> 00:41:39,380
OK.

872
00:41:39,380 --> 00:41:39,880
OK.

873
00:41:39,880 --> 00:41:40,880
OK.

874
00:41:40,880 --> 00:41:41,380
OK.

875
00:41:41,380 --> 00:41:41,880
OK.

876
00:41:41,880 --> 00:41:42,380
OK.

877
00:41:42,380 --> 00:41:42,880
OK.

878
00:41:42,880 --> 00:41:43,880
OK.

879
00:41:43,880 --> 00:41:44,380
OK.

880
00:41:44,380 --> 00:41:44,880
OK.

881
00:41:44,880 --> 00:41:45,380
OK.

882
00:41:45,380 --> 00:41:45,880
OK.

883
00:41:45,880 --> 00:41:46,380
OK.

884
00:41:46,380 --> 00:41:46,880
OK.

885
00:41:46,880 --> 00:41:47,380
OK.

886
00:41:47,380 --> 00:41:47,880
OK.

887
00:41:47,880 --> 00:41:48,380
OK.

888
00:41:48,380 --> 00:41:48,880
OK.

889
00:41:48,880 --> 00:41:49,380
OK.

890
00:41:49,380 --> 00:41:49,880
OK.

891
00:41:49,880 --> 00:41:50,380
OK.

892
00:41:50,380 --> 00:41:50,880
OK.

893
00:41:50,880 --> 00:41:51,380
OK.

894
00:41:51,380 --> 00:41:51,880
OK.

895
00:41:51,880 --> 00:41:52,380
OK.

896
00:41:52,380 --> 00:41:52,880
OK.

897
00:41:52,880 --> 00:41:53,380
OK.

898
00:41:53,380 --> 00:41:53,880
OK.

899
00:41:53,880 --> 00:41:54,380
OK.

900
00:41:54,380 --> 00:41:54,880
OK.

901
00:41:54,880 --> 00:41:55,380
OK.

902
00:41:55,380 --> 00:41:55,880
OK.

903
00:41:55,880 --> 00:41:56,380
OK.

904
00:41:56,380 --> 00:41:56,880
OK.

905
00:41:56,880 --> 00:41:57,380
OK.

906
00:41:57,380 --> 00:41:57,880
OK.

907
00:41:57,880 --> 00:41:58,380
OK.

908
00:41:58,380 --> 00:41:58,880
OK.

909
00:41:58,880 --> 00:41:59,380
OK.

910
00:41:59,380 --> 00:41:59,880
OK.

911
00:41:59,880 --> 00:42:00,380
OK.

912
00:42:00,380 --> 00:42:00,880
OK.

913
00:42:00,880 --> 00:42:01,380
OK.

914
00:42:01,380 --> 00:42:01,880
OK.

915
00:42:01,880 --> 00:42:02,380
OK.

916
00:42:02,380 --> 00:42:02,880
OK.

917
00:42:02,880 --> 00:42:03,380
OK.

918
00:42:03,380 --> 00:42:03,880
OK.

919
00:42:03,880 --> 00:42:04,380
OK.

920
00:42:04,380 --> 00:42:04,880
OK.

921
00:42:04,880 --> 00:42:05,380
OK.

922
00:42:05,380 --> 00:42:05,880
OK.

923
00:42:05,880 --> 00:42:06,380
OK.

924
00:42:06,380 --> 00:42:06,880
OK.

925
00:42:06,880 --> 00:42:07,380
OK.

926
00:42:07,380 --> 00:42:07,880
OK.

927
00:42:07,880 --> 00:42:08,380
OK.

928
00:42:08,380 --> 00:42:09,380
OK.

929
00:42:09,380 --> 00:42:09,880
OK.

930
00:42:09,880 --> 00:42:10,380
OK.

931
00:42:10,380 --> 00:42:10,880
OK.

932
00:42:10,880 --> 00:42:11,380
OK.

933
00:42:11,380 --> 00:42:12,380
OK.

934
00:42:12,380 --> 00:42:12,880
OK.

935
00:42:12,880 --> 00:42:13,380
OK.

936
00:42:13,380 --> 00:42:13,880
OK.

937
00:42:13,880 --> 00:42:14,380
OK.

938
00:42:14,380 --> 00:42:14,880
OK.

939
00:42:14,880 --> 00:42:15,380
OK.

940
00:42:15,380 --> 00:42:15,880
OK.

941
00:42:15,880 --> 00:42:16,380
OK.

942
00:42:16,380 --> 00:42:16,880
OK.

943
00:42:16,880 --> 00:42:17,380
OK.

944
00:42:17,380 --> 00:42:17,880
OK.

945
00:42:17,880 --> 00:42:18,380
OK.

946
00:42:18,380 --> 00:42:18,880
OK.

947
00:42:18,880 --> 00:42:19,380
OK.

948
00:42:19,380 --> 00:42:19,880
OK.

949
00:42:19,880 --> 00:42:20,380
OK.

950
00:42:20,380 --> 00:42:20,880
OK.

951
00:42:20,880 --> 00:42:21,380
OK.

952
00:42:21,380 --> 00:42:21,880
OK.

953
00:42:21,880 --> 00:42:22,380
OK.

954
00:42:22,380 --> 00:42:22,880
OK.

955
00:42:22,880 --> 00:42:23,380
OK.

956
00:42:23,380 --> 00:42:23,880
OK.

957
00:42:23,880 --> 00:42:24,380
OK.

958
00:42:24,380 --> 00:42:24,880
OK.

959
00:42:24,880 --> 00:42:25,380
OK.

960
00:42:25,380 --> 00:42:25,880
OK.

961
00:42:25,880 --> 00:42:26,380
OK.

962
00:42:26,380 --> 00:42:26,880
OK.

963
00:42:26,880 --> 00:42:27,380
OK.

964
00:42:27,380 --> 00:42:27,880
OK.

965
00:42:27,880 --> 00:42:28,380
OK.

966
00:42:28,380 --> 00:42:28,880
OK.

967
00:42:28,880 --> 00:42:29,380
OK.

968
00:42:29,380 --> 00:42:29,880
OK.

969
00:42:29,880 --> 00:42:30,380
OK.

970
00:42:30,380 --> 00:42:30,880
OK.

971
00:42:30,880 --> 00:42:31,380
OK.

972
00:42:31,380 --> 00:42:31,880
OK.

973
00:42:31,880 --> 00:42:32,380
OK.

974
00:42:32,380 --> 00:42:32,880
OK.

975
00:42:32,880 --> 00:42:33,380
OK.

976
00:42:33,380 --> 00:42:33,880
OK.

977
00:42:33,880 --> 00:42:34,380
OK.

978
00:42:34,380 --> 00:42:34,880
OK.

979
00:42:34,880 --> 00:42:35,380
OK.

980
00:42:35,380 --> 00:42:35,880
OK.

981
00:42:35,880 --> 00:42:36,380
OK.

982
00:42:36,380 --> 00:42:36,880
OK.

983
00:42:36,880 --> 00:42:37,880
OK.

984
00:42:37,880 --> 00:42:38,380
OK.

985
00:42:38,380 --> 00:42:38,880
OK.

986
00:42:38,880 --> 00:42:39,380
OK.

987
00:42:39,380 --> 00:42:39,880
OK.

988
00:42:39,880 --> 00:42:40,880
OK.

989
00:42:40,880 --> 00:42:41,380
OK.

990
00:42:41,380 --> 00:42:41,880
OK.

991
00:42:41,880 --> 00:42:42,380
OK.

992
00:42:42,380 --> 00:42:42,880
OK.

993
00:42:42,880 --> 00:42:43,380
OK.

994
00:42:43,380 --> 00:42:43,880
OK.

995
00:42:43,880 --> 00:42:44,380
OK.

996
00:42:44,380 --> 00:42:44,880
OK.

997
00:42:44,880 --> 00:42:45,380
OK.

998
00:42:45,380 --> 00:42:45,880
OK.

999
00:42:45,880 --> 00:42:46,380
OK.

1000
00:42:46,380 --> 00:42:46,880
OK.

1001
00:42:46,880 --> 00:42:47,380
OK.

1002
00:42:47,380 --> 00:42:47,880
OK.

1003
00:42:47,880 --> 00:42:48,380
OK.

1004
00:42:48,380 --> 00:42:48,880
OK.

1005
00:42:48,880 --> 00:42:49,380
OK.

1006
00:42:49,380 --> 00:42:49,880
OK.

1007
00:42:49,880 --> 00:42:50,380
OK.

1008
00:42:50,380 --> 00:42:50,880
OK.

1009
00:42:50,880 --> 00:42:51,380
OK.

1010
00:42:51,380 --> 00:42:51,880
OK.

1011
00:42:51,880 --> 00:42:52,380
OK.

1012
00:42:52,380 --> 00:42:52,880
OK.

1013
00:42:52,880 --> 00:42:53,380
OK.

1014
00:42:53,380 --> 00:42:53,880
OK.

1015
00:42:53,880 --> 00:42:54,380
OK.

1016
00:42:54,380 --> 00:42:54,880
OK.

1017
00:42:54,880 --> 00:42:55,380
OK.

1018
00:42:55,380 --> 00:42:55,880
OK.

1019
00:42:55,880 --> 00:42:56,380
OK.

1020
00:42:56,380 --> 00:42:56,880
OK.

1021
00:42:56,880 --> 00:42:57,380
OK.

1022
00:42:57,380 --> 00:42:57,880
OK.

1023
00:42:57,880 --> 00:42:58,380
OK.

1024
00:42:58,380 --> 00:42:58,880
OK.

1025
00:42:58,880 --> 00:42:59,380
OK.

1026
00:42:59,380 --> 00:42:59,880
OK.

1027
00:42:59,880 --> 00:43:00,380
OK.

1028
00:43:00,380 --> 00:43:00,880
OK.

1029
00:43:00,880 --> 00:43:01,380
OK.

1030
00:43:01,380 --> 00:43:01,880
OK.

1031
00:43:01,880 --> 00:43:02,380
OK.

1032
00:43:02,380 --> 00:43:02,880
OK.

1033
00:43:02,880 --> 00:43:03,380
OK.

1034
00:43:03,380 --> 00:43:03,880
OK.

1035
00:43:03,880 --> 00:43:04,380
OK.

1036
00:43:04,380 --> 00:43:04,880
OK.

1037
00:43:04,880 --> 00:43:05,380
OK.

1038
00:43:05,380 --> 00:43:06,380
OK.

1039
00:43:06,380 --> 00:43:06,880
OK.

1040
00:43:06,880 --> 00:43:07,380
OK.

1041
00:43:07,380 --> 00:43:07,880
OK.

1042
00:43:07,880 --> 00:43:08,380
OK.

1043
00:43:08,380 --> 00:43:09,380
OK.

1044
00:43:09,380 --> 00:43:09,880
OK.

1045
00:43:09,880 --> 00:43:10,380
OK.

1046
00:43:10,380 --> 00:43:10,880
OK.

1047
00:43:10,880 --> 00:43:11,380
OK.

1048
00:43:11,380 --> 00:43:11,880
OK.

1049
00:43:11,880 --> 00:43:12,880
OK.

1050
00:43:12,880 --> 00:43:13,380
OK.

1051
00:43:13,380 --> 00:43:13,880
OK.

1052
00:43:13,880 --> 00:43:14,380
OK.

1053
00:43:14,380 --> 00:43:15,380
OK.

1054
00:43:15,380 --> 00:43:15,880
OK.

1055
00:43:15,880 --> 00:43:16,380
OK.

1056
00:43:16,380 --> 00:43:16,880
OK.

1057
00:43:16,880 --> 00:43:17,380
OK.

1058
00:43:17,380 --> 00:43:17,880
OK.

1059
00:43:17,880 --> 00:43:18,380
OK.

1060
00:43:18,380 --> 00:43:18,880
OK.

1061
00:43:18,880 --> 00:43:19,380
OK.

1062
00:43:19,380 --> 00:43:19,880
OK.

1063
00:43:19,880 --> 00:43:20,380
Ok.

1064
00:43:20,380 --> 00:43:20,880
OK.

1065
00:43:20,880 --> 00:43:21,380
OK.

1066
00:43:21,380 --> 00:43:21,880
OK.

1067
00:43:21,880 --> 00:43:22,380
OK.

1068
00:43:22,380 --> 00:43:23,380
OK.

1069
00:43:23,380 --> 00:43:34,380
So we'll try to do it.

1070
00:43:34,380 --> 00:43:37,880
Otherwise, we wouldn't be able to finish today.

1071
00:43:37,880 --> 00:43:41,960
So I hope everyone got a little bit of a taste of what

1072
00:43:41,960 --> 00:43:44,420
PyTorch is about, and you understand

1073
00:43:44,420 --> 00:43:46,440
some of the strengths of it.

1074
00:43:46,440 --> 00:43:48,360
That's basically the idea of the tutorial,

1075
00:43:48,360 --> 00:43:50,520
just for you to familiarize yourself.

1076
00:43:50,520 --> 00:43:53,360
You don't need to know everything in this notebook.

1077
00:43:53,360 --> 00:43:54,200
It's not important.

1078
00:43:54,200 --> 00:43:57,080
As we dive deeper, you will see that these are just

1079
00:43:57,080 --> 00:43:58,680
basic operations that we're doing.

1080
00:43:58,680 --> 00:44:01,000
But you won't need all of this.

1081
00:44:01,000 --> 00:44:02,720
There are more important operations

1082
00:44:02,720 --> 00:44:04,840
that you would have to familiarize yourself with,

1083
00:44:04,840 --> 00:44:07,560
but just the idea of PyTorch.

1084
00:44:07,560 --> 00:44:08,400
So let's just move on.

1085
00:44:12,280 --> 00:44:17,280
OK, so we have part two, which is what I call getting started

1086
00:44:17,280 --> 00:44:18,440
with PyTorch.

1087
00:44:18,440 --> 00:44:23,640
So the reason why the previous tutorial isn't the starting

1088
00:44:23,640 --> 00:44:30,640
point in PyTorch, in my opinion, is that PyTorch is usually,

1089
00:44:30,640 --> 00:44:32,800
the strength of it is not really the tensor operation.

1090
00:44:32,800 --> 00:44:33,960
The tensor operation is basically

1091
00:44:33,960 --> 00:44:36,320
what you get from non-Py, what you get from TensorFlow,

1092
00:44:36,320 --> 00:44:37,880
and all these other libraries.

1093
00:44:37,880 --> 00:44:40,360
I think one of the strengths of PyTorch,

1094
00:44:40,360 --> 00:44:43,800
initially when it was built, was that it has this thing called

1095
00:44:43,800 --> 00:44:49,600
automatic differentiation, which is just a very complex concept

1096
00:44:49,600 --> 00:44:54,000
for what we do in deep learning, which is just having

1097
00:44:54,000 --> 00:44:57,680
a bunch of operations, doing some forward propagation,

1098
00:44:57,680 --> 00:44:59,200
and doing some backward propagation,

1099
00:44:59,200 --> 00:45:00,840
if you're familiar with those terms,

1100
00:45:00,840 --> 00:45:03,080
and you will get some output in the end.

1101
00:45:03,080 --> 00:45:04,880
So that's just basically the idea

1102
00:45:04,880 --> 00:45:07,200
of the automatic differentiation, which

1103
00:45:07,200 --> 00:45:09,280
includes the backward process.

1104
00:45:09,280 --> 00:45:14,160
So here, just to make this concept a lot much easier

1105
00:45:14,160 --> 00:45:16,720
for someone that's beginning to use PyTorch, what I did

1106
00:45:16,720 --> 00:45:18,720
is I captured this from a book.

1107
00:45:18,720 --> 00:45:19,880
I don't remember which book.

1108
00:45:19,880 --> 00:45:21,400
I'll put the source later on.

1109
00:45:21,400 --> 00:45:26,280
But I have, so there are a lot of these type of, like,

1110
00:45:26,280 --> 00:45:26,780
graph.

1111
00:45:26,780 --> 00:45:31,120
But I like this one a lot, because it actually shows,

1112
00:45:31,120 --> 00:45:33,680
we have a tensor here, a scalar.

1113
00:45:33,680 --> 00:45:35,440
And you see some transformation being done.

1114
00:45:35,440 --> 00:45:36,200
It's pretty clean.

1115
00:45:36,200 --> 00:45:36,800
I like this.

1116
00:45:36,800 --> 00:45:39,040
I use it for all my presentations.

1117
00:45:39,040 --> 00:45:42,920
But it shows exactly what is going on at each step, right?

1118
00:45:42,920 --> 00:45:44,520
So if you understand the basic math,

1119
00:45:44,520 --> 00:45:47,800
how to do the partial derivatives of a variable,

1120
00:45:47,800 --> 00:45:50,960
you will see that that's the answer.

1121
00:45:50,960 --> 00:45:54,880
So what I did was, so in the book, you only see this.

1122
00:45:54,880 --> 00:45:57,320
And they explain what it is, what the concept is.

1123
00:45:57,320 --> 00:45:59,040
But I would just take it a step forward

1124
00:45:59,040 --> 00:46:01,440
and actually implement it using PyTorch.

1125
00:46:01,440 --> 00:46:03,640
So what I did is just implement it here.

1126
00:46:03,640 --> 00:46:05,320
You have some implementation here

1127
00:46:05,320 --> 00:46:07,360
if you have access to the notebook.

1128
00:46:07,360 --> 00:46:09,760
It's a very simple implementation.

1129
00:46:09,760 --> 00:46:14,480
But it's, how should I say, it's pretty powerful.

1130
00:46:14,480 --> 00:46:16,600
When you actually implement all these lines of code,

1131
00:46:16,600 --> 00:46:19,960
you will see exactly what the strength of this tool is,

1132
00:46:19,960 --> 00:46:24,760
which is all the other stuff, all the crazy stuff going on

1133
00:46:24,760 --> 00:46:28,040
behind the neural network is being done automatically,

1134
00:46:28,040 --> 00:46:29,240
behind the scenes.

1135
00:46:29,240 --> 00:46:34,360
And all you need to do is just make an explicit line

1136
00:46:34,360 --> 00:46:36,640
like that, and you should be able to do

1137
00:46:36,640 --> 00:46:39,000
a lot of different operations behind the scenes.

1138
00:46:39,000 --> 00:46:42,480
So I can go on and on and talk about what the strengths are

1139
00:46:42,480 --> 00:46:47,200
and what are some of the wonderful parts of PyTorch.

1140
00:46:47,200 --> 00:46:50,080
But the idea of this tutorial will

1141
00:46:50,080 --> 00:46:54,200
be just to make it explicit, try to build your own graph,

1142
00:46:54,200 --> 00:46:57,200
your own competition graph, and you get the idea of what it does.

1143
00:46:57,200 --> 00:46:58,580
And from there, I think you'll be

1144
00:46:58,580 --> 00:47:01,580
able to extend this into a neural network

1145
00:47:01,580 --> 00:47:06,260
and even to a deep learning model.

1146
00:47:06,260 --> 00:47:08,420
So I have some example here.

1147
00:47:08,420 --> 00:47:10,860
And then there's an exercise here

1148
00:47:10,860 --> 00:47:14,700
in which you compute some derivatives of some variables.

1149
00:47:14,700 --> 00:47:17,420
Don't pay too much attention to this complicated stuff

1150
00:47:17,420 --> 00:47:19,300
if you're not pretty familiar with math.

1151
00:47:19,300 --> 00:47:21,500
And so I think it's pretty easy to understand.

1152
00:47:21,500 --> 00:47:23,620
Once you go through the code and you code it yourself,

1153
00:47:23,620 --> 00:47:25,140
you will see what it does.

1154
00:47:25,140 --> 00:47:27,540
And these figures really help out a lot.

1155
00:47:31,940 --> 00:47:34,660
So each component, each important component,

1156
00:47:34,660 --> 00:47:36,460
so we know that the most important component,

1157
00:47:36,460 --> 00:47:38,860
as I mentioned, is a competition graph.

1158
00:47:38,860 --> 00:47:41,700
But every competition graph has to have an input.

1159
00:47:41,700 --> 00:47:44,820
So in this case, it's just a scalar.

1160
00:47:44,820 --> 00:47:47,380
And you will see that I have some block here

1161
00:47:47,380 --> 00:47:49,580
talking about what the input is and have

1162
00:47:49,580 --> 00:47:51,180
some explanation in your time.

1163
00:47:51,180 --> 00:47:54,620
You can always go through the brief explanations

1164
00:47:54,620 --> 00:47:55,500
I put there.

1165
00:47:55,500 --> 00:47:57,780
And you have an explanation here that says, OK,

1166
00:47:57,780 --> 00:47:59,460
in order for you to have a sample,

1167
00:47:59,460 --> 00:48:01,340
you need to have some kind of dimension.

1168
00:48:01,340 --> 00:48:05,060
And usually in NLP, when you're doing NLP deep learning models,

1169
00:48:05,060 --> 00:48:06,980
you have some kind of dimension.

1170
00:48:06,980 --> 00:48:10,180
So you have a batch size, max sequence length,

1171
00:48:10,180 --> 00:48:11,740
times the vocabulary size.

1172
00:48:11,740 --> 00:48:13,180
So that's usually what you use.

1173
00:48:13,180 --> 00:48:15,380
In computer vision, you use something different,

1174
00:48:15,380 --> 00:48:16,260
some different size.

1175
00:48:16,260 --> 00:48:19,180
If you use videos, it's completely different dimensions

1176
00:48:19,180 --> 00:48:20,060
you're dealing with.

1177
00:48:20,060 --> 00:48:23,060
So that's just the point of this full lines of code

1178
00:48:23,060 --> 00:48:25,900
here, just to show that you can create some random information

1179
00:48:25,900 --> 00:48:27,180
of that size.

1180
00:48:27,180 --> 00:48:30,220
And you can use the properties to get access to it,

1181
00:48:30,220 --> 00:48:33,260
just to see what the size of it is.

1182
00:48:33,260 --> 00:48:37,340
And you can do some sample also, just with random information.

1183
00:48:37,340 --> 00:48:38,620
You can reduce it to 100.

1184
00:48:38,620 --> 00:48:41,060
You don't really need to do the 10,000.

1185
00:48:41,060 --> 00:48:44,220
So you have that there also.

1186
00:48:44,220 --> 00:48:48,740
And the point was that in order for you to have some kind of,

1187
00:48:48,740 --> 00:48:51,740
usually in NLP, what we do is you

1188
00:48:51,740 --> 00:48:54,820
don't want to work with those dimensions above 10,000.

1189
00:48:54,820 --> 00:48:56,060
That's going to be crazy.

1190
00:48:56,060 --> 00:48:58,180
That's going to be too intensive for your CPU

1191
00:48:58,180 --> 00:49:00,020
if you only have a CPU.

1192
00:49:00,020 --> 00:49:01,980
So what you usually do is we want to reduce that

1193
00:49:01,980 --> 00:49:03,460
to something like 100.

1194
00:49:03,460 --> 00:49:08,740
So with that idea and that need came these things

1195
00:49:08,740 --> 00:49:10,260
called word embeddings.

1196
00:49:10,260 --> 00:49:13,180
So now we're not really dealing with that 10,000 dimension.

1197
00:49:13,180 --> 00:49:16,300
Now we need something like 100 dimensions.

1198
00:49:16,300 --> 00:49:18,620
So if you have heard about word embeddings,

1199
00:49:18,620 --> 00:49:20,540
that's the whole idea behind it, just

1200
00:49:20,540 --> 00:49:23,500
to have something more efficient to work with.

1201
00:49:23,500 --> 00:49:26,820
I explained it there in some paragraphs.

1202
00:49:26,820 --> 00:49:29,140
And I just gave some exercise here

1203
00:49:29,140 --> 00:49:32,860
on how to the kind of things that you need.

1204
00:49:32,860 --> 00:49:34,520
You need to use something like permute.

1205
00:49:34,520 --> 00:49:37,860
Sometimes you will need to change.

1206
00:49:37,860 --> 00:49:42,660
Usually some models would want or require a certain dimension.

1207
00:49:42,660 --> 00:49:45,020
So usually sometimes they would have something

1208
00:49:45,020 --> 00:49:47,620
like SequenceNet at the beginning and then Bash Size.

1209
00:49:47,620 --> 00:49:50,860
So we need to transform the tensor itself.

1210
00:49:50,860 --> 00:49:52,420
So I would use something like permute.

1211
00:49:52,420 --> 00:49:55,180
But there are other functions you can use as well.

1212
00:49:55,180 --> 00:49:56,900
So that's just a method.

1213
00:49:56,900 --> 00:49:59,740
And then so we have the competition graph.

1214
00:49:59,740 --> 00:50:00,860
We have the input.

1215
00:50:00,860 --> 00:50:03,540
Then there's something called linear transformation.

1216
00:50:03,540 --> 00:50:04,300
So what is that?

1217
00:50:04,300 --> 00:50:08,980
Linear transformation is just I think we saw a bit of it here.

1218
00:50:08,980 --> 00:50:09,900
Actually not.

1219
00:50:09,900 --> 00:50:11,780
That's a very simple figure.

1220
00:50:11,780 --> 00:50:13,980
So linear transformation is something like this.

1221
00:50:13,980 --> 00:50:15,580
So there's a mat here.

1222
00:50:15,580 --> 00:50:18,940
It's a weight times the input plus bias.

1223
00:50:18,940 --> 00:50:24,340
So that's kind of the basic component of a neural network.

1224
00:50:24,340 --> 00:50:28,140
And how do we put this mat into code?

1225
00:50:28,140 --> 00:50:30,580
So you can imagine there's a lot of things

1226
00:50:30,580 --> 00:50:31,420
that you need to know.

1227
00:50:31,420 --> 00:50:36,620
But basically what we're doing is we just need some input.

1228
00:50:36,620 --> 00:50:38,140
And I have the input here already.

1229
00:50:38,140 --> 00:50:39,980
I just specify my input.

1230
00:50:39,980 --> 00:50:41,580
And then I have some transformation

1231
00:50:41,580 --> 00:50:43,580
that I want to do.

1232
00:50:43,580 --> 00:50:45,700
So I have my input, and I want to transform it

1233
00:50:45,700 --> 00:50:48,500
for it to give some y value.

1234
00:50:48,500 --> 00:50:51,460
And the Wx plus b is actually encapsulated

1235
00:50:51,460 --> 00:50:55,420
in this object here.

1236
00:50:55,420 --> 00:50:58,380
So all you need to do is just pass in your dimensions.

1237
00:50:58,380 --> 00:51:00,620
And all of that is taken care of.

1238
00:51:00,620 --> 00:51:03,020
By the way, when you do something like this, what you do

1239
00:51:03,020 --> 00:51:04,500
is you have to initialize.

1240
00:51:04,500 --> 00:51:06,340
If you are familiar with neural networks,

1241
00:51:06,340 --> 00:51:07,620
you have to initialize your weights.

1242
00:51:07,620 --> 00:51:08,940
You have to initialize your bias.

1243
00:51:08,940 --> 00:51:11,420
And then you have to do the transformation later on.

1244
00:51:11,420 --> 00:51:14,620
But with these objects here, with these classes,

1245
00:51:14,620 --> 00:51:17,620
you already have those things going on behind the scenes.

1246
00:51:17,620 --> 00:51:19,420
So you don't need to explicitly do it.

1247
00:51:19,420 --> 00:51:21,660
All you need to do, or you'll find out,

1248
00:51:21,660 --> 00:51:23,180
all you need to do is you just need

1249
00:51:23,180 --> 00:51:24,420
to specify the dimensions.

1250
00:51:24,420 --> 00:51:25,900
Because it's the same transformation

1251
00:51:25,900 --> 00:51:27,020
you need to do anyways.

1252
00:51:27,020 --> 00:51:30,580
So you don't need to explicitly say that.

1253
00:51:30,580 --> 00:51:31,780
So you have an output here.

1254
00:51:34,340 --> 00:51:36,060
And then there's nonlinear transformation,

1255
00:51:36,060 --> 00:51:40,220
which is what's called the activation functions.

1256
00:51:40,220 --> 00:51:42,100
So after you have some linear transformation,

1257
00:51:42,100 --> 00:51:46,580
you want to apply some nonlinear transformation on that output.

1258
00:51:46,580 --> 00:51:49,220
And so you have a different function for that.

1259
00:51:49,220 --> 00:51:50,900
Usually we use sigmoids.

1260
00:51:50,900 --> 00:51:54,300
We can use tanh, ReLU, all those concepts in neural networks.

1261
00:51:54,300 --> 00:51:57,460
I'm not really going to go deep into those concepts.

1262
00:51:57,460 --> 00:52:01,100
I think you can read about them later on.

1263
00:52:01,100 --> 00:52:02,680
It's not so important in this tutorial.

1264
00:52:05,780 --> 00:52:07,940
OK, so any questions so far?

1265
00:52:07,940 --> 00:52:09,180
Anything that you would like?

1266
00:52:09,180 --> 00:52:09,700
Yeah.

1267
00:52:09,700 --> 00:52:11,180
You used the variable fc.

1268
00:52:14,260 --> 00:52:15,220
Yes.

1269
00:52:15,220 --> 00:52:16,780
Yes.

1270
00:52:16,780 --> 00:52:19,060
So yeah, so the idea is just that it's a fully connected

1271
00:52:19,060 --> 00:52:19,580
layer.

1272
00:52:19,580 --> 00:52:22,100
And you would achieve that just using that line of code.

1273
00:52:29,940 --> 00:52:31,500
I didn't get the question, sorry.

1274
00:52:31,500 --> 00:52:32,980
How does it match the dimension?

1275
00:52:32,980 --> 00:52:36,940
How does it match the dimension?

1276
00:52:36,940 --> 00:52:37,420
Oh, yeah.

1277
00:52:37,420 --> 00:52:38,780
Why do you like to do that?

1278
00:52:38,780 --> 00:52:39,280
Oh, yeah.

1279
00:52:39,280 --> 00:52:43,020
So usually what you do is that's something.

1280
00:52:43,020 --> 00:52:45,020
So I struggled with this when I was initially

1281
00:52:45,020 --> 00:52:47,620
learning PyTorch.

1282
00:52:47,620 --> 00:52:50,140
That's one of the things that I think

1283
00:52:50,140 --> 00:52:52,460
was lacking at the beginning.

1284
00:52:52,460 --> 00:52:55,220
You didn't know which dimension you needed to put first,

1285
00:52:55,220 --> 00:52:58,260
or what dimension goes second, and so forth.

1286
00:52:58,260 --> 00:53:00,900
So usually what you do is you have

1287
00:53:01,020 --> 00:53:04,580
So usually what you do is, since it's a chain of operations,

1288
00:53:04,580 --> 00:53:06,700
usually what you do is you put the last dimension.

1289
00:53:06,700 --> 00:53:08,260
If you don't know which size to put,

1290
00:53:08,260 --> 00:53:11,220
you would put the last dimension coming in.

1291
00:53:11,220 --> 00:53:15,140
That would be the first here for the fully connected layer.

1292
00:53:15,140 --> 00:53:16,820
So when you're designing neural networks,

1293
00:53:16,820 --> 00:53:20,580
that's something that you need to be aware of.

1294
00:53:20,580 --> 00:53:23,820
That if you put a different size there,

1295
00:53:23,820 --> 00:53:24,900
it will be incompatible.

1296
00:53:24,900 --> 00:53:26,940
And usually it throws an error.

1297
00:53:26,940 --> 00:53:28,620
So yes.

1298
00:53:30,900 --> 00:53:35,340
So you could put the one that's the second size

1299
00:53:35,340 --> 00:53:39,340
tied into the same parameter.

1300
00:53:39,340 --> 00:53:41,980
So it's already encapsulated using this one,

1301
00:53:41,980 --> 00:53:45,220
using the linear object here.

1302
00:53:45,220 --> 00:53:47,420
It's already packaged.

1303
00:53:47,420 --> 00:53:49,540
But if you wanted to do it explicitly, then you

1304
00:53:49,540 --> 00:53:54,340
would have to do it on your own, like create your weights.

1305
00:53:54,340 --> 00:53:57,900
So I just put a layer that's using the linear space here.

1306
00:53:57,900 --> 00:53:59,380
Yeah.

1307
00:53:59,380 --> 00:54:04,860
So it's set to 90% by putting a 10% to it's call

1308
00:54:04,860 --> 00:54:09,340
where you're including the key.

1309
00:54:09,340 --> 00:54:11,220
That's why I tried to explain.

1310
00:54:11,220 --> 00:54:14,420
That it's already done behind the scenes for you.

1311
00:54:14,420 --> 00:54:17,300
It already knows which dimension, what's the size of it.

1312
00:54:17,300 --> 00:54:20,940
And it just uses some initialization already for you.

1313
00:54:20,940 --> 00:54:22,020
Oh, that's different.

1314
00:54:22,020 --> 00:54:22,540
Yeah.

1315
00:54:22,540 --> 00:54:23,020
Yeah.

1316
00:54:23,020 --> 00:54:24,620
What value do you choose?

1317
00:54:24,620 --> 00:54:25,940
Yeah, what values do you choose.

1318
00:54:25,940 --> 00:54:26,620
It's a good question.

1319
00:54:26,620 --> 00:54:27,940
And you can see in the documentation

1320
00:54:27,940 --> 00:54:29,900
that it has all the different choices.

1321
00:54:29,900 --> 00:54:31,620
You can change.

1322
00:54:31,620 --> 00:54:32,580
Yeah.

1323
00:54:32,580 --> 00:54:35,460
Yeah, it's a default. Yeah.

1324
00:54:35,460 --> 00:54:40,260
Do you use more attention when you're doing things like this?

1325
00:54:40,260 --> 00:54:41,620
No, you don't.

1326
00:54:41,620 --> 00:54:44,100
Yeah, this is like, I forgot to mention,

1327
00:54:44,100 --> 00:54:45,980
but everything is just eager execution.

1328
00:54:45,980 --> 00:54:48,300
So yeah, everything is done automatically.

1329
00:54:48,300 --> 00:54:48,800
Yes.

1330
00:54:48,800 --> 00:54:49,300
Yeah.

1331
00:54:57,040 --> 00:54:58,360
Sure.

1332
00:54:58,360 --> 00:54:59,960
Sure.

1333
00:54:59,960 --> 00:55:04,280
So we have an input of that size.

1334
00:55:04,280 --> 00:55:10,880
And so we have 64 by 60 by 100.

1335
00:55:10,880 --> 00:55:11,380
Right?

1336
00:55:11,380 --> 00:55:15,000
So this is like a range three times the length.

1337
00:55:15,000 --> 00:55:17,560
Yes.

1338
00:55:17,560 --> 00:55:20,880
It's hard to picture, but you can just imagine this.

1339
00:55:20,880 --> 00:55:24,400
It has a lot of dimensions here.

1340
00:55:24,400 --> 00:55:25,120
Yes.

1341
00:55:25,120 --> 00:55:27,160
Yes.

1342
00:55:27,160 --> 00:55:27,760
OK.

1343
00:55:27,760 --> 00:55:31,240
And let's say you wanted to build a neural network,

1344
00:55:31,240 --> 00:55:34,040
a very simple neural network that only has one layer, one

1345
00:55:34,040 --> 00:55:35,800
fully connected layer.

1346
00:55:35,800 --> 00:55:39,160
Usually what you do is basically that is what is represented up

1347
00:55:39,160 --> 00:55:41,200
here in the equation.

1348
00:55:41,200 --> 00:55:43,360
And what you do is you have to explicitly define

1349
00:55:43,360 --> 00:55:46,880
what the weights are, what the biases are, and all of that.

1350
00:55:46,880 --> 00:55:50,200
But what I was saying is that once you have this line of code

1351
00:55:50,200 --> 00:55:55,280
here, this linear, and you specify the dimensions of it,

1352
00:55:55,280 --> 00:55:59,040
you have already everything done behind the scenes.

1353
00:55:59,040 --> 00:56:00,720
But the most important part is someone.

1354
00:56:06,720 --> 00:56:07,720
Sorry, I didn't.

1355
00:56:07,920 --> 00:56:11,600
You have to use the

1356
00:56:11,600 --> 00:56:18,800
So it's just like, as I said, it's just like a transformation

1357
00:56:18,800 --> 00:56:20,920
on x, right, that we're doing.

1358
00:56:20,920 --> 00:56:25,360
But we're just applying weights and biases, right?

1359
00:56:25,360 --> 00:56:25,860
Yeah.

1360
00:56:29,320 --> 00:56:32,800
You have to use the weights.

1361
00:56:32,800 --> 00:56:37,280
You have to use the weights for all of the weights.

1362
00:56:38,080 --> 00:56:41,080
Yes.

1363
00:56:41,080 --> 00:56:44,320
Yes.

1364
00:56:44,320 --> 00:56:49,800
So 50 would be the size of the layer, the number of units.

1365
00:56:55,520 --> 00:57:01,640
So I don't have a figure here, but if you know neural networks,

1366
00:57:01,640 --> 00:57:05,880
when you have some input, you would have some layer,

1367
00:57:05,880 --> 00:57:09,240
fully connected layer, but you have to specify a dimension for it.

1368
00:57:09,240 --> 00:57:11,920
It could be one, it could be two, it could be,

1369
00:57:11,920 --> 00:57:13,600
how many neurons do you want, basically?

1370
00:57:13,600 --> 00:57:14,720
That's the question.

1371
00:57:14,720 --> 00:57:16,600
And in this case, we just choose 50.

1372
00:57:16,600 --> 00:57:17,320
It can be any.

1373
00:57:23,160 --> 00:57:25,320
Oh, you want to see?

1374
00:57:25,320 --> 00:57:26,840
OK.

1375
00:57:26,840 --> 00:57:27,360
We have it.

1376
00:57:27,360 --> 00:57:28,800
We're going to actually, let's see.

1377
00:57:31,640 --> 00:57:33,200
Yeah, we can do that.

1378
00:57:33,200 --> 00:57:33,960
That's not wrong.

1379
00:57:35,880 --> 00:57:40,600
Right.

1380
00:57:40,600 --> 00:57:41,520
So we want x.

1381
00:57:41,520 --> 00:57:42,000
Let's see.

1382
00:58:00,200 --> 00:58:02,200
Yes, just random information.

1383
00:58:02,200 --> 00:58:04,760
It's just for illustration purposes.

1384
00:58:04,760 --> 00:58:09,720
It's not really anything important or special about it.

1385
00:58:24,040 --> 00:58:25,840
Yes, that was a sign.

1386
00:58:25,840 --> 00:58:28,000
That was a sign.

1387
00:58:28,000 --> 00:58:32,160
Yeah, that was a, yeah, that was an exact information.

1388
00:58:35,760 --> 00:58:37,240
It's an exact information.

1389
00:58:37,240 --> 00:58:38,720
It's an exact thing.

1390
00:58:38,720 --> 00:58:41,120
So do neural networks take non-sensory,

1391
00:58:41,120 --> 00:58:42,600
or are they all non-sensory?

1392
00:58:42,600 --> 00:58:43,120
Yes.

1393
00:58:43,120 --> 00:58:46,520
So it's really hard to specify.

1394
00:58:46,520 --> 00:58:47,560
Yes, exactly.

1395
00:58:55,960 --> 00:58:57,320
Yeah, and I printed out the sample.

1396
00:58:57,320 --> 00:58:59,520
It looks, all right, it's crazy.

1397
00:58:59,520 --> 00:59:01,720
This is random information, yeah.

1398
00:59:01,720 --> 00:59:03,520
I just wanted to have an example here.

1399
00:59:06,520 --> 00:59:07,040
I'm sorry.

1400
00:59:07,040 --> 00:59:09,240
I'm trying to go a little bit faster, because I could say,

1401
00:59:09,240 --> 00:59:10,320
this is just spider stuff.

1402
00:59:10,320 --> 00:59:12,480
We're not even touching the NLP stuff yet.

1403
00:59:12,480 --> 00:59:16,360
So because of time, we need to move.

1404
00:59:16,360 --> 00:59:19,600
All right, so other component is softmax classifier.

1405
00:59:19,600 --> 00:59:22,240
Another type of transformation here.

1406
00:59:22,240 --> 00:59:24,760
It's just, I have some explanation here.

1407
00:59:24,760 --> 00:59:28,560
It's just a normalized representation of the output.

1408
00:59:28,560 --> 00:59:29,920
The output would be basically what

1409
00:59:29,960 --> 00:59:32,960
I'm outputting from the linear transformation.

1410
00:59:32,960 --> 00:59:35,080
So just another transformation.

1411
00:59:35,080 --> 00:59:38,600
You apply some function again, and you keep doing this.

1412
00:59:38,600 --> 00:59:41,000
So you can see that it's just a chain of operations

1413
00:59:41,000 --> 00:59:41,920
that they're doing.

1414
00:59:41,920 --> 00:59:43,440
And you're just calling a function,

1415
00:59:43,440 --> 00:59:46,160
and everything is just being done behind the scenes.

1416
00:59:46,160 --> 00:59:49,760
And if you ever need to change what softmax do,

1417
00:59:49,760 --> 00:59:52,760
you can actually change that behavior of the softmax

1418
00:59:52,760 --> 00:59:53,760
function.

1419
00:59:53,760 --> 00:59:57,960
So yeah, that's basically what it does.

1420
00:59:57,960 --> 01:00:01,880
And the softmax, of course, is just a distribution, right?

1421
01:00:01,880 --> 01:00:03,520
The range 0 to 1.

1422
01:00:03,520 --> 01:00:07,400
You see how the values are from 0 to 1.

1423
01:00:07,400 --> 01:00:09,160
So and then I have some exercise here,

1424
01:00:09,160 --> 01:00:11,640
just to get some more practice on how

1425
01:00:11,640 --> 01:00:16,520
to chain those linear layers and nonlinear layers also.

1426
01:00:16,520 --> 01:00:18,120
You can practice that.

1427
01:00:18,120 --> 01:00:24,360
So I think it's a good time to try to go over the notebook.

1428
01:00:24,360 --> 01:00:26,440
I didn't go over this part so much.

1429
01:00:26,440 --> 01:00:29,200
I think this one is very important in this notebook.

1430
01:00:33,160 --> 01:00:35,880
Let me just take two minutes to explain it.

1431
01:00:35,880 --> 01:00:38,040
So we have two tensors, right?

1432
01:00:38,040 --> 01:00:38,800
We have two tensors.

1433
01:00:38,800 --> 01:00:45,240
And I want to do some forward transformations.

1434
01:00:45,240 --> 01:00:48,520
So what I do is just define layer 1, and I have two tensors.

1435
01:00:48,520 --> 01:00:52,360
And I say, OK, I want to require gradient equal true, which

1436
01:00:52,360 --> 01:00:55,160
means I want to apply.

1437
01:00:55,160 --> 01:00:59,440
I want to apply some kind of some gradients

1438
01:00:59,440 --> 01:01:00,800
transformation on that.

1439
01:01:00,800 --> 01:01:02,600
So I say true.

1440
01:01:02,600 --> 01:01:04,720
If this was false, I cannot really

1441
01:01:04,720 --> 01:01:07,920
obtain those values from this.

1442
01:01:07,920 --> 01:01:09,880
And you have to explicitly say.

1443
01:01:09,880 --> 01:01:13,160
So and then layer 2 is, so again, it's more visually

1444
01:01:13,160 --> 01:01:14,720
more clear here on the figure.

1445
01:01:14,720 --> 01:01:16,360
And that's why I didn't want to go over it,

1446
01:01:16,360 --> 01:01:17,960
because I think it would take a little while

1447
01:01:17,960 --> 01:01:18,840
to go over each part.

1448
01:01:18,840 --> 01:01:21,120
What do you mean by gradient equal true?

1449
01:01:21,120 --> 01:01:24,080
So gradient is just like some values

1450
01:01:24,080 --> 01:01:26,040
that you get from an equation.

1451
01:01:26,040 --> 01:01:27,800
So the equation is just specified here.

1452
01:01:33,520 --> 01:01:35,680
So let's try to do the exercises.

1453
01:01:35,680 --> 01:01:38,800
I think that's a better choice here.

1454
01:01:38,800 --> 01:01:41,400
Just to get you more familiar with how

1455
01:01:41,400 --> 01:01:45,080
to do more tensor transformations,

1456
01:01:45,080 --> 01:01:48,840
I think the other exercise was on how to change linear layers.

1457
01:01:48,840 --> 01:01:51,800
So try to combine five linear layers.

1458
01:01:51,800 --> 01:01:53,320
Just use different dimensions just

1459
01:01:53,320 --> 01:01:54,560
to get some practice.

1460
01:01:54,560 --> 01:01:55,960
You saw that I just used one.

1461
01:01:55,960 --> 01:01:57,400
We tried to change some of them just

1462
01:01:57,400 --> 01:02:00,000
to get the idea on how you would chain all these operations.

1463
01:02:00,000 --> 01:02:03,360
So just try that for now and maybe five minutes or so.

1464
01:02:06,480 --> 01:02:08,480
Always, if you have questions, just let me know

1465
01:02:08,480 --> 01:02:09,480
and I will come to you.

1466
01:02:09,480 --> 01:02:11,080
I know this is going pretty fast,

1467
01:02:11,080 --> 01:02:14,680
but I have to keep up the pace, because we have a lot more

1468
01:02:14,680 --> 01:02:15,600
content to cover.

1469
01:02:15,600 --> 01:02:17,080
Sorry, I'm just going to.

1470
01:03:15,600 --> 01:03:16,100
OK.

1471
01:03:45,600 --> 01:03:46,100
OK.

1472
01:04:15,600 --> 01:04:16,100
OK.

1473
01:04:45,600 --> 01:04:46,100
OK.

1474
01:05:02,320 --> 01:05:07,840
OK, so I'm going to move on.

1475
01:05:07,840 --> 01:05:08,840
So I'm going to move on.

1476
01:05:08,840 --> 01:05:10,880
Sorry.

1477
01:05:10,880 --> 01:05:12,960
So these two notebooks, basically,

1478
01:05:12,960 --> 01:05:16,320
I just want to say that in your time, you can go through them.

1479
01:05:16,320 --> 01:05:17,240
Try to understand them.

1480
01:05:17,240 --> 01:05:18,480
I have long descriptions.

1481
01:05:18,480 --> 01:05:19,520
You can go through them.

1482
01:05:19,520 --> 01:05:20,840
If you need any more information,

1483
01:05:20,840 --> 01:05:22,720
you can always search for the terminologies

1484
01:05:22,720 --> 01:05:24,160
that I included there.

1485
01:05:24,160 --> 01:05:26,960
It'll make it much easier for you to understand.

1486
01:05:26,960 --> 01:05:31,000
I would say that if you want to do the rest of the notebooks

1487
01:05:31,000 --> 01:05:33,320
that I have here, it'd be really important to go

1488
01:05:33,320 --> 01:05:34,760
through those two notebooks.

1489
01:05:34,760 --> 01:05:36,440
I don't want to really emphasize too much

1490
01:05:36,440 --> 01:05:38,920
or put too much time into it, because I believe

1491
01:05:38,920 --> 01:05:40,320
that you can do this on your own.

1492
01:05:40,320 --> 01:05:42,800
I truly believe that.

1493
01:05:42,840 --> 01:05:45,120
I think the more unique part and the more important part

1494
01:05:45,120 --> 01:05:48,640
of this tutorial will be the other segments that we will cover.

1495
01:05:48,640 --> 01:05:51,680
And you will see that we're not going to do all the tensor stuff anymore.

1496
01:05:51,680 --> 01:05:54,880
You will see that everything is just very high level.

1497
01:05:54,880 --> 01:05:57,720
So let's just continue so that we make it for the break,

1498
01:05:57,720 --> 01:06:00,000
because we have a break at 3 o'clock.

1499
01:06:00,000 --> 01:06:02,080
So let's go.

1500
01:06:02,080 --> 01:06:04,100
And for this, I will take my time.

1501
01:06:04,100 --> 01:06:06,280
I will explain parts more in depth.

1502
01:06:06,280 --> 01:06:09,120
It won't be so fast like the first two tutorials.

1503
01:06:12,800 --> 01:06:18,000
So in NLP, we have different kind of tasks that we can work on.

1504
01:06:18,000 --> 01:06:21,440
These tutorials that I prepared here are more specific to classification,

1505
01:06:21,440 --> 01:06:24,680
because classification is something that's much easier to explain.

1506
01:06:24,680 --> 01:06:27,200
If you have something like language generation,

1507
01:06:27,200 --> 01:06:30,160
that would definitely be something more advanced.

1508
01:06:30,160 --> 01:06:32,840
But I think classification is something you

1509
01:06:32,840 --> 01:06:35,760
can use to easily explain concepts.

1510
01:06:35,760 --> 01:06:41,320
So the first thing that we want to do is load a data set.

1511
01:06:41,320 --> 01:06:45,120
In that folder I shared, there's a data set that I used.

1512
01:06:45,120 --> 01:06:49,320
Actually, that data set is part of my, I wasn't supposed to make it public,

1513
01:06:49,320 --> 01:06:51,000
but I made it public for this tutorial.

1514
01:06:51,000 --> 01:06:54,840
I think it's really nice to have a real world data set.

1515
01:06:54,840 --> 01:06:57,760
The only thing I would say is that don't try to make it public.

1516
01:06:57,760 --> 01:07:02,440
Just try to use it for your own purposes and educational purposes.

1517
01:07:02,440 --> 01:07:05,440
That's the only thing I ask.

1518
01:07:05,440 --> 01:07:07,960
So we have a data set.

1519
01:07:07,960 --> 01:07:09,720
So here is just some functions.

1520
01:07:09,720 --> 01:07:15,480
I can just, we have some functions that I declared here.

1521
01:07:15,480 --> 01:07:17,360
Not so important.

1522
01:07:17,360 --> 01:07:23,560
But when you're doing Google Collab, how do you connect with your data?

1523
01:07:23,560 --> 01:07:27,000
So you have a data stored in a Google Drive, right?

1524
01:07:27,000 --> 01:07:28,800
How do you get access to it?

1525
01:07:28,800 --> 01:07:30,840
This is not your local machine.

1526
01:07:30,840 --> 01:07:33,320
So what you do is you have these two lines of code here.

1527
01:07:33,320 --> 01:07:35,080
If you have the notebook in front of you.

1528
01:07:35,080 --> 01:07:40,080
And if you try to run those two cells, you

1529
01:07:40,080 --> 01:07:45,720
would see that there's a message here with some URL here.

1530
01:07:45,720 --> 01:07:48,400
So basically, it's asking for authorization

1531
01:07:48,400 --> 01:07:49,800
to get access to Google Drive.

1532
01:07:49,800 --> 01:07:53,280
Obviously, this one, you cannot access my Google Drive.

1533
01:07:53,280 --> 01:07:57,400
If this was the case, then that would be really bad.

1534
01:07:57,400 --> 01:08:01,200
But yeah, the cool thing is you can make a copy of it,

1535
01:08:01,200 --> 01:08:03,040
and then you can access your own Google Drive.

1536
01:08:03,280 --> 01:08:07,960
So if you try it, so it asks you for signing.

1537
01:08:07,960 --> 01:08:11,680
If you sign in with your Google account, you give it access.

1538
01:08:11,680 --> 01:08:13,040
It's perfectly safe, by the way.

1539
01:08:13,040 --> 01:08:16,480
It's your account, so you should be fine.

1540
01:08:16,480 --> 01:08:20,600
So then you paste that code.

1541
01:08:20,600 --> 01:08:23,240
And basically, what I'm doing is just mounting the drive.

1542
01:08:23,240 --> 01:08:28,040
Like you would mount your local drives.

1543
01:08:28,040 --> 01:08:30,600
So now it says it has mounted the drive.

1544
01:08:30,600 --> 01:08:32,640
This is very important, because most of the time,

1545
01:08:32,640 --> 01:08:33,840
we need a data set anyways.

1546
01:08:33,840 --> 01:08:37,000
You won't want to work with random information.

1547
01:08:37,000 --> 01:08:41,640
So this part is very important for Collab.

1548
01:08:41,640 --> 01:08:47,400
So you have the drive mounted.

1549
01:08:47,400 --> 01:08:51,360
And my data is stored in the folder in my Google Drive.

1550
01:08:51,360 --> 01:08:54,840
So it's like merge training.

1551
01:08:54,840 --> 01:08:55,960
It's a pickle file.

1552
01:08:55,960 --> 01:08:57,080
It's already stored there.

1553
01:08:57,080 --> 01:08:59,000
And you have access to that already in your Google Drive.

1554
01:08:59,000 --> 01:09:01,280
If you made a copy of it, you should have a copy of it.

1555
01:09:03,600 --> 01:09:04,680
If you don't, don't worry.

1556
01:09:04,680 --> 01:09:07,120
We will have enough time to make a copy of it

1557
01:09:07,120 --> 01:09:09,960
and go through all the different steps.

1558
01:09:09,960 --> 01:09:13,200
So I will load the data.

1559
01:09:13,200 --> 01:09:17,000
And notice that for those people that are familiar with Pandas,

1560
01:09:17,000 --> 01:09:20,800
it's basically a data frame that I'm using here.

1561
01:09:20,800 --> 01:09:23,320
One thing I found when I do a lot of research,

1562
01:09:23,320 --> 01:09:25,440
I would always want to use a data frame.

1563
01:09:25,440 --> 01:09:26,940
Every time, I would use a data frame,

1564
01:09:26,940 --> 01:09:30,560
because it has those nice functionalities to render

1565
01:09:30,560 --> 01:09:32,280
a plot or something like that.

1566
01:09:32,280 --> 01:09:34,600
I wouldn't initially just go jump straight

1567
01:09:34,600 --> 01:09:35,820
into working with the tensor.

1568
01:09:35,820 --> 01:09:37,280
I want to know more about my data.

1569
01:09:37,280 --> 01:09:39,680
So usually, I would store it in that kind of format.

1570
01:09:39,680 --> 01:09:42,360
And so you have access to that one in a data frame.

1571
01:09:42,360 --> 01:09:46,400
And yeah, I just plot here the distribution of the data set

1572
01:09:46,400 --> 01:09:47,280
classes.

1573
01:09:47,280 --> 01:09:50,880
So it's an emotion labeled data set.

1574
01:09:50,880 --> 01:09:54,760
So you have six emotions here.

1575
01:09:54,760 --> 01:09:57,460
It's not the entire data set, because my real data set

1576
01:09:57,460 --> 01:10:00,280
has more emotions than that, more emotion classes.

1577
01:10:00,280 --> 01:10:04,040
But just for this tutorial case, I just used these six emotions.

1578
01:10:04,040 --> 01:10:06,160
You can see also the distribution here,

1579
01:10:06,160 --> 01:10:08,400
how many records are for each class.

1580
01:10:08,400 --> 01:10:11,200
It's a pretty huge data set when you think about it,

1581
01:10:11,200 --> 01:10:14,960
compared to what you have access to.

1582
01:10:14,960 --> 01:10:17,640
So it's a bunch of tweets.

1583
01:10:17,640 --> 01:10:20,560
Basically, what you get is some tweets already

1584
01:10:20,560 --> 01:10:21,960
pre-processed for you.

1585
01:10:21,960 --> 01:10:23,360
So you have it something like that.

1586
01:10:23,360 --> 01:10:27,920
There's a data.head, render 10 samples.

1587
01:10:27,920 --> 01:10:29,200
You can see the 10 samples here.

1588
01:10:29,200 --> 01:10:30,880
We have text.

1589
01:10:30,880 --> 01:10:32,320
And we have the emotion class.

1590
01:10:35,200 --> 01:10:38,080
And it's not so important how I got those emotion class.

1591
01:10:38,080 --> 01:10:40,960
If you really want to know, I have a paper for it.

1592
01:10:40,960 --> 01:10:42,240
All those details are there.

1593
01:10:42,240 --> 01:10:45,880
But it's just important that it's just the emotion of that text.

1594
01:10:50,080 --> 01:10:51,640
And then there is some pre-processing

1595
01:10:51,640 --> 01:10:52,400
that you need to do.

1596
01:10:52,400 --> 01:10:55,440
You have it very nice in a very nice format.

1597
01:10:55,440 --> 01:10:58,320
It's already shuffled and everything for you.

1598
01:10:58,320 --> 01:10:59,820
You want to do some pre-processing.

1599
01:10:59,820 --> 01:11:02,880
But you notice that the text is already very clean, right?

1600
01:11:02,880 --> 01:11:04,840
Usually tweets don't come like that.

1601
01:11:04,840 --> 01:11:10,360
So I already did a lot of the heavy work, right?

1602
01:11:10,360 --> 01:11:13,320
If you want access to that, all of the real data set,

1603
01:11:13,320 --> 01:11:17,080
and you want to know how to do this kind of pre-processing,

1604
01:11:17,080 --> 01:11:18,480
I also have a notebook for that.

1605
01:11:18,480 --> 01:11:20,560
But I thought it would be so time consuming.

1606
01:11:20,560 --> 01:11:25,640
So other information, I would definitely share it.

1607
01:11:25,640 --> 01:11:29,080
I have an extended thing that I will do.

1608
01:11:29,080 --> 01:11:31,000
And I will share everything on the slides.

1609
01:11:31,000 --> 01:11:32,800
Yes.

1610
01:11:32,800 --> 01:11:34,240
You're welcome.

1611
01:11:34,240 --> 01:11:37,220
So after the pre-processing, which there's

1612
01:11:37,220 --> 01:11:40,600
no pre-processing here, we have tokenizing and sampling,

1613
01:11:40,600 --> 01:11:41,080
right?

1614
01:11:41,080 --> 01:11:44,960
These are two very important parts of the transformation

1615
01:11:44,960 --> 01:11:47,440
that you're going to do to your data set.

1616
01:11:47,440 --> 01:11:53,040
And tokenizing part is basically splitting your text

1617
01:11:53,040 --> 01:11:55,760
into the word components, right?

1618
01:11:55,760 --> 01:11:59,400
So that's basically the idea of tokenizing.

1619
01:11:59,400 --> 01:12:03,240
And sampling is just sampling that data set all over again.

1620
01:12:03,240 --> 01:12:05,800
So in this case, because this is a huge data set,

1621
01:12:05,800 --> 01:12:07,920
and I say that after 10,000, you will

1622
01:12:07,920 --> 01:12:10,200
see that you will start to face some challenges

1623
01:12:10,200 --> 01:12:12,080
with the competition part.

1624
01:12:12,080 --> 01:12:14,480
So what I do is just sample 50,000.

1625
01:12:14,480 --> 01:12:16,280
I don't want the entire data set.

1626
01:12:16,280 --> 01:12:21,520
And in fact, if I try 100,000, I think the notebook will break.

1627
01:12:21,520 --> 01:12:24,200
So that's about your free to play around

1628
01:12:24,200 --> 01:12:27,200
with the entire data set and find ways to make it run.

1629
01:12:27,200 --> 01:12:29,440
I'll be happy if you can do that.

1630
01:12:29,440 --> 01:12:31,800
But I didn't get to do it, so.

1631
01:12:31,800 --> 01:12:33,920
OK, so we have a sample.

1632
01:12:33,920 --> 01:12:37,360
And we have the tokens.

1633
01:12:37,360 --> 01:12:40,360
So let me just print out what I mean by tokens.

1634
01:12:40,360 --> 01:12:41,120
So let's see.

1635
01:12:46,280 --> 01:13:00,600
Right, so you can see that it's the token size, right?

1636
01:13:00,600 --> 01:13:02,080
Not really the token itself.

1637
01:13:02,080 --> 01:13:04,120
I didn't store the token itself.

1638
01:13:04,120 --> 01:13:05,880
I think, yeah, it didn't store.

1639
01:13:05,880 --> 01:13:07,600
So you only have the token size, which

1640
01:13:07,600 --> 01:13:10,760
is the number of words in each sentence, right?

1641
01:13:10,760 --> 01:13:13,600
But why do I want that information?

1642
01:13:13,600 --> 01:13:15,720
So that information is very important

1643
01:13:15,720 --> 01:13:17,280
when you're doing batches.

1644
01:13:17,280 --> 01:13:21,280
You want to put the batches in terms of the token size

1645
01:13:21,280 --> 01:13:22,440
together.

1646
01:13:22,440 --> 01:13:24,680
So that's something that you would want to do.

1647
01:13:24,680 --> 01:13:27,440
So like I say, it's just part of the preprocessing step.

1648
01:13:31,040 --> 01:13:31,540
Sorry?

1649
01:13:34,200 --> 01:13:38,320
So usually, depending on what kind of task you're working on,

1650
01:13:38,320 --> 01:13:41,600
for this classification task, it's not so important.

1651
01:13:41,600 --> 01:13:43,360
You can actually just do it random.

1652
01:13:43,400 --> 01:13:46,600
But because I had designed this notebook for machine

1653
01:13:46,600 --> 01:13:48,560
translation, I had to do something like this

1654
01:13:48,560 --> 01:13:51,760
and get that information so that I can put batches together

1655
01:13:51,760 --> 01:13:52,960
of similar size.

1656
01:13:52,960 --> 01:13:55,480
Because that's usually very good for machine translation.

1657
01:13:55,480 --> 01:13:57,560
But for this one, we won't use that information.

1658
01:13:57,560 --> 01:13:59,760
I just had it here just to show.

1659
01:13:59,760 --> 01:14:02,280
Sometimes you would need this information,

1660
01:14:02,280 --> 01:14:04,280
depending on the task you're working on.

1661
01:14:08,120 --> 01:14:12,920
So we have a, now I think this is very important.

1662
01:14:12,920 --> 01:14:16,360
So we want to construct a vocabulary out of our data set.

1663
01:14:16,360 --> 01:14:20,080
We have all our text here, very nicely formatted.

1664
01:14:20,080 --> 01:14:22,320
And we want to create or construct

1665
01:14:22,320 --> 01:14:24,640
a vocabulary and some indexing.

1666
01:14:24,640 --> 01:14:25,800
Why is that important?

1667
01:14:25,800 --> 01:14:29,280
Because the next step would be to establish features.

1668
01:14:29,280 --> 01:14:31,920
What kind of features are we going to feed into the model?

1669
01:14:31,920 --> 01:14:34,160
And usually what we do is, in NLP,

1670
01:14:34,160 --> 01:14:35,640
usually we use word embeddings.

1671
01:14:35,640 --> 01:14:38,560
That's the first thing that we're going to use.

1672
01:14:38,560 --> 01:14:42,840
That's like a very typical use case, use word embeddings.

1673
01:14:42,840 --> 01:14:45,440
So we need to construct a vocabulary first.

1674
01:14:45,440 --> 01:14:46,960
Before we even talk about embeddings,

1675
01:14:46,960 --> 01:14:48,680
we just need to construct a vocabulary.

1676
01:14:48,680 --> 01:14:52,800
Because we need to tell the model what we want to embed.

1677
01:14:52,800 --> 01:14:57,440
What kind of words do we need to get those embeddings for?

1678
01:14:57,440 --> 01:14:59,200
So that's just basically the vocabulary.

1679
01:14:59,200 --> 01:15:00,400
And this class does that.

1680
01:15:00,400 --> 01:15:02,480
I don't want to go over the details

1681
01:15:02,480 --> 01:15:04,400
in what this class does.

1682
01:15:04,400 --> 01:15:06,160
You can basically tell what it does,

1683
01:15:06,160 --> 01:15:11,720
but it's just rendering the vocabulary itself.

1684
01:15:11,720 --> 01:15:16,480
So after I do, let's see, just run it.

1685
01:15:21,360 --> 01:15:25,440
So there's some strange word here.

1686
01:15:25,440 --> 01:15:27,400
I hope that doesn't break the code.

1687
01:15:27,400 --> 01:15:28,640
I never saw it before.

1688
01:15:28,640 --> 01:15:32,080
But anyways, so you can see.

1689
01:15:32,080 --> 01:15:33,120
Yes.

1690
01:15:33,120 --> 01:15:35,600
Yeah, you're right.

1691
01:15:35,600 --> 01:15:39,400
And usually that kind of words, as a best practice,

1692
01:15:39,400 --> 01:15:44,240
usually you would have to normalize that kind of word.

1693
01:15:44,240 --> 01:15:47,280
You remove the A's, remove all the repetitive R's.

1694
01:15:47,280 --> 01:15:50,760
But it just so happens that in this case, I missed that one.

1695
01:15:50,760 --> 01:15:52,400
Something happened.

1696
01:15:52,400 --> 01:15:55,360
It happens.

1697
01:15:55,360 --> 01:15:58,000
So hopefully nothing breaks later on.

1698
01:15:58,000 --> 01:16:00,440
So after we have that vocabulary,

1699
01:16:00,440 --> 01:16:02,680
you would use that information.

1700
01:16:02,680 --> 01:16:05,240
You have that information accessible

1701
01:16:05,240 --> 01:16:06,320
to transform your data.

1702
01:16:06,320 --> 01:16:09,560
Now you have all the words that you need.

1703
01:16:09,560 --> 01:16:13,680
And you need to convert each sentence into a tensor.

1704
01:16:13,680 --> 01:16:14,480
How do we do that?

1705
01:16:14,480 --> 01:16:17,080
How do we convert each sentence into a tensor?

1706
01:16:17,080 --> 01:16:19,280
A tensor of values.

1707
01:16:19,280 --> 01:16:23,440
And each value in that tensor would be basically an ID,

1708
01:16:23,440 --> 01:16:26,000
an index, into that vocabulary.

1709
01:16:26,000 --> 01:16:27,040
So what I mean by that.

1710
01:16:27,040 --> 01:16:27,680
So let's see.

1711
01:16:31,960 --> 01:16:33,960
So we have two sentences.

1712
01:16:33,960 --> 01:16:35,080
I select two sentences.

1713
01:16:35,080 --> 01:16:38,880
And notice that when I print it out,

1714
01:16:38,880 --> 01:16:41,320
some weird numbers show up.

1715
01:16:41,320 --> 01:16:43,880
So we're not dealing with text anymore.

1716
01:16:43,880 --> 01:16:45,400
We're dealing with numbers.

1717
01:16:45,400 --> 01:16:47,560
We have done all the transformation already.

1718
01:16:47,560 --> 01:16:49,040
So this is a sentence.

1719
01:16:49,040 --> 01:16:51,000
No one can say with the sentences.

1720
01:16:51,000 --> 01:16:51,840
No one.

1721
01:16:51,840 --> 01:16:52,560
I can bet on that.

1722
01:16:52,560 --> 01:16:54,200
No one can.

1723
01:16:54,200 --> 01:16:59,560
But you can see that it's an ID, an ID into that vocabulary.

1724
01:16:59,560 --> 01:17:02,440
And each ID represents a word in that vocabulary.

1725
01:17:05,720 --> 01:17:07,360
So far so good?

1726
01:17:07,360 --> 01:17:09,160
OK.

1727
01:17:09,160 --> 01:17:12,200
So after that, let's say we want.

1728
01:17:12,200 --> 01:17:13,400
So there's some exercise here.

1729
01:17:13,400 --> 01:17:15,800
You can try it on your own.

1730
01:17:15,800 --> 01:17:20,200
We'll do that after I finish the notebook.

1731
01:17:20,200 --> 01:17:22,440
Or you can try it.

1732
01:17:22,440 --> 01:17:24,720
So after that, we have to do some padding.

1733
01:17:24,720 --> 01:17:28,360
Notice that some sentences are longer.

1734
01:17:28,360 --> 01:17:30,920
Because you can see the first one is a lot longer

1735
01:17:30,920 --> 01:17:32,360
than the other one.

1736
01:17:32,360 --> 01:17:33,400
It's longer.

1737
01:17:33,400 --> 01:17:34,320
See?

1738
01:17:34,320 --> 01:17:35,920
The size, right?

1739
01:17:35,920 --> 01:17:39,200
So what you do in that case is you need to pad it.

1740
01:17:39,200 --> 01:17:41,120
Because if you want to do something

1741
01:17:41,120 --> 01:17:43,680
like a standard neural network or a deep learning model,

1742
01:17:43,680 --> 01:17:46,880
you have to have those data of the same size, the inputs

1743
01:17:46,880 --> 01:17:49,280
at least.

1744
01:17:49,280 --> 01:17:53,320
And that's very important when you're dealing with models

1745
01:17:53,320 --> 01:17:54,480
like RNN.

1746
01:17:54,480 --> 01:17:58,240
You'd have to have that already normalized already,

1747
01:17:58,240 --> 01:18:01,400
the same size, each input.

1748
01:18:01,400 --> 01:18:03,880
So that's basically the concept of padding.

1749
01:18:03,960 --> 01:18:05,160
I do a padding.

1750
01:18:05,160 --> 01:18:08,680
First, what I want to find out, what's the max length

1751
01:18:08,680 --> 01:18:10,400
of all the sentences?

1752
01:18:10,400 --> 01:18:13,200
Just collect the size of all the sentences

1753
01:18:13,200 --> 01:18:19,360
and check what the max length of the longest sentence is.

1754
01:18:19,360 --> 01:18:22,800
And in my data set, it's 69.

1755
01:18:22,800 --> 01:18:26,360
So we have a tweet of 69 words.

1756
01:18:26,360 --> 01:18:28,120
At the beginning, when I started at 69,

1757
01:18:28,120 --> 01:18:29,360
I thought that was wrong.

1758
01:18:29,360 --> 01:18:31,880
But after Twitter changed their policies,

1759
01:18:31,880 --> 01:18:35,280
now you have longer texts.

1760
01:18:35,280 --> 01:18:39,480
So you have some sentences with 69 words.

1761
01:18:42,720 --> 01:18:45,600
So now you have some padding that you need to do.

1762
01:18:45,600 --> 01:18:47,840
So this function takes care of that.

1763
01:18:52,240 --> 01:18:52,960
So let's see.

1764
01:18:52,960 --> 01:18:53,460
OK.

1765
01:19:00,280 --> 01:19:05,320
So what I did here is I padded the inputs.

1766
01:19:05,320 --> 01:19:10,160
So now you have all the same size, all the inputs

1767
01:19:10,160 --> 01:19:12,040
of the same size now.

1768
01:19:12,040 --> 01:19:16,560
And you can see the padding was done just using zeros.

1769
01:19:16,560 --> 01:19:18,200
So that's standard.

1770
01:19:18,200 --> 01:19:20,720
Your data is standardized now using the padding.

1771
01:19:23,840 --> 01:19:26,400
I know it's hard to understand what it means,

1772
01:19:26,400 --> 01:19:29,120
but just think of it that it's just a sentence.

1773
01:19:32,520 --> 01:19:36,680
So after that, we have a binarization step.

1774
01:19:36,680 --> 01:19:38,920
And this one is going to take care of.

1775
01:19:38,920 --> 01:19:42,280
So we took care of our sentences, our input.

1776
01:19:42,280 --> 01:19:44,120
Now we have our target classes that we

1777
01:19:44,120 --> 01:19:46,560
need to take care of.

1778
01:19:46,560 --> 01:19:49,160
We cannot train a model if we don't have classes in this case,

1779
01:19:49,160 --> 01:19:50,640
because this is supervised learning.

1780
01:19:50,640 --> 01:19:53,600
So we need classes.

1781
01:19:53,600 --> 01:19:55,880
So all this code takes care of.

1782
01:19:55,880 --> 01:19:57,400
So this is like Scikit-learn.

1783
01:19:57,400 --> 01:19:59,840
If you're familiar with Scikit-learn, I hope you are.

1784
01:19:59,840 --> 01:20:02,800
So Scikit-learn is just have a lot of utility functions

1785
01:20:02,800 --> 01:20:05,960
that you can use to just transform your data,

1786
01:20:05,960 --> 01:20:08,120
do some kind of special operations in your data,

1787
01:20:08,120 --> 01:20:10,760
and you get a lot of functions there.

1788
01:20:13,240 --> 01:20:18,680
So what I did there is just to obtain my target values.

1789
01:20:18,680 --> 01:20:20,720
So my target looks something like this.

1790
01:20:20,720 --> 01:20:24,000
Again, we're dealing with numbers.

1791
01:20:24,000 --> 01:20:27,800
Everything is numerical values.

1792
01:20:27,800 --> 01:20:32,080
So now it's something like this.

1793
01:20:32,080 --> 01:20:33,600
The classes are like this.

1794
01:20:33,600 --> 01:20:39,480
So for example, let's say this was anger, and this was,

1795
01:20:39,480 --> 01:20:42,200
I don't know, disgust maybe.

1796
01:20:42,200 --> 01:20:45,040
I don't remember really the label number,

1797
01:20:45,040 --> 01:20:47,280
but each one is a different class,

1798
01:20:47,280 --> 01:20:49,040
because it's a one-hat encoding.

1799
01:20:49,040 --> 01:20:52,640
And these are just two records, two different classes.

1800
01:20:57,320 --> 01:21:00,480
So our data looks like this again, just to refresh.

1801
01:21:00,480 --> 01:21:02,680
I have a text, a motion, and a token size.

1802
01:21:02,680 --> 01:21:03,840
That's the Pandas data frame.

1803
01:21:06,520 --> 01:21:12,560
And now we want to, so as I said at the beginning,

1804
01:21:12,560 --> 01:21:18,520
what was the, I didn't know what was the emotion class here.

1805
01:21:18,520 --> 01:21:20,760
So I just created a function here just

1806
01:21:20,760 --> 01:21:24,880
to get access to what was the emotion label,

1807
01:21:24,880 --> 01:21:26,320
just to keep track of it.

1808
01:21:26,320 --> 01:21:27,960
These are just things that you do

1809
01:21:27,960 --> 01:21:31,200
to just keep checking that you're in the right direction.

1810
01:21:31,200 --> 01:21:33,760
Not so important for this tutorial,

1811
01:21:33,760 --> 01:21:37,760
but it's just something that you need to keep track of.

1812
01:21:37,760 --> 01:21:39,320
And I have some more additional code

1813
01:21:39,320 --> 01:21:43,320
that I use just to run more checks that I'm

1814
01:21:43,320 --> 01:21:44,800
in the right direction.

1815
01:21:44,800 --> 01:21:48,320
So you can see here that I'm looking at the first sentence

1816
01:21:48,320 --> 01:21:50,680
just to make sure.

1817
01:21:50,680 --> 01:21:54,080
Yeah, I'm there, the same sentence.

1818
01:21:57,320 --> 01:22:02,920
So after binarization, we have the input represented

1819
01:22:02,920 --> 01:22:03,800
in numerical values.

1820
01:22:03,800 --> 01:22:06,920
We have the target represented in numerical values as well.

1821
01:22:06,920 --> 01:22:11,240
What do we do next is we need to establish batches in order

1822
01:22:11,240 --> 01:22:14,360
for us to do a training.

1823
01:22:14,360 --> 01:22:16,600
So these are more functions that I use here.

1824
01:22:19,240 --> 01:22:23,040
So you can see that I'm using this train test split function

1825
01:22:23,040 --> 01:22:27,320
that's provided by CycleLearn, another very helpful function.

1826
01:22:27,320 --> 01:22:30,720
And that just splits the data into a training, test,

1827
01:22:30,720 --> 01:22:31,840
and validation data set.

1828
01:22:31,840 --> 01:22:36,480
So we have three portions.

1829
01:22:36,480 --> 01:22:40,920
And each one has a different intention.

1830
01:22:40,920 --> 01:22:43,560
So the training data set is for training the model.

1831
01:22:43,560 --> 01:22:46,240
Testing data set is for basically testing

1832
01:22:46,240 --> 01:22:47,880
the model on the go while we train.

1833
01:22:47,880 --> 01:22:50,280
And the validation is a held-out data set

1834
01:22:50,280 --> 01:22:54,480
just to check whether the model really generalized well

1835
01:22:54,480 --> 01:22:59,800
or is doing well in the prediction.

1836
01:22:59,800 --> 01:23:03,560
So that code takes care of that.

1837
01:23:03,560 --> 01:23:06,200
Let's see again just to make sure.

1838
01:23:07,000 --> 01:23:08,240
It's divided like this.

1839
01:23:08,240 --> 01:23:12,040
So remember, it was 50,000 records.

1840
01:23:12,040 --> 01:23:15,760
So our training data set is 40,000.

1841
01:23:15,760 --> 01:23:17,880
Our validation, 5,000.

1842
01:23:17,880 --> 01:23:20,000
And our test, 5,000.

1843
01:23:20,000 --> 01:23:21,400
Why do you see a bunch of numbers?

1844
01:23:21,400 --> 01:23:26,040
Because I actually printed out together with the tensor

1845
01:23:26,040 --> 01:23:27,440
the target and the input.

1846
01:23:27,440 --> 01:23:30,040
So you saw that those are just pairs, right?

1847
01:23:30,040 --> 01:23:33,040
So now I think this is the core thing.

1848
01:23:33,040 --> 01:23:35,040
The thing that I really like about PyTorch

1849
01:23:35,040 --> 01:23:40,040
is that initially it was built with these very nice classes.

1850
01:23:40,040 --> 01:23:44,040
So usually when we work with training data sets

1851
01:23:44,040 --> 01:23:46,040
and validation data sets, we have the data set

1852
01:23:46,040 --> 01:23:48,040
in a really nice format.

1853
01:23:48,040 --> 01:23:51,040
But you will notice that even if you have your data

1854
01:23:51,040 --> 01:23:56,040
split and everything, you still need to do extra stuff.

1855
01:23:56,040 --> 01:23:59,040
So you can see that the data sets are really nice.

1856
01:23:59,040 --> 01:24:02,040
Just like extra steps on that data.

1857
01:24:02,040 --> 01:24:04,040
A lot of things that you need to do to get it right

1858
01:24:04,040 --> 01:24:08,040
in the right format before you feed it into a model.

1859
01:24:08,040 --> 01:24:13,040
And PyTorch provides very good support for that.

1860
01:24:13,040 --> 01:24:17,040
There are a lot of nice classes that you can use

1861
01:24:17,040 --> 01:24:19,040
to take care of a lot of the things that you need to do

1862
01:24:19,040 --> 01:24:20,040
with the data.

1863
01:24:20,040 --> 01:24:22,040
So I will just show what I mean by that here.

1864
01:24:22,040 --> 01:24:26,040
So I just have some parameters here like the batch size,

1865
01:24:26,040 --> 01:24:28,040
which is 64.

1866
01:24:28,040 --> 01:24:31,040
And then there is the different sizes of the trained,

1867
01:24:31,040 --> 01:24:35,040
validation, and the testing data set.

1868
01:24:35,040 --> 01:24:40,040
And then I would import those classes that I want access to.

1869
01:24:40,040 --> 01:24:42,040
So why do I want to do all of...

1870
01:24:42,040 --> 01:24:46,040
You have my data in batches already.

1871
01:24:46,040 --> 01:24:47,040
But I don't have them in batches.

1872
01:24:47,040 --> 01:24:51,040
I have them training, validation, and testing.

1873
01:24:51,040 --> 01:24:52,040
No batches yet.

1874
01:24:52,040 --> 01:24:55,040
How do I make sure that I have my batches

1875
01:24:55,040 --> 01:24:58,040
and that this is done very efficiently?

1876
01:24:58,040 --> 01:24:59,040
How do I do that?

1877
01:24:59,040 --> 01:25:02,040
Usually what I would do is before what I did is use NumPy.

1878
01:25:02,040 --> 01:25:05,040
I would just manually do it, manually do my batches,

1879
01:25:05,040 --> 01:25:07,040
and then I would feed it into the model.

1880
01:25:07,040 --> 01:25:09,040
But what I realized is that I did a lot of mistakes

1881
01:25:09,040 --> 01:25:10,040
while doing this.

1882
01:25:10,040 --> 01:25:11,040
Usually I would have some error.

1883
01:25:11,040 --> 01:25:14,040
Usually I don't know if you have faced this before,

1884
01:25:14,040 --> 01:25:17,040
but sometimes I would even feed the same batch again

1885
01:25:17,040 --> 01:25:20,040
into the model, and this would create a lot of problems

1886
01:25:20,040 --> 01:25:21,040
during training.

1887
01:25:21,040 --> 01:25:24,040
So there are some efficient classes provided by PyTorch

1888
01:25:24,040 --> 01:25:26,040
to take care of all of that.

1889
01:25:26,040 --> 01:25:29,040
So one of them is a data set class.

1890
01:25:29,040 --> 01:25:35,040
So what I did here was just provide a class

1891
01:25:35,040 --> 01:25:41,040
that specifies how I want to set up my data.

1892
01:25:41,040 --> 01:25:44,040
So here I have this very nice class,

1893
01:25:44,040 --> 01:25:47,040
and I'm specifying how do I initialize it.

1894
01:25:47,040 --> 01:25:51,040
I want the self-laid data to have the input,

1895
01:25:51,040 --> 01:25:55,040
then I want that to be the target,

1896
01:25:55,040 --> 01:25:56,040
and then I want the length of it.

1897
01:25:56,040 --> 01:25:57,040
I want that information.

1898
01:25:57,040 --> 01:25:59,040
So this is going to encapsulate all the information

1899
01:25:59,040 --> 01:26:04,040
that you want to use during training time.

1900
01:26:04,040 --> 01:26:09,040
And when you specify to get that item function,

1901
01:26:09,040 --> 01:26:10,040
what do you want to get from it?

1902
01:26:10,040 --> 01:26:15,040
So you want to get something that's like in a tuple, right?

1903
01:26:15,040 --> 01:26:19,040
You want it to be the input, the output,

1904
01:26:20,040 --> 01:26:22,040
and the length of the text.

1905
01:26:22,040 --> 01:26:24,040
So just like the data frame I showed,

1906
01:26:24,040 --> 01:26:26,040
that's the same thing that you're seeing here,

1907
01:26:26,040 --> 01:26:28,040
but now we're using just tensors.

1908
01:26:28,040 --> 01:26:31,040
We're not using the actual real text.

1909
01:26:31,040 --> 01:26:34,040
So just imagine it that way.

1910
01:26:34,040 --> 01:26:36,040
And I can establish more functions,

1911
01:26:36,040 --> 01:26:39,040
and I can create various kinds of functions,

1912
01:26:39,040 --> 01:26:41,040
and everything is all taken care of here.

1913
01:26:41,040 --> 01:26:45,040
So let's say this is a very simple class that I built

1914
01:26:45,040 --> 01:26:47,040
with the data set module,

1915
01:26:47,040 --> 01:26:50,040
and I would just try it here and see what it does.

1916
01:26:50,040 --> 01:26:53,040
And that's the best way of showing, right?

1917
01:26:53,040 --> 01:27:00,040
So let's see what this class does.

1918
01:27:00,040 --> 01:27:05,040
So we have it here, and we provide the input tensor train.

1919
01:27:05,040 --> 01:27:07,040
We have the target, we have the pairs, right?

1920
01:27:07,040 --> 01:27:09,040
Input, output, pairs.

1921
01:27:09,040 --> 01:27:13,040
And we pass it to this as object here.

1922
01:27:13,040 --> 01:27:15,040
And now we're supposed to have something here

1923
01:27:15,040 --> 01:27:16,040
in the train data set.

1924
01:27:16,040 --> 01:27:19,040
But what's in that train data set?

1925
01:27:19,040 --> 01:27:21,040
What's encapsulated there?

1926
01:27:21,040 --> 01:27:33,040
So let's try to print that out and see what happens.

1927
01:27:33,040 --> 01:27:36,040
So let's try to print data set.

1928
01:27:36,040 --> 01:27:39,040
What you would expect to see is the tensor again, right?

1929
01:27:39,040 --> 01:27:45,040
The tensor, the input, the target, and the size.

1930
01:27:45,040 --> 01:27:47,040
But what you get is something like this.

1931
01:27:47,040 --> 01:27:49,040
It's an object.

1932
01:27:49,040 --> 01:27:53,040
All of that is contained already, self-contained.

1933
01:27:53,040 --> 01:27:55,040
So let's see.

1934
01:27:55,040 --> 01:27:57,040
What's in this object?

1935
01:27:57,040 --> 01:28:02,040
So we have different functions in that object.

1936
01:28:02,040 --> 01:28:03,040
Let's see.

1937
01:28:03,040 --> 01:28:06,040
How about batch size?

1938
01:28:06,040 --> 01:28:13,040
Sorry.

1939
01:28:13,040 --> 01:28:25,040
Right, so we have that.

1940
01:28:25,040 --> 01:28:31,040
Whoa.

1941
01:28:31,040 --> 01:28:35,040
Yeah, I never got this open before.

1942
01:28:35,040 --> 01:28:36,040
Yeah, it's too much.

1943
01:28:36,040 --> 01:28:38,040
Yeah, so that's the reason why I never had it there

1944
01:28:38,040 --> 01:28:40,040
in the first place, right?

1945
01:28:40,040 --> 01:28:42,040
But so, let's just continue.

1946
01:28:42,040 --> 01:28:44,040
So we have all that information encoded already,

1947
01:28:44,040 --> 01:28:46,040
and I would like to print something out, right?

1948
01:28:46,040 --> 01:28:50,040
So what I do is I want to establish those batches.

1949
01:28:50,040 --> 01:28:53,040
So what I do is use this other class here, data loader,

1950
01:28:53,040 --> 01:28:56,040
and say, okay, I want to establish batch sizes of this size,

1951
01:28:56,040 --> 01:28:57,040
which is 64.

1952
01:28:57,040 --> 01:28:59,040
I specify that.

1953
01:28:59,040 --> 01:29:00,040
And I want to drop the last.

1954
01:29:00,040 --> 01:29:03,040
Sometimes you have batches that are not the same size,

1955
01:29:03,040 --> 01:29:05,040
usually the remainder, so you have 64.

1956
01:29:05,040 --> 01:29:07,040
Maybe you have 63 in the last batch.

1957
01:29:07,040 --> 01:29:08,040
We don't want that.

1958
01:29:08,040 --> 01:29:09,040
We want to discard that.

1959
01:29:09,040 --> 01:29:11,040
So we say drop last equal true.

1960
01:29:11,040 --> 01:29:13,040
Then we want to shuffle again.

1961
01:29:13,040 --> 01:29:15,040
Okay, that's very important.

1962
01:29:15,040 --> 01:29:20,040
So now we have, again, train data set.

1963
01:29:20,040 --> 01:29:22,040
So let's just try again.

1964
01:29:22,040 --> 01:29:29,040
So the validation data set, what's the batch size?

1965
01:29:29,040 --> 01:29:30,040
This is 64.

1966
01:29:30,040 --> 01:29:32,040
Now everything is encoded here, and I can even print it out.

1967
01:29:32,040 --> 01:29:38,040
Let's see.

1968
01:29:38,040 --> 01:29:40,040
Let's see how I would print that out.

1969
01:29:40,040 --> 01:29:43,040
So I have all these different functions.

1970
01:29:43,040 --> 01:29:46,040
Let's see, data set.

1971
01:29:46,040 --> 01:29:49,040
Do you see some object?

1972
01:29:49,040 --> 01:29:51,040
Right?

1973
01:29:51,040 --> 01:29:56,040
If I try to print the data, do you think it will crash?

1974
01:29:56,040 --> 01:29:57,040
Great.

1975
01:29:57,040 --> 01:30:00,040
So now when I try to print it out, it prints, right?

1976
01:30:00,040 --> 01:30:03,040
So you can see that all the different steps what you're doing

1977
01:30:03,040 --> 01:30:07,040
is just have that very efficient way of encoding your data

1978
01:30:07,040 --> 01:30:10,040
and making sure that you have it in the right dimension

1979
01:30:10,040 --> 01:30:12,040
and size that you want.

1980
01:30:12,040 --> 01:30:15,040
Okay?

1981
01:30:15,040 --> 01:30:21,040
So you have access to all the different methods from that class.

1982
01:30:21,040 --> 01:30:28,040
Yeah, it's a whole, yeah, because I never specified.

1983
01:30:28,040 --> 01:30:31,040
Yeah, I never specified the batch.

1984
01:30:31,040 --> 01:30:33,040
But I can do that as well.

1985
01:30:33,040 --> 01:30:35,040
Let me just...

1986
01:30:36,040 --> 01:30:40,040
I don't want to print that out.

1987
01:30:40,040 --> 01:30:42,040
Right.

1988
01:30:42,040 --> 01:30:46,040
So again, let's double check that the number of emotions is six, right?

1989
01:30:46,040 --> 01:30:50,040
That's the number of classes.

1990
01:30:50,040 --> 01:30:52,040
Again, I have some utility functions here just to convert to pickles

1991
01:30:52,040 --> 01:30:53,040
because I work with pickles.

1992
01:30:53,040 --> 01:30:55,040
I prefer to do that.

1993
01:30:55,040 --> 01:30:57,040
And that's the end of the tutorial.

1994
01:30:57,040 --> 01:31:00,040
So what I did here was just encode the data, having a batch.

1995
01:31:00,040 --> 01:31:03,040
Now this is just ready to be used, right?

1996
01:31:03,040 --> 01:31:05,040
Ready to be used, train a model with it.

1997
01:31:05,040 --> 01:31:07,040
That's your input, right?

1998
01:31:07,040 --> 01:31:08,040
That's what we did.

1999
01:31:08,040 --> 01:31:10,040
That's why I labeled this as NLP basis

2000
01:31:10,040 --> 01:31:12,040
because you're not doing really NLP.

2001
01:31:12,040 --> 01:31:15,040
All you're doing is just a lot of preprocessing and encoding.

2002
01:31:15,040 --> 01:31:18,040
But that's a very important part of the whole framework, right?

2003
01:31:18,040 --> 01:31:22,040
So I just stored my data here.

2004
01:31:22,040 --> 01:31:25,040
And this should store into Google Drive, right?

2005
01:31:25,040 --> 01:31:31,040
If you run those lines of code, that will start to go into your Google Drive.

2006
01:31:31,040 --> 01:31:34,040
So I have training, validation, and testing data set.

2007
01:31:34,040 --> 01:31:38,040
So any questions before we go to a break?

2008
01:31:38,040 --> 01:31:39,040
I think we have a break.

2009
01:31:39,040 --> 01:31:41,040
We have 10 minutes more.

2010
01:31:41,040 --> 01:31:43,040
There are some exercises here, I believe.

2011
01:31:43,040 --> 01:31:46,040
Did I have some exercises?

2012
01:31:46,040 --> 01:31:48,040
Yeah, I did have one.

2013
01:31:48,040 --> 01:31:53,040
So maybe just you can go back and review it if you have any questions.

2014
01:31:53,040 --> 01:31:57,040
Just try to do this exercise here just to review the code

2015
01:31:57,040 --> 01:32:01,040
and understand the different steps that I took to reach the last step.

2016
01:32:01,040 --> 01:32:04,040
But this one is just for doing the encoding itself,

2017
01:32:04,040 --> 01:32:07,040
getting the vocabulary and indexing it

2018
01:32:07,040 --> 01:32:11,040
and have that input tensor in the format that I showed.

2019
01:32:11,040 --> 01:32:12,040
So just try to do that.

2020
01:32:12,040 --> 01:32:17,040
And I think you can just reuse the code at the top here to get that answer.

2021
01:32:17,040 --> 01:32:18,040
So just try it.

2022
01:32:18,040 --> 01:32:21,040
If you get that, I think you should be okay.

2023
01:32:21,040 --> 01:32:25,040
And if you have any questions, just ask.

2024
01:32:25,040 --> 01:32:28,040
So I think it's just 10 minutes to the break.

2025
01:32:28,040 --> 01:32:29,040
Yeah.

2026
01:32:51,040 --> 01:32:52,040
No, not yet.

2027
01:32:52,040 --> 01:32:53,040
Not yet.

2028
01:32:53,040 --> 01:32:55,040
That would be the other step.

2029
01:33:23,040 --> 01:33:24,040
Okay.

2030
01:33:53,040 --> 01:33:54,040
Okay.

2031
01:34:23,040 --> 01:34:25,040
Okay.

2032
01:34:53,040 --> 01:34:55,040
Okay.

2033
01:35:23,040 --> 01:35:25,040
Okay.

2034
01:35:53,040 --> 01:35:55,040
Okay.

2035
01:36:23,040 --> 01:36:25,040
Okay.

2036
01:36:53,040 --> 01:36:55,040
Okay.

2037
01:37:23,040 --> 01:37:25,040
Okay.

2038
01:37:53,040 --> 01:37:55,040
Okay.

2039
01:38:23,040 --> 01:38:25,040
Okay.

2040
01:38:53,040 --> 01:38:55,040
Okay.

2041
01:39:23,040 --> 01:39:25,040
Okay.

2042
01:39:53,040 --> 01:39:55,040
Okay.

2043
01:40:23,040 --> 01:40:25,040
Okay.

2044
01:40:53,040 --> 01:40:55,040
Okay.

2045
01:41:23,040 --> 01:41:25,040
Okay.

2046
01:41:53,040 --> 01:41:55,040
Okay.

2047
01:42:23,040 --> 01:42:25,040
Okay.

2048
01:42:53,040 --> 01:42:55,040
Okay.

2049
01:43:23,040 --> 01:43:25,040
Okay.

2050
01:43:53,040 --> 01:43:55,040
Okay.

2051
01:44:23,040 --> 01:44:25,040
Okay.

2052
01:44:53,040 --> 01:44:55,040
Okay.

2053
01:45:23,040 --> 01:45:25,040
Okay.

2054
01:45:53,040 --> 01:45:55,040
Okay.

2055
01:46:23,040 --> 01:46:25,040
Okay.

2056
01:46:53,040 --> 01:46:55,040
Okay.

2057
01:47:23,040 --> 01:47:25,040
Okay.

2058
01:47:53,040 --> 01:47:55,040
Okay.

2059
01:48:23,040 --> 01:48:25,040
Okay.

2060
01:48:53,040 --> 01:48:55,040
Okay.

2061
01:49:23,040 --> 01:49:25,040
Okay.

2062
01:49:53,040 --> 01:49:55,040
Okay.

2063
01:50:23,040 --> 01:50:25,040
Okay.

2064
01:50:53,040 --> 01:50:54,040
Okay.

2065
01:51:23,040 --> 01:51:24,040
Okay.

2066
01:51:53,040 --> 01:51:55,040
Okay.

2067
01:52:23,040 --> 01:52:25,040
Okay.

2068
01:52:53,040 --> 01:52:55,040
Okay.

2069
01:53:23,040 --> 01:53:24,040
Okay.

2070
01:53:53,040 --> 01:53:55,040
Okay.

2071
01:54:23,040 --> 01:54:25,040
Okay.

2072
01:54:53,040 --> 01:54:54,040
Okay.

2073
01:55:23,040 --> 01:55:25,040
Okay.

2074
01:55:53,040 --> 01:55:55,040
Okay.

2075
01:56:23,040 --> 01:56:24,040
Okay.

2076
01:56:53,040 --> 01:56:55,040
Okay.

2077
01:57:23,040 --> 01:57:24,040
Okay.

2078
01:57:53,040 --> 01:57:55,040
Okay.

2079
01:58:23,040 --> 01:58:25,040
Okay.

2080
01:58:53,040 --> 01:58:54,040
Okay.

2081
01:59:23,040 --> 01:59:24,040
Okay.

2082
01:59:53,040 --> 01:59:54,040
Okay.

2083
02:00:23,040 --> 02:00:30,040
Let's see this.

2084
02:00:30,040 --> 02:00:52,960
Let's, I guess we can start to try to wrap up with our notebooks here.

2085
02:00:52,960 --> 02:00:53,960
Okay.

2086
02:01:23,200 --> 02:01:27,680
Then you can just store those pickle files to your folder and then access them.

2087
02:01:27,680 --> 02:01:30,640
You need to change the code later on.

2088
02:01:30,640 --> 02:01:32,960
Right?

2089
02:01:32,960 --> 02:01:37,960
So just be aware of the permissions of that folder.

2090
02:01:37,960 --> 02:01:39,040
Okay.

2091
02:01:39,040 --> 02:01:42,000
So, let's move on to part four.

2092
02:01:42,000 --> 02:01:43,280
Okay.

2093
02:01:43,280 --> 02:01:48,540
So, here we're going to, like, do the deep learning that I have been talking about a

2094
02:01:48,540 --> 02:01:49,540
lot.

2095
02:01:49,540 --> 02:01:53,260
So we already did the basics, which is the preprocessing.

2096
02:01:53,260 --> 02:01:54,860
Here what we're going to do is we're

2097
02:01:54,860 --> 02:01:58,020
going to build a model, train a model,

2098
02:01:58,020 --> 02:02:00,340
construct a model, train it.

2099
02:02:00,340 --> 02:02:02,460
And we're going to use this RNN.

2100
02:02:02,460 --> 02:02:05,580
It's a very popular deep learning model that's

2101
02:02:05,580 --> 02:02:08,260
commonly used for NLP tests.

2102
02:02:08,260 --> 02:02:11,380
And you will see that we can use RNN, LSTMs, GRUs.

2103
02:02:11,380 --> 02:02:14,180
You can use any of those models.

2104
02:02:14,180 --> 02:02:16,180
You can see how easily later on you

2105
02:02:16,180 --> 02:02:18,620
can see how easy you can use those.

2106
02:02:18,660 --> 02:02:21,660
So the first thing we want to do is load a data set.

2107
02:02:21,660 --> 02:02:25,660
In the previous notebook, we had all the data sets stored

2108
02:02:25,660 --> 02:02:27,300
in the proper format.

2109
02:02:27,300 --> 02:02:29,540
So what we do now is load it again.

2110
02:02:29,540 --> 02:02:31,980
So you can see that each notebook is just

2111
02:02:31,980 --> 02:02:34,780
a continuation of the previous one.

2112
02:02:34,780 --> 02:02:39,900
So now we load it again, the same.

2113
02:02:39,900 --> 02:02:41,780
So let me just try to do something here.

2114
02:02:41,780 --> 02:02:45,260
Because at some point, because I don't have access,

2115
02:02:45,260 --> 02:02:49,820
well, Google Collab only gives you access to 11 gigabytes

2116
02:02:49,820 --> 02:02:50,660
of RAM.

2117
02:02:50,660 --> 02:02:53,860
So at some point, because I'm working on 50,000 records,

2118
02:02:53,860 --> 02:02:55,740
at some point it's going to break

2119
02:02:55,740 --> 02:02:57,580
because I'm using one machine.

2120
02:02:57,580 --> 02:03:08,660
So what I would do here is I would just close this runtime.

2121
02:03:08,660 --> 02:03:09,660
I'll just interrupt.

2122
02:03:10,500 --> 02:03:12,900
And you would need to do the same as well,

2123
02:03:12,900 --> 02:03:17,460
because you're running multiple notebooks on the same data set.

2124
02:03:17,460 --> 02:03:20,020
You can imagine it's using a lot of memory already.

2125
02:03:20,020 --> 02:03:21,860
So it's good to always.

2126
02:03:26,860 --> 02:03:28,660
OK, so load the data.

2127
02:03:28,660 --> 02:03:29,180
Let's see.

2128
02:03:29,180 --> 02:03:30,940
Right, it's the same steps.

2129
02:03:30,940 --> 02:03:33,940
I have to mount my Google Drive all over again,

2130
02:03:33,940 --> 02:03:38,700
because I deleted the session from before.

2131
02:03:38,700 --> 02:03:40,660
And that's how you will give it permission.

2132
02:03:40,660 --> 02:03:44,300
I didn't show the step, but now you can see it.

2133
02:03:44,300 --> 02:03:46,140
Right, and then you can see it.

2134
02:03:46,140 --> 02:03:48,580
So I'm going to close the data set.

2135
02:03:48,580 --> 02:03:51,260
So I'm going to go back to the previous notebook.

2136
02:03:51,260 --> 02:03:53,860
So I'm going to go back to the previous notebook.

2137
02:03:53,860 --> 02:03:56,420
And then I'm going to go back to the previous notebook.

2138
02:03:57,420 --> 02:04:04,300
Right, and then you would enter that code again.

2139
02:04:10,620 --> 02:04:12,500
OK, so it's mounted.

2140
02:04:12,500 --> 02:04:16,500
And then again, I would use the same class all over again

2141
02:04:16,500 --> 02:04:17,780
to load the data set.

2142
02:04:17,780 --> 02:04:20,580
This is just redundant, because I just

2143
02:04:20,580 --> 02:04:23,900
want to make sure that we are just doing the steps right.

2144
02:04:27,380 --> 02:04:29,660
Yeah, so the core part of this tutorial

2145
02:04:29,660 --> 02:04:31,740
is the implementation of the model.

2146
02:04:31,740 --> 02:04:34,020
So we're going to build this model here.

2147
02:04:34,020 --> 02:04:35,860
Let's see if I can get that on the screen.

2148
02:04:56,580 --> 02:04:58,500
So that's what we're going to build.

2149
02:04:58,500 --> 02:05:01,540
So we have an input.

2150
02:05:01,540 --> 02:05:03,380
We have the input, which is x.

2151
02:05:03,380 --> 02:05:05,580
And then we have a layer here.

2152
02:05:05,580 --> 02:05:09,060
We're going to use the GRU unit for that layer.

2153
02:05:09,060 --> 02:05:13,260
And that's just a variation of RNN, a very efficient variation

2154
02:05:13,260 --> 02:05:14,020
of that.

2155
02:05:14,020 --> 02:05:16,260
And then we have a fully connected layer at the top.

2156
02:05:16,260 --> 02:05:18,660
And then we have a SAP master to do the classification.

2157
02:05:18,660 --> 02:05:21,300
There's very standard things that we do in classification,

2158
02:05:21,300 --> 02:05:22,900
very standard model.

2159
02:05:22,900 --> 02:05:26,780
And so now I'm going to jump into the implementation,

2160
02:05:26,780 --> 02:05:28,860
the actual implementation of it.

2161
02:05:28,860 --> 02:05:32,220
So here we have some code.

2162
02:05:32,220 --> 02:05:33,700
It's pretty small over there.

2163
02:05:42,100 --> 02:05:45,340
So we have a model that we want to build here.

2164
02:05:45,340 --> 02:05:47,660
And we have a name.

2165
02:05:47,660 --> 02:05:51,420
It's M-O-G-R-U, which almost stands for emotion.

2166
02:05:51,420 --> 02:05:56,460
And GRU is the type of RNN that we're using here.

2167
02:05:56,460 --> 02:06:00,380
So we have an init function here.

2168
02:06:00,380 --> 02:06:04,900
And notice there is a super class here.

2169
02:06:04,900 --> 02:06:08,340
What we're doing here is we want to inherit from this module,

2170
02:06:08,340 --> 02:06:11,140
which means we have access to all the different classes.

2171
02:06:11,140 --> 02:06:14,660
We inherit all the different methods in this class, this

2172
02:06:14,660 --> 02:06:15,780
module.

2173
02:06:15,780 --> 02:06:19,060
So the first thing we want to do is establish what

2174
02:06:19,060 --> 02:06:20,820
goes into the init layer.

2175
02:06:20,820 --> 02:06:22,180
Sorry, the init method.

2176
02:06:22,180 --> 02:06:26,300
The init method is basically where you specify the

2177
02:06:26,300 --> 02:06:29,300
parameters of the model, which is the batch size, the hidden

2178
02:06:29,300 --> 02:06:32,460
units, the number of hidden units, the embedding

2179
02:06:32,460 --> 02:06:33,540
dimension.

2180
02:06:33,540 --> 02:06:37,220
And now we're going to use embeddings, the vocabulary

2181
02:06:37,220 --> 02:06:39,540
size, the output size, all of those different types of

2182
02:06:39,540 --> 02:06:41,780
information that we need for the model.

2183
02:06:41,780 --> 02:06:44,260
And that's where you would put that information.

2184
02:06:44,260 --> 02:06:49,180
And also for the layers, we're going to specify what kind of

2185
02:06:49,220 --> 02:06:50,900
layers we're going to use.

2186
02:06:50,900 --> 02:06:53,500
So here we're going to use an embedding layer.

2187
02:06:53,500 --> 02:06:56,380
And I'll explain more in detail what that means.

2188
02:06:56,380 --> 02:06:58,620
We're going to use a dropout layer.

2189
02:06:58,620 --> 02:07:02,100
It's really good for regularization, which is

2190
02:07:02,100 --> 02:07:06,180
allowing our model to more efficiently train.

2191
02:07:06,180 --> 02:07:07,660
And then we have a GRU layer.

2192
02:07:07,660 --> 02:07:10,900
And then we have a linear layer at the end here.

2193
02:07:10,900 --> 02:07:13,460
So you can see it's like a chain of different operations

2194
02:07:13,460 --> 02:07:14,380
going on here.

2195
02:07:14,380 --> 02:07:16,660
So a lot of things are hidden here.

2196
02:07:16,660 --> 02:07:20,660
But the most important thing to know here is that you can

2197
02:07:20,660 --> 02:07:24,020
see that it's just a chain of operations.

2198
02:07:24,020 --> 02:07:30,340
And that's how you would build your deep learning model here.

2199
02:07:30,340 --> 02:07:34,900
And another important part of when you're using RNN is that

2200
02:07:34,900 --> 02:07:40,140
you need to initialize the beginning of the training.

2201
02:07:40,140 --> 02:07:42,260
So what you do is a hidden state.

2202
02:07:42,260 --> 02:07:43,740
It's called a hidden state.

2203
02:07:43,740 --> 02:07:45,700
And what you do is initialize those to zero.

2204
02:07:45,700 --> 02:07:49,540
So it's not so important what goes here.

2205
02:07:49,540 --> 02:07:52,900
We just want to initialize those to zeroes.

2206
02:07:52,900 --> 02:07:54,340
But there is a tutorial.

2207
02:07:54,340 --> 02:07:55,460
I would strongly recommend.

2208
02:07:55,460 --> 02:07:58,820
There's a Python tutorial where you can go and read more

2209
02:07:58,820 --> 02:08:00,420
about what this function is doing.

2210
02:08:04,140 --> 02:08:06,220
And this is the forward.

2211
02:08:06,220 --> 02:08:10,900
This is where the input enters the neural network.

2212
02:08:10,900 --> 02:08:16,420
So here we have first we want to embed the inputs, which

2213
02:08:16,420 --> 02:08:20,580
means that we're going to get embeddings from both inputs.

2214
02:08:20,580 --> 02:08:27,060
And then we have a hidden state that we're initializing.

2215
02:08:27,060 --> 02:08:32,540
And then we have the GRU layer here.

2216
02:08:32,540 --> 02:08:33,900
And then we have some outputs.

2217
02:08:33,900 --> 02:08:36,020
After the transformation, we have some output.

2218
02:08:36,020 --> 02:08:39,500
And then we have an extra dropout layer that we added.

2219
02:08:39,500 --> 02:08:41,100
And then we have the fully connected layer.

2220
02:08:41,100 --> 02:08:45,020
So when you look at it, it's basically this, right?

2221
02:08:45,020 --> 02:08:47,140
That's what we're building here.

2222
02:08:47,140 --> 02:08:50,060
The only thing that was missing here is the softmax.

2223
02:08:50,060 --> 02:08:51,820
We didn't specify the softmax here.

2224
02:08:51,820 --> 02:08:54,900
So typically it's like a common practice

2225
02:08:54,900 --> 02:08:59,140
to have the softmax being executed later on,

2226
02:08:59,140 --> 02:09:02,580
like in a separate function.

2227
02:09:02,580 --> 02:09:04,980
So I'll show later where that is.

2228
02:09:04,980 --> 02:09:07,420
So that's basically your model here.

2229
02:09:07,420 --> 02:09:09,540
That's your basic GRU model.

2230
02:09:09,540 --> 02:09:11,740
And it only takes a couple lines of code.

2231
02:09:11,740 --> 02:09:14,260
I actually have extra lines of code

2232
02:09:14,260 --> 02:09:15,540
that I don't really need here.

2233
02:09:15,540 --> 02:09:17,740
I could have just made one line.

2234
02:09:17,740 --> 02:09:19,060
It's pretty simple.

2235
02:09:19,060 --> 02:09:20,700
And that's why I said at the beginning

2236
02:09:20,700 --> 02:09:22,620
that the chaining was the most important part,

2237
02:09:22,620 --> 02:09:25,460
because you basically add more dropout maybe,

2238
02:09:25,460 --> 02:09:27,300
or add another fully connected layer,

2239
02:09:27,300 --> 02:09:30,660
or whatever you want to add here.

2240
02:09:30,660 --> 02:09:31,340
Yes?

2241
02:09:31,340 --> 02:09:36,620
So if you follow up to the feature of the model,

2242
02:09:36,620 --> 02:09:39,020
you have four GRUs, right?

2243
02:09:39,020 --> 02:09:39,540
Yes.

2244
02:09:39,540 --> 02:09:42,740
So I guess one question is, there's a solid line

2245
02:09:42,740 --> 02:09:45,420
and then there's a dashed line.

2246
02:09:45,420 --> 02:09:46,980
What's the difference between them?

2247
02:09:46,980 --> 02:09:50,820
And also, what's the difference between x0 to x1?

2248
02:09:50,820 --> 02:09:51,320
Yeah.

2249
02:09:51,320 --> 02:09:54,020
What's the difference between x0 to x6?

2250
02:09:54,020 --> 02:09:54,620
OK.

2251
02:09:54,620 --> 02:10:00,060
So x67 means it's just the size of the input.

2252
02:10:00,060 --> 02:10:00,980
Is 67 units?

2253
02:10:00,980 --> 02:10:02,300
Yeah.

2254
02:10:02,300 --> 02:10:04,900
Yeah, 67 is the size of the input.

2255
02:10:04,900 --> 02:10:06,540
So imagine the tensor that I showed

2256
02:10:06,540 --> 02:10:08,580
with the numerical values without the padding

2257
02:10:08,580 --> 02:10:09,540
and everything.

2258
02:10:09,540 --> 02:10:12,180
So the max size was 68.

2259
02:10:12,180 --> 02:10:15,260
That's what we have from 0 to 67.

2260
02:10:15,260 --> 02:10:17,020
That's what the figure explains.

2261
02:10:17,020 --> 02:10:19,580
Although the figure is pretty high level,

2262
02:10:19,580 --> 02:10:22,020
so you won't get that information from here.

2263
02:10:22,020 --> 02:10:25,900
And I'm glad you asked, because yeah, it's a clarify.

2264
02:10:25,900 --> 02:10:27,460
And then the GRU, of course, is just

2265
02:10:27,460 --> 02:10:33,260
like the whatever you want to make that layer, whatever size.

2266
02:10:33,260 --> 02:10:38,740
Here, I only have each input going into each unit.

2267
02:10:38,740 --> 02:10:42,020
Because at the end of the day, when you're doing an RNN,

2268
02:10:42,020 --> 02:10:43,620
you have to do it for each input.

2269
02:10:43,620 --> 02:10:45,420
You would have to have a unit.

2270
02:10:45,420 --> 02:10:50,060
Other transformations have to happen for each component

2271
02:10:50,060 --> 02:10:52,500
of the input.

2272
02:10:52,500 --> 02:10:54,820
That's just how the RNN is designed.

2273
02:10:58,620 --> 02:11:02,300
But if you were using something like a standard neural network,

2274
02:11:02,340 --> 02:11:05,940
then the unit size doesn't have to be of that same size.

2275
02:11:05,940 --> 02:11:07,780
It should not be consistent with that.

2276
02:11:07,780 --> 02:11:08,980
It could be a different size.

2277
02:11:11,860 --> 02:11:14,660
So when you find the model, it seems

2278
02:11:14,660 --> 02:11:17,500
like you have GRU only listed once,

2279
02:11:17,500 --> 02:11:20,460
rather than multiple times, or in dynamic.

2280
02:11:20,460 --> 02:11:22,660
Right.

2281
02:11:22,660 --> 02:11:25,660
So here, you saw that.

2282
02:11:25,660 --> 02:11:27,700
So let me just explain a little bit more on that.

2283
02:11:27,700 --> 02:11:30,460
So we have this, which is a GRU.

2284
02:11:30,460 --> 02:11:34,140
You notice that the GRU, the first dimension

2285
02:11:34,140 --> 02:11:38,140
is the embedding dimension, which should be the input.

2286
02:11:38,140 --> 02:11:40,580
And then the other one is the hitting units,

2287
02:11:40,580 --> 02:11:41,980
the number of hitting units.

2288
02:11:41,980 --> 02:11:44,260
So these two are separate things.

2289
02:11:44,260 --> 02:11:49,180
So the hitting units can be any size you wish,

2290
02:11:49,180 --> 02:11:51,940
just like a fully connected layer can be any size,

2291
02:11:51,940 --> 02:11:52,820
like what we said.

2292
02:11:52,820 --> 02:11:54,740
But the embedding dimension is very important

2293
02:11:54,740 --> 02:11:58,660
that it's very consistent and represents the input dimension.

2294
02:12:00,660 --> 02:12:01,380
Is that right?

2295
02:12:01,380 --> 02:12:01,860
Yeah.

2296
02:12:01,860 --> 02:12:02,340
OK.

2297
02:12:07,260 --> 02:12:08,980
I mean, I think the documentation

2298
02:12:08,980 --> 02:12:12,100
does a pretty good job of explaining it very well.

2299
02:12:12,100 --> 02:12:14,900
That's why I always recommend that if you really

2300
02:12:14,900 --> 02:12:16,460
want to know more of what it's doing,

2301
02:12:16,460 --> 02:12:18,140
because this is just very high level,

2302
02:12:18,140 --> 02:12:20,060
if you want to know more, I really

2303
02:12:20,060 --> 02:12:21,660
recommend that you look at it.

2304
02:12:26,060 --> 02:12:26,580
Right.

2305
02:12:26,580 --> 02:12:30,020
So then we have, let's see.

2306
02:12:30,020 --> 02:12:31,940
Let's just try to run the block of code.

2307
02:12:34,500 --> 02:12:36,380
And one of the things that I really

2308
02:12:36,380 --> 02:12:40,420
like to do when building a model is pre-testing.

2309
02:12:40,420 --> 02:12:42,220
It's something I call pre-testing.

2310
02:12:42,220 --> 02:12:45,140
So what you usually do after you implement your model

2311
02:12:45,140 --> 02:12:46,740
is just go straight into training.

2312
02:12:46,740 --> 02:12:49,180
That's the typical thing that you would do.

2313
02:12:49,180 --> 02:12:52,660
But what I want to know is exactly what's going on,

2314
02:12:52,660 --> 02:12:54,420
what each transformation is doing.

2315
02:12:54,420 --> 02:12:57,140
I just like, I'm very curious what's going on, right?

2316
02:12:57,140 --> 02:12:58,980
So I like to pre-test the model.

2317
02:12:58,980 --> 02:13:00,940
So what I do is just generate some code.

2318
02:13:07,700 --> 02:13:10,180
So these are just the parameters of the model,

2319
02:13:10,180 --> 02:13:12,660
like the batch size, the embedding dimension,

2320
02:13:12,660 --> 02:13:15,140
the number of units for the layers.

2321
02:13:15,140 --> 02:13:15,640
Yeah.

2322
02:13:15,640 --> 02:13:18,620
Do you need to say that it's 6A or something?

2323
02:13:18,620 --> 02:13:21,260
Because the maximum size of the model is 6A?

2324
02:13:21,260 --> 02:13:21,860
No.

2325
02:13:21,860 --> 02:13:23,460
So one thing I didn't explain.

2326
02:13:23,460 --> 02:13:27,060
So we have the inputs, and the max length of the input

2327
02:13:27,060 --> 02:13:28,340
is 67.

2328
02:13:28,340 --> 02:13:31,420
But after that, after we have that input,

2329
02:13:31,420 --> 02:13:34,220
then we have to generate embeddings.

2330
02:13:34,220 --> 02:13:36,860
So embedding, that's why we call it, is like a layer.

2331
02:13:36,860 --> 02:13:39,780
So here we're going to generate embeddings for each one

2332
02:13:39,780 --> 02:13:40,740
of those tokens.

2333
02:13:40,740 --> 02:13:42,900
So now we don't have that max length, which

2334
02:13:42,900 --> 02:13:44,140
is going to be the 67.

2335
02:13:44,140 --> 02:13:47,580
Now the max length is going to be 256.

2336
02:13:47,580 --> 02:13:51,100
That's going to be the vector for each word.

2337
02:13:51,100 --> 02:13:53,540
So all of that, you can see that it's hidden here.

2338
02:13:53,540 --> 02:13:55,340
It's not really, it's not explicit.

2339
02:13:55,340 --> 02:13:57,820
You can see that it's just a line of code that's

2340
02:13:57,820 --> 02:13:59,020
doing all of that.

2341
02:13:59,020 --> 02:14:01,420
From how we went from the tensors

2342
02:14:01,420 --> 02:14:04,220
with just the numerical values into the embeddings.

2343
02:14:04,220 --> 02:14:06,940
And it's just line one of code that takes care of that.

2344
02:14:10,380 --> 02:14:13,940
So we're taking, you have a vocabulary,

2345
02:14:13,940 --> 02:14:18,100
and you want to embed in a space all these words.

2346
02:14:18,100 --> 02:14:21,980
And you want to have these words to have some kind of relationship

2347
02:14:21,980 --> 02:14:24,400
with each other, some kind of what we call a semantic

2348
02:14:24,400 --> 02:14:25,220
relationship.

2349
02:14:26,180 --> 02:14:28,780
What's the relationship between king and queen?

2350
02:14:28,780 --> 02:14:30,540
So in an embedding space, these words

2351
02:14:30,540 --> 02:14:33,140
would probably be closer to each other.

2352
02:14:33,140 --> 02:14:34,980
And so in order for us to do that,

2353
02:14:34,980 --> 02:14:37,620
we need to encode these words.

2354
02:14:37,620 --> 02:14:39,860
We have to have some kind of encoding.

2355
02:14:39,860 --> 02:14:43,620
And that encoding is basically like what we call Latin features.

2356
02:14:43,620 --> 02:14:45,860
It's just features that the model just

2357
02:14:45,860 --> 02:14:48,940
chooses automatically, because it's a deep learning model

2358
02:14:48,940 --> 02:14:49,740
anyways.

2359
02:14:49,740 --> 02:14:51,820
So it chooses some Latin features,

2360
02:14:51,820 --> 02:14:54,980
which basically explains the semantic relationship

2361
02:14:54,980 --> 02:14:56,860
between words in that space.

2362
02:14:56,860 --> 02:15:02,340
So we have the king and queen having some kind of relationship.

2363
02:15:02,340 --> 02:15:05,140
And so you have that the distance between king and queen

2364
02:15:05,140 --> 02:15:08,060
will be much closer to what it is to maybe say something

2365
02:15:08,060 --> 02:15:12,580
like dog, that will have a further relationship.

2366
02:15:12,580 --> 02:15:14,420
So that's basically the concept of embedding.

2367
02:15:14,420 --> 02:15:16,700
I mean, there's a lot more information about embeddings

2368
02:15:16,700 --> 02:15:18,260
that I'm not sharing here.

2369
02:15:18,260 --> 02:15:22,700
But the point is that we need to have an efficient way of encoding

2370
02:15:22,700 --> 02:15:24,020
that input.

2371
02:15:24,020 --> 02:15:27,540
We're not going to use just the basic indices.

2372
02:15:27,540 --> 02:15:29,820
We can use those features as just the basic indices,

2373
02:15:29,820 --> 02:15:32,340
but we want to use something more efficient.

2374
02:15:32,340 --> 02:15:34,100
So that's why we use word embeddings.

2375
02:15:39,260 --> 02:15:42,020
So yeah, and I mean, I didn't mention at the beginning

2376
02:15:42,020 --> 02:15:45,060
that I'm really not going to go into the theory of all

2377
02:15:45,060 --> 02:15:49,420
of the different options and design choices that I use here.

2378
02:15:49,420 --> 02:15:52,340
The whole point is that you can get an idea of how

2379
02:15:52,340 --> 02:15:56,260
to use all the building blocks that PyTorch offers

2380
02:15:56,260 --> 02:15:59,340
to get the LMP task done.

2381
02:15:59,340 --> 02:16:02,100
That's the idea.

2382
02:16:02,100 --> 02:16:05,340
So sometimes it may feel like I'm going a little bit fast,

2383
02:16:05,340 --> 02:16:09,700
but I'm just trying to share some of my experience

2384
02:16:09,700 --> 02:16:13,820
and the kind of things that I do building LMP models.

2385
02:16:13,820 --> 02:16:18,020
So after that, we have everything.

2386
02:16:18,020 --> 02:16:19,420
We have the model.

2387
02:16:19,420 --> 02:16:24,820
And now we want to do, we have the batches, right?

2388
02:16:24,820 --> 02:16:26,700
I spoke about the batches earlier on.

2389
02:16:26,700 --> 02:16:29,460
We have all the different batches

2390
02:16:29,460 --> 02:16:31,220
that we're going to feed them all.

2391
02:16:31,220 --> 02:16:32,580
And we have this function here.

2392
02:16:32,580 --> 02:16:35,220
Now, this function, I don't really use it here,

2393
02:16:35,220 --> 02:16:37,940
but this is a very useful function for in case.

2394
02:16:37,940 --> 02:16:40,100
I remember earlier on I said that sometimes I

2395
02:16:40,100 --> 02:16:43,820
would want the batches to be of the same size.

2396
02:16:43,820 --> 02:16:46,260
So for some tasks, that works.

2397
02:16:46,260 --> 02:16:48,100
So I would want them to be of the same size.

2398
02:16:48,100 --> 02:16:50,860
So this function takes care of making sure

2399
02:16:50,860 --> 02:16:55,620
that some of those batches are consistent,

2400
02:16:55,620 --> 02:16:57,220
like the size is consistent.

2401
02:16:57,220 --> 02:16:59,700
So that very simple function does that.

2402
02:16:59,700 --> 02:17:01,700
And also what it does, it transforms

2403
02:17:01,700 --> 02:17:05,580
the dimensions of the batches.

2404
02:17:05,580 --> 02:17:06,580
But we won't use it here.

2405
02:17:06,580 --> 02:17:08,900
This code is more useful for when

2406
02:17:08,900 --> 02:17:10,620
I'm doing something like machine translation.

2407
02:17:10,620 --> 02:17:12,300
But when I was preparing this tutorial,

2408
02:17:12,300 --> 02:17:14,100
I just kind of borrowed some code from there,

2409
02:17:14,100 --> 02:17:16,180
and I just adopted it to the classification test.

2410
02:17:16,180 --> 02:17:16,820
So I show it here.

2411
02:17:16,820 --> 02:17:20,780
Maybe it could be useful if you want to ever try that.

2412
02:17:20,780 --> 02:17:23,180
So I'm not really going to explain that, no, exactly

2413
02:17:23,180 --> 02:17:25,740
what it does, and I'm not going to use it.

2414
02:17:25,740 --> 02:17:28,380
So yeah, I did have some example on what it does.

2415
02:17:28,380 --> 02:17:31,700
So I have code for here.

2416
02:17:31,700 --> 02:17:36,500
So now let's explain the details of this block here.

2417
02:17:36,500 --> 02:17:37,580
So we have a device.

2418
02:17:37,580 --> 02:17:39,620
Now, the first thing you want to do before training

2419
02:17:39,620 --> 02:17:41,180
is what kind of device you want to use.

2420
02:17:41,180 --> 02:17:42,020
You want to use a CPU.

2421
02:17:42,020 --> 02:17:44,220
If you have only access to that, then you use that.

2422
02:17:44,260 --> 02:17:48,100
But if you have access to GPU, then you want to use that.

2423
02:17:48,100 --> 02:17:52,500
So typically, what we do is we choose the selected device

2424
02:17:52,500 --> 02:17:56,140
that we're going to use, and then we have the model.

2425
02:17:56,140 --> 02:17:58,380
Now we'll create an instance of the model here.

2426
02:17:58,380 --> 02:18:04,860
Remember, the emoji here represents this one, right?

2427
02:18:04,860 --> 02:18:07,740
And we're specifying all the different parameters

2428
02:18:07,740 --> 02:18:09,900
that we need here.

2429
02:18:09,900 --> 02:18:13,740
And those parameters are specified here.

2430
02:18:14,060 --> 02:18:17,420
The target size, batch size, the number of units,

2431
02:18:17,420 --> 02:18:19,900
the embedding dimension, and the vocabulary also.

2432
02:18:19,900 --> 02:18:23,820
All this information that we collected already previously.

2433
02:18:23,820 --> 02:18:26,580
And then we say, OK, we want to put that model

2434
02:18:26,580 --> 02:18:28,420
into the device that's available.

2435
02:18:28,420 --> 02:18:30,020
In this case, we have access to GPU

2436
02:18:30,020 --> 02:18:33,380
because the Google guys are very generous.

2437
02:18:33,380 --> 02:18:35,180
So they provide GPU for us.

2438
02:18:35,180 --> 02:18:36,260
Why not use it?

2439
02:18:36,260 --> 02:18:37,380
So we use it.

2440
02:18:37,380 --> 02:18:41,900
Then we have a sample here.

2441
02:18:41,900 --> 02:18:43,380
I just generate the sample.

2442
02:18:43,420 --> 02:18:46,980
So notice that there is an iterator here.

2443
02:18:46,980 --> 02:18:50,580
An iterator, which means that I want to just select

2444
02:18:50,580 --> 02:18:52,980
an example of the training data set.

2445
02:18:52,980 --> 02:18:56,180
So that means I want to collect one batch, at least.

2446
02:18:56,180 --> 02:18:58,940
And we want to check what that batch has.

2447
02:18:58,940 --> 02:19:00,580
So now we have that batch.

2448
02:19:00,580 --> 02:19:04,140
And I'm going to sort that batch.

2449
02:19:04,140 --> 02:19:05,900
I have that batch here, and I sort it.

2450
02:19:05,900 --> 02:19:09,780
And so now, let's see.

2451
02:19:09,780 --> 02:19:15,740
So the size of it is this, which is the 69 again.

2452
02:19:15,740 --> 02:19:17,820
Again, it's 69.

2453
02:19:17,820 --> 02:19:18,820
Not consistent, right?

2454
02:19:18,820 --> 02:19:19,500
Because this is 68.

2455
02:19:19,500 --> 02:19:20,220
No, it's 69.

2456
02:19:20,220 --> 02:19:24,260
But 69 because we added a padding, which is the 0, right?

2457
02:19:24,260 --> 02:19:27,340
So we have that extra element that we

2458
02:19:27,340 --> 02:19:29,860
didn't talk about much.

2459
02:19:29,860 --> 02:19:32,100
So that's the size of the batch.

2460
02:19:32,100 --> 02:19:34,180
So it's 69 by 64.

2461
02:19:34,180 --> 02:19:35,700
Notice the dimension, right?

2462
02:19:35,700 --> 02:19:37,980
So it's not batch size first.

2463
02:19:37,980 --> 02:19:41,580
It's max length first.

2464
02:19:41,580 --> 02:19:43,660
So that's key when you're going to train,

2465
02:19:43,660 --> 02:19:46,380
because we want to make sure that those dimensions are

2466
02:19:46,380 --> 02:19:48,540
in the right places.

2467
02:19:48,540 --> 02:19:51,100
So that's why I say it's good to always do the pretesting,

2468
02:19:51,100 --> 02:19:53,220
what I call the pretesting, because just making sure

2469
02:19:53,220 --> 02:19:55,220
that before we train the model, that we

2470
02:19:55,220 --> 02:19:57,340
have everything in the proper dimensions,

2471
02:19:57,340 --> 02:19:59,900
in the proper sizes.

2472
02:19:59,900 --> 02:20:03,740
And then here, we set up a training.

2473
02:20:03,740 --> 02:20:07,780
Again, I'm just noticing I'm creating another instance

2474
02:20:07,780 --> 02:20:10,140
again, and just repeating the instance again,

2475
02:20:10,140 --> 02:20:11,500
because again, this is separate.

2476
02:20:11,500 --> 02:20:14,060
Pretesting is separate from the training.

2477
02:20:14,060 --> 02:20:16,420
Now we're going to do the training.

2478
02:20:16,420 --> 02:20:19,020
And we have two things here, which is the cross-entropy

2479
02:20:19,020 --> 02:20:21,780
loss, which is basically just computing

2480
02:20:21,780 --> 02:20:25,060
what the loss of the model is after a couple iterations

2481
02:20:25,060 --> 02:20:26,220
of training.

2482
02:20:26,220 --> 02:20:28,500
Now, the details of it are not so important.

2483
02:20:28,500 --> 02:20:30,580
You can read about what each loss,

2484
02:20:30,580 --> 02:20:33,980
because there's so many different losses that you can use.

2485
02:20:33,980 --> 02:20:36,700
Just go ahead and read up on the tutorial,

2486
02:20:36,700 --> 02:20:39,500
on the documentation, and see what each one represents.

2487
02:20:39,500 --> 02:20:40,420
But we need a loss.

2488
02:20:40,420 --> 02:20:42,140
We need to compute that loss, because we

2489
02:20:42,140 --> 02:20:45,100
need to know if the model is optimizing well.

2490
02:20:45,100 --> 02:20:46,380
And then we have an optimizer.

2491
02:20:46,380 --> 02:20:49,740
An optimizer is just what takes care

2492
02:20:49,740 --> 02:20:54,220
of the training, the back propagation part.

2493
02:20:54,220 --> 02:20:58,140
Again, you have a lot of options on what kind of optimizers

2494
02:20:58,140 --> 02:20:59,140
you want to use.

2495
02:20:59,140 --> 02:21:01,540
There's several of them.

2496
02:21:01,540 --> 02:21:03,460
And then these are the functions, loss function

2497
02:21:03,460 --> 02:21:09,460
and accuracy, just to check what the loss is.

2498
02:21:09,460 --> 02:21:11,580
The accuracy is just the accuracy of the model.

2499
02:21:11,580 --> 02:21:14,300
So two separate values that we want

2500
02:21:14,300 --> 02:21:18,100
to collect after training, after each round of training.

2501
02:21:18,100 --> 02:21:22,260
So we have the training now here, finally.

2502
02:21:22,260 --> 02:21:26,420
And you see it's a lot of code here.

2503
02:21:27,420 --> 02:21:35,140
So the most important thing is that we want to do accuracy.

2504
02:21:35,140 --> 02:21:37,020
We want to collect loss.

2505
02:21:37,020 --> 02:21:41,220
Those are the two things we want to keep track of.

2506
02:21:41,220 --> 02:21:43,540
We're going to run this for 10 epochs, which

2507
02:21:43,540 --> 02:21:46,140
is 10 iterations.

2508
02:21:46,140 --> 02:21:47,740
So we do that.

2509
02:21:47,740 --> 02:21:49,300
So notice a for loop here.

2510
02:21:49,300 --> 02:21:51,220
That's going to be 10 iterations.

2511
02:21:51,220 --> 02:21:53,140
And then we have the training, finally.

2512
02:21:53,140 --> 02:21:58,060
So here we have the training data set instance here.

2513
02:21:58,060 --> 02:22:02,340
And now we can just iterate over each batch

2514
02:22:02,340 --> 02:22:04,420
and start to do the training.

2515
02:22:04,420 --> 02:22:07,460
Now, before, what you used to do with this one

2516
02:22:07,460 --> 02:22:10,100
is you have to do all the preparatory batches here

2517
02:22:10,100 --> 02:22:10,940
in this code.

2518
02:22:10,940 --> 02:22:12,340
So it's pretty messy that you have

2519
02:22:12,340 --> 02:22:14,100
to do all the batches here.

2520
02:22:14,100 --> 02:22:16,740
And then this code can get pretty long

2521
02:22:16,740 --> 02:22:19,300
if you are doing all that code here.

2522
02:22:19,300 --> 02:22:22,580
So while I was saying that the data set classes and the data

2523
02:22:22,580 --> 02:22:25,340
loader makes it so easy because now I'm just

2524
02:22:25,340 --> 02:22:27,460
using that train data set iterator

2525
02:22:27,460 --> 02:22:32,180
and I'm just getting the batches that I need to train.

2526
02:22:32,180 --> 02:22:33,620
Right, so there's more code here.

2527
02:22:33,620 --> 02:22:35,260
And then we have the model, of course.

2528
02:22:35,260 --> 02:22:39,660
Then we're passing in the input, passing in the length,

2529
02:22:39,660 --> 02:22:41,300
and the kind of device we want to use.

2530
02:22:41,300 --> 02:22:42,980
If we want to use a GPU or a CPU,

2531
02:22:42,980 --> 02:22:46,220
in this case we want to use a GPU for faster training.

2532
02:22:46,220 --> 02:22:48,540
And then we're just collecting loss,

2533
02:22:48,540 --> 02:22:50,460
computing the loss properly here.

2534
02:22:50,460 --> 02:22:53,020
And then we have this code here that we say,

2535
02:22:53,020 --> 02:22:54,100
OK, now it's time.

2536
02:22:54,100 --> 02:22:55,500
We have done the forward.

2537
02:22:55,500 --> 02:22:57,900
Now we want to do the backward part,

2538
02:22:57,900 --> 02:23:03,140
specify what the optimizer is here, the backward part.

2539
02:23:03,140 --> 02:23:04,100
And we want to do this.

2540
02:23:04,100 --> 02:23:06,740
So this is just pretty standard code

2541
02:23:06,740 --> 02:23:09,340
that you use when you're training models with PyTorch.

2542
02:23:09,340 --> 02:23:10,980
So this is not something that I made up.

2543
02:23:10,980 --> 02:23:12,140
It's just very common.

2544
02:23:12,140 --> 02:23:13,580
If you go to the tutorial, you will

2545
02:23:13,580 --> 02:23:17,100
see that these three lines of code are always there.

2546
02:23:17,100 --> 02:23:18,260
You always need them.

2547
02:23:18,260 --> 02:23:19,260
They're always there.

2548
02:23:19,260 --> 02:23:22,220
It means that you can write lines in one of the code.

2549
02:23:22,220 --> 02:23:23,220
You can do it in the new code.

2550
02:23:23,220 --> 02:23:25,220
If you want to do the code, you would have to write

2551
02:23:25,220 --> 02:23:26,700
the same three lines.

2552
02:23:26,700 --> 02:23:27,180
I get it.

2553
02:23:27,180 --> 02:23:28,300
That's a great question.

2554
02:23:28,300 --> 02:23:28,820
I guess.

2555
02:23:28,820 --> 02:23:31,220
You can narrow the pair of code and do it better.

2556
02:23:31,220 --> 02:23:32,340
Yeah.

2557
02:23:32,340 --> 02:23:34,580
I think that would be something that they

2558
02:23:34,580 --> 02:23:37,340
would want to work on, because yeah, it's just something

2559
02:23:37,340 --> 02:23:40,100
that you need to put in there.

2560
02:23:40,100 --> 02:23:40,740
Very standard.

2561
02:23:44,460 --> 02:23:45,900
And if you're missing one of these,

2562
02:23:45,900 --> 02:23:47,980
you're probably not going to get the training done.

2563
02:23:48,940 --> 02:23:49,900
Very important.

2564
02:23:49,900 --> 02:23:51,340
So you have accuracy as well.

2565
02:23:51,340 --> 02:23:54,660
The collected loss, we have accuracy as well.

2566
02:23:54,660 --> 02:23:56,540
It's not very important to know all the details,

2567
02:23:56,540 --> 02:23:59,660
but you'll have time to go over it.

2568
02:23:59,660 --> 02:24:04,500
And then we have, yeah, we want to have some just,

2569
02:24:04,500 --> 02:24:06,820
we don't want to do it for every iteration,

2570
02:24:06,820 --> 02:24:08,860
but we want to every 100 batches,

2571
02:24:08,860 --> 02:24:13,100
we want to know a good track of the loss and the accuracy.

2572
02:24:13,100 --> 02:24:16,900
And notice that here, I'm using some special functionality

2573
02:24:16,900 --> 02:24:18,500
here.

2574
02:24:18,500 --> 02:24:21,460
So the batch loss, I need to put it into the CPU.

2575
02:24:21,460 --> 02:24:24,060
It's more efficient to do that, because it's just a number.

2576
02:24:24,060 --> 02:24:26,980
Why would I want to put this into a GPU?

2577
02:24:26,980 --> 02:24:29,540
So you can see that I put it in the CPU, and then I detach it,

2578
02:24:29,540 --> 02:24:33,580
and then I just convert it into NumPy, and it's just a value.

2579
02:24:33,580 --> 02:24:35,900
So I have the flexibility to do whatever

2580
02:24:35,900 --> 02:24:39,620
I want doing training with my data.

2581
02:24:39,620 --> 02:24:43,220
Then we have validation, the same code.

2582
02:24:43,220 --> 02:24:45,420
It's almost the same code that you use.

2583
02:24:45,420 --> 02:24:47,460
The only part is that you would see this missing,

2584
02:24:47,460 --> 02:24:51,140
because we are just checking what's the accuracy of the model.

2585
02:24:51,140 --> 02:24:53,540
We are not going to do training again.

2586
02:24:53,540 --> 02:24:56,340
We're not going to do it backward again.

2587
02:24:56,340 --> 02:24:59,020
We cannot do that in validation, because that's just

2588
02:24:59,020 --> 02:25:01,820
typical things in deep learning.

2589
02:25:01,820 --> 02:25:04,540
So I have it already.

2590
02:25:04,540 --> 02:25:06,860
I don't want to run it, because I'm just

2591
02:25:06,860 --> 02:25:08,460
suspicious that it will break.

2592
02:25:08,460 --> 02:25:10,780
So I have trained it already.

2593
02:25:10,780 --> 02:25:14,380
So you can see here that the training accuracy is 93,

2594
02:25:14,380 --> 02:25:17,100
and the validation accuracy is 92.

2595
02:25:17,100 --> 02:25:20,140
That's pretty good, I guess.

2596
02:25:20,140 --> 02:25:23,580
For this kind of task, this is a multi-class classification.

2597
02:25:23,580 --> 02:25:27,900
This is not a binary, so it's a pretty complicated task,

2598
02:25:27,900 --> 02:25:29,820
I would say, in my opinion.

2599
02:25:29,820 --> 02:25:32,340
But you can see that a very, very simple model,

2600
02:25:32,340 --> 02:25:36,500
like a GRU with dropout, a fully connected layer,

2601
02:25:36,500 --> 02:25:40,140
and a softmax layer, can achieve this accuracy

2602
02:25:40,140 --> 02:25:42,940
on that very complex data set.

2603
02:25:42,940 --> 02:25:48,780
It could show you the power of deep learning here for MLP.

2604
02:25:48,780 --> 02:25:51,060
And you can see that after each epoch,

2605
02:25:51,060 --> 02:25:55,180
you can see that it stays relatively the same.

2606
02:25:55,180 --> 02:25:59,340
And at some point, it will overfit at some point.

2607
02:25:59,340 --> 02:26:02,380
Overfitting means that there is some huge difference

2608
02:26:02,380 --> 02:26:04,540
between the validation and training accuracy.

2609
02:26:04,540 --> 02:26:07,660
So see that.

2610
02:26:07,660 --> 02:26:09,260
And then you need to stop it.

2611
02:26:09,260 --> 02:26:11,180
But I just have 10 iterations here,

2612
02:26:11,180 --> 02:26:15,300
and you notice that we have a very high accuracy.

2613
02:26:15,300 --> 02:26:16,740
At some point, you want to stop it.

2614
02:26:16,740 --> 02:26:19,340
And there are methods that you can use, like early stopping.

2615
02:26:19,340 --> 02:26:20,940
I'm not going to go into that details.

2616
02:26:20,940 --> 02:26:23,620
I have another notebook that talks about this,

2617
02:26:23,620 --> 02:26:27,300
so I will share that with you later on.

2618
02:26:27,300 --> 02:26:29,860
And the other thing that we want to do after we have trained

2619
02:26:29,860 --> 02:26:32,460
the model, we want to store it.

2620
02:26:32,460 --> 02:26:35,140
Sometimes we want to do some inferencing on the model,

2621
02:26:35,140 --> 02:26:36,180
and so we store it.

2622
02:26:36,180 --> 02:26:39,780
So there's a very convenient function that we can use.

2623
02:26:39,780 --> 02:26:41,180
PyTorch offers this one.

2624
02:26:41,180 --> 02:26:44,260
So it's just like torch.save, and we have the model.

2625
02:26:44,260 --> 02:26:48,220
And now everything is saved and serialized properly.

2626
02:26:48,220 --> 02:26:51,980
So the model name is just emoji.ru here.

2627
02:26:51,980 --> 02:26:54,860
I know if I want to use that or share it with someone,

2628
02:26:54,860 --> 02:26:55,780
I could just use that.

2629
02:26:55,780 --> 02:26:59,820
And then that person would just need to load the model,

2630
02:26:59,820 --> 02:27:01,780
basically, and do inferencing on it.

2631
02:27:01,780 --> 02:27:04,860
That's basically what would happen.

2632
02:27:04,860 --> 02:27:07,220
So that's the end of the notebook itself.

2633
02:27:07,220 --> 02:27:11,060
And the other part is how to load this model

2634
02:27:11,060 --> 02:27:12,420
and how to properly evaluate it.

2635
02:27:12,420 --> 02:27:14,980
That's kind of the last segment of the tutorial.

2636
02:27:14,980 --> 02:27:18,140
I think let's see how much time do we have.

2637
02:27:18,140 --> 02:27:20,820
So the end of the tutorial is 420, right?

2638
02:27:20,820 --> 02:27:21,780
420.

2639
02:27:21,780 --> 02:27:27,620
OK, so we have roughly 440.

2640
02:27:27,620 --> 02:27:28,740
OK.

2641
02:27:28,740 --> 02:27:29,900
OK.

2642
02:27:29,900 --> 02:27:33,220
So I do have a special group exercise

2643
02:27:33,220 --> 02:27:35,660
that I really want us to work on,

2644
02:27:35,660 --> 02:27:39,420
just to put everything together that we have discussed today

2645
02:27:39,420 --> 02:27:40,940
and that we have learned today.

2646
02:27:40,940 --> 02:27:42,500
So one of the things that I really

2647
02:27:42,500 --> 02:27:44,820
wanted to try with this model, because I showed

2648
02:27:44,820 --> 02:27:47,020
a very simple model, a GRU.

2649
02:27:47,020 --> 02:27:48,900
You saw in the figure it's very simple.

2650
02:27:48,900 --> 02:27:51,940
But you can do extreme deep learning.

2651
02:27:51,940 --> 02:27:53,660
You can do many layers of GRUs.

2652
02:27:53,660 --> 02:27:54,940
You can use LSTMs.

2653
02:27:54,940 --> 02:27:56,620
You can use all the different components

2654
02:27:56,620 --> 02:27:58,700
that are offered by PyTorch.

2655
02:27:58,700 --> 02:28:01,660
So what I would like is that we can just join,

2656
02:28:01,660 --> 02:28:03,420
make some groups.

2657
02:28:03,420 --> 02:28:05,580
I would say large groups, because maybe I'm

2658
02:28:05,580 --> 02:28:07,660
thinking something like seven or eight groups.

2659
02:28:07,660 --> 02:28:11,420
And then I would like that you guys can come and present

2660
02:28:11,420 --> 02:28:12,940
your model.

2661
02:28:12,940 --> 02:28:15,980
And what I would suggest you to do

2662
02:28:15,980 --> 02:28:18,620
is use the same model that we have here,

2663
02:28:18,620 --> 02:28:20,220
the same model that I showed here.

2664
02:28:20,220 --> 02:28:23,620
And look, I could just show you how easy it is.

2665
02:28:23,620 --> 02:28:25,780
Let me show you.

2666
02:28:25,780 --> 02:28:29,180
So we have the model here, right?

2667
02:28:29,180 --> 02:28:33,620
All we need to do is just go back to this function here,

2668
02:28:33,620 --> 02:28:35,820
and then you can add whatever you want to add.

2669
02:28:35,820 --> 02:28:38,580
So you can add another fully connected layer if you want.

2670
02:28:38,580 --> 02:28:39,220
You can do that.

2671
02:28:39,220 --> 02:28:41,380
You can just copy, paste.

2672
02:28:41,380 --> 02:28:44,540
Then you can just continue building on top of that.

2673
02:28:44,540 --> 02:28:47,020
So I would really like that if we can get into some groups

2674
02:28:47,020 --> 02:28:50,380
and try to go into the documentation,

2675
02:28:50,380 --> 02:28:53,380
just read up on it a bit more, and just have that practice

2676
02:28:53,380 --> 02:28:56,900
that you can go read up more on the documentation.

2677
02:28:56,900 --> 02:28:59,460
And you can add more GRUs if you want.

2678
02:28:59,460 --> 02:29:01,460
You can change it to LSTM if you want.

2679
02:29:01,460 --> 02:29:03,340
The documentation will tell you exactly what

2680
02:29:03,340 --> 02:29:06,460
you need to change when you're using different components.

2681
02:29:06,460 --> 02:29:09,580
So I would really like you guys to try it out.

2682
02:29:09,580 --> 02:29:10,500
Just change it.

2683
02:29:10,500 --> 02:29:11,980
Don't be afraid to change the code,

2684
02:29:11,980 --> 02:29:14,620
because you will have access to original code here.

2685
02:29:14,620 --> 02:29:19,700
And then see if you can improve that accuracy.

2686
02:29:19,700 --> 02:29:21,180
This is 97.

2687
02:29:21,180 --> 02:29:23,060
Maybe you can improve it.

2688
02:29:23,060 --> 02:29:24,780
I believe you can improve it.

2689
02:29:24,780 --> 02:29:31,660
All you need to do is just change that model that I showed.

2690
02:29:31,660 --> 02:29:33,660
So do you want to try it out?

2691
02:29:33,660 --> 02:29:34,940
I think that would be nice.

2692
02:29:34,940 --> 02:29:39,540
And I would just want you guys to just share what you learned.

2693
02:29:39,540 --> 02:29:43,540
Just share whatever you learned so far, and just try it out.

2694
02:29:46,620 --> 02:29:47,620
Can we do just groups?

2695
02:29:47,620 --> 02:29:51,340
I mean, just I'm thinking like seven groups or eight groups.

2696
02:29:51,340 --> 02:29:54,060
And we can just share what we have made.

2697
02:30:00,060 --> 02:30:00,560
OK.

2698
02:30:02,660 --> 02:30:03,160
Sure.

2699
02:30:09,980 --> 02:30:13,260
So do you prefer that, before we move on to the other?

2700
02:30:13,260 --> 02:30:14,540
OK, perfect.

2701
02:30:14,540 --> 02:30:15,820
Because I have the other one.

2702
02:30:15,820 --> 02:30:16,320
So.

2703
02:30:32,620 --> 02:30:35,100
Yeah, so I would definitely have to change forward,

2704
02:30:35,100 --> 02:30:38,620
and then add it here.

2705
02:30:38,620 --> 02:30:39,700
Yeah.

2706
02:30:39,700 --> 02:30:43,020
So here, I would say FC2.

2707
02:30:43,020 --> 02:30:43,520
Yeah.

2708
02:30:54,860 --> 02:30:58,220
OK, so we can just do port 5, I guess.

2709
02:30:58,220 --> 02:31:02,180
The initialize couldn't state a function in that model.

2710
02:31:02,180 --> 02:31:04,420
That's something that's overwritten from base model,

2711
02:31:04,420 --> 02:31:04,900
right?

2712
02:31:04,900 --> 02:31:06,500
Like you didn't implement that in the model.

2713
02:31:06,500 --> 02:31:07,020
No.

2714
02:31:07,020 --> 02:31:10,020
Yeah, I just use the standard thing that's using RNNs.

2715
02:31:12,980 --> 02:31:15,140
OK, let's jump into part 5.

2716
02:31:15,140 --> 02:31:18,660
And part 5 is why I had it hidden here, optional,

2717
02:31:18,660 --> 02:31:21,020
is because it's more like what you

2718
02:31:21,020 --> 02:31:22,500
would do when you're doing research.

2719
02:31:22,500 --> 02:31:23,260
It's very important.

2720
02:31:23,260 --> 02:31:26,220
OK, so you have an accuracy, but you don't really

2721
02:31:26,220 --> 02:31:28,500
know why you got that accuracy, right?

2722
02:31:28,500 --> 02:31:31,340
How do you explain why that accuracy is so high?

2723
02:31:31,340 --> 02:31:34,420
So something you want to do is properly evaluate the model,

2724
02:31:34,420 --> 02:31:36,340
and that's just the part 5.

2725
02:31:36,340 --> 02:31:37,500
So I'll go over it.

2726
02:31:37,500 --> 02:31:39,020
Definitely want to share that as well.

2727
02:31:50,700 --> 02:31:53,220
OK.

2728
02:31:53,220 --> 02:31:56,860
All right, so part 5, evaluating NLP models.

2729
02:31:56,860 --> 02:32:00,300
So I stored the model the last time, right?

2730
02:32:00,300 --> 02:32:04,140
And then now, again, I'm just doing the necessary steps

2731
02:32:04,140 --> 02:32:08,140
to make sure that I'm loading the model.

2732
02:32:08,140 --> 02:32:13,300
Notice that one of the things that a lot of these programmers

2733
02:32:13,300 --> 02:32:14,700
are working on these tools is just

2734
02:32:14,700 --> 02:32:19,140
trying to make sure that we have tools that you can properly

2735
02:32:19,140 --> 02:32:22,700
use for sharing models.

2736
02:32:22,700 --> 02:32:24,100
OK, we have a model that I train,

2737
02:32:24,100 --> 02:32:25,460
and I want to share with someone,

2738
02:32:25,460 --> 02:32:27,420
and I want that person to just load that model

2739
02:32:27,420 --> 02:32:28,380
and quickly use it.

2740
02:32:28,380 --> 02:32:29,860
That's something that I would love.

2741
02:32:29,860 --> 02:32:31,420
I think when we were working in teams,

2742
02:32:31,420 --> 02:32:33,220
that's something that would be perfect.

2743
02:32:33,220 --> 02:32:35,820
But it's so hard to get these things right

2744
02:32:35,820 --> 02:32:38,500
when you're building these deep learning frameworks.

2745
02:32:38,500 --> 02:32:42,540
So one of the things that PyTorch offers is just, OK,

2746
02:32:42,540 --> 02:32:44,220
you have all the model, but you still

2747
02:32:44,220 --> 02:32:46,580
need to do some extra work in order

2748
02:32:46,580 --> 02:32:49,140
for you to use that model for inferencing or evaluating

2749
02:32:49,140 --> 02:32:50,300
later on.

2750
02:32:50,300 --> 02:32:53,780
So you'll notice here that I'm using, so I unloaded,

2751
02:32:53,780 --> 02:33:00,340
so here I have to declare again what the class is

2752
02:33:00,340 --> 02:33:03,500
and everything included in the class.

2753
02:33:03,500 --> 02:33:05,260
Notice that there is no code before that.

2754
02:33:05,260 --> 02:33:09,100
It's just the same code that I used to specify the model.

2755
02:33:09,100 --> 02:33:11,140
I just basically copy and paste it here.

2756
02:33:11,140 --> 02:33:12,660
And then I have this special line here,

2757
02:33:12,660 --> 02:33:14,340
this important line here.

2758
02:33:14,340 --> 02:33:16,100
Let me just try to make this a little.

2759
02:33:21,300 --> 02:33:21,800
Right.

2760
02:33:21,800 --> 02:33:25,940
We have this line here where you load the model.

2761
02:33:25,940 --> 02:33:27,740
So now you have the model stored again,

2762
02:33:27,740 --> 02:33:30,500
and you can now use to do inferencing.

2763
02:33:30,500 --> 02:33:31,000
Yeah.

2764
02:33:31,000 --> 02:33:32,940
So you load the model, and you have to know

2765
02:33:32,940 --> 02:33:34,340
that it's actually a model.

2766
02:33:34,340 --> 02:33:35,180
Yes.

2767
02:33:35,180 --> 02:33:37,220
That's kind of like a drawback.

2768
02:33:37,220 --> 02:33:37,820
Yeah.

2769
02:33:37,820 --> 02:33:39,180
That's kind of a drawback.

2770
02:33:39,180 --> 02:33:42,780
And there's so many reasons why it's designed that way.

2771
02:33:42,780 --> 02:33:44,020
So many reasons.

2772
02:33:44,020 --> 02:33:47,220
What do you have to know to load the model?

2773
02:33:47,220 --> 02:33:49,380
So you would have to know the properties of the model.

2774
02:33:49,380 --> 02:33:51,020
You can see embedding there.

2775
02:33:51,020 --> 02:33:53,500
Probably put your name in there.

2776
02:33:53,500 --> 02:33:54,000
Yeah.

2777
02:33:54,000 --> 02:33:55,500
And then you can find it.

2778
02:33:55,500 --> 02:33:56,000
Yes.

2779
02:33:56,000 --> 02:33:58,980
Yeah.

2780
02:33:58,980 --> 02:34:01,180
The other ways you can store the model and load it,

2781
02:34:01,180 --> 02:34:04,620
I just find that this is, for me, the easiest way

2782
02:34:04,620 --> 02:34:07,100
to get things just running fast and easy.

2783
02:34:07,100 --> 02:34:09,460
But there are other ways that PyTorch offers for you

2784
02:34:09,460 --> 02:34:11,220
to store and load the model.

2785
02:34:11,220 --> 02:34:13,220
There are other practices that you can use.

2786
02:34:13,220 --> 02:34:14,580
This is just one way you can do.

2787
02:34:14,580 --> 02:34:18,060
All right.

2788
02:34:18,060 --> 02:34:21,540
So let's see.

2789
02:34:27,020 --> 02:34:31,220
Right, so I have that model here.

2790
02:34:31,220 --> 02:34:32,820
And now I want to evaluate the model.

2791
02:34:32,820 --> 02:34:34,060
What do I mean by that?

2792
02:34:34,060 --> 02:34:37,260
So I want to know why this model does so well.

2793
02:34:37,260 --> 02:34:41,180
Like, it has a high accuracy for a multi-class problem.

2794
02:34:41,180 --> 02:34:42,900
Why does it do so well?

2795
02:34:42,900 --> 02:34:47,740
So one way you can do that is do some inferencing on it.

2796
02:34:47,740 --> 02:34:51,940
So I have a validation data set, like a held out data set

2797
02:34:51,940 --> 02:34:54,340
that I can use to test the model.

2798
02:34:54,340 --> 02:34:56,580
And one of the things that I'm looking forward to do

2799
02:34:56,580 --> 02:34:59,500
is, so you can see again, this is standard code again,

2800
02:34:59,500 --> 02:35:00,180
all over again.

2801
02:35:00,180 --> 02:35:02,660
I'm just repeating the code here.

2802
02:35:02,660 --> 02:35:06,580
That's something I do when I'm doing experimenting a lot.

2803
02:35:06,580 --> 02:35:09,540
I like just to redo things, just making sure everything is

2804
02:35:09,540 --> 02:35:10,300
right.

2805
02:35:10,300 --> 02:35:12,900
So you don't have to do all this code repeated again.

2806
02:35:12,900 --> 02:35:15,180
You can just use one notebook and do everything in one

2807
02:35:15,180 --> 02:35:17,140
notebook.

2808
02:35:17,140 --> 02:35:18,140
OK, so here again.

2809
02:35:18,140 --> 02:35:23,460
So now we have noticed that I want

2810
02:35:23,460 --> 02:35:27,420
to inference on a set, like a part of an entire validation

2811
02:35:27,420 --> 02:35:29,580
data set, right?

2812
02:35:29,580 --> 02:35:31,820
Sorry, the test data set.

2813
02:35:31,820 --> 02:35:33,660
So what I do is I don't need to do training.

2814
02:35:33,660 --> 02:35:36,460
I don't need to do like, OK, epochs and other stuff,

2815
02:35:36,460 --> 02:35:37,900
because I just want to do one shot.

2816
02:35:37,900 --> 02:35:40,180
I want to just check the accuracy of the model

2817
02:35:40,180 --> 02:35:43,900
at one shot, because this is the inferencing part.

2818
02:35:43,900 --> 02:35:48,580
And one key thing is, notice I have the model running

2819
02:35:48,580 --> 02:35:51,660
on CUDA, on the GPU, right?

2820
02:35:51,660 --> 02:35:57,500
But do I really need it to be in CUDA or a GPU

2821
02:35:57,500 --> 02:35:59,260
when I'm doing testing?

2822
02:35:59,260 --> 02:36:03,420
That's something that you may want to ask yourself, right?

2823
02:36:03,420 --> 02:36:04,820
So in this case, I would just use it,

2824
02:36:04,820 --> 02:36:06,020
because I have access to it.

2825
02:36:06,020 --> 02:36:08,420
But if you are paying cloud services,

2826
02:36:08,420 --> 02:36:12,220
and you don't have that amount of money to do that,

2827
02:36:12,220 --> 02:36:14,540
you just want to make sure.

2828
02:36:14,540 --> 02:36:17,140
Because initially, this model was trained on a GPU,

2829
02:36:17,140 --> 02:36:19,580
so you want to make sure that you change it to CPU,

2830
02:36:19,580 --> 02:36:22,020
like that, right?

2831
02:36:22,020 --> 02:36:24,540
You're doing inferencing on that one.

2832
02:36:24,540 --> 02:36:26,620
And this is very useful if you are sharing models

2833
02:36:26,620 --> 02:36:28,220
with someone, right?

2834
02:36:28,220 --> 02:36:31,220
In this case, just leave it as that.

2835
02:36:31,220 --> 02:36:32,700
OK, so I didn't want to do this.

2836
02:36:38,940 --> 02:36:45,020
I missed out on some of these.

2837
02:36:45,020 --> 02:36:46,500
Other stuff I'm not explaining again,

2838
02:36:46,500 --> 02:36:47,940
because it's just repeated code.

2839
02:36:50,380 --> 02:36:52,140
OK, yeah, at some point, it told me

2840
02:36:52,140 --> 02:36:57,300
that I need to get rid of all the,

2841
02:36:57,300 --> 02:36:59,380
because I'm just overusing the stuff, right?

2842
02:36:59,380 --> 02:37:00,940
I just need to do it all over again.

2843
02:37:06,060 --> 02:37:06,560
OK.

2844
02:37:08,420 --> 02:37:13,420
There we go.

2845
02:37:13,420 --> 02:37:13,900
Valuating.

2846
02:37:22,900 --> 02:37:23,380
Oh, wow.

2847
02:37:28,860 --> 02:37:31,340
This one, actually.

2848
02:37:31,340 --> 02:37:32,820
I think I've done something wrong.

2849
02:37:38,420 --> 02:37:39,420
I got the same value?

2850
02:37:53,340 --> 02:37:54,420
Yeah, yeah.

2851
02:37:54,420 --> 02:37:59,460
I may have overwritten the model itself and the weights,

2852
02:37:59,460 --> 02:38:01,900
so you would see that it would change.

2853
02:38:01,900 --> 02:38:04,500
But yeah, if you do it right, if you do all the notebooks right

2854
02:38:04,500 --> 02:38:07,100
and you run them in that sequence,

2855
02:38:07,100 --> 02:38:09,460
you'll get a high test accuracy, for sure.

2856
02:38:09,460 --> 02:38:11,620
And one of the things that I want to show

2857
02:38:11,620 --> 02:38:15,460
is it probably won't work so well here.

2858
02:38:15,460 --> 02:38:16,900
So I have a lot of code here that I

2859
02:38:16,900 --> 02:38:18,700
show that I typically use when I want

2860
02:38:18,700 --> 02:38:20,820
to evaluate a classifier.

2861
02:38:20,820 --> 02:38:23,060
So just a lot of code.

2862
02:38:23,060 --> 02:38:25,300
And basically, what I want to do is just

2863
02:38:25,300 --> 02:38:27,420
plot the confusion matrix.

2864
02:38:27,420 --> 02:38:29,980
So a confusion matrix looks something like this.

2865
02:38:29,980 --> 02:38:33,660
So you can see that that model didn't do so well, right?

2866
02:38:33,660 --> 02:38:39,940
But generally, what you want is that you want all these values

2867
02:38:39,940 --> 02:38:41,180
to be super high.

2868
02:38:41,180 --> 02:38:43,220
And then you know that your model does well.

2869
02:38:49,420 --> 02:38:52,980
So you can see that the accuracy is not so good.

2870
02:38:52,980 --> 02:38:55,580
Because I did something wrong with the one

2871
02:38:55,580 --> 02:38:58,020
when I executed those notebooks.

2872
02:38:58,020 --> 02:38:58,940
I did something wrong.

2873
02:38:58,940 --> 02:39:00,460
So the model wasn't stored properly.

2874
02:39:00,460 --> 02:39:02,260
I used a different model, probably.

2875
02:39:02,260 --> 02:39:05,300
And I stopped training maybe at some point.

2876
02:39:05,300 --> 02:39:07,620
So it's probably not a really good model here.

2877
02:39:07,620 --> 02:39:10,660
But yeah, you wouldn't get a high accuracy if you did right,

2878
02:39:10,660 --> 02:39:13,060
if you run all the notebooks right.

2879
02:39:13,060 --> 02:39:15,260
And yeah.

2880
02:39:15,260 --> 02:39:18,860
So you have access to all that code here,

2881
02:39:18,860 --> 02:39:21,300
all that code that you can generate the prediction,

2882
02:39:21,300 --> 02:39:23,060
and you can do the inferencing and everything.

2883
02:39:23,060 --> 02:39:25,340
And it's just a good way when you're

2884
02:39:25,340 --> 02:39:28,900
doing research in NLP just to properly evaluate your models.

2885
02:39:28,900 --> 02:39:31,420
You just don't want to focus too much on the accuracy

2886
02:39:31,420 --> 02:39:33,660
or the loss, but you want to properly evaluate it,

2887
02:39:33,660 --> 02:39:35,780
especially for a multi-class problem.

2888
02:39:35,780 --> 02:39:37,020
That's really important.

2889
02:39:37,020 --> 02:39:37,520
Yeah.

2890
02:39:37,520 --> 02:39:38,500
Can you just show that?

2891
02:39:38,500 --> 02:39:39,980
I think you can do it in the middle.

2892
02:39:39,980 --> 02:39:40,480
OK.

2893
02:39:40,480 --> 02:39:40,980
Here.

2894
02:39:44,980 --> 02:39:46,460
I'm sorry to hear that.

2895
02:39:46,460 --> 02:39:49,140
So if you have a better one, you can

2896
02:39:49,140 --> 02:39:50,620
see that something is going wrong.

2897
02:39:50,620 --> 02:39:52,100
The system is really bad.

2898
02:39:52,100 --> 02:39:55,100
You can find one that's good, but what can you do?

2899
02:39:55,100 --> 02:39:58,260
Yeah, so what I would do is, for this kind of problem,

2900
02:39:58,260 --> 02:39:59,180
it's a good question.

2901
02:39:59,180 --> 02:40:00,900
For this kind of problem, what I would do

2902
02:40:01,220 --> 02:40:07,940
I know, for example, in my case, it was like I had love and joy.

2903
02:40:07,940 --> 02:40:11,420
There's some overlap in the language used for joy and love.

2904
02:40:11,420 --> 02:40:12,700
That's very common.

2905
02:40:12,700 --> 02:40:15,660
So something I would do, and I think someone brought it up

2906
02:40:15,660 --> 02:40:21,220
already, I would definitely want to do a lot more preprocessing.

2907
02:40:21,220 --> 02:40:23,780
I would definitely want to try to see if I, or even

2908
02:40:23,780 --> 02:40:24,740
obtain more data.

2909
02:40:24,740 --> 02:40:26,860
For deep learning cases, I think it's

2910
02:40:26,860 --> 02:40:29,420
really good to have a bigger data set just

2911
02:40:29,420 --> 02:40:32,900
to make sure that the model has enough data to properly learn

2912
02:40:32,900 --> 02:40:35,660
those features and weights.

2913
02:40:35,660 --> 02:40:37,620
That's one way when you're using deep learning,

2914
02:40:37,620 --> 02:40:39,620
but when you're using the classical way of machine

2915
02:40:39,620 --> 02:40:42,540
learning, then your engineering features,

2916
02:40:42,540 --> 02:40:45,260
and I really like that because you can use some domain

2917
02:40:45,260 --> 02:40:46,300
knowledge.

2918
02:40:46,300 --> 02:40:49,420
For example, I know love and joy will have a lot of language

2919
02:40:49,420 --> 02:40:51,420
overlapping, so I would probably want

2920
02:40:51,420 --> 02:40:56,100
to inject more of the language that's

2921
02:40:56,100 --> 02:40:59,380
going to distinguish those two classes.

2922
02:40:59,900 --> 02:41:02,580
What I mean by that is get more instances, for example,

2923
02:41:02,580 --> 02:41:04,220
in this case.

2924
02:41:04,220 --> 02:41:07,380
Definitely it would be more clear if I had all the steps

2925
02:41:07,380 --> 02:41:09,460
that I did for collecting this specific data set.

2926
02:41:09,460 --> 02:41:12,940
Because here I'm just showing, OK, this is a clean data set,

2927
02:41:12,940 --> 02:41:14,700
but I actually cleaned the data set

2928
02:41:14,700 --> 02:41:17,260
and reprocessed it already with many different steps.

2929
02:41:17,260 --> 02:41:19,100
And as I said earlier, I have that notebook

2930
02:41:19,100 --> 02:41:20,620
that I want to share because I think

2931
02:41:20,620 --> 02:41:25,100
it has a lot of information that can help you answer that question.

2932
02:41:25,100 --> 02:41:28,300
But for every case, you need that domain knowledge.

2933
02:41:28,300 --> 02:41:31,060
So yeah, there are many ways you can do that.

2934
02:41:33,860 --> 02:41:35,380
But since now we're only focusing

2935
02:41:35,380 --> 02:41:39,580
on deep learning in notice, I guess a bigger data set

2936
02:41:39,580 --> 02:41:43,660
would be able to help with the classification accuracy.

2937
02:41:47,140 --> 02:41:52,060
So didn't you say in the deep learning case

2938
02:41:52,060 --> 02:41:55,060
that you didn't do things like release the software,

2939
02:41:55,060 --> 02:42:00,060
and then you kind of did it again?

2940
02:42:00,060 --> 02:42:00,540
Yeah.

2941
02:42:00,540 --> 02:42:02,660
Did you comment on that?

2942
02:42:02,660 --> 02:42:06,860
Yeah, so for this kind of task, I

2943
02:42:06,860 --> 02:42:09,140
didn't have to do a lot of that, if you notice,

2944
02:42:09,140 --> 02:42:10,940
because the data was super clean.

2945
02:42:10,940 --> 02:42:13,700
I mean, if you analyze the data more in depth,

2946
02:42:13,700 --> 02:42:17,460
you would see that the data has removed stop words.

2947
02:42:17,460 --> 02:42:19,700
It has emojis removed.

2948
02:42:19,700 --> 02:42:21,860
I think emojis are removed as well.

2949
02:42:21,860 --> 02:42:23,060
Emoticons were kept.

2950
02:42:23,060 --> 02:42:24,420
Some emoticons were kept.

2951
02:42:24,460 --> 02:42:27,620
But I just kind of specified what I want to keep

2952
02:42:27,620 --> 02:42:31,980
and what I want to move, to filter.

2953
02:42:31,980 --> 02:42:32,820
Yeah.

2954
02:42:32,820 --> 02:42:33,340
Yeah.

2955
02:42:33,340 --> 02:42:35,340
I already did all of that already with the data.

2956
02:42:35,340 --> 02:42:37,500
That's why it's so clean.

2957
02:42:37,500 --> 02:42:41,300
But you would have to do all of that yourself.

2958
02:42:41,300 --> 02:42:44,420
It would change for every use case.

2959
02:42:44,420 --> 02:42:45,900
It would be different, I guess.

2960
02:42:49,380 --> 02:42:52,580
Yeah, so the labels were like, so it's a Twitter data set.

2961
02:42:52,580 --> 02:42:56,420
And each label has, yeah, it's very important.

2962
02:42:56,420 --> 02:42:58,220
So it's connected to your question as well.

2963
02:42:58,220 --> 02:43:00,100
Because for my case, I just had to collect

2964
02:43:00,100 --> 02:43:03,500
like a series of hashtags.

2965
02:43:03,500 --> 02:43:05,180
For example, like surprise.

2966
02:43:05,180 --> 02:43:08,100
I would have like hashtag surprise or hashtag wow.

2967
02:43:08,100 --> 02:43:12,500
And I would use that as a weak label, like a noisy label,

2968
02:43:12,500 --> 02:43:15,860
to basically determine the class of the record.

2969
02:43:15,860 --> 02:43:16,360
Yeah.

2970
02:43:19,820 --> 02:43:21,780
Can we see the things that we want to see?

2971
02:43:21,780 --> 02:43:22,780
Is it an eye?

2972
02:43:22,780 --> 02:43:27,140
Yeah, you definitely want to see that diagonal, that eye

2973
02:43:27,140 --> 02:43:28,020
that I showed.

2974
02:43:28,020 --> 02:43:29,740
But yeah.

2975
02:43:29,740 --> 02:43:32,260
We don't have the time, I guess.

2976
02:43:32,260 --> 02:43:34,740
I guess we can try to fix it.

2977
02:43:34,740 --> 02:43:37,300
So let me just try to rerun this notebook.

2978
02:43:37,300 --> 02:43:40,900
And while it loads, maybe I can answer more questions

2979
02:43:40,900 --> 02:43:43,580
and see if it works.

2980
02:43:43,580 --> 02:43:44,100
Yeah.

2981
02:43:44,100 --> 02:43:47,540
So on the data collection side, I

2982
02:43:47,540 --> 02:43:50,020
don't know how many changes you have.

2983
02:43:50,020 --> 02:43:52,500
But you said the more you have, the better.

2984
02:43:52,500 --> 02:43:54,460
What if you don't have a lot?

2985
02:43:54,460 --> 02:43:56,420
You have something that you want to do.

2986
02:43:56,420 --> 02:43:59,420
And you want to be kind of a label.

2987
02:43:59,420 --> 02:44:01,420
What are some values or techniques

2988
02:44:01,420 --> 02:44:05,900
to know about a data set?

2989
02:44:05,900 --> 02:44:08,380
Yeah.

2990
02:44:08,380 --> 02:44:11,260
There are some techniques, like in LMP.

2991
02:44:11,260 --> 02:44:13,380
Data augmentation is one of them.

2992
02:44:13,380 --> 02:44:15,100
It's not so clear.

2993
02:44:15,100 --> 02:44:18,060
I've been following this research space a lot.

2994
02:44:18,060 --> 02:44:21,540
It's not so clear how you can generate text,

2995
02:44:21,540 --> 02:44:22,700
keep generating text.

2996
02:44:22,700 --> 02:44:25,780
So for computer vision, it would be much easier.

2997
02:44:25,780 --> 02:44:26,620
Yeah.

2998
02:44:26,620 --> 02:44:27,740
Shift, the CAD.

2999
02:44:27,740 --> 02:44:29,020
Yeah, definitely.

3000
02:44:29,020 --> 02:44:30,940
So that's something that's not so clear in LMP,

3001
02:44:30,940 --> 02:44:32,740
how you go about doing that.

3002
02:44:32,740 --> 02:44:34,180
Because everything is discrete.

3003
02:44:34,180 --> 02:44:37,060
So yeah, so that's the kind of challenge.

3004
02:44:37,060 --> 02:44:40,860
But I've heard some people working on using embeddings

3005
02:44:40,860 --> 02:44:41,380
to do that.

3006
02:44:41,380 --> 02:44:44,380
I think embedding is the key thing,

3007
02:44:44,380 --> 02:44:49,340
to properly be able to auto-generate text,

3008
02:44:49,340 --> 02:44:50,300
synthetic text.

3009
02:44:50,300 --> 02:44:52,740
So basically, yeah.

3010
02:44:52,740 --> 02:44:55,300
But for my case, why I didn't focus on that

3011
02:44:55,300 --> 02:44:58,180
is because I have that domain expertise, which

3012
02:44:58,180 --> 02:45:01,860
I can use to collect larger data sets.

3013
02:45:01,860 --> 02:45:03,460
But it's a lot of tedious work.

3014
02:45:03,460 --> 02:45:06,020
In my case, I have to do more hashtags,

3015
02:45:06,020 --> 02:45:08,900
find out from people manually what people are texting

3016
02:45:08,900 --> 02:45:10,620
or what people are tweeting.

3017
02:45:10,660 --> 02:45:12,940
And if that is related to emotions,

3018
02:45:12,940 --> 02:45:14,460
and I have to do some evaluation,

3019
02:45:14,460 --> 02:45:16,980
and I have to use Amazon Turk to ask people

3020
02:45:16,980 --> 02:45:19,780
if those hashtags are really what they are.

3021
02:45:19,780 --> 02:45:22,420
And so I have to get a lot of that domain expertise

3022
02:45:22,420 --> 02:45:25,020
and evaluation from others.

3023
02:45:25,020 --> 02:45:26,700
So it takes a lot of work.

3024
02:45:26,700 --> 02:45:29,620
But yeah, there's no clear way how

3025
02:45:29,620 --> 02:45:33,820
you can go about generating larger data sets.

3026
02:45:33,820 --> 02:45:36,260
How do you know if you have enough data?

3027
02:45:36,260 --> 02:45:38,980
Maybe somebody's doing something like, OK, I mean,

3028
02:45:39,020 --> 02:45:41,580
like you're doing this thing on a kind of a curve, right?

3029
02:45:41,580 --> 02:45:43,340
So when do you stop?

3030
02:45:43,340 --> 02:45:47,340
1,000 instances, 10,000?

3031
02:45:47,340 --> 02:45:49,900
When do you stop?

3032
02:45:49,900 --> 02:45:52,420
I guess it depends on every task, I guess.

3033
02:45:52,420 --> 02:45:55,940
This is not a very simple question to answer.

3034
02:45:55,940 --> 02:45:59,660
I guess for me, it's like, OK, I tried with a sample, by the way.

3035
02:45:59,660 --> 02:46:02,220
I didn't try the entire data set, right?

3036
02:46:02,220 --> 02:46:05,780
And you can see how good the classification is.

3037
02:46:05,780 --> 02:46:07,620
I guess that's an indicator that I can stop,

3038
02:46:07,620 --> 02:46:09,780
because we have a high accuracy.

3039
02:46:09,780 --> 02:46:11,180
We have a low loss.

3040
02:46:11,180 --> 02:46:12,340
That's a good indicator.

3041
02:46:12,340 --> 02:46:14,140
But what do you do in cases where

3042
02:46:14,140 --> 02:46:16,060
you don't have such a clean data set,

3043
02:46:16,060 --> 02:46:17,740
don't have good accuracy?

3044
02:46:17,740 --> 02:46:18,940
That's your question, right?

3045
02:46:18,940 --> 02:46:20,260
What would you do in that case?

3046
02:46:20,260 --> 02:46:24,220
Yeah, like I mean, when you say, like, hey, I'm good, right?

3047
02:46:24,220 --> 02:46:25,820
And you can always get more, right?

3048
02:46:25,820 --> 02:46:28,020
But if you're starting with a small amount,

3049
02:46:28,020 --> 02:46:30,020
at some point, you can use a hey.

3050
02:46:32,780 --> 02:46:34,340
Yeah.

3051
02:46:34,340 --> 02:46:38,140
I guess, yeah, like I say, it's properly evaluating it,

3052
02:46:38,140 --> 02:46:39,620
I guess.

3053
02:46:39,620 --> 02:46:42,300
Doing the confusion matrix for me helped.

3054
02:46:42,300 --> 02:46:44,300
That's the indicator for me.

3055
02:46:44,300 --> 02:46:45,860
If I do have something.

3056
02:46:45,860 --> 02:46:49,620
So again, it's like, you need some kind of domain knowledge

3057
02:46:49,620 --> 02:46:50,980
at the end of the day.

3058
02:46:50,980 --> 02:46:51,780
That's my opinion.

3059
02:46:51,780 --> 02:46:53,180
You need some domain knowledge.

3060
02:46:53,180 --> 02:46:56,380
Because if you're going to do something in computer vision,

3061
02:46:56,380 --> 02:46:59,020
you have to explain why that model is doing so good.

3062
02:46:59,020 --> 02:47:00,860
You have to be able to explain that, right?

3063
02:47:00,860 --> 02:47:02,420
I mean, you can apply it if you like,

3064
02:47:02,420 --> 02:47:04,100
and a lot of people can't do that.

3065
02:47:04,100 --> 02:47:06,420
But if you really want to go to the extreme

3066
02:47:06,420 --> 02:47:12,220
and be innovative, you would have to just learn exactly

3067
02:47:12,220 --> 02:47:14,580
what the machine learning model is doing.

3068
02:47:14,580 --> 02:47:16,980
So on the vision side, right?

3069
02:47:16,980 --> 02:47:20,460
Some people might do some kind of segmentation, right?

3070
02:47:20,460 --> 02:47:21,340
Right, right.

3071
02:47:21,340 --> 02:47:23,660
You can find out a kind of cap of things.

3072
02:47:23,660 --> 02:47:24,180
Right.

3073
02:47:24,180 --> 02:47:26,580
Even a feature that they're using to recognize

3074
02:47:26,580 --> 02:47:27,540
that this is a cap.

3075
02:47:27,540 --> 02:47:28,040
Right.

3076
02:47:28,040 --> 02:47:33,420
Like, each map that shows you the elephant in there.

3077
02:47:33,420 --> 02:47:34,420
Yeah.

3078
02:47:34,420 --> 02:47:35,420
Yeah.

3079
02:47:35,420 --> 02:47:37,900
Is there any kind of de-tapping applied with like NLP?

3080
02:47:37,900 --> 02:47:39,660
Like, these are the features, and these

3081
02:47:39,660 --> 02:47:43,540
are the words that it's using to recognize joy or sadness?

3082
02:47:43,540 --> 02:47:44,460
Yeah, we do.

3083
02:47:44,460 --> 02:47:44,980
Let's see.

3084
02:47:44,980 --> 02:47:48,380
Let me try to show you like a notebook here.

3085
02:47:48,380 --> 02:47:51,100
So this notebook here, that I have this extra notebook here,

3086
02:47:51,100 --> 02:47:53,220
is basically text summarization.

3087
02:47:53,220 --> 02:47:55,460
So it's just another NLP task.

3088
02:47:55,460 --> 02:47:59,420
And what I do in this notebook uses the same code, by the way.

3089
02:47:59,420 --> 02:48:02,060
Model time is just different.

3090
02:48:02,060 --> 02:48:03,060
So it's a lot of code.

3091
02:48:03,060 --> 02:48:05,860
Let's see.

3092
02:48:05,860 --> 02:48:11,900
So I have this really wonderful visualization that I can use.

3093
02:48:11,900 --> 02:48:15,300
In this case, it's basically visualizing the same thing

3094
02:48:15,300 --> 02:48:17,140
you're talking about here.

3095
02:48:17,140 --> 02:48:18,580
Here, you can see that.

3096
02:48:18,580 --> 02:48:20,900
So this is like an attention weight.

3097
02:48:20,900 --> 02:48:23,100
These are attention weights, or we call it attention weights.

3098
02:48:23,100 --> 02:48:25,460
And it's basically indicating to you

3099
02:48:25,460 --> 02:48:27,860
what parts the model is looking at that are

3100
02:48:27,860 --> 02:48:30,260
important during training.

3101
02:48:30,260 --> 02:48:33,140
So in this case, you see that here, it's

3102
02:48:33,140 --> 02:48:35,180
focusing a lot on Bush-Argy.

3103
02:48:35,180 --> 02:48:38,340
So those words are important for this summary.

3104
02:48:38,340 --> 02:48:41,900
And so here, you see, original summary, and then

3105
02:48:41,900 --> 02:48:44,180
the predicted summary is this.

3106
02:48:44,180 --> 02:48:46,980
So what the model used to make that proper prediction.

3107
02:48:46,980 --> 02:48:48,580
So that's what it means here.

3108
02:48:48,580 --> 02:48:50,060
Yeah, I mean, there's a lot of information

3109
02:48:50,060 --> 02:48:50,820
that I can share here.

3110
02:48:50,820 --> 02:48:52,700
But I guess, just to answer your question,

3111
02:48:52,700 --> 02:48:54,300
there are techniques.

3112
02:48:54,300 --> 02:48:57,260
They're not fully developed, I guess, in my opinion.

3113
02:48:57,260 --> 02:48:59,260
You can use them to get an idea of what

3114
02:48:59,260 --> 02:49:01,020
the model is doing, like interpret

3115
02:49:01,020 --> 02:49:03,700
what the model is learning, what areas are important.

3116
02:49:03,700 --> 02:49:06,140
In computer vision, I guess it's easier to do that.

3117
02:49:06,140 --> 02:49:09,020
But in NLP, we're still struggling with that area.

3118
02:49:09,020 --> 02:49:09,520
So yeah.

3119
02:49:14,500 --> 02:49:16,300
Yeah, I will share all of it.

3120
02:49:16,300 --> 02:49:17,660
I have a series of notebooks.

3121
02:49:17,660 --> 02:49:20,820
I have may have up to 20 notebooks that I worked on.

3122
02:49:20,820 --> 02:49:23,260
And the good thing is all of them are consistent.

3123
02:49:23,260 --> 02:49:27,180
Use similar code should be very easy to understand, I guess.

3124
02:49:27,180 --> 02:49:28,380
Just a data set changes.

3125
02:49:37,260 --> 02:49:37,760
Right.

3126
02:49:49,100 --> 02:49:52,380
And I have another one here for neural machine translation.

3127
02:49:52,380 --> 02:49:55,460
And this one uses the attention mechanism again.

3128
02:49:55,460 --> 02:49:59,300
And it's the same code, the same functionality,

3129
02:49:59,300 --> 02:50:01,220
it's just a different data set.

3130
02:50:01,220 --> 02:50:05,780
I will also share with this if you're really interested in it.

3131
02:50:05,780 --> 02:50:07,060
Yeah, I will share this.

3132
02:50:07,060 --> 02:50:09,620
It takes a lot of time to explain all of this stuff.

3133
02:50:09,620 --> 02:50:11,820
But I think these notebooks were pretty

3134
02:50:11,820 --> 02:50:13,100
helpful for a lot of students.

3135
02:50:13,100 --> 02:50:15,460
I shared them over the past.

3136
02:50:15,460 --> 02:50:17,380
I really got a lot of good feedback from the,

3137
02:50:17,380 --> 02:50:19,380
because I spent a lot of time trying to explain,

3138
02:50:19,380 --> 02:50:21,420
you know, write a paragraph about it,

3139
02:50:21,420 --> 02:50:23,740
and try to explain it in a very intuitive way.

3140
02:50:23,740 --> 02:50:25,220
These things are very complicated.

3141
02:50:25,220 --> 02:50:27,940
But it's important, like I say, to always start

3142
02:50:27,940 --> 02:50:30,340
from the beginning, and just go step by step.

3143
02:50:30,340 --> 02:50:33,300
Just don't rush into just building that model.

3144
02:50:33,300 --> 02:50:36,380
So let's see if I can get that model running,

3145
02:50:36,380 --> 02:50:38,820
and see if I can get that accuracy.

3146
02:50:38,820 --> 02:50:40,140
Let's see.

3147
02:50:40,140 --> 02:50:42,620
I think it runs, so let's just try.

3148
02:50:45,260 --> 02:50:46,300
So I might have to.

3149
02:50:46,300 --> 02:50:47,140
Let's see.

3150
02:50:57,620 --> 02:50:58,460
Let's see.

3151
02:51:08,740 --> 02:51:10,900
If you have any other questions you want to ask,

3152
02:51:10,900 --> 02:51:14,460
or maybe after the session, feel free to approach me

3153
02:51:14,460 --> 02:51:15,580
and ask me.

3154
02:51:15,580 --> 02:51:18,100
I'll be really glad to answer your questions.

3155
02:51:24,260 --> 02:51:25,660
Wonderful, got it to work.

3156
02:51:27,100 --> 02:51:32,100
Okay, so, yeah, so I guess that was a very important

3157
02:51:32,180 --> 02:51:36,380
question, the one you asked, because I was gonna speak

3158
02:51:36,380 --> 02:51:39,540
about that here when I was showing the confusion matrix.

3159
02:51:39,540 --> 02:51:43,700
So you see here for joy and love, there is some overlap

3160
02:51:43,700 --> 02:51:45,380
there in the language used.

3161
02:51:46,220 --> 02:51:48,980
So you can see that the model is confusing those labels

3162
02:51:48,980 --> 02:51:52,020
here, so that's the true label and the predicted label.

3163
02:51:52,020 --> 02:51:55,540
And for surprise, usually it doesn't do so well.

3164
02:51:55,540 --> 02:51:59,500
And one of the reasons is that there's a lack of examples

3165
02:51:59,500 --> 02:52:02,860
for surprise, and let's go back to see the distribution.

3166
02:52:02,860 --> 02:52:05,420
So for surprise, you will see that it's 78,

3167
02:52:05,420 --> 02:52:06,540
and for love, it's 84.

3168
02:52:06,540 --> 02:52:10,060
So let's go back and see what the distribution

3169
02:52:10,060 --> 02:52:11,380
of this dataset was.

3170
02:52:15,580 --> 02:52:18,580
So you see surprise is like the class,

3171
02:52:18,580 --> 02:52:21,100
like with the lowest examples, right?

3172
02:52:21,100 --> 02:52:25,060
So small, it's like maybe less than 10,000 records

3173
02:52:25,060 --> 02:52:26,300
compared to all the rest.

3174
02:52:28,260 --> 02:52:31,500
And love also is the second, this class.

3175
02:52:31,500 --> 02:52:34,660
So you can see that the training time,

3176
02:52:34,660 --> 02:52:37,420
it will affect the model at some point.

3177
02:52:37,420 --> 02:52:39,740
It's something that I try to always try to do,

3178
02:52:39,740 --> 02:52:42,220
and I think that's the reason why I'm doing this.

3179
02:52:42,700 --> 02:52:44,220
At some point.

3180
02:52:44,220 --> 02:52:48,100
It's something that I try to argue when I wrote my thesis.

3181
02:52:48,100 --> 02:52:51,580
I try to argue this a lot, and I try to run

3182
02:52:51,580 --> 02:52:54,420
a lot of experimentation to make sure that

3183
02:52:54,420 --> 02:52:55,460
what I'm claiming is right,

3184
02:52:55,460 --> 02:52:59,460
because there may be other things that I'm not considering.

3185
02:52:59,460 --> 02:53:02,020
But yeah, this stuff that I'm sharing here

3186
02:53:02,020 --> 02:53:04,580
is just part of my PhD thesis,

3187
02:53:04,580 --> 02:53:07,500
so this stuff that I properly evaluated.

3188
02:53:08,500 --> 02:53:09,340
Yeah.

3189
02:53:09,340 --> 02:53:11,060
So when you change the model,

3190
02:53:11,060 --> 02:53:15,100
surprise is very low relative to some of the others, right?

3191
02:53:15,100 --> 02:53:17,380
Would you sum the examples together,

3192
02:53:17,380 --> 02:53:19,820
or would you add more if you did?

3193
02:53:19,820 --> 02:53:22,820
In that case, just try the examples, right?

3194
02:53:22,820 --> 02:53:24,260
That's something you can try.

3195
02:53:25,540 --> 02:53:27,540
You can definitely do that.

3196
02:53:27,540 --> 02:53:30,580
For our methods, for sure that help,

3197
02:53:30,580 --> 02:53:33,140
trying to balance all the datasets.

3198
02:53:33,140 --> 02:53:35,780
But in that case, you would lose, right?

3199
02:53:35,780 --> 02:53:38,300
You would lose some, like,

3200
02:53:38,300 --> 02:53:41,940
if you really wanna focus on some emotions,

3201
02:53:41,940 --> 02:53:43,420
some people, what they do is just reduce

3202
02:53:43,420 --> 02:53:44,980
the number of classes as well.

3203
02:53:44,980 --> 02:53:46,820
So instead of having the surprise and love,

3204
02:53:46,820 --> 02:53:48,940
maybe merge it into one, or something like that.

3205
02:53:48,940 --> 02:53:51,100
In this kind of task, you can do that.

3206
02:53:52,900 --> 02:53:55,980
Yeah, there are many ways you can try to fix it.

3207
02:53:55,980 --> 02:53:56,980
Any ways to fix it.

3208
02:54:01,340 --> 02:54:02,220
Sorry, the.

3209
02:54:02,220 --> 02:54:06,180
A testing library.

3210
02:54:06,180 --> 02:54:08,140
Yeah, another great question.

3211
02:54:08,140 --> 02:54:10,900
I guess for most people that use PyTorch,

3212
02:54:10,900 --> 02:54:13,580
that's the number one thing they would complain about.

3213
02:54:14,420 --> 02:54:19,260
There are not a lot of good functionalities for testing.

3214
02:54:19,260 --> 02:54:21,900
It's because PyTorch was actually built

3215
02:54:21,900 --> 02:54:24,020
for research purposes at the beginning.

3216
02:54:24,020 --> 02:54:26,980
And so most of the things you only have access to

3217
02:54:26,980 --> 02:54:30,180
are just the fully connected layers, the RNN,

3218
02:54:30,180 --> 02:54:31,900
and the other stuff you have to do it on your own,

3219
02:54:31,900 --> 02:54:33,420
which is the evaluation part.

3220
02:54:33,420 --> 02:54:35,860
So I have all that code that I have to, you know,

3221
02:54:35,860 --> 02:54:38,940
implement myself to properly evaluate them all.

3222
02:54:38,940 --> 02:54:42,060
I think that's done intentionally.

3223
02:54:42,060 --> 02:54:45,220
I mean, at some point you have to have the responsibility

3224
02:54:45,220 --> 02:54:47,420
to actually evaluate them all yourself.

3225
02:54:50,140 --> 02:54:52,820
Yeah, but there is some tool that they're working on.

3226
02:54:52,820 --> 02:54:55,900
I think it's called PyTex, I believe.

3227
02:54:55,900 --> 02:55:00,380
So it's a relatively new part of the ecosystem

3228
02:55:00,460 --> 02:55:03,140
that they're trying to address these issues,

3229
02:55:03,140 --> 02:55:06,260
like how to evaluate them all easily with PyTorch.

3230
02:55:06,260 --> 02:55:08,900
You don't have to use an external library to do that.

3231
02:55:14,380 --> 02:55:17,700
So any more questions that I can answer for you?

3232
02:55:22,940 --> 02:55:25,980
So the other part I really wanted to share

3233
02:55:25,980 --> 02:55:29,580
were those three, machine translation,

3234
02:55:29,620 --> 02:55:32,180
which is I think machine translation,

3235
02:55:32,180 --> 02:55:34,500
if you're really interested in NLP,

3236
02:55:34,500 --> 02:55:37,100
this is one area I would strongly suggest

3237
02:55:37,100 --> 02:55:38,140
for you to explore.

3238
02:55:39,140 --> 02:55:41,260
It is really challenging and interesting

3239
02:55:41,260 --> 02:55:45,540
because the whole setup is so different from classification.

3240
02:55:45,540 --> 02:55:46,620
There are a lot of techniques

3241
02:55:46,620 --> 02:55:48,140
that you need to be aware of using,

3242
02:55:48,140 --> 02:55:50,140
and some of these techniques, you know,

3243
02:55:51,420 --> 02:55:53,860
you can use them in different types of NLP tasks.

3244
02:55:53,860 --> 02:55:55,980
Like I said, most of the things that I share here

3245
02:55:55,980 --> 02:55:59,020
are actually machine translation stuff that I just use,

3246
02:55:59,020 --> 02:56:00,980
and I use it for classification.

3247
02:56:00,980 --> 02:56:02,940
So it's a very interesting field.

3248
02:56:02,940 --> 02:56:04,460
I guess you can learn a lot from it.

3249
02:56:04,460 --> 02:56:08,500
So yeah, if you don't have any other questions,

3250
02:56:08,500 --> 02:56:12,580
I guess it'd be good to stop it here.

3251
02:56:17,820 --> 02:56:18,660
Yes.

3252
02:56:20,900 --> 02:56:22,980
Okay, thank you for reminding me.

3253
02:56:29,020 --> 02:56:29,860
Okay.

3254
02:56:40,220 --> 02:56:41,060
Okay.

3255
02:56:41,580 --> 02:56:44,620
So, do you have to learn other models

3256
02:56:44,620 --> 02:56:47,460
that you need to learn how to use?

3257
02:56:51,860 --> 02:56:53,940
Do you watch other models?

3258
02:56:58,340 --> 02:57:00,060
I didn't get your question, sorry.

3259
02:57:00,060 --> 02:57:02,140
Do you watch other models?

3260
02:57:06,180 --> 02:57:07,740
For which kind of models?

3261
02:57:07,740 --> 02:57:09,660
Like what do you have?

3262
02:57:13,940 --> 02:57:16,540
Yeah, so there are definitely a lot of the things

3263
02:57:16,540 --> 02:57:19,380
that I didn't really talk about here.

3264
02:57:19,380 --> 02:57:21,260
You mean like GRU?

3265
02:57:22,140 --> 02:57:27,140
So, yeah, so GRU is just like a very simplified version

3266
02:57:29,260 --> 02:57:33,940
of an RNN, and then LSTM is a lot more complicated.

3267
02:57:35,900 --> 02:57:37,500
It does things more efficiently.

3268
02:57:38,260 --> 02:57:40,620
It adds actually more components to the RNN,

3269
02:57:40,620 --> 02:57:43,740
the standard RNN, so it's just a lot of things

3270
02:57:43,740 --> 02:57:45,340
that are changing in the model.

3271
02:57:45,340 --> 02:57:47,780
And I didn't really focus on that here

3272
02:57:47,780 --> 02:57:50,260
because like I said, it wasn't like a deep learning,

3273
02:57:50,260 --> 02:57:52,620
you know, deep learning course or tutorial.

3274
02:57:52,620 --> 02:57:54,580
But yeah, those things are very important to know,

3275
02:57:54,580 --> 02:57:56,420
especially if you wanna use them here.

3276
02:57:57,780 --> 02:57:59,540
But yeah, I would suggest like the,

3277
02:57:59,540 --> 02:58:01,620
if you're really interested in it

3278
02:58:01,620 --> 02:58:03,540
and you want a good place to start,

3279
02:58:03,540 --> 02:58:05,900
that documentation for PyTorch,

3280
02:58:05,900 --> 02:58:08,820
I think is the number one thing I would recommend.

3281
02:58:08,820 --> 02:58:09,980
I wouldn't even recommend books.

3282
02:58:09,980 --> 02:58:12,700
I would just recommend go to a documentation

3283
02:58:12,700 --> 02:58:15,940
because the documentation actually has like explanations.

3284
02:58:15,940 --> 02:58:20,380
It has like references, like the proper references

3285
02:58:20,380 --> 02:58:22,500
of where you can, of what was used,

3286
02:58:22,500 --> 02:58:24,940
what paper was used to implement that model, for example.

3287
02:58:24,940 --> 02:58:27,660
And it has like a really, really nice explanation

3288
02:58:27,660 --> 02:58:30,420
of what the model is about, yeah.

3289
02:58:34,460 --> 02:58:35,300
Yeah.

3290
02:58:35,300 --> 02:58:37,700
How do you use the machine learning

3291
02:58:37,700 --> 02:58:41,980
to kind of understand the value of the model?

3292
02:58:41,980 --> 02:58:42,820
Yeah.

3293
02:58:43,740 --> 02:58:47,820
So for me, it's like, initially I started

3294
02:58:47,820 --> 02:58:49,660
in machine learning, basic machine learning,

3295
02:58:49,660 --> 02:58:52,740
like doing like hand, I still enjoy that.

3296
02:58:52,740 --> 02:58:55,620
I still enjoy like establishing my own features, you know,

3297
02:58:55,620 --> 02:58:57,860
because I have a lot of domain expertise in this space.

3298
02:58:57,860 --> 02:59:00,340
So I just, you know, try something like an SVM

3299
02:59:00,340 --> 02:59:02,060
or try something like a naive Bayesian classifier.

3300
02:59:02,060 --> 02:59:03,060
It does well.

3301
02:59:03,060 --> 02:59:04,740
It actually does well for the task.

3302
02:59:04,740 --> 02:59:07,180
There was no need for me to do deep learning.

3303
02:59:07,180 --> 02:59:10,540
But as you can see that one of the things

3304
02:59:10,540 --> 02:59:14,420
that I showed here was that if you have a lot of data set,

3305
02:59:14,420 --> 02:59:16,100
you can actually achieve it without having

3306
02:59:16,100 --> 02:59:18,540
to do that tedious work, right?

3307
02:59:18,540 --> 02:59:21,260
If you're all interested in accuracy and this kind of thing,

3308
02:59:21,260 --> 02:59:24,820
you would get that kind of result.

3309
02:59:24,820 --> 02:59:26,780
It took quite a bit of time.

3310
02:59:26,780 --> 02:59:30,740
Most of my work was basically trying to combine

3311
02:59:30,740 --> 02:59:34,020
like traditional methods with deep learning methods.

3312
02:59:34,060 --> 02:59:37,500
So I have some papers in that I managed

3313
02:59:37,500 --> 02:59:39,820
to publish this year, past year I mean.

3314
02:59:40,940 --> 02:59:43,020
I discuss a lot about that, like what are some

3315
02:59:43,020 --> 02:59:45,260
of the drawbacks, because I use, I don't know

3316
02:59:45,260 --> 02:59:47,140
if you're familiar, there are a lot of trending models

3317
02:59:47,140 --> 02:59:51,100
like BERT and OpenAI, GPT, and all these new models

3318
02:59:51,100 --> 02:59:53,740
that are coming out from these companies.

3319
02:59:53,740 --> 02:59:56,700
I compare all these models with this kind of task

3320
02:59:56,700 --> 02:59:59,700
that I showed here today, and I show the differences.

3321
02:59:59,700 --> 03:00:03,380
And it's unbelievable that something like an OpenAI GPT,

3322
03:00:03,380 --> 03:00:05,780
which was trained on a Wikipedia data set,

3323
03:00:05,780 --> 03:00:10,220
can actually all perform all the other models that I trained.

3324
03:00:10,220 --> 03:00:13,660
Even models that I trained on like data sets,

3325
03:00:13,660 --> 03:00:16,580
like domain-based data set, like a Twitter data set,

3326
03:00:16,580 --> 03:00:20,020
even those models were performed by a GPT,

3327
03:00:20,020 --> 03:00:21,940
OpenAI GPT, which is outstanding.

3328
03:00:21,940 --> 03:00:25,100
It means that deep learning is actually working,

3329
03:00:25,100 --> 03:00:26,580
because you can see, right?

3330
03:00:26,580 --> 03:00:29,060
But it raises questions as well,

3331
03:00:29,060 --> 03:00:31,260
because what is a model learning, like you said, right?

3332
03:00:31,260 --> 03:00:33,140
Like you pointed out, but the model is learning.

3333
03:00:33,140 --> 03:00:36,820
So that's an area that we are working a lot in NLP,

3334
03:00:36,820 --> 03:00:38,020
trying to understand.

3335
03:00:38,020 --> 03:00:40,220
Because I think that's a very important point,

3336
03:00:40,220 --> 03:00:43,260
very important part of the work that we do.

3337
03:00:43,260 --> 03:00:44,360
I try to explain it.

3338
03:00:44,360 --> 03:00:46,820
Otherwise, we are just coding, coding, coding,

3339
03:00:46,820 --> 03:00:50,140
and just combining GRUs and just doing that,

3340
03:00:50,140 --> 03:00:52,940
but it's no fun if you don't understand it very well.

3341
03:00:57,540 --> 03:00:58,380
Sorry, sorry.

3342
03:01:03,140 --> 03:01:06,940
No, I did it.

3343
03:01:06,940 --> 03:01:11,140
So I did my thesis in like three years.

3344
03:01:11,140 --> 03:01:11,980
Yeah, three years.

3345
03:01:12,860 --> 03:01:15,100
I was very fortunate, because I can say,

3346
03:01:15,100 --> 03:01:17,140
I was working in a task that not a lot of people

3347
03:01:17,140 --> 03:01:17,980
are working on.

3348
03:01:17,980 --> 03:01:20,020
I'm not sure if you have ever seen like

3349
03:01:20,020 --> 03:01:22,020
emotion classification, maybe you have heard about it.

3350
03:01:22,020 --> 03:01:24,260
But not a lot of people are working in this space.

3351
03:01:24,260 --> 03:01:25,100
Thank you.

3352
03:01:25,100 --> 03:01:28,980
Yeah, so I was fortunate because of that.

3353
03:01:28,980 --> 03:01:31,360
I mean, when I built my models,

3354
03:01:31,360 --> 03:01:33,480
there wasn't a lot of papers to compare with,

3355
03:01:33,480 --> 03:01:35,760
so it was good for me, I got lucky.

3356
03:01:35,760 --> 03:01:38,720
But yeah, I know if you want to get into this space,

3357
03:01:38,720 --> 03:01:40,820
and you want to do this kind of work,

3358
03:01:40,820 --> 03:01:42,280
you will see how competitive it is,

3359
03:01:42,280 --> 03:01:44,760
because companies have a lot of GPU power,

3360
03:01:44,760 --> 03:01:47,800
and deep learning is working, so it's pretty tough.

3361
03:01:52,440 --> 03:01:53,440
Any other questions?

3362
03:01:53,440 --> 03:01:54,280
Yeah.

3363
03:02:01,360 --> 03:02:06,360
Oh, not too sure of that.

3364
03:02:08,720 --> 03:02:11,720
Maybe contact me after, I'll definitely answer.

3365
03:02:12,640 --> 03:02:14,400
Yeah, yeah, yeah, definitely.

3366
03:02:14,400 --> 03:02:17,400
I definitely would like to chat to you more about it.

3367
03:02:22,720 --> 03:02:24,360
Let's check that again.

3368
03:02:31,360 --> 03:02:36,360
Yeah, that's the one.

3369
03:02:42,080 --> 03:02:43,320
Doesn't work?

3370
03:02:43,320 --> 03:02:44,600
Well, they shared the wrong one.

3371
03:02:44,600 --> 03:02:46,600
Maybe someone can help me double check.

3372
03:02:47,600 --> 03:02:50,600
Double check, see if they're right.

3373
03:02:54,600 --> 03:02:55,440
Ha ha.

3374
03:02:56,760 --> 03:02:57,600
Wow.

3375
03:02:58,560 --> 03:02:59,560
It's gonna be small.

3376
03:03:02,360 --> 03:03:04,240
Yeah, so there was an R missing.

3377
03:03:20,760 --> 03:03:23,560
Yeah, so one more thing that I wanted to share with you,

3378
03:03:23,560 --> 03:03:26,400
let me see, one more quick thing.

3379
03:03:27,240 --> 03:03:30,440
Just quick advice is, I would say,

3380
03:03:30,480 --> 03:03:33,320
I always say this, I was fortunate, like I say,

3381
03:03:33,320 --> 03:03:35,400
that I got to do a PhD in LLP,

3382
03:03:35,400 --> 03:03:37,280
and I get to teach like this, I'm very fortunate,

3383
03:03:37,280 --> 03:03:41,320
and I'm really happy, and I really like the turnout of today,

3384
03:03:41,320 --> 03:03:43,480
and I really appreciate you being here

3385
03:03:43,480 --> 03:03:45,040
and trying to learn this stuff.

3386
03:03:45,040 --> 03:03:47,040
One thing I would say is don't rush,

3387
03:03:47,040 --> 03:03:48,680
there's plenty of time to learn.

3388
03:03:49,560 --> 03:03:51,960
A lot of the exciting results, as you see today,

3389
03:03:51,960 --> 03:03:53,880
are just stuff that, you know,

3390
03:03:53,880 --> 03:03:56,400
it's just like a lot of repetitive stuff that's going on,

3391
03:03:56,400 --> 03:03:58,480
and companies have a lot of GPU power,

3392
03:03:58,480 --> 03:04:00,440
so they're able to get those good results,

3393
03:04:00,440 --> 03:04:03,240
but there's a lot of things you can work on and improve.

3394
03:04:03,240 --> 03:04:07,000
And enrolling courses, but be sure to practice on your own,

3395
03:04:07,000 --> 03:04:08,480
that's something that I try to highlight

3396
03:04:08,480 --> 03:04:10,200
during this tutorial.

3397
03:04:10,200 --> 03:04:12,760
Enrolling math courses, most people would tell you

3398
03:04:12,760 --> 03:04:14,800
don't worry about the math,

3399
03:04:14,800 --> 03:04:18,360
but you can see that Python actually makes it super easy

3400
03:04:18,360 --> 03:04:21,800
to go through the math, do the derivations,

3401
03:04:21,800 --> 03:04:24,360
yeah, do the computations in that computation graph,

3402
03:04:24,360 --> 03:04:26,160
I think that it's really helpful.

3403
03:04:26,680 --> 03:04:29,320
Try to implement papers on your own,

3404
03:04:29,320 --> 03:04:32,320
that's the way I started, I just went and I started

3405
03:04:32,320 --> 03:04:33,960
with something like the word embedding paper,

3406
03:04:33,960 --> 03:04:36,200
like a skip gram, I have like a notebook for that

3407
03:04:36,200 --> 03:04:37,680
where I implemented myself,

3408
03:04:37,680 --> 03:04:40,320
and like CBO algorithm for word embeddings,

3409
03:04:40,320 --> 03:04:42,040
I implemented that myself as well.

3410
03:04:42,920 --> 03:04:45,320
There's a lot of like cool stuff

3411
03:04:45,320 --> 03:04:46,800
in the Python documentation,

3412
03:04:46,800 --> 03:04:49,240
which you can use also as a really nice guide

3413
03:04:49,240 --> 03:04:51,120
to know that you're going in the right direction,

3414
03:04:51,120 --> 03:04:54,040
because sometimes that's kind of like a challenge, right?

3415
03:04:54,040 --> 03:04:56,240
Where to start, that's a big question.

3416
03:04:56,240 --> 03:05:00,320
So I guess these documentations really help for that.

3417
03:05:00,320 --> 03:05:02,920
There are many books, here is good to learn as well.

3418
03:05:03,840 --> 03:05:06,240
It's also nice to combine these different learning resources,

3419
03:05:06,240 --> 03:05:07,880
if you're only learning from videos,

3420
03:05:07,880 --> 03:05:08,720
it's pretty difficult,

3421
03:05:08,720 --> 03:05:11,280
for me, in my personal experience, it was difficult.

3422
03:05:12,160 --> 03:05:14,680
I try to go to meet-ups a lot and talk with people

3423
03:05:14,680 --> 03:05:16,400
and try to understand the use cases

3424
03:05:16,400 --> 03:05:17,880
that they're working on,

3425
03:05:17,880 --> 03:05:19,760
and I appreciate the questions that you ask

3426
03:05:19,760 --> 03:05:22,280
about the data sets, you know, and the kind of challenges,

3427
03:05:22,280 --> 03:05:24,080
because all these things really help me

3428
03:05:24,080 --> 03:05:26,880
to understand the problem better, so I appreciate that.

3429
03:05:28,200 --> 03:05:31,760
For me, future work is like, I want to extend these notebooks,

3430
03:05:31,760 --> 03:05:34,160
eventually I want to make it like a book,

3431
03:05:34,160 --> 03:05:36,560
so feel free to just keep up with me,

3432
03:05:36,560 --> 03:05:39,440
I have a Twitter that I constantly use,

3433
03:05:39,440 --> 03:05:42,080
I'm always trying to help students

3434
03:05:42,080 --> 03:05:44,120
to learn this stuff from the beginning,

3435
03:05:44,120 --> 03:05:46,720
so I'm always trying to adventure into a new task,

3436
03:05:46,720 --> 03:05:48,640
for me, it's just exploring the area.

3437
03:05:48,640 --> 03:05:50,840
Yeah, and there are other sophisticated techniques

3438
03:05:50,840 --> 03:05:53,120
that I didn't discuss here, like GANs,

3439
03:05:53,120 --> 03:05:55,800
which someone raised earlier,

3440
03:05:55,800 --> 03:05:58,800
like what do you use for doing natural language generation,

3441
03:05:58,800 --> 03:06:01,880
so you can use those generative models,

3442
03:06:01,880 --> 03:06:04,600
so that's a really interesting area as well.

3443
03:06:04,600 --> 03:06:07,720
These are just some references, so these are just some books,

3444
03:06:07,720 --> 03:06:10,080
I don't usually recommend books, that's not my thing,

3445
03:06:10,080 --> 03:06:13,320
but I would say that these books really help me

3446
03:06:14,960 --> 03:06:15,800
in my learning,

3447
03:06:16,640 --> 03:06:17,680
in my learning.

3448
03:06:18,680 --> 03:06:22,000
This is a recent one, NLP with PyTorch,

3449
03:06:22,000 --> 03:06:24,720
that's a recent book, I haven't really read this book,

3450
03:06:24,720 --> 03:06:29,200
but I heard it's good, I guess you can try that,

3451
03:06:29,200 --> 03:06:31,280
and Deep Learning with PyTorch from Chalet,

3452
03:06:31,280 --> 03:06:32,600
it's a really, really good book,

3453
03:06:32,600 --> 03:06:34,120
if you want to learn deep learning,

3454
03:06:34,120 --> 03:06:37,360
I would strongly recommend this book as a starting point.

3455
03:06:37,360 --> 03:06:38,720
There are other really good books here,

3456
03:06:38,720 --> 03:06:40,880
for learning theory, this one is really great,

3457
03:06:40,880 --> 03:06:43,680
neural network methods for NLP,

3458
03:06:44,640 --> 03:06:46,840
it would help to answer questions like why,

3459
03:06:46,840 --> 03:06:49,160
what's the difference between LSTM, GRU,

3460
03:06:49,160 --> 03:06:51,080
which sometimes you want to know,

3461
03:06:51,080 --> 03:06:52,560
that book will really help you

3462
03:06:52,560 --> 03:06:55,800
and create a really solid foundation for your theory.

3463
03:06:57,520 --> 03:06:59,840
Yeah, and there are some websites I share here,

3464
03:06:59,840 --> 03:07:02,600
one more last thing I want to share before you go,

3465
03:07:02,600 --> 03:07:05,760
so I partnered up with another researcher

3466
03:07:05,760 --> 03:07:10,760
to create this really nice website here,

3467
03:07:11,480 --> 03:07:15,600
and basically it's the foundation of all the stuff

3468
03:07:15,600 --> 03:07:18,600
that I'm doing here, I'm trying to teach things about NLP

3469
03:07:18,600 --> 03:07:20,440
and using deep learning techniques,

3470
03:07:20,440 --> 03:07:24,640
so this is, if you ever want to know where to start,

3471
03:07:24,640 --> 03:07:27,560
we created this project, it's like an ebook,

3472
03:07:27,560 --> 03:07:29,960
where you can basically know where to start,

3473
03:07:29,960 --> 03:07:33,160
so we start at word embeddings, and character embeddings,

3474
03:07:33,160 --> 03:07:36,040
then we go to CNNs, then we go to recurrent neural networks,

3475
03:07:36,040 --> 03:07:38,040
then we go to recursive neural networks,

3476
03:07:38,040 --> 03:07:40,720
and all these models, right, it's a step by step,

3477
03:07:41,720 --> 03:07:43,160
learning the theory, eventually we plan to incorporate

3478
03:07:43,160 --> 03:07:44,520
all these notebooks into this,

3479
03:07:44,520 --> 03:07:47,560
so we have like a full-fledged learning resource

3480
03:07:47,560 --> 03:07:49,800
that you can use, that you don't have to go all over

3481
03:07:49,800 --> 03:07:51,200
the place to learn this stuff,

3482
03:07:51,200 --> 03:07:54,000
so that's kind of the intention behind this project,

3483
03:07:54,000 --> 03:07:56,120
most of the things are thought, yeah.

3484
03:07:58,720 --> 03:08:01,360
So it's in the, so it's here?

3485
03:08:02,880 --> 03:08:04,520
Yeah, the last one, yeah.

3486
03:08:06,120 --> 03:08:08,320
So it's pretty useful for that,

3487
03:08:08,320 --> 03:08:10,320
it has some of the equations I use here,

3488
03:08:11,240 --> 03:08:14,840
let's see, and it has like, what of the state of the art,

3489
03:08:14,840 --> 03:08:17,240
models used for what kind of tasks,

3490
03:08:17,240 --> 03:08:21,880
it's a really intuitive, like ebook,

3491
03:08:21,880 --> 03:08:25,160
definitely check it out if you're interested in this space,

3492
03:08:25,160 --> 03:08:27,360
if you ever have questions, please email me,

3493
03:08:27,360 --> 03:08:30,520
you have access to my email, you have my Twitter,

3494
03:08:30,520 --> 03:08:32,440
just feel free to have a discussion with me,

3495
03:08:32,440 --> 03:08:35,080
I'm always free to ask, it's my job, educating,

3496
03:08:35,080 --> 03:08:37,920
so I'm always free to, and excited to always

3497
03:08:37,920 --> 03:08:40,080
answer your questions, thanks a lot.

