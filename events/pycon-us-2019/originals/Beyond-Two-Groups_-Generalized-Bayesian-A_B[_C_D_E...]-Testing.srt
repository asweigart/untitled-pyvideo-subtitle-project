1
00:00:00,000 --> 00:00:01,220
Hello, everyone.

2
00:00:01,220 --> 00:00:03,900
How's everyone doing?

3
00:00:03,900 --> 00:00:04,860
Awesome.

4
00:00:04,860 --> 00:00:05,660
All right.

5
00:00:05,660 --> 00:00:10,500
This talk is Beyond Two Groups, General Bayesian Testing

6
00:00:10,500 --> 00:00:12,060
by Eric Ma.

7
00:00:12,060 --> 00:00:17,060
He has an open space later today that he'll

8
00:00:17,060 --> 00:00:18,060
be telling you about.

9
00:00:18,060 --> 00:00:20,740
And that is where you can go and ask questions.

10
00:00:20,740 --> 00:00:25,780
So please hold your questions for the open space later.

11
00:00:25,780 --> 00:00:27,980
And Eric Ma.

12
00:00:27,980 --> 00:00:35,840
Hello, everybody.

13
00:00:35,840 --> 00:00:37,840
How are you all doing?

14
00:00:37,840 --> 00:00:40,000
And how has PyCon been?

15
00:00:40,000 --> 00:00:40,760
Pretty awesome?

16
00:00:40,760 --> 00:00:41,240
Yeah.

17
00:00:41,240 --> 00:00:41,560
Yeah.

18
00:00:41,560 --> 00:00:41,880
Cool.

19
00:00:41,880 --> 00:00:44,080
I've been on staff for a few years myself.

20
00:00:44,080 --> 00:00:46,640
PyCon's one of the highlights of my year.

21
00:00:46,640 --> 00:00:48,880
And it's really amazing that we were able to get

22
00:00:48,880 --> 00:00:50,000
so many people here.

23
00:00:50,000 --> 00:00:53,440
I'd also like to thank you all for coming to my talk today.

24
00:00:53,440 --> 00:00:56,400
The title, you can see it on the screen, Beyond Two Groups,

25
00:00:56,400 --> 00:00:59,620
Generalized Bayesian A, B, C, D, E, F, G Testing.

26
00:01:02,260 --> 00:01:04,740
At the end of it, I hope it's pretty clear to you

27
00:01:04,740 --> 00:01:07,920
that this is basically my rant against canned

28
00:01:07,920 --> 00:01:10,340
statistical procedures.

29
00:01:10,340 --> 00:01:12,700
We do not want canned statistical procedures.

30
00:01:12,700 --> 00:01:16,980
We want to do our statistics in a more principled fashion.

31
00:01:16,980 --> 00:01:20,820
And Python can be a great part of that.

32
00:01:20,820 --> 00:01:22,920
A little bit about myself, just so that you

33
00:01:22,920 --> 00:01:24,620
have a flavor of who I am.

34
00:01:24,620 --> 00:01:26,360
Currently, I work for the Novartis Institutes

35
00:01:26,360 --> 00:01:27,880
for Biomedical Research.

36
00:01:27,880 --> 00:01:29,000
We do data science.

37
00:01:29,000 --> 00:01:31,480
I'm on a team called Scientific Data Analysis.

38
00:01:31,480 --> 00:01:33,320
My official title is investigator,

39
00:01:33,320 --> 00:01:36,480
which sort of means I'm allowed to do researchy things.

40
00:01:36,480 --> 00:01:38,120
Scientific Data Analysis is basically

41
00:01:38,120 --> 00:01:39,480
one of the most generic names you

42
00:01:39,480 --> 00:01:41,880
can think of for a team that's embedded

43
00:01:41,880 --> 00:01:45,480
in a company whose bread and butter is science.

44
00:01:45,480 --> 00:01:46,680
All right?

45
00:01:46,680 --> 00:01:49,680
Prior to that, I was an Insight Data Science fellow.

46
00:01:49,680 --> 00:01:52,780
And prior to that, I did my thesis

47
00:01:52,780 --> 00:01:56,720
in the Department of Biological Engineering at MIT.

48
00:01:56,720 --> 00:01:58,760
It was a journey for me to go in through the back door

49
00:01:58,760 --> 00:01:59,720
and out the front door.

50
00:01:59,720 --> 00:02:01,840
And then if people want to hear it,

51
00:02:01,840 --> 00:02:03,160
open space is open for that.

52
00:02:03,160 --> 00:02:04,400
All right?

53
00:02:04,400 --> 00:02:08,040
Finally, a little bit about me and Python.

54
00:02:08,040 --> 00:02:10,880
Python, for me, has been a very productive language

55
00:02:10,880 --> 00:02:13,120
because it's been the language in which I've

56
00:02:13,120 --> 00:02:16,120
been able to learn a lot of math, things from Bayesian

57
00:02:16,120 --> 00:02:19,000
stats to machine learning to network science.

58
00:02:19,000 --> 00:02:21,400
Without the programming language as a tool

59
00:02:21,400 --> 00:02:24,060
to aid in my learning, it would have been much harder

60
00:02:24,060 --> 00:02:26,860
because this interactive nature of programming

61
00:02:26,860 --> 00:02:28,340
really aids in the learning.

62
00:02:28,340 --> 00:02:31,340
And beyond that, a little note about Python.

63
00:02:31,340 --> 00:02:33,620
It's one of those things where I've

64
00:02:33,620 --> 00:02:35,420
heard people say that Python is just

65
00:02:35,420 --> 00:02:38,260
like the second best language for everything.

66
00:02:38,260 --> 00:02:41,740
And the keyword is not second best.

67
00:02:41,740 --> 00:02:43,860
The keyword is everything, right?

68
00:02:43,860 --> 00:02:46,740
There's rarely one language that lets you go from the web

69
00:02:46,740 --> 00:02:50,660
to data science, to munging data, to DevOps, to SysOps,

70
00:02:50,660 --> 00:02:53,000
et cetera, et cetera, et cetera, all while sticking

71
00:02:53,000 --> 00:02:54,280
within one language.

72
00:02:54,280 --> 00:02:55,800
So it's great.

73
00:02:55,800 --> 00:02:59,360
And I hope you all have the same sentiments.

74
00:02:59,360 --> 00:03:01,440
OK, so why did I make this talk?

75
00:03:01,440 --> 00:03:05,720
I made this talk because I saw a gap in the data science

76
00:03:05,720 --> 00:03:09,880
so-called talk literature in which most talks tell you

77
00:03:09,880 --> 00:03:14,600
how to do an A-B test, Bayesian style or frequentist style.

78
00:03:14,600 --> 00:03:16,240
And they show you two groups.

79
00:03:16,240 --> 00:03:18,120
And they tell you about the T test.

80
00:03:18,460 --> 00:03:21,100
Then they tell you how to apply this to your business case,

81
00:03:21,100 --> 00:03:22,500
et cetera, et cetera.

82
00:03:22,500 --> 00:03:28,180
So in response to that, I basically have three messages.

83
00:03:28,180 --> 00:03:30,700
The first message is you don't have to be stuck with two

84
00:03:30,700 --> 00:03:32,220
groups.

85
00:03:32,220 --> 00:03:33,800
And more importantly, you don't have

86
00:03:33,800 --> 00:03:36,460
to be stuck with the assumptions of the T test, which are

87
00:03:36,460 --> 00:03:39,640
basically normal distribution assumptions.

88
00:03:39,640 --> 00:03:41,340
The second message I have for you

89
00:03:41,340 --> 00:03:43,300
is related to the language.

90
00:03:43,300 --> 00:03:46,420
Python, and more specifically, PyMC3,

91
00:03:46,420 --> 00:03:49,360
is a really great language for probabilistic modeling

92
00:03:49,360 --> 00:03:52,280
and for doing your stats in a principled fashion.

93
00:03:52,280 --> 00:03:55,200
And that's the third point, that with Bayesian modeling,

94
00:03:55,200 --> 00:03:59,120
we can move away from these canned statistical procedures

95
00:03:59,120 --> 00:04:01,600
that have been handed to us in our university classes

96
00:04:01,600 --> 00:04:03,720
and our university stats education

97
00:04:03,720 --> 00:04:07,800
and finally start doing stats in a more principled fashion.

98
00:04:10,240 --> 00:04:15,520
OK, so this is a talk that is a statistics talk disguised

99
00:04:15,520 --> 00:04:16,660
with Python.

100
00:04:16,660 --> 00:04:20,740
And so I am assuming a few pieces of knowledge here.

101
00:04:20,740 --> 00:04:23,220
And I'm putting this out here for the benefit of you,

102
00:04:23,220 --> 00:04:26,580
the audience, so you can make an informed decision as to whether

103
00:04:26,580 --> 00:04:28,180
you would like to continue in this room

104
00:04:28,180 --> 00:04:30,220
or if you'd like to move to a different room.

105
00:04:30,220 --> 00:04:32,780
And I don't mean this in a way to exclude people.

106
00:04:32,780 --> 00:04:35,500
I just want you to have an informed decision.

107
00:04:35,500 --> 00:04:39,100
So first off, I talk a lot about probability distributions,

108
00:04:39,100 --> 00:04:41,420
and I name them in passing in this talk.

109
00:04:41,420 --> 00:04:44,560
So I am assuming that when I name a probability

110
00:04:44,560 --> 00:04:46,980
distribution, you're able to conjure up in your mind

111
00:04:46,980 --> 00:04:50,060
what its shape roughly looks like, what it's

112
00:04:50,060 --> 00:04:53,860
used to be modeling, what it's used for in modeling work,

113
00:04:53,860 --> 00:04:57,740
and what values for which this probability distribution are

114
00:04:57,740 --> 00:04:59,820
valid.

115
00:04:59,820 --> 00:05:01,660
Probability distributions, by the way,

116
00:05:01,660 --> 00:05:04,260
as a working definition for today's talk,

117
00:05:04,260 --> 00:05:07,580
we're just going to call it credibility points assigned

118
00:05:07,580 --> 00:05:09,020
on the number line.

119
00:05:09,020 --> 00:05:13,780
And so when I talk about strong belief or weak belief

120
00:05:13,780 --> 00:05:17,400
inside a strong or weak belief for the value

121
00:05:17,400 --> 00:05:21,560
of a particular parameter, a strong belief, all I'm saying

122
00:05:21,560 --> 00:05:25,040
is that I'm willing to assign very few, a small range

123
00:05:25,040 --> 00:05:27,240
of numbers, credibility points.

124
00:05:27,240 --> 00:05:29,600
So we have a narrow distribution.

125
00:05:29,600 --> 00:05:32,400
And a weak belief says, I'm willing to assign

126
00:05:32,400 --> 00:05:36,820
a large number of, a large range of numbers, credibility points,

127
00:05:36,820 --> 00:05:40,360
and therefore we have a wide distribution.

128
00:05:40,360 --> 00:05:43,160
So the distinction between continuous and discrete

129
00:05:43,160 --> 00:05:46,220
is also important because the two examples that I will show

130
00:05:46,220 --> 00:05:50,620
involve one continuous and one discrete data.

131
00:05:50,620 --> 00:05:52,300
Now, because this is a talk on Bayes,

132
00:05:52,300 --> 00:05:54,940
we have the obligatory Bayes rule slide.

133
00:05:54,940 --> 00:05:56,820
And so what is Bayes rule?

134
00:05:56,820 --> 00:06:00,940
Bayes rule is a neon sign that lives on the, on the,

135
00:06:00,940 --> 00:06:01,940
oh, sorry, no.

136
00:06:01,940 --> 00:06:05,660
Bayes rule is a mathematical routine

137
00:06:05,660 --> 00:06:07,100
for working definition for today.

138
00:06:07,100 --> 00:06:09,340
Bayes rule is a mathematical routine

139
00:06:09,340 --> 00:06:12,940
that lets us update our beliefs about a particular

140
00:06:12,940 --> 00:06:15,720
parameter having seen new data.

141
00:06:15,720 --> 00:06:16,320
All right?

142
00:06:16,320 --> 00:06:17,400
That's all it is.

143
00:06:17,400 --> 00:06:20,160
We don't need to worry about what the mathematical routine is,

144
00:06:20,160 --> 00:06:24,880
thankfully, because PyMC3, which is the package in which I have

145
00:06:24,880 --> 00:06:27,940
implemented all of these models, PyMC3

146
00:06:27,940 --> 00:06:31,560
does all of the fancy math for lazy programmers like me.

147
00:06:31,560 --> 00:06:32,320
All right?

148
00:06:32,320 --> 00:06:35,720
So PyMC3 is a great language, a great package

149
00:06:35,720 --> 00:06:37,680
written in the Python language for doing that.

150
00:06:37,680 --> 00:06:41,200
And I will assume some familiarity with Python,

151
00:06:41,200 --> 00:06:42,400
with PyMC3 syntax.

152
00:06:42,420 --> 00:06:44,700
Now, this isn't to say that if you don't understand

153
00:06:44,700 --> 00:06:47,060
PyMC3 syntax, you shouldn't be here.

154
00:06:47,060 --> 00:06:49,540
It just means I will go a little bit slower

155
00:06:49,540 --> 00:06:54,580
during the code portion for the benefit of those who need it.

156
00:06:54,580 --> 00:06:56,660
That said, if you've never touched PyMC3 before,

157
00:06:56,660 --> 00:06:59,860
my hope is that by looking at the PyMC3 code,

158
00:06:59,860 --> 00:07:01,660
you'll intuit how we've told the data

159
00:07:01,660 --> 00:07:05,980
generative story of our models in such a plain language way

160
00:07:05,980 --> 00:07:07,780
that we can just intuit it from the code.

161
00:07:07,780 --> 00:07:09,940
All right?

162
00:07:09,940 --> 00:07:10,460
All right.

163
00:07:10,460 --> 00:07:12,480
So comparison.

164
00:07:12,480 --> 00:07:16,280
Comparison is the basis of how we do science.

165
00:07:16,280 --> 00:07:19,360
And this video here shows the late Hans Rosling,

166
00:07:19,360 --> 00:07:22,680
a statistician who's great at storytelling.

167
00:07:22,680 --> 00:07:25,800
And in one of his books, he wrote this quote.

168
00:07:25,800 --> 00:07:29,160
When I see a lonely number in a news report,

169
00:07:29,160 --> 00:07:31,920
it always triggers an alarm.

170
00:07:31,920 --> 00:07:34,840
What should this lonely number be compared to?

171
00:07:34,840 --> 00:07:38,420
Implicitly, what he's saying is, what is the baseline?

172
00:07:38,420 --> 00:07:39,880
And what is the baseline that we're

173
00:07:39,880 --> 00:07:42,800
comparing our intervention against is basically

174
00:07:42,800 --> 00:07:45,060
what we're doing whenever we're doing science

175
00:07:45,060 --> 00:07:48,660
and when we do the statistical analysis of that data?

176
00:07:48,660 --> 00:07:50,060
Typically, we're thrown it.

177
00:07:50,060 --> 00:07:52,100
We're thrown two groups.

178
00:07:52,100 --> 00:07:56,420
And it's my hope to tell you, we don't have to stick with two.

179
00:07:56,420 --> 00:08:01,820
So I'm going to tell you two analyses that are lifelike.

180
00:08:01,820 --> 00:08:03,860
I'm clearly not showing real data here because

181
00:08:03,860 --> 00:08:06,500
of confidentiality issues or because they're borrowed

182
00:08:06,500 --> 00:08:08,700
from a different source.

183
00:08:08,700 --> 00:08:11,460
But I'm going to show you two analyses that are lifelike

184
00:08:11,460 --> 00:08:15,300
and will simulate for you what they look like.

185
00:08:15,300 --> 00:08:18,420
The first vignette will tell you how

186
00:08:18,420 --> 00:08:21,680
we do an analysis that moves beyond two groups.

187
00:08:21,680 --> 00:08:23,380
And the second vignette will tell you

188
00:08:23,380 --> 00:08:25,620
how we move beyond two distributed,

189
00:08:25,620 --> 00:08:29,740
t-distributed data in addition to that.

190
00:08:29,740 --> 00:08:32,100
So let's look at vignette one.

191
00:08:32,100 --> 00:08:36,980
This story comes from a paper written by John Kresge in 2013.

192
00:08:36,980 --> 00:08:42,820
And it's a paper that talks about the intervention where

193
00:08:42,820 --> 00:08:46,140
we give participants in a trial a drug.

194
00:08:46,140 --> 00:08:50,100
And then we measure their IQ post-intervention.

195
00:08:50,100 --> 00:08:52,340
So we get data that looks something like this.

196
00:08:52,340 --> 00:08:53,340
Just a quick poll.

197
00:08:53,340 --> 00:08:55,640
How many of you have seen a chart that looks like this?

198
00:08:55,640 --> 00:08:57,700
This is a cumulative distribution plot.

199
00:08:57,700 --> 00:08:58,420
Great.

200
00:08:58,420 --> 00:09:00,300
So I don't really need to explain too much

201
00:09:00,300 --> 00:09:01,500
about what it says.

202
00:09:01,500 --> 00:09:03,500
Just very quickly for the benefit of those

203
00:09:03,500 --> 00:09:06,140
who haven't seen it, the x-axis, we

204
00:09:06,140 --> 00:09:08,460
have the values that we've measured.

205
00:09:08,460 --> 00:09:14,900
On the y-axis, we plot what cumulative fraction of samples

206
00:09:14,900 --> 00:09:17,380
have that value or less.

207
00:09:17,380 --> 00:09:18,900
And so the tails of the distribution

208
00:09:18,900 --> 00:09:20,180
are on the left and the right.

209
00:09:20,180 --> 00:09:22,860
The meaty part is in the middle.

210
00:09:22,860 --> 00:09:24,900
And we have two groups shown over here.

211
00:09:24,900 --> 00:09:27,820
We have the placebo group, which is our control group.

212
00:09:27,820 --> 00:09:29,440
And then we have the drug-treated group,

213
00:09:29,440 --> 00:09:31,000
which is our intervention group.

214
00:09:31,000 --> 00:09:32,880
Marketers call this AB testing.

215
00:09:32,880 --> 00:09:35,640
I don't know why we have to invent yet a new term

216
00:09:35,640 --> 00:09:36,760
for case control testing.

217
00:09:36,760 --> 00:09:40,760
But yes, that's the alternative term that people will use.

218
00:09:40,760 --> 00:09:44,880
So when you look at this, you think, well, yeah, OK.

219
00:09:44,880 --> 00:09:48,720
What kind of statistical procedure could I use here?

220
00:09:48,720 --> 00:09:51,240
So we go back into our university textbooks,

221
00:09:51,240 --> 00:09:55,440
flip back our notes from college, find stats 201,

222
00:09:55,440 --> 00:09:58,140
and they tell us, use the t-test.

223
00:09:58,180 --> 00:09:59,860
So we run a t-test.

224
00:09:59,860 --> 00:10:02,700
And how many of you see that p-value, that juicy, juicy

225
00:10:02,700 --> 00:10:06,780
p-value of 0.025?

226
00:10:06,780 --> 00:10:09,220
Things are statistically significant.

227
00:10:09,220 --> 00:10:10,380
It's awesome.

228
00:10:10,380 --> 00:10:12,660
We can publish this paper, right?

229
00:10:12,660 --> 00:10:14,740
This drug can go to the trials, right?

230
00:10:14,740 --> 00:10:15,240
No.

231
00:10:18,340 --> 00:10:22,260
As it turns out, we missed out that there

232
00:10:22,260 --> 00:10:25,040
were two other groups.

233
00:10:25,040 --> 00:10:27,620
And if we were a dishonest scientist,

234
00:10:27,620 --> 00:10:29,660
we'd stop there.

235
00:10:29,660 --> 00:10:33,900
But we couldn't mask the fact that we've got four groups here.

236
00:10:33,900 --> 00:10:37,620
So now what's the canned statistical procedure

237
00:10:37,620 --> 00:10:39,740
for four groups?

238
00:10:39,740 --> 00:10:42,180
So we go back to our textbooks, go back to our notes.

239
00:10:42,180 --> 00:10:45,660
Stats 201 didn't teach that, so we go to stats 211.

240
00:10:45,660 --> 00:10:49,460
Stats 211 says, there's this thing called an ANOVA.

241
00:10:49,460 --> 00:10:52,380
And you might be able to do a post-hoc t-test.

242
00:10:52,380 --> 00:10:54,300
So we whip out SciPy.

243
00:10:54,300 --> 00:10:55,820
And then we do the ANOVA.

244
00:10:55,980 --> 00:10:57,620
Oh, no.

245
00:10:57,620 --> 00:10:59,540
That p-value is 0.09.

246
00:10:59,540 --> 00:11:01,340
This thing is not statistically significant.

247
00:11:01,340 --> 00:11:02,300
What are we going to do?

248
00:11:02,300 --> 00:11:04,700
The whole thing has gone crumbling down

249
00:11:04,700 --> 00:11:05,780
in front of my eyes.

250
00:11:05,780 --> 00:11:07,180
Well, it's OK.

251
00:11:07,180 --> 00:11:09,980
Even if this thing is not statistically significant,

252
00:11:09,980 --> 00:11:10,980
that's fine.

253
00:11:10,980 --> 00:11:14,260
Let's just do the post-hoc t-test anyways, right?

254
00:11:14,260 --> 00:11:16,620
And so how many post-hoc t-tests do we have to do?

255
00:11:16,620 --> 00:11:18,320
Well, we have four groups.

256
00:11:18,320 --> 00:11:20,460
We do pairwise t-tests.

257
00:11:20,460 --> 00:11:23,420
And so we have four choose two number of tests, right?

258
00:11:23,420 --> 00:11:24,780
And so we put this in.

259
00:11:24,820 --> 00:11:27,660
And since we have more than one test, one hypothesis testing

260
00:11:27,660 --> 00:11:30,140
going, we do the French Revolution thing.

261
00:11:30,140 --> 00:11:31,780
We bring out the guillotine.

262
00:11:31,780 --> 00:11:33,500
And we start chopping out hypotheses

263
00:11:33,500 --> 00:11:34,940
that are not significant.

264
00:11:34,940 --> 00:11:37,100
And we tremble before the guillotine,

265
00:11:37,100 --> 00:11:43,260
asking, when will we get a significant hypothesis test?

266
00:11:43,260 --> 00:11:44,540
Oh, my.

267
00:11:44,540 --> 00:11:46,580
What an emotional roller coaster.

268
00:11:46,580 --> 00:11:49,580
Is this the right way to do stats?

269
00:11:49,580 --> 00:11:52,100
Is this the right way to do science?

270
00:11:52,100 --> 00:11:54,580
Do we really have to bring out the French Revolution

271
00:11:54,580 --> 00:11:58,500
guillotine in front every time we have multiple groups?

272
00:11:58,500 --> 00:12:02,380
Moreover, think about the case where we don't have just four

273
00:12:02,380 --> 00:12:03,740
groups tested.

274
00:12:03,740 --> 00:12:05,940
We have 50 groups tested.

275
00:12:05,940 --> 00:12:08,860
Suddenly, we have more than 1,000 pairwise tests.

276
00:12:08,860 --> 00:12:13,340
And that guillotine is going to have a field day chopping out

277
00:12:13,340 --> 00:12:15,980
hypotheses, pairwise hypotheses tests

278
00:12:15,980 --> 00:12:18,340
that are not significant.

279
00:12:18,340 --> 00:12:20,300
There's got to be a better way.

280
00:12:20,300 --> 00:12:22,980
And when I see a scientific article that says,

281
00:12:22,980 --> 00:12:25,820
or someone says that science has proven this and proven that,

282
00:12:25,820 --> 00:12:28,940
first question I ask is, what have they disproven?

283
00:12:28,940 --> 00:12:31,700
Second question is, how did they do the stats?

284
00:12:31,700 --> 00:12:33,980
Was it a p-value that they calculated?

285
00:12:33,980 --> 00:12:37,300
I don't quite believe that just yet.

286
00:12:37,300 --> 00:12:39,980
So there's got to be a better way.

287
00:12:39,980 --> 00:12:44,260
In particular, let's think carefully about the assumptions

288
00:12:44,260 --> 00:12:47,820
that we make every time we do the t-test.

289
00:12:47,820 --> 00:12:50,940
Every time we do the t-test, we are assuming a bunch of things,

290
00:12:50,940 --> 00:12:52,660
these three being the key.

291
00:12:52,660 --> 00:12:56,740
Firstly, that our data are normally distributed.

292
00:12:56,740 --> 00:12:58,900
IQ, you probably could make a case.

293
00:12:58,900 --> 00:13:02,380
But what if you had other things?

294
00:13:02,380 --> 00:13:04,060
Not necessarily true.

295
00:13:04,060 --> 00:13:06,980
Why would we throw the canned statistical procedure

296
00:13:06,980 --> 00:13:10,020
at non-normally distributed data when we're assuming

297
00:13:10,020 --> 00:13:11,820
normally distributed data?

298
00:13:11,820 --> 00:13:14,580
Secondly, we assume that the standard deviations

299
00:13:14,580 --> 00:13:17,740
between the groups are equal.

300
00:13:17,740 --> 00:13:18,860
What if that's not true?

301
00:13:18,860 --> 00:13:21,540
What if we need a way to flexibly model that?

302
00:13:21,540 --> 00:13:25,060
Thirdly, what if our samples were not independent?

303
00:13:25,060 --> 00:13:27,420
Now, in a randomized controlled trial,

304
00:13:27,420 --> 00:13:29,140
hopefully someone checked.

305
00:13:29,140 --> 00:13:32,420
Otherwise, our drug trials by our company would be done wrong.

306
00:13:32,420 --> 00:13:34,460
Hopefully someone checked.

307
00:13:34,460 --> 00:13:37,220
But what if that assumption doesn't hold true?

308
00:13:37,220 --> 00:13:39,020
We can't do the t-test.

309
00:13:39,020 --> 00:13:42,420
So what canned statistical procedure is there?

310
00:13:42,420 --> 00:13:45,860
So I propose we stop assuming things

311
00:13:45,860 --> 00:13:49,940
and stop implicitly violating these assumptions.

312
00:13:49,940 --> 00:13:52,980
And instead, we do it Bayesian style instead.

313
00:13:52,980 --> 00:13:56,740
So let's carefully model the key components of this problem.

314
00:13:56,740 --> 00:13:58,460
What are the key components of this problem?

315
00:13:58,460 --> 00:13:59,900
Well, we got data.

316
00:13:59,900 --> 00:14:02,260
We need to know what its likelihood is

317
00:14:02,260 --> 00:14:03,460
under a particular model.

318
00:14:03,460 --> 00:14:05,780
And then we need to find out and compute

319
00:14:05,780 --> 00:14:08,580
what the key parameters of this model are

320
00:14:08,580 --> 00:14:11,040
and what their most likely parameters are.

321
00:14:11,040 --> 00:14:13,860
And then for each group, do the same thing

322
00:14:13,860 --> 00:14:15,580
and compare those key parameters.

323
00:14:15,580 --> 00:14:17,460
That's all we need to do.

324
00:14:17,460 --> 00:14:21,580
So we go in.

325
00:14:21,580 --> 00:14:24,100
We write some PMC3 code.

326
00:14:24,100 --> 00:14:26,340
PMC3 code is written like this.

327
00:14:26,340 --> 00:14:29,420
You'll notice, first off, there are these things

328
00:14:29,420 --> 00:14:34,380
called PM.normal, PM.halfKoshi, PM.exponential.

329
00:14:34,380 --> 00:14:37,380
And there are all of these great first class citizens

330
00:14:37,380 --> 00:14:39,540
called probability distributions.

331
00:14:39,540 --> 00:14:41,020
So we can assign them to variables

332
00:14:41,020 --> 00:14:42,700
and we can do math with them.

333
00:14:42,700 --> 00:14:46,220
So for example, I'll model our data,

334
00:14:46,220 --> 00:14:48,980
maybe as a starting point, with a student t distribution.

335
00:14:48,980 --> 00:14:51,980
Because there's a case that multiple factors affect your IQ

336
00:14:51,980 --> 00:14:56,140
and so as a result, the resultant IQ may potentially,

337
00:14:56,140 --> 00:14:59,260
in a hand wavy way, be t distributed.

338
00:14:59,260 --> 00:15:01,980
The student t distribution has a few key parameters.

339
00:15:01,980 --> 00:15:04,580
It's got a mu, it's got a standard deviation,

340
00:15:04,580 --> 00:15:06,540
and it's got a degree of freedom parameter.

341
00:15:06,540 --> 00:15:08,820
Remember that point that I made about having

342
00:15:08,820 --> 00:15:12,400
non-equal, not necessarily equal standard deviations

343
00:15:12,400 --> 00:15:13,340
per group?

344
00:15:13,340 --> 00:15:19,700
Well, as it turns out, we can model standard deviations

345
00:15:19,700 --> 00:15:22,420
on a per group basis using PMC3.

346
00:15:22,420 --> 00:15:24,620
All we have to do is specify that it's

347
00:15:24,620 --> 00:15:29,060
a vector of standard deviations, one per group,

348
00:15:29,060 --> 00:15:32,940
and then pass that into the student t distribution.

349
00:15:32,940 --> 00:15:33,980
So those are the key things.

350
00:15:33,980 --> 00:15:39,060
We can explicitly encode what we are assuming about our data

351
00:15:39,060 --> 00:15:41,020
inside the probability model.

352
00:15:41,020 --> 00:15:44,140
In other words, lay out all the cards on the table.

353
00:15:44,140 --> 00:15:46,540
And if you're not happy with those assumptions,

354
00:15:46,540 --> 00:15:48,260
bring me a different distribution

355
00:15:48,260 --> 00:15:50,020
and a justification for that.

356
00:15:52,820 --> 00:15:55,060
How do we analyze the results of a Bayesian model?

357
00:15:55,060 --> 00:15:59,380
Well, first off, there's no K choose 2 pairwise hypotheses

358
00:15:59,380 --> 00:16:00,820
test that we need to do.

359
00:16:00,820 --> 00:16:04,380
All we have to look at are posterior distributions.

360
00:16:04,380 --> 00:16:06,440
This is one particular representation

361
00:16:06,440 --> 00:16:08,780
of how we look at posterior distributions.

362
00:16:08,780 --> 00:16:11,820
This is essentially what we call a forest plot.

363
00:16:11,820 --> 00:16:15,060
And in a forest plot per group, each of the groups

364
00:16:15,060 --> 00:16:18,500
are the horizontal, are the rows here.

365
00:16:18,500 --> 00:16:22,300
Per group, we can plot out what the median of the posterior

366
00:16:22,300 --> 00:16:25,640
distribution is, what the interquartile range is,

367
00:16:25,640 --> 00:16:29,780
and what a percentage credible interval would be as well.

368
00:16:29,780 --> 00:16:33,760
And those are the dots, the thick bars, and the thin lines,

369
00:16:33,760 --> 00:16:35,340
respectively.

370
00:16:35,340 --> 00:16:38,220
And all you have to do is look at that and say,

371
00:16:38,220 --> 00:16:42,620
yes, studying turned out to be the better intervention.

372
00:16:42,620 --> 00:16:45,060
And actually, I don't even know what this drug is doing.

373
00:16:45,060 --> 00:16:46,520
Is it making you stoned or something?

374
00:16:46,520 --> 00:16:51,660
Because compared to drug plus studying compared to studying,

375
00:16:51,660 --> 00:16:53,900
you actually have lower IQ, right?

376
00:16:53,900 --> 00:16:56,740
So I don't know if this is the thing that was most recently

377
00:16:56,740 --> 00:17:00,580
legalized in Massachusetts.

378
00:17:00,580 --> 00:17:02,580
So all we have to do is compare posteriors.

379
00:17:02,580 --> 00:17:05,020
And compare this, by the way.

380
00:17:05,020 --> 00:17:08,460
And look at this versus that.

381
00:17:08,460 --> 00:17:10,860
Would you rather kneel beneath the guillotine,

382
00:17:10,860 --> 00:17:13,220
waiting for a hypothesis to be proven true,

383
00:17:13,220 --> 00:17:16,700
or would you rather just look at the posterior distributions?

384
00:17:16,700 --> 00:17:18,540
And moreover, what's really interesting,

385
00:17:18,540 --> 00:17:20,860
what's really neat here is the posterior distributions

386
00:17:20,860 --> 00:17:22,940
tell you the credibility assigned

387
00:17:22,940 --> 00:17:24,660
to the particular values.

388
00:17:24,660 --> 00:17:27,420
Plain and simple, nothing fancy.

389
00:17:27,420 --> 00:17:31,740
So two key lessons from this particular analysis.

390
00:17:31,740 --> 00:17:35,700
Firstly, all you need are posterior distributions.

391
00:17:35,700 --> 00:17:37,740
You didn't have to do the multiple hypothesis

392
00:17:37,740 --> 00:17:40,100
correction procedure.

393
00:17:40,100 --> 00:17:41,060
Could we still be wrong?

394
00:17:41,060 --> 00:17:42,860
Yeah, of course, because our data are

395
00:17:42,860 --> 00:17:45,020
assumed to be generated by a random process.

396
00:17:45,020 --> 00:17:49,420
And so if we, by chance, had the wrong sample,

397
00:17:49,420 --> 00:17:52,060
a non-representative sample, well, we

398
00:17:52,060 --> 00:17:54,220
wouldn't have the correct inference.

399
00:17:54,220 --> 00:18:00,780
But it's harder to be wrong when you go with Bayesian procedures.

400
00:18:00,780 --> 00:18:02,340
Secondly, there should be nothing that

401
00:18:02,340 --> 00:18:04,900
stops you with two groups.

402
00:18:04,900 --> 00:18:07,940
PIME C3 code basically lets you write these things called

403
00:18:07,940 --> 00:18:10,220
a vector of probability distributions

404
00:18:10,220 --> 00:18:13,940
in which you can use each slot in the vector to model one group

405
00:18:13,940 --> 00:18:19,260
and do all of your inference in a nice, principled fashion.

406
00:18:19,260 --> 00:18:19,860
Cool.

407
00:18:19,860 --> 00:18:22,140
So I sort of gave you a semi-cheating example

408
00:18:22,140 --> 00:18:25,300
in which IQ, there's sort of a case

409
00:18:25,300 --> 00:18:27,940
to be made that it's maybe normally distributed.

410
00:18:27,940 --> 00:18:30,420
But what if our data were not t-distributed?

411
00:18:30,540 --> 00:18:32,020
t-distributions and normal distributions

412
00:18:32,020 --> 00:18:34,140
are in the same family.

413
00:18:34,140 --> 00:18:36,460
Well, that's when we come to the second vignette.

414
00:18:36,460 --> 00:18:39,740
And this is one where we're talking about clicking on websites.

415
00:18:39,740 --> 00:18:41,740
And this is an example that came to me last year

416
00:18:41,740 --> 00:18:45,580
after I did my tutorial with my co-instructor Hugo.

417
00:18:45,580 --> 00:18:48,540
I was holding office hours in the Tejas room in the AT&T

418
00:18:48,540 --> 00:18:49,740
center in Texas.

419
00:18:49,740 --> 00:18:53,860
And this guy comes up to me and asks, hey, I got some data.

420
00:18:53,860 --> 00:18:55,300
Turned out to be his company data.

421
00:18:55,300 --> 00:18:57,340
I got some data.

422
00:18:57,340 --> 00:18:59,300
And I need some help looking at it.

423
00:18:59,300 --> 00:19:00,340
Would you have some time?

424
00:19:00,340 --> 00:19:01,140
And they're like, yeah, sure.

425
00:19:01,140 --> 00:19:01,900
I got half an hour.

426
00:19:01,900 --> 00:19:03,520
We can totally sit down and look at it.

427
00:19:03,520 --> 00:19:06,300
Three hours later, we were satisfied.

428
00:19:06,300 --> 00:19:08,820
And the process was very illuminating.

429
00:19:08,820 --> 00:19:11,260
So let's take a look at what that data looks like.

430
00:19:11,260 --> 00:19:13,180
The data looks something like this.

431
00:19:13,180 --> 00:19:14,860
This is a company that has a website.

432
00:19:14,860 --> 00:19:16,460
They're helping with booking of hotels.

433
00:19:16,460 --> 00:19:19,460
I won't say which company it is.

434
00:19:19,460 --> 00:19:21,620
There's basically things that they

435
00:19:21,620 --> 00:19:26,060
did to the website to try to improve customer engagement

436
00:19:26,060 --> 00:19:28,380
and clicking and revenue.

437
00:19:28,380 --> 00:19:31,860
So they have the control, which is don't change anything as is.

438
00:19:31,860 --> 00:19:33,460
Then they have a first intervention,

439
00:19:33,460 --> 00:19:38,580
which is let's sort all of our results by price by default.

440
00:19:38,580 --> 00:19:40,620
By default, we sort them by price.

441
00:19:40,620 --> 00:19:43,780
And the second intervention was completely independent.

442
00:19:43,780 --> 00:19:46,660
Let's just put images of the hotel rooms

443
00:19:46,660 --> 00:19:48,820
on the search results, like little thumbnails

444
00:19:48,820 --> 00:19:50,900
that you can see.

445
00:19:50,900 --> 00:19:52,980
And how do we measure customer engagement?

446
00:19:52,980 --> 00:19:54,340
Well, there's a way to do it.

447
00:19:54,340 --> 00:19:56,900
We will measure how many times they click

448
00:19:56,900 --> 00:19:59,220
before they stop clicking.

449
00:19:59,220 --> 00:20:01,860
And so basically, we want to know

450
00:20:01,860 --> 00:20:04,140
which intervention helped our customers find

451
00:20:04,140 --> 00:20:07,860
what they're looking for, and that's how we measured it.

452
00:20:07,860 --> 00:20:09,660
So what are you going to do with that data?

453
00:20:09,660 --> 00:20:10,940
Are you going to do the t-test?

454
00:20:10,940 --> 00:20:13,940
Are you really going to do the t-test?

455
00:20:13,940 --> 00:20:16,300
Does the t-test stand here?

456
00:20:16,300 --> 00:20:18,060
Firstly, I don't think so.

457
00:20:18,060 --> 00:20:20,300
Firstly, we have zero inflation in the data.

458
00:20:20,300 --> 00:20:21,500
Look at the data.

459
00:20:21,500 --> 00:20:25,220
There's this big bar on a log scale

460
00:20:25,220 --> 00:20:28,900
that says that there's a bunch of stuff with zero clicks.

461
00:20:28,900 --> 00:20:31,600
So the t-distribution completely does not hold.

462
00:20:31,600 --> 00:20:35,860
If you did the t-test, you are wrong.

463
00:20:35,860 --> 00:20:38,060
Because the t-distribution assumes

464
00:20:38,060 --> 00:20:40,380
that you can take values from negative infinity

465
00:20:40,380 --> 00:20:43,060
to positive infinity, and that you are continuous.

466
00:20:43,060 --> 00:20:44,260
What are these data?

467
00:20:44,260 --> 00:20:47,580
Zero to positive infinity and discrete.

468
00:20:47,580 --> 00:20:52,220
So again, if you did the t-test, you're wrong.

469
00:20:52,220 --> 00:20:54,620
We need a model that allows us to account for this thing

470
00:20:54,620 --> 00:20:56,580
that we call zero inflation, and we

471
00:20:56,580 --> 00:21:00,100
need a discrete distribution model for this data.

472
00:21:00,100 --> 00:21:04,620
All right, so three hours of modeling later,

473
00:21:04,620 --> 00:21:07,500
we arrive at the conclusion that the zero inflated negative

474
00:21:07,500 --> 00:21:11,140
binomial distribution, that long mouthful name,

475
00:21:11,140 --> 00:21:15,180
is actually the right thing to describe the data.

476
00:21:15,180 --> 00:21:16,740
And why is that the case?

477
00:21:16,740 --> 00:21:19,900
It's because the zero inflated, the ZinB model,

478
00:21:19,900 --> 00:21:22,780
the ZinB distribution is a distribution that

479
00:21:22,780 --> 00:21:25,180
lets us model two processes.

480
00:21:25,180 --> 00:21:28,320
The first process is the group of customers

481
00:21:28,320 --> 00:21:30,540
that have no intent to click whatsoever.

482
00:21:30,540 --> 00:21:32,580
They're just looking around.

483
00:21:32,580 --> 00:21:35,100
The second process is the process

484
00:21:35,100 --> 00:21:38,100
that describes the customers who are looking around.

485
00:21:38,100 --> 00:21:40,260
Sometimes they see the result and they make the purchase

486
00:21:40,260 --> 00:21:41,980
straight away, so zero clicks.

487
00:21:41,980 --> 00:21:44,180
And sometimes they need a first and a second and a third

488
00:21:44,180 --> 00:21:46,420
and a fourth, clicking around before they actually

489
00:21:46,420 --> 00:21:49,260
make that purchase.

490
00:21:49,260 --> 00:21:52,140
So the zero inflated negative binomial distribution

491
00:21:52,140 --> 00:21:54,700
turns out to be the distribution that models this process.

492
00:21:54,700 --> 00:21:57,980
Now, our friend here was not convinced.

493
00:21:57,980 --> 00:21:59,860
He said, we really got to try the T distribution

494
00:21:59,860 --> 00:22:00,940
and I let him try it.

495
00:22:00,940 --> 00:22:02,300
It didn't turn out well.

496
00:22:02,300 --> 00:22:06,180
But he needed to see it in order for him to be convinced

497
00:22:06,180 --> 00:22:07,180
that this was true.

498
00:22:08,480 --> 00:22:12,740
So in PMC3 code, it turns out, look,

499
00:22:12,740 --> 00:22:16,560
we have the zero inflated negative binomial distribution.

500
00:22:16,560 --> 00:22:19,300
A lot of probability distributions are first class citizens

501
00:22:19,780 --> 00:22:22,860
and this particular so-called mixture distribution

502
00:22:22,860 --> 00:22:23,940
is also one of them.

503
00:22:23,940 --> 00:22:27,340
And in fact, you can make a mixture distribution of your own.

504
00:22:27,340 --> 00:22:29,700
Moreover, here's what's really cool.

505
00:22:29,700 --> 00:22:31,780
Because of the data generating process,

506
00:22:31,780 --> 00:22:33,620
there are two very interpretable parameters

507
00:22:33,620 --> 00:22:34,820
that we're interested in.

508
00:22:34,820 --> 00:22:39,340
Firstly, we're interested in knowing whether the intervention changes

509
00:22:39,340 --> 00:22:44,100
the probability of a customer just searching around

510
00:22:44,100 --> 00:22:46,900
to actually being intended to click.

511
00:22:46,900 --> 00:22:48,740
Click around and make a purchase.

512
00:22:48,740 --> 00:22:50,580
And the second thing that we're interested in

513
00:22:50,580 --> 00:22:55,100
is whether the clicking process is affected

514
00:22:55,100 --> 00:22:57,900
or the probability of making a purchase is affected

515
00:22:57,900 --> 00:23:00,420
each time the customer clicks.

516
00:23:02,780 --> 00:23:04,060
And those are calculatable

517
00:23:04,060 --> 00:23:05,620
and I'd invite you to take a look at the code.

518
00:23:05,620 --> 00:23:07,420
I won't dwell too much on it.

519
00:23:07,420 --> 00:23:10,620
But that's modelable if you know just a little bit

520
00:23:10,620 --> 00:23:13,220
about the math behind the equation,

521
00:23:13,220 --> 00:23:15,620
the negative binomial distribution.

522
00:23:15,620 --> 00:23:18,260
So what did we find?

523
00:23:18,260 --> 00:23:21,860
We found that putting images of rooms

524
00:23:21,860 --> 00:23:25,300
actually increases the probability of your customers

525
00:23:25,300 --> 00:23:27,580
clicking on anything at all, right?

526
00:23:27,580 --> 00:23:31,060
So they're moving away from this mode

527
00:23:31,060 --> 00:23:33,100
where they're just looking around into this mode.

528
00:23:33,100 --> 00:23:34,340
We're capturing more customers

529
00:23:34,340 --> 00:23:38,180
that are actually actively seeking to purchase things.

530
00:23:38,180 --> 00:23:41,140
And that if you put an image of the room on,

531
00:23:41,140 --> 00:23:43,700
compared to sorting by price,

532
00:23:43,700 --> 00:23:44,660
you actually increase,

533
00:23:44,660 --> 00:23:47,300
you maintain the probability of purchasing an item.

534
00:23:47,300 --> 00:23:49,660
So it actually reveals something interesting

535
00:23:49,660 --> 00:23:51,340
about the customer behavior.

536
00:23:51,340 --> 00:23:53,660
These are not your price-sensitive people.

537
00:23:53,660 --> 00:23:55,460
These are your value-sensitive people,

538
00:23:55,460 --> 00:23:57,180
the people who want value for money

539
00:23:57,180 --> 00:23:59,220
more than the best price, right?

540
00:23:59,220 --> 00:24:02,700
And so we would not have been able to do this

541
00:24:02,700 --> 00:24:05,300
if we just stuck with the t-test

542
00:24:05,300 --> 00:24:08,140
and blindly assumed all of the things

543
00:24:08,140 --> 00:24:09,780
that the t-test assumed.

544
00:24:09,780 --> 00:24:13,380
Instead, because we sat down and we did the hard thing,

545
00:24:13,380 --> 00:24:15,060
but the right thing,

546
00:24:15,060 --> 00:24:17,860
and carefully modeled the data-generating process

547
00:24:17,860 --> 00:24:19,180
and did our checks to make sure

548
00:24:19,180 --> 00:24:21,180
that it was modeling it correctly,

549
00:24:21,180 --> 00:24:25,260
we're able to arrive at this new insight onto his data.

550
00:24:27,220 --> 00:24:28,820
And this insight on his data

551
00:24:28,820 --> 00:24:31,580
was illuminating for the business and valuable.

552
00:24:31,580 --> 00:24:33,740
And you can actually tag a loss function on top of that

553
00:24:33,740 --> 00:24:36,820
and calculate how much this is tied to dollar amounts

554
00:24:36,820 --> 00:24:39,140
and make that interpretable for executives.

555
00:24:39,140 --> 00:24:43,620
You couldn't do that with the p-value, could you, right?

556
00:24:44,500 --> 00:24:45,980
Okay, so the key takeaways.

557
00:24:45,980 --> 00:24:48,340
Firstly, from this example,

558
00:24:48,340 --> 00:24:51,340
three or more groups are possible, totally possible.

559
00:24:51,340 --> 00:24:55,060
There's no reason again to limit yourself to two groups.

560
00:24:55,060 --> 00:24:59,580
Secondly, all we're doing is comparisons.

561
00:24:59,580 --> 00:25:00,980
And who said that t-tests

562
00:25:00,980 --> 00:25:03,660
were the only way to do comparisons?

563
00:25:03,660 --> 00:25:06,300
As long as you're comparing the consistent quantity,

564
00:25:07,180 --> 00:25:08,060
that works fine.

565
00:25:09,460 --> 00:25:12,460
Finally, this was a neat insight for me,

566
00:25:12,460 --> 00:25:15,900
sitting down, carefully constructing the white box model

567
00:25:15,900 --> 00:25:18,780
lets us tease apart the interpretable things

568
00:25:18,780 --> 00:25:20,820
that are important to the business.

569
00:25:20,820 --> 00:25:22,580
And me coming in as like an outsider

570
00:25:22,580 --> 00:25:23,580
and not even knowing any of this,

571
00:25:23,580 --> 00:25:25,180
this for me was an epiphany.

572
00:25:25,180 --> 00:25:26,380
Now this might be a bit more obvious

573
00:25:26,380 --> 00:25:27,500
to the guy who I was working with,

574
00:25:27,500 --> 00:25:30,180
but for me, wow, this is powerful.

575
00:25:30,180 --> 00:25:32,060
This is really powerful stuff.

576
00:25:32,060 --> 00:25:33,660
All right, so back to my three messages.

577
00:25:33,660 --> 00:25:35,180
I hope it's clear.

578
00:25:35,180 --> 00:25:38,180
From this talk, you can totally do your analysis

579
00:25:38,180 --> 00:25:40,420
without needing to be stuck with two groups

580
00:25:40,420 --> 00:25:42,740
and normal distribution assumptions.

581
00:25:42,740 --> 00:25:46,460
Secondly, PiMC3 is a great flexible language

582
00:25:46,460 --> 00:25:48,060
for probabilistic modeling.

583
00:25:48,060 --> 00:25:50,300
There's a wealth of tops that are out there

584
00:25:50,300 --> 00:25:53,380
and tutorials even on how do you use PiMC3

585
00:25:53,380 --> 00:25:55,140
to do your probabilistic modeling.

586
00:25:55,140 --> 00:25:57,180
I encourage you to check those out.

587
00:25:57,180 --> 00:25:59,620
And finally, when we go base,

588
00:25:59,620 --> 00:26:03,420
we can move away from canned statistical procedures

589
00:26:03,420 --> 00:26:06,940
and instead now do statistics in a principled fashion.

590
00:26:07,980 --> 00:26:09,420
The bonus message here

591
00:26:09,420 --> 00:26:12,860
is you can better interpret your data with Bayesian methods.

592
00:26:14,020 --> 00:26:15,220
Thank you.

593
00:26:15,220 --> 00:26:16,100
.

594
00:26:16,100 --> 00:26:16,940
.

595
00:26:16,940 --> 00:26:17,780
.

596
00:26:17,780 --> 00:26:18,620
.

597
00:26:18,620 --> 00:26:19,460
.

598
00:26:19,460 --> 00:26:20,300
.

599
00:26:20,300 --> 00:26:21,140
.

600
00:26:21,140 --> 00:26:21,980
.

601
00:26:21,980 --> 00:26:22,820
.

602
00:26:22,820 --> 00:26:23,660
.

603
00:26:23,660 --> 00:26:24,500
.

604
00:26:24,500 --> 00:26:25,340
.

605
00:26:25,340 --> 00:26:26,340
.

606
00:26:26,340 --> 00:26:27,180
.

607
00:26:27,180 --> 00:26:28,020
.

608
00:26:28,020 --> 00:26:28,860
.

609
00:26:28,860 --> 00:26:29,700
.

610
00:26:29,700 --> 00:26:30,700
.

611
00:26:30,700 --> 00:26:31,700
.

612
00:26:31,700 --> 00:26:32,700
.

613
00:26:32,700 --> 00:26:33,700
.

614
00:26:33,700 --> 00:26:34,700
.

615
00:26:34,700 --> 00:26:35,700
.

616
00:26:35,700 --> 00:26:36,700
.

617
00:26:36,700 --> 00:26:37,700
.

618
00:26:37,700 --> 00:26:38,700
.

619
00:26:38,740 --> 00:26:39,740
.

620
00:26:39,740 --> 00:26:41,500
.

621
00:26:41,500 --> 00:26:42,500
..

622
00:26:42,500 --> 00:26:43,500
.

623
00:26:43,500 --> 00:26:44,500
.

624
00:26:44,500 --> 00:26:45,500
.

625
00:26:45,500 --> 00:26:46,500
.

626
00:26:46,500 --> 00:26:47,500
.

627
00:26:47,500 --> 00:26:48,500
.

628
00:26:48,540 --> 00:26:49,540
.

629
00:26:49,540 --> 00:26:50,540
.

630
00:26:50,540 --> 00:26:51,540
.

631
00:26:51,540 --> 00:26:52,540
.

632
00:26:52,540 --> 00:26:53,540
.

633
00:26:53,540 --> 00:26:54,540
.

634
00:26:54,540 --> 00:26:55,540
.

635
00:26:55,540 --> 00:26:56,540
.

636
00:26:56,540 --> 00:26:57,540
.

637
00:26:57,540 --> 00:26:58,540
.

638
00:26:58,540 --> 00:26:59,540
.

639
00:26:59,540 --> 00:27:00,540
.

640
00:27:00,540 --> 00:27:01,540
.

641
00:27:01,540 --> 00:27:02,540
.

642
00:27:02,540 --> 00:27:03,540
.

643
00:27:03,540 --> 00:27:04,540
.

644
00:27:04,540 --> 00:27:05,540
.

645
00:27:05,540 --> 00:27:06,540
.

646
00:27:06,540 --> 00:27:07,540
.

647
00:27:07,540 --> 00:27:14,200
So the first way of picking distributions is to make sure that you pick a distribution that has the correct support

648
00:27:14,200 --> 00:27:16,200
That is the values for which it is valid

649
00:27:16,280 --> 00:27:19,180
So if you got something that can never take on negative values

650
00:27:19,780 --> 00:27:22,000
If you got data that can never take on negative values

651
00:27:22,000 --> 00:27:27,520
Don't pick a distribution for which you can the distribution actually can take on values negative values

652
00:27:27,720 --> 00:27:32,260
The second way to do this is to know the probability distribution stories

653
00:27:32,260 --> 00:27:36,120
And so there's this instructor at Caltech who have met at the sci-pi conferences

654
00:27:36,120 --> 00:27:42,480
His name is Justin Boyce Bo is and he wrote this really great resource online about

655
00:27:43,060 --> 00:27:45,480
probability distributions and their stories

656
00:27:45,780 --> 00:27:52,680
So he tells us what the story is for the binomial distribution the negative binomial distribution the exponential distribution

657
00:27:53,200 --> 00:27:56,280
the normal distribution etc etc etc and

658
00:27:56,740 --> 00:28:04,200
Once you know the distribution stories it becomes really easy to pick out the appropriate distribution for the problem at hand

659
00:28:04,500 --> 00:28:09,440
Now finally there is some model checking that's involved and I've glossed over that over here

660
00:28:09,640 --> 00:28:15,240
You got to do that model checking to make sure that the model is correct and sometimes it's not a single distribution

661
00:28:15,240 --> 00:28:19,200
It's a mixture distribution. We got to do the right checks to make sure that's correct

662
00:28:19,920 --> 00:28:21,920
next question

663
00:28:22,000 --> 00:28:26,260
Quick disclaimer, I'm not trying to start a frequentist Bayesian war but

664
00:28:27,080 --> 00:28:31,680
My question is if if you're doing multiple hypotheses at once

665
00:28:32,400 --> 00:28:35,400
Isn't some sort of correction still needed because

666
00:28:36,120 --> 00:28:42,440
Just by chance you'd expect that you know one of your hundred or two hundred or whatever different high

667
00:28:43,280 --> 00:28:49,920
Categories just happens to be an outlier in what sample you assign to it. And so it will

668
00:28:52,480 --> 00:28:59,100
It will perform it will appear to perform better than the control and you might spend a lot of work trying to implement that

669
00:28:59,140 --> 00:29:05,160
When really it is the same right it speaks to the point that I had made earlier on that we might be wrong

670
00:29:05,500 --> 00:29:11,460
About our inferences because the data as you mentioned just came by random chance from a non random sample

671
00:29:12,260 --> 00:29:18,420
Non-representative sample so I actually had that question when I first made my first notebook with three comparisons

672
00:29:18,580 --> 00:29:21,860
and I went online and asked Alan Downey professor of computer science and

673
00:29:22,220 --> 00:29:24,820
statistics and data science at the Olin College of Engineering and

674
00:29:25,300 --> 00:29:28,100
I asked him do we need multiple hypothesis correction?

675
00:29:28,740 --> 00:29:30,740
and his answer was no because

676
00:29:31,420 --> 00:29:37,780
And he didn't give me the rigorous answer and I'm unfortunately not able to give you the rigorous answer either because I don't have the necessary

677
00:29:37,900 --> 00:29:42,220
Requisite training. I'm I'm a statistician midway through grad school

678
00:29:42,500 --> 00:29:50,540
But but the the answer there the reasoning there was because we've summarized everything that we need to know having seen the data

679
00:29:52,060 --> 00:29:55,900
In the posterior distribution and so the posterior distribution

680
00:29:56,420 --> 00:30:04,860
Tells us everything and there's no hypotheses that we've constructed that we're explicitly testing all we're doing is looking and comparing

681
00:30:05,500 --> 00:30:07,500
What the what the data tell us?

682
00:30:07,700 --> 00:30:12,780
Conditioned on our priors conditioned on the data and I'm happy to talk about it more during the open space

683
00:30:13,180 --> 00:30:18,420
So there will be a I have Bayesian open space or Bayesian office hours as an open space

684
00:30:18,820 --> 00:30:23,280
Later in the day and I'm happy to talk with more of you if you'd like to talk about it

685
00:30:23,280 --> 00:30:25,280
And please don't bring your data to me

686
00:30:25,900 --> 00:30:28,100
Thank you

