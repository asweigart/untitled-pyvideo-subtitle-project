1
00:00:00,000 --> 00:00:03,480
Hello, everyone.

2
00:00:03,480 --> 00:00:07,920
Please join me in welcoming William Horton for Quda and your Python, effective parallel

3
00:00:07,920 --> 00:00:10,920
programming on the GPU.

4
00:00:10,920 --> 00:00:16,260
Hi, everyone.

5
00:00:16,260 --> 00:00:18,000
This is Quda and your Python.

6
00:00:18,000 --> 00:00:19,000
My name is William.

7
00:00:19,000 --> 00:00:20,760
I'm really excited to be here at PyCon.

8
00:00:20,760 --> 00:00:22,380
This is my first PyCon ever, actually.

9
00:00:22,380 --> 00:00:24,600
How many people is this your first PyCon?

10
00:00:24,600 --> 00:00:25,600
Wow.

11
00:00:25,600 --> 00:00:26,600
That's a lot of hands.

12
00:00:26,600 --> 00:00:27,600
Yeah.

13
00:00:27,600 --> 00:00:28,600
Everyone enjoying it so far?

14
00:00:28,600 --> 00:00:29,600
Sweet.

15
00:00:29,600 --> 00:00:30,600
Thank you.

16
00:00:30,600 --> 00:00:36,440
So today, what I'm going to be talking about is basically various ways that we as Python

17
00:00:36,440 --> 00:00:42,920
developers can start to use the processing power of the GPU from our Python programs.

18
00:00:42,920 --> 00:00:46,960
And unfortunately, I have to start out with some bad news.

19
00:00:46,960 --> 00:00:49,280
Moore's law is dead.

20
00:00:49,280 --> 00:00:50,840
Sad face.

21
00:00:50,840 --> 00:00:56,960
Basically, Moore's law was the idea formulated by this guy, Gordon Moore, that the number

22
00:00:56,960 --> 00:01:01,400
of transistors in an integrated circuit would double every two years.

23
00:01:01,400 --> 00:01:05,360
And originally, he formulated it as an even shorter period, and he kind of adjusted the

24
00:01:05,360 --> 00:01:06,360
law.

25
00:01:06,360 --> 00:01:12,560
But this is something that has kind of been behind a lot of the advances in hardware.

26
00:01:12,560 --> 00:01:14,040
And he originally based it off of this data.

27
00:01:14,040 --> 00:01:18,920
So you can see this covers the period from the 50s, 60s, 70s.

28
00:01:18,920 --> 00:01:21,400
And on the left side, that's a log scale.

29
00:01:21,400 --> 00:01:26,040
So the linear nature of the graph means that it's growing at an exponential rate.

30
00:01:26,120 --> 00:01:28,160
And it actually continued to grow.

31
00:01:28,160 --> 00:01:31,680
This graph goes basically all the way up until 2016.

32
00:01:31,680 --> 00:01:39,200
And you can see that basically that same kind of trend of doubling every two years continued

33
00:01:39,200 --> 00:01:40,920
to hold.

34
00:01:40,920 --> 00:01:43,240
And then something happened.

35
00:01:43,240 --> 00:01:44,240
Physics happened.

36
00:01:44,240 --> 00:01:46,240
And basically, once you get...

37
00:01:46,240 --> 00:01:50,240
So this is actually a representation of the last time I took a physics class, which is

38
00:01:50,240 --> 00:01:52,000
why I threw it up there.

39
00:01:52,000 --> 00:01:58,040
So I'll give you a really high level version of why Moore's Law has started to fail.

40
00:01:58,040 --> 00:02:03,000
Basically, as we get down to smaller levels, I mean, the things now are on the nanometer

41
00:02:03,000 --> 00:02:04,160
scale.

42
00:02:04,160 --> 00:02:06,560
There's problems with fitting the transistors on the chips.

43
00:02:06,560 --> 00:02:11,720
There's problem with basically the power coming off of it and dissipation of the heat.

44
00:02:11,720 --> 00:02:16,040
So there's a lot of limitations that are starting to challenge this trend.

45
00:02:16,040 --> 00:02:19,120
A lot of people writing about it.

46
00:02:19,240 --> 00:02:25,240
People like to talk about the death of Moore's Law.

47
00:02:25,240 --> 00:02:30,160
So the one authority I look to is this person who said, I guess I see Moore's Law dying

48
00:02:30,160 --> 00:02:33,760
here in the next decade or so, but that's not surprising.

49
00:02:33,760 --> 00:02:37,640
And the reason why I trust this person so much is because this is Gordon Moore, and

50
00:02:37,640 --> 00:02:40,040
he said that in 2015.

51
00:02:40,040 --> 00:02:45,520
So I think we can be relatively certain that there's something going on here.

52
00:02:45,920 --> 00:02:49,160
So the question is then, why GPUs?

53
00:02:49,160 --> 00:02:55,160
How can this new kind of device help us break through some of the challenges we're seeing

54
00:02:55,160 --> 00:02:57,640
in hardware manufacturing?

55
00:02:57,640 --> 00:02:59,440
So a little bit about the history of the GPU.

56
00:02:59,440 --> 00:03:04,080
They were originally manufactured for gaming purposes, for graphics rendering.

57
00:03:04,080 --> 00:03:08,720
And because of that, they were designed to be really good at vector and matrix operations.

58
00:03:08,720 --> 00:03:14,800
Basically, operating on sets of pixels or on objects in a certain scene, you can imagine

59
00:03:14,880 --> 00:03:21,400
that that is requiring large matrices and then transformations on those.

60
00:03:21,400 --> 00:03:26,000
And so that was the original purpose of this device.

61
00:03:26,000 --> 00:03:31,120
So we can start to understand a little bit more about the differences between GPU and

62
00:03:31,120 --> 00:03:32,360
CPU by taking a look.

63
00:03:32,360 --> 00:03:39,640
These are two pretty recent high-end models of an NVIDIA GPU versus an Intel CPU.

64
00:03:39,640 --> 00:03:41,200
And we can take a look at the specs.

65
00:03:41,600 --> 00:03:46,880
And we can see that the GPU, on the one hand, has many more cores, many more processing

66
00:03:46,880 --> 00:03:47,880
units.

67
00:03:47,880 --> 00:03:50,440
It has over 4,000 CUDA cores.

68
00:03:50,440 --> 00:03:55,200
The Intel CPU, eight cores or up to 16 with hyper threads.

69
00:03:55,200 --> 00:03:59,880
So you can see there's a huge disparity there in the number of processing units.

70
00:03:59,880 --> 00:04:04,600
While on the other hand, if you look at the base clock and boost clock rates, you can

71
00:04:04,600 --> 00:04:09,720
see in terms of clock speed that the Intel CPU is achieving roughly three times faster

72
00:04:10,600 --> 00:04:11,400
processing.

73
00:04:11,400 --> 00:04:15,640
So there you start to understand some of the differences between the GPU on the one hand,

74
00:04:15,640 --> 00:04:22,160
more processing but slower, versus the CPU, which has fewer processing units but can do

75
00:04:22,160 --> 00:04:24,000
things faster.

76
00:04:24,000 --> 00:04:29,560
And this is a diagram from the NVIDIA documentation that basically gets to the heart of the difference

77
00:04:29,560 --> 00:04:33,220
between CPU versus GPU on an architecture level.

78
00:04:33,220 --> 00:04:39,900
And basically, you can see on the CPU side, there's large portions of the chip that are

79
00:04:39,900 --> 00:04:41,660
dedicated to control.

80
00:04:41,660 --> 00:04:46,140
So that's your if statements, your while statements, things like that.

81
00:04:46,140 --> 00:04:52,540
Whereas on the GPU, there's not so many pieces being devoted to that.

82
00:04:52,540 --> 00:04:57,180
You can also see the CPU has a much larger cache, whereas on the GPU, again, that's relatively

83
00:04:57,180 --> 00:04:58,420
small.

84
00:04:58,700 --> 00:05:03,300
The green bits that are dominating the GPU are arithmetic units.

85
00:05:03,300 --> 00:05:10,740
So basically, the GPU is packed with these smaller units that are built to do simple

86
00:05:10,740 --> 00:05:12,480
arithmetic operations.

87
00:05:12,480 --> 00:05:18,660
And so on a hardware level, this is the difference between CPU and GPU.

88
00:05:18,660 --> 00:05:24,940
And what people have started to realize is that the GPU isn't just useful for gaming.

89
00:05:24,940 --> 00:05:30,260
So there's the rise of this paradigm that is called GPGPU, the idea that we can do general

90
00:05:30,260 --> 00:05:33,540
purpose workloads on a GPU.

91
00:05:33,540 --> 00:05:36,540
And this is from a paper published by NVIDIA.

92
00:05:36,540 --> 00:05:42,460
But they're saying the GPU could be just a truly parallel processor.

93
00:05:42,460 --> 00:05:45,840
And different companies started coming up with different models to be able to do this

94
00:05:45,840 --> 00:05:48,440
kind of programming on their GPU.

95
00:05:48,440 --> 00:05:51,660
So NVIDIA did CUDA, which we're going to talk about today.

96
00:05:51,660 --> 00:05:53,380
But there are other standards.

97
00:05:53,740 --> 00:05:55,260
AMD has their own.

98
00:05:55,260 --> 00:05:59,340
And OpenCL is a developing open standard.

99
00:05:59,340 --> 00:06:04,220
But the main idea is people starting to realize that we've built these devices for a certain

100
00:06:04,220 --> 00:06:05,900
kind of workload.

101
00:06:05,900 --> 00:06:10,620
And that might be useful across many different domains.

102
00:06:10,620 --> 00:06:13,740
So why am I interested in this?

103
00:06:13,740 --> 00:06:16,620
Part of it has to do with my day job, which I'll tell you about.

104
00:06:16,620 --> 00:06:21,500
I work as a software engineer at Compass, which is a real estate technology company.

105
00:06:21,500 --> 00:06:25,980
We're putting tools in the hands of agents to make them more productive.

106
00:06:25,980 --> 00:06:29,360
And there I work on data pipelines and data ingestion.

107
00:06:29,360 --> 00:06:35,500
So we use some pretty common Python tools, including PySpark, Kafka, Airflow.

108
00:06:35,500 --> 00:06:40,700
And I will mention that we're hiring in NYC and Seattle.

109
00:06:40,700 --> 00:06:44,460
And I like to keep my ear to the ground in terms of things and new technologies that

110
00:06:44,460 --> 00:06:47,420
are happening kind of in this domain.

111
00:06:47,420 --> 00:06:53,620
And so one thing that's been proposed and is being worked on by Rapids is the idea of

112
00:06:53,620 --> 00:06:55,380
GPUs for data pipelines.

113
00:06:55,380 --> 00:07:00,180
So the idea that you might already be training your model on the GPU.

114
00:07:00,180 --> 00:07:01,820
And so why not do some other steps?

115
00:07:01,820 --> 00:07:03,820
Why not do data pre-processing?

116
00:07:03,820 --> 00:07:05,580
Why not do feature engineering?

117
00:07:05,580 --> 00:07:06,940
Maybe even do your visualization.

118
00:07:06,940 --> 00:07:11,540
So this is one thing that's kind of coming down the pipeline in terms of using the GPU

119
00:07:11,540 --> 00:07:12,860
for even more.

120
00:07:12,860 --> 00:07:17,420
This is another pretty recent article from this year where Uber actually used GPUs to

121
00:07:17,420 --> 00:07:19,860
come up with an analytics database.

122
00:07:19,860 --> 00:07:25,260
So these are just examples of ways in which the GPU is starting to be used in this space.

123
00:07:25,260 --> 00:07:30,340
But the talk also has something to do with my hobbies, which include deep learning.

124
00:07:30,340 --> 00:07:32,700
So I got into it through the Fast AI program.

125
00:07:32,700 --> 00:07:37,900
I've done probably way too many Kaggle competitions, mostly using PyTorch.

126
00:07:37,900 --> 00:07:43,540
And you can see here, this is a picture of my home rig that I lovingly assembled.

127
00:07:43,540 --> 00:07:46,940
I've got two Nvidia GPUs in there.

128
00:07:46,940 --> 00:07:53,500
Basically I had to do that because one weekend I left my GPU instance running on AWS, forgot

129
00:07:53,500 --> 00:07:54,500
to turn it off.

130
00:07:54,500 --> 00:07:57,500
Monday I woke up to a really, really big bill.

131
00:07:57,500 --> 00:07:58,820
And that's not good.

132
00:07:58,820 --> 00:08:01,260
So I decided to build something at home.

133
00:08:01,260 --> 00:08:03,820
I actually came up with a little bit of a law.

134
00:08:03,820 --> 00:08:05,900
I've kind of repurposed the graph from before.

135
00:08:06,500 --> 00:08:10,300
But I'm calling this one Horton's law, which is basically the doubling every month of your

136
00:08:10,300 --> 00:08:14,300
AWS bill with your interest in deep learning.

137
00:08:14,300 --> 00:08:16,140
So this is just something to keep in mind.

138
00:08:16,140 --> 00:08:28,740
So another thing is last year I made a pull request to PyTorch, basically moving a function

139
00:08:28,740 --> 00:08:33,940
that had been in the purely Python side of the library into the C++ side.

140
00:08:33,980 --> 00:08:38,500
So this also got me thinking about these kinds of problems.

141
00:08:38,500 --> 00:08:42,140
If you ever look into the internals there, they're doing some really interesting things

142
00:08:42,140 --> 00:08:48,020
in terms of how they bring together the Python API with the C++ extensions, as well as access

143
00:08:48,020 --> 00:08:49,020
and CUDA.

144
00:08:49,020 --> 00:08:53,580
So this is another thing that inspired this talk and got me thinking about these kinds

145
00:08:53,580 --> 00:08:55,180
of problems.

146
00:08:55,180 --> 00:08:58,580
So now we get to the question that you probably all came here to answer.

147
00:08:58,580 --> 00:09:02,300
How can you start programming the GPU?

148
00:09:02,300 --> 00:09:08,260
So to illustrate how you might do it, I first wanted to give you a CPU example.

149
00:09:08,260 --> 00:09:09,820
So this is from NumPy.

150
00:09:09,820 --> 00:09:16,260
We're basically creating two vectors and adding them together, so two pretty large vectors.

151
00:09:16,260 --> 00:09:17,940
And so this is a NumPy example.

152
00:09:17,940 --> 00:09:23,020
I'm going to show you the example of how to do this exact same operation on the GPU.

153
00:09:23,020 --> 00:09:24,860
There it is.

154
00:09:24,860 --> 00:09:28,020
In case you missed it, okay, I'll do it one more time.

155
00:09:28,020 --> 00:09:29,020
Here's NumPy.

156
00:09:29,020 --> 00:09:32,020
Okay, CPU, GPU.

157
00:09:32,700 --> 00:09:36,940
There's the differences highlighted.

158
00:09:36,940 --> 00:09:37,940
That's it.

159
00:09:37,940 --> 00:09:43,500
Thank you.

160
00:09:43,500 --> 00:09:48,220
So what we're doing here, also you can see I benchmarked this, and just that drop in

161
00:09:48,220 --> 00:09:55,100
replacement, you can get up to a 30 times speed up using CUDA and using the GPU.

162
00:09:55,100 --> 00:09:59,820
So this is a really powerful approach.

163
00:09:59,820 --> 00:10:03,180
So to give you a little bit of overview of what we're going through, what I just demonstrated

164
00:10:03,180 --> 00:10:06,220
was a drop in replacement, so I'm going to talk about that a little bit.

165
00:10:06,220 --> 00:10:10,620
And that's kind of the simplest way, is there are libraries out there that will let you

166
00:10:10,620 --> 00:10:17,780
plug in to your existing code with very minimal changes and start to access this processing.

167
00:10:17,780 --> 00:10:20,260
The second one is a little bit more complex.

168
00:10:20,260 --> 00:10:23,480
It's basically compiling CUDA strings within your Python program.

169
00:10:23,520 --> 00:10:30,480
And then the last one is basically building your CUDA kernels as a C or C++ extension

170
00:10:30,480 --> 00:10:32,600
to your Python programs.

171
00:10:32,600 --> 00:10:37,360
And basically, as with a lot of things in software engineering, these are kind of in

172
00:10:37,360 --> 00:10:43,040
increasing complexity, but at the same time, as you go through these steps, they'll give

173
00:10:43,040 --> 00:10:48,240
you greater control over your programs and maybe access to certain features of CUDA that

174
00:10:48,240 --> 00:10:51,360
you couldn't have with just a drop in.

175
00:10:51,360 --> 00:10:56,280
So to talk about the drop in, the library that I just showed you is KooPy.

176
00:10:56,280 --> 00:10:59,320
It's basically designed as a drop in for NumPy.

177
00:10:59,320 --> 00:11:02,760
It was developed for a deep learning framework called Chainer, and it supports a lot of the

178
00:11:02,760 --> 00:11:07,840
features that you might know and love about NumPy, including indexing, different data

179
00:11:07,840 --> 00:11:10,400
types, as well as broadcasting.

180
00:11:10,400 --> 00:11:15,360
And they've done a really a lot of work to make this work really smoothly, just like

181
00:11:15,360 --> 00:11:16,600
NumPy.

182
00:11:16,600 --> 00:11:20,920
But there are a couple API differences that I wanted to shout out, just in case you want

183
00:11:20,960 --> 00:11:22,640
to drop this into your programs.

184
00:11:22,640 --> 00:11:27,720
So one, there are certain data types that you can't use, which are strings and objects,

185
00:11:27,720 --> 00:11:31,560
just because the GPU would have a hard time processing those.

186
00:11:31,560 --> 00:11:37,920
The other thing is, if you do np.array, you can do that around a Python list, and that

187
00:11:37,920 --> 00:11:39,160
will work.

188
00:11:39,160 --> 00:11:44,280
But in this case, it won't work.

189
00:11:44,280 --> 00:11:50,000
And then the last thing is, basically, NumPy.sum is going to return you a scalar, so just one

190
00:11:50,040 --> 00:11:56,640
number, and KooPy is going to return you, it's going to be an array.

191
00:11:56,640 --> 00:12:00,040
So you just have to watch out for those gotchas.

192
00:12:00,040 --> 00:12:04,960
But otherwise, it should be pretty smooth to plug this in and start getting speed ups.

193
00:12:04,960 --> 00:12:09,960
So I just wanted to mention a couple other drop in libraries, and this is from Rapids,

194
00:12:09,960 --> 00:12:11,880
which I mentioned earlier before.

195
00:12:11,880 --> 00:12:15,160
There's KooDF, which is a drop in for Pandas data frames.

196
00:12:15,160 --> 00:12:19,760
There's KooML they're working on, which is aspiring to be a Kuda-powered scikit learn.

197
00:12:19,760 --> 00:12:23,440
I know they've already got some algorithms implemented, so definitely check that out

198
00:12:23,440 --> 00:12:28,280
if you're interested in bringing this into your workflow.

199
00:12:28,280 --> 00:12:34,000
So the next thing we're going to talk about is compiling Kuda strings in your Python program.

200
00:12:34,000 --> 00:12:37,660
And so when we get into this, we have to start talking about what is the Kuda API?

201
00:12:37,660 --> 00:12:40,000
What does a Kuda program actually look like?

202
00:12:40,000 --> 00:12:42,180
We're moving beyond drop ins.

203
00:12:42,180 --> 00:12:45,160
And so this is the basic API of Kuda.

204
00:12:45,160 --> 00:12:49,400
A program is organized into threads, blocks, and grids.

205
00:12:49,400 --> 00:12:53,760
And so at the bottom there, you've got a bunch of threads in a block.

206
00:12:53,760 --> 00:12:55,920
The top is a bunch of blocks in a grid.

207
00:12:55,920 --> 00:12:59,120
And we're going to talk a little bit about basically how this is organized and also why

208
00:12:59,120 --> 00:13:03,560
it's important that things are organized in this way.

209
00:13:03,560 --> 00:13:06,140
So what is a thread in this paradigm?

210
00:13:06,140 --> 00:13:09,500
The thread is the thing that's actually executing your Kuda code.

211
00:13:09,500 --> 00:13:14,160
And it also has a thread index that goes basically up to three dimensions.

212
00:13:14,160 --> 00:13:18,520
And the thread index is important because it's used to specify which part of the data

213
00:13:18,520 --> 00:13:20,140
you're doing work on.

214
00:13:20,140 --> 00:13:24,800
And so the question might be, why can you index it in three dimensions?

215
00:13:24,800 --> 00:13:29,160
And this is a really useful thing in terms of understanding this programming paradigm

216
00:13:29,160 --> 00:13:31,740
and how you can use it most effectively.

217
00:13:31,740 --> 00:13:36,920
So GPUs are best at doing data parallel operations.

218
00:13:36,920 --> 00:13:42,160
So basically, if you have a bunch of data, let's say you have like a million elements

219
00:13:42,160 --> 00:13:44,840
in a vector, and you want to do the same thing.

220
00:13:44,840 --> 00:13:46,300
Let's say you want to double them.

221
00:13:46,300 --> 00:13:48,180
You want to double each of those numbers.

222
00:13:48,180 --> 00:13:52,880
You can just split those up, assign different ones to different processors, and then have

223
00:13:52,880 --> 00:13:53,880
them do it.

224
00:13:53,880 --> 00:13:59,560
And that's completely parallelizable and basically only limited then by the number of processors

225
00:13:59,560 --> 00:14:00,560
that you have.

226
00:14:00,560 --> 00:14:05,960
And so a lot of operations you do on vectors or matrices can be done in this way.

227
00:14:05,960 --> 00:14:07,380
So this is one example.

228
00:14:07,380 --> 00:14:09,820
Let's say you want to perform an operation on this.

229
00:14:09,820 --> 00:14:10,900
There's nine elements.

230
00:14:10,900 --> 00:14:12,500
You have four threads.

231
00:14:12,500 --> 00:14:15,460
So you start to figure out, well, which one is the thread going to operate?

232
00:14:15,460 --> 00:14:19,280
And this is one way that you can assign it.

233
00:14:19,280 --> 00:14:23,800
To put it more clearly, basically the threads are working on different indices.

234
00:14:23,800 --> 00:14:26,660
And so thread 0 ends up being 0, 4, and 8.

235
00:14:26,660 --> 00:14:28,640
Let's say thread 1, 1, and 5.

236
00:14:28,640 --> 00:14:29,640
You can see.

237
00:14:29,640 --> 00:14:34,080
And the nice thing about this system is that you can code a rule for that, which is each

238
00:14:34,080 --> 00:14:40,120
thread up until you get to the end of the vector is going to work on one that is its

239
00:14:40,120 --> 00:14:43,760
own index plus the number of threads times i.

240
00:14:43,820 --> 00:14:47,460
And you would just go through i until you got to the end.

241
00:14:47,460 --> 00:14:52,220
And so this is a nice way of organizing these kinds of operations.

242
00:14:52,220 --> 00:14:56,940
But then the question becomes, what do you do if it's two-dimensional?

243
00:14:56,940 --> 00:15:01,760
It becomes a lot trickier to start to assign indices to each thread.

244
00:15:01,760 --> 00:15:03,720
You could use modulo.

245
00:15:03,720 --> 00:15:07,740
And then you start getting all these kinds of weird calculations to just figure out what

246
00:15:07,740 --> 00:15:09,380
index you might use.

247
00:15:09,680 --> 00:15:16,160
A 3D example would get even weirder if you have a single index for your threads, but

248
00:15:16,160 --> 00:15:18,480
then your data is in 3D.

249
00:15:18,480 --> 00:15:20,140
That starts getting very tricky.

250
00:15:20,140 --> 00:15:25,880
And so what CUDA lets you do is start looking at your threads in multiple dimensions as

251
00:15:25,880 --> 00:15:26,880
well.

252
00:15:26,880 --> 00:15:32,200
So if you have 2D data, why not also assign your threads in a two-dimensional way?

253
00:15:32,200 --> 00:15:34,700
So your threads also have two indexes.

254
00:15:34,700 --> 00:15:39,160
And that makes it a lot easier to map from a thread to an element.

255
00:15:39,180 --> 00:15:43,100
And basically, this is what this piece of the CUDA API is.

256
00:15:43,100 --> 00:15:45,580
So your threads are in a block.

257
00:15:45,580 --> 00:15:48,500
And you can index them in 1D.

258
00:15:48,500 --> 00:15:49,920
In this case, it's 2D.

259
00:15:49,920 --> 00:15:55,380
You can index them in up to 3D to be able to map your processing to your data.

260
00:15:55,380 --> 00:15:59,380
And that makes your programs a lot simpler and a lot clearer.

261
00:15:59,380 --> 00:16:02,700
So then we've talked kind of about threads, how those are organized.

262
00:16:02,700 --> 00:16:04,920
So then we also have blocks and grids.

263
00:16:04,920 --> 00:16:06,820
Blocks are organizations of groups of threads.

264
00:16:06,840 --> 00:16:08,920
So this is one block.

265
00:16:08,920 --> 00:16:13,500
Basically, your blocks have to execute independently.

266
00:16:13,500 --> 00:16:17,020
So within a block, your threads are going to be executing together.

267
00:16:17,020 --> 00:16:20,820
But then your blocks have to execute independently.

268
00:16:20,820 --> 00:16:22,600
You should be able to do them in any order.

269
00:16:22,600 --> 00:16:27,440
And the other thing is the threads within the blocks can share data between each other.

270
00:16:27,440 --> 00:16:30,960
But then across blocks, there is no shared memory.

271
00:16:30,960 --> 00:16:34,800
And so your blocks can also be indexed in up to three dimensions.

272
00:16:34,820 --> 00:16:39,580
And they also have a block dimension, which is also used in calculating these indexes.

273
00:16:39,580 --> 00:16:42,100
And then grids are basically just groups of blocks.

274
00:16:42,100 --> 00:16:45,460
So this is the whole API again.

275
00:16:45,460 --> 00:16:48,380
So at the top, you have a grid, which is full of blocks.

276
00:16:48,380 --> 00:16:53,700
And then you take one block, which is full of threads that you can assign to these tasks.

277
00:16:53,700 --> 00:16:58,220
And it kind of matches up well with this architecture diagram we saw before, where each of your

278
00:16:59,120 --> 00:17:05,960
being assigned to the small green units on the right in the GPU architecture.

279
00:17:05,960 --> 00:17:07,680
So this is one example of a CUDA kernel.

280
00:17:07,680 --> 00:17:11,160
And we're going to dive into a couple particulars about this code.

281
00:17:11,160 --> 00:17:20,800
Basically, to understand it, we have to understand how does CUDA work across your CPU and your

282
00:17:20,800 --> 00:17:21,800
GPU.

283
00:17:21,800 --> 00:17:27,040
And you need both the CPU and the GPU, because the CPU is going to be running some code in

284
00:17:27,060 --> 00:17:30,740
terms of getting the data, maybe performing some sort of logic.

285
00:17:30,740 --> 00:17:34,460
And then when you want to perform your operation, you're going to send it to the GPU.

286
00:17:34,460 --> 00:17:37,940
And so in the CUDA language, your CPU is called your host.

287
00:17:37,940 --> 00:17:41,980
The GPU, you could have multiple of them, are called your device or your devices.

288
00:17:41,980 --> 00:17:47,500
And basically, the CPU is transferring data to the GPU, and the GPU can send it back.

289
00:17:47,500 --> 00:17:49,420
So this is kind of the setup.

290
00:17:49,420 --> 00:17:57,020
And so what you have is a C or C++ style syntax, but with some additions to make you able to

291
00:17:57,180 --> 00:18:01,740
specify what code runs on the device versus what code runs on the host.

292
00:18:01,740 --> 00:18:07,020
And so that's the global is going to let you specify the actual kernel function.

293
00:18:07,020 --> 00:18:11,840
And then there's some additional syntax to be able to set up grid size and block size.

294
00:18:11,840 --> 00:18:12,840
And so this is it again.

295
00:18:12,840 --> 00:18:15,020
You can see it's using the global up top.

296
00:18:15,020 --> 00:18:18,340
And that means that function is going to run on the GPU.

297
00:18:18,340 --> 00:18:22,060
This main function is basically regular C++.

298
00:18:22,060 --> 00:18:25,980
That's going to run on your CPU.

299
00:18:26,000 --> 00:18:27,760
So let's talk about PyCuda.

300
00:18:27,760 --> 00:18:29,880
What does it let you do?

301
00:18:29,880 --> 00:18:34,560
It was built by a researcher originally, and it's described in this paper.

302
00:18:34,560 --> 00:18:38,120
It's one of the few libraries I've encountered that actually have a research paper dedicated

303
00:18:38,120 --> 00:18:39,120
to it.

304
00:18:39,120 --> 00:18:41,800
So if you're interested in the internals, you can definitely dive into that.

305
00:18:41,800 --> 00:18:45,160
And it's been used in a lot of academic projects.

306
00:18:45,160 --> 00:18:47,120
So this is an example code of PyCuda.

307
00:18:47,120 --> 00:18:51,960
And what it lets you do is it has this source module class that takes in a string that is

308
00:18:51,960 --> 00:18:57,780
this Cuda code and then from within your Python code lets you get that function and call

309
00:18:57,780 --> 00:19:00,100
it on your arrays.

310
00:19:00,100 --> 00:19:02,940
So this is the basic setup of PyCuda.

311
00:19:02,940 --> 00:19:06,720
And what are the benefits of it compared to manual Cuda?

312
00:19:06,720 --> 00:19:09,080
One is automatic memory management.

313
00:19:09,080 --> 00:19:13,700
There's also benefits in data transfer, automatic error checking, and metaprogramming.

314
00:19:13,700 --> 00:19:19,700
So automatic memory management, basically the memory management in PyCuda conforms to

315
00:19:19,700 --> 00:19:21,480
the Python way of managing memory.

316
00:19:21,480 --> 00:19:26,960
So once an object goes outside of its lifetime, it's going to be cleaned up.

317
00:19:26,960 --> 00:19:28,320
This is not how Cuda works.

318
00:19:28,320 --> 00:19:34,360
So if you're actually working directly with it, you'd have to do a lot of manual allocation

319
00:19:34,360 --> 00:19:36,600
of memory, which we'll see later.

320
00:19:36,600 --> 00:19:40,620
Another thing that PyCuda is nice about is data transfer.

321
00:19:40,620 --> 00:19:42,360
So this is some example code.

322
00:19:42,360 --> 00:19:46,560
It's using this class, Cuda.inout, that PyCuda provides.

323
00:19:46,560 --> 00:19:52,720
And basically what you would originally have to do in data transfer between CPU and GPU

324
00:19:52,720 --> 00:19:56,080
is you have to create a NumPy array maybe on the CPU.

325
00:19:56,080 --> 00:20:00,560
You need to allocate GPU memory, transfer the data between the devices, run the kernel,

326
00:20:00,560 --> 00:20:02,000
and then send it back.

327
00:20:02,000 --> 00:20:06,560
And so these classes basically will do that automatically for you.

328
00:20:06,560 --> 00:20:11,040
And so in, data is going in, out, data is coming out, in out does both of them.

329
00:20:11,040 --> 00:20:14,080
And that just wraps the array in a nice way.

330
00:20:14,080 --> 00:20:16,000
Another thing is automatic error checking.

331
00:20:16,000 --> 00:20:18,120
This is from the Cuda documentation.

332
00:20:18,120 --> 00:20:23,140
If an asynchronous error occurs, it will be reported by some subsequent unrelated runtime

333
00:20:23,140 --> 00:20:24,140
function call.

334
00:20:24,140 --> 00:20:27,760
That sounds really confusing to me personally.

335
00:20:27,760 --> 00:20:31,740
And so PyCuda is going to wrap that, check the errors at the right time, and then surface

336
00:20:31,740 --> 00:20:33,440
them as Python exceptions.

337
00:20:33,440 --> 00:20:35,400
And finally, metaprogramming.

338
00:20:35,400 --> 00:20:41,040
This is a really interesting feature of PyCuda and how you're basically using strings in

339
00:20:41,040 --> 00:20:44,840
Python to create your Cuda code.

340
00:20:44,840 --> 00:20:49,320
I talked about threads, blocks, and grids, and the size of those things can be hard to

341
00:20:49,320 --> 00:20:50,320
tune.

342
00:20:50,320 --> 00:20:55,080
So basically for a given problem, a given workload, you want to decide how many of those

343
00:20:55,080 --> 00:20:56,660
to allocate.

344
00:20:56,660 --> 00:21:00,560
And that can be difficult and it can impact the performance of your code.

345
00:21:00,560 --> 00:21:05,760
And so what you might want to do is benchmark different parameters of that.

346
00:21:05,760 --> 00:21:10,540
And because PyCuda is compiling strings, what you can do is templating.

347
00:21:10,540 --> 00:21:13,140
So this is just normal string templating.

348
00:21:13,140 --> 00:21:19,100
And you're templating in different parameters for maybe the size of your block or the size

349
00:21:19,100 --> 00:21:20,180
of your grid.

350
00:21:20,180 --> 00:21:24,460
And so you can very easily write a Python program that loops over different parameters

351
00:21:24,460 --> 00:21:29,600
and then templates them into your Cuda kernel and then use that kernel and time it and do

352
00:21:29,600 --> 00:21:32,200
some profiling there.

353
00:21:32,200 --> 00:21:36,700
So finally, we have Cuda as a C or C++ extension.

354
00:21:36,700 --> 00:21:39,760
And so when someone was reviewing this talk and I got to this part, they're like, why

355
00:21:39,760 --> 00:21:42,440
not just use C++?

356
00:21:42,440 --> 00:21:48,080
And I said, I'm speaking at PyCon.

357
00:21:48,080 --> 00:21:51,480
But the real answer is this is a really common paradigm across libraries.

358
00:21:51,480 --> 00:21:56,520
And this is what you'd want to dig into if you are actually building something for Python

359
00:21:56,520 --> 00:21:57,520
users.

360
00:21:57,520 --> 00:22:00,400
So this is what PyTorch does is what TensorFlow does.

361
00:22:00,400 --> 00:22:05,280
But in a way, this is also what NumPy does just with C or C++ extensions.

362
00:22:06,280 --> 00:22:12,280
So you're letting you use a Python API, but behind the scenes, it's calling more powerful

363
00:22:12,280 --> 00:22:14,280
parts of your system.

364
00:22:14,280 --> 00:22:20,280
And so what I call this kind of version of using Cuda is Python API and Cuda performance.

365
00:22:20,280 --> 00:22:24,280
And so if this is a desirable thing for you, especially if you're building libraries, then

366
00:22:24,280 --> 00:22:26,280
you might want to go this route.

367
00:22:26,280 --> 00:22:31,280
So this is kind of the documentation about Python C extensions.

368
00:22:31,280 --> 00:22:38,280
So you're building a module in C or C++ that you have then the ability to import into your Python code and call.

369
00:22:38,280 --> 00:22:49,280
And what this requires from Cuda is to be able to somehow translate it from Cuda to C or C++ to be able to make that connection to Python.

370
00:22:49,280 --> 00:22:56,280
And luckily, NVIDIA provides a compiler called NVCC, which basically takes in your Cuda code.

371
00:22:56,280 --> 00:22:58,280
And it does a couple things.

372
00:22:58,280 --> 00:23:03,280
One, the kernel part that's going to run on your GPU device is turned into GPU assembly.

373
00:23:03,280 --> 00:23:07,280
And then it basically takes the rest of the code that's meant to run on the CPU.

374
00:23:07,280 --> 00:23:11,280
It replaces the special syntax that it's using.

375
00:23:11,280 --> 00:23:15,280
And then it can also just compile your CPU code for you.

376
00:23:15,280 --> 00:23:20,280
So everything can happen through NVCC, and that's pretty nice.

377
00:23:20,280 --> 00:23:25,280
In terms of the Python side, if you're interested, this is a really good place to start understanding.

378
00:23:25,280 --> 00:23:27,280
And this will be in my slides.

379
00:23:27,280 --> 00:23:30,280
Basically, what you can do, there's a couple ways you can do it.

380
00:23:30,280 --> 00:23:36,280
This way, which makes it a little bit easier, is you can use Cython to generate a C++ class.

381
00:23:36,280 --> 00:23:43,280
And then by hacking a little bit in your setup tools, you can get NVCC to compile and link everything together.

382
00:23:43,280 --> 00:23:49,280
And this is just, if you've had to do extensions or worked in setup tools, this is just one snippet from that repo.

383
00:23:49,280 --> 00:23:54,280
Basically, how they're setting up the sources, which is some Cuda code, some C++ code.

384
00:23:54,280 --> 00:24:01,280
And you can basically specify that you want the NVCC compiler to compile and link it together.

385
00:24:01,280 --> 00:24:06,280
But definitely check that out if this is something you're interested in, especially, like I said,

386
00:24:06,280 --> 00:24:09,280
if you're building libraries that are being exposed to users.

387
00:24:09,280 --> 00:24:13,280
And so some of the benefits of this is manual memory management.

388
00:24:13,280 --> 00:24:15,280
And you might say, well, is that really a benefit?

389
00:24:15,280 --> 00:24:18,280
So this is what you might have to do.

390
00:24:18,280 --> 00:24:21,280
You can see there's like Matlocks and Memcpies.

391
00:24:22,280 --> 00:24:25,280
And you also have to do freeze, which is annoying.

392
00:24:25,280 --> 00:24:28,280
But there are some benefits to manual memory management.

393
00:24:28,280 --> 00:24:30,280
There's Cuda features.

394
00:24:30,280 --> 00:24:34,280
So like page locked memory, mapped memory can be really powerful.

395
00:24:34,280 --> 00:24:39,280
These are basically ways to use the memory of both your host and your device together.

396
00:24:39,280 --> 00:24:45,280
And so you can get better performance out of these and save some of the writing that's going on back and forth.

397
00:24:45,280 --> 00:24:47,280
So this can be really powerful.

398
00:24:47,280 --> 00:24:54,280
And in order to access you, you'd need to kind of drop into the C and C++ extension.

399
00:24:54,280 --> 00:24:56,280
Another thing is a compiler.

400
00:24:56,280 --> 00:24:59,280
And I know I shouldn't say this because it's a Python conference.

401
00:24:59,280 --> 00:25:04,280
But I will say, as someone who is kind of putting this together and you're working with this Cuda code,

402
00:25:04,280 --> 00:25:11,280
it can be nice for it to just tell you you did something wrong, especially if you're unfamiliar with the API.

403
00:25:11,280 --> 00:25:15,280
And so I think this is a plus to doing it the extension way.

404
00:25:15,280 --> 00:25:19,280
And so finally, I just want to talk about how to get started.

405
00:25:19,280 --> 00:25:23,280
You know, you've seen this talk. You're interested in using the GPU.

406
00:25:24,280 --> 00:25:26,280
So the first thing is access.

407
00:25:26,280 --> 00:25:31,280
And the nice thing is there's a lot of resources out there that can let you access a GPU.

408
00:25:31,280 --> 00:25:33,280
There's Google Colab, which I think is awesome.

409
00:25:33,280 --> 00:25:34,280
And it's free.

410
00:25:34,280 --> 00:25:37,280
Calgo kernels have free GPUs as well.

411
00:25:37,280 --> 00:25:41,280
And then even if you want to host it on your own instance,

412
00:25:41,280 --> 00:25:45,280
both AWS and Google Cloud have pretty cheap offerings.

413
00:25:45,280 --> 00:25:50,280
But I will say, if you go down this route, always remember Horton's Law.

414
00:25:50,280 --> 00:25:52,280
Don't forget to turn it off.

415
00:25:54,280 --> 00:25:56,280
Finally, you know, where can you go next?

416
00:25:56,280 --> 00:26:01,280
One, I would definitely say, how can you apply Cuda to your own workflow?

417
00:26:01,280 --> 00:26:05,280
So like I said, it's starting to be applied in a lot of different domains.

418
00:26:05,280 --> 00:26:08,280
So you can think about how can it work for you.

419
00:26:08,280 --> 00:26:12,280
The other thing to get into is now that you have these tools to do parallel programming,

420
00:26:12,280 --> 00:26:19,280
you can start to do some research into actual parallel programming algorithms you can use with this paradigm.

421
00:26:19,280 --> 00:26:23,280
And finally, if you're interested in this kind of stuff from a hardware or device level,

422
00:26:23,280 --> 00:26:25,280
there's not just GPUs.

423
00:26:25,280 --> 00:26:29,280
There's TPUs, other XPUs, I guess, is a term people use now.

424
00:26:29,280 --> 00:26:34,280
And also you can get into maybe FPGAs, which is kind of the extreme,

425
00:26:34,280 --> 00:26:38,280
going to the extreme of having the device that actually executes your algorithm.

426
00:26:38,280 --> 00:26:43,280
And so that, if you're interested in the hardware side, is definitely another place to go.

427
00:26:43,280 --> 00:26:47,280
And that is at the actual end. Thank you so much. And go program the GPU.

