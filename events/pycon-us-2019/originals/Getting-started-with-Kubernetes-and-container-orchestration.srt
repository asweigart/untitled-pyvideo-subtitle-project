1
00:00:00,000 --> 00:00:02,600
Okay, it's nine, so we're gonna start.

2
00:00:07,600 --> 00:00:08,520
Good morning, everyone.

3
00:00:08,520 --> 00:00:10,960
Welcome to PyCon and to getting started

4
00:00:10,960 --> 00:00:13,280
with Kubernetes and Container Orchestration.

5
00:00:14,200 --> 00:00:17,480
You will find the slides that I will present this morning

6
00:00:17,480 --> 00:00:22,480
at this URL here, pycon2019.container.training.

7
00:00:22,720 --> 00:00:25,960
You will also find that URL on the little cards

8
00:00:25,960 --> 00:00:27,520
that we've put on your tables.

9
00:00:28,400 --> 00:00:32,280
So I'm Jerome and I'm helped today by AJ

10
00:00:32,280 --> 00:00:35,320
who is on the back of the room over there.

11
00:00:35,320 --> 00:00:39,480
So we will run this until about 12.20.

12
00:00:39,480 --> 00:00:41,200
There will be a coffee break.

13
00:00:41,200 --> 00:00:43,920
It's more likely to be around like 10.15.

14
00:00:43,920 --> 00:00:46,120
I'm going to give a heavy responsibility

15
00:00:46,120 --> 00:00:47,840
to the people in the first row.

16
00:00:47,840 --> 00:00:49,760
If you see that at 10.15,

17
00:00:49,760 --> 00:00:52,680
I'm still obliviously going through slides,

18
00:00:52,680 --> 00:00:54,160
please throw something at me

19
00:00:54,200 --> 00:00:58,280
so that we can make a short break, have coffee, et cetera.

20
00:00:58,280 --> 00:01:02,080
Feel free to interrupt if you have questions at any time.

21
00:01:02,080 --> 00:01:03,600
And between each chapter,

22
00:01:03,600 --> 00:01:06,600
we've put full screen pictures of containers

23
00:01:06,600 --> 00:01:09,400
to remind us how we're about to start a new chapter

24
00:01:09,400 --> 00:01:10,960
and we should take a deep breath

25
00:01:10,960 --> 00:01:13,480
and let people ask questions.

26
00:01:13,480 --> 00:01:16,200
In case you feel shy about raising your hand

27
00:01:16,200 --> 00:01:18,880
asking questions, we've also thought about setting up

28
00:01:18,880 --> 00:01:21,120
a chat room using Gitter.

29
00:01:21,120 --> 00:01:23,600
There will be the link to that chat room

30
00:01:23,640 --> 00:01:24,880
in just a few slides

31
00:01:24,880 --> 00:01:28,680
and you will be able to ask questions here as well live.

32
00:01:28,680 --> 00:01:31,560
And of course, I won't be able to simultaneously

33
00:01:31,560 --> 00:01:35,160
show slides and make demos and watch a chat room,

34
00:01:35,160 --> 00:01:37,600
but AJ on the back is going to monitor that chat room

35
00:01:37,600 --> 00:01:40,640
for your questions during the entire workshop.

36
00:01:40,640 --> 00:01:43,560
So the content that I'll present this morning

37
00:01:43,560 --> 00:01:47,880
was initially written for in-person training like this,

38
00:01:47,880 --> 00:01:52,360
but it can also be used or reused for self-paced training.

39
00:01:52,360 --> 00:01:55,120
The materials are going to remain online,

40
00:01:55,120 --> 00:01:58,720
maybe not forever, but at least for a really long time.

41
00:01:58,720 --> 00:02:01,800
So you are welcome to use and reuse them later.

42
00:02:01,800 --> 00:02:05,600
You can even look at the other materials that we deliver.

43
00:02:05,600 --> 00:02:09,400
They are also like one day, two day, four days workshops

44
00:02:09,400 --> 00:02:11,400
and training materials.

45
00:02:11,400 --> 00:02:15,360
So all the content is stored in a public GitHub repo.

46
00:02:15,360 --> 00:02:19,960
That repo has these slides, but also the demo,

47
00:02:19,960 --> 00:02:22,600
the code for the demo app that we will use this morning,

48
00:02:22,600 --> 00:02:25,960
as well as a bunch of scripts and other tools

49
00:02:25,960 --> 00:02:28,400
that can be useful to deliver content

50
00:02:28,400 --> 00:02:30,320
or follow this workshop.

51
00:02:31,320 --> 00:02:34,360
The source of the slides is in Markdown.

52
00:02:34,360 --> 00:02:38,280
And if you go at any slide to the bottom edge of the screen,

53
00:02:38,280 --> 00:02:40,560
you will see this black bar showing up.

54
00:02:40,560 --> 00:02:43,960
And if you click on it, it will take you to GitHub

55
00:02:43,960 --> 00:02:47,400
to the source of these slides,

56
00:02:47,400 --> 00:02:50,520
which will show up in just a hot second

57
00:02:50,520 --> 00:02:55,080
once the Wi-Fi gives me access to said material.

58
00:02:55,080 --> 00:02:56,280
Here we go.

59
00:02:56,280 --> 00:02:59,640
So the slides sources are in Markdown.

60
00:03:00,520 --> 00:03:05,520
And the idea is to version then, keep track of changes,

61
00:03:06,000 --> 00:03:09,240
allow for easy pull requests, et cetera, et cetera.

62
00:03:11,440 --> 00:03:15,040
You will see, okay, so normally in the top left corner here,

63
00:03:15,040 --> 00:03:17,600
there should be a little magnifying glass.

64
00:03:17,600 --> 00:03:19,800
I don't know why it's not showing up, maybe.

65
00:03:19,800 --> 00:03:20,800
Oh, there we go.

66
00:03:20,800 --> 00:03:24,120
It was just a small network glitch.

67
00:03:24,120 --> 00:03:28,360
You will see that these slides have two layers.

68
00:03:28,360 --> 00:03:29,440
There is the first layer

69
00:03:29,440 --> 00:03:31,920
that we're going to cover extensively today.

70
00:03:31,920 --> 00:03:33,320
And there is a second layer,

71
00:03:33,320 --> 00:03:35,540
which is indicated by these slides

72
00:03:35,540 --> 00:03:37,320
with the little magnifying glass.

73
00:03:37,320 --> 00:03:42,320
These slides have extra content that I will skip today,

74
00:03:42,800 --> 00:03:45,640
but that I wanted to have in the slides

75
00:03:45,640 --> 00:03:48,400
so that if you just check on them later,

76
00:03:48,400 --> 00:03:52,000
or if we are covering a chapter about concepts

77
00:03:52,000 --> 00:03:53,280
that you already know,

78
00:03:53,280 --> 00:03:57,140
they will provide extra information and extra depth.

79
00:03:57,140 --> 00:03:59,760
But don't panic if you see me just skip slides

80
00:03:59,760 --> 00:04:01,200
with that little symbol.

81
00:04:01,200 --> 00:04:02,460
It's normal, it's expected,

82
00:04:02,460 --> 00:04:04,000
because I don't expect otherwise

83
00:04:04,000 --> 00:04:08,260
that we would cover 300 slides in just half a day.

84
00:04:08,260 --> 00:04:10,320
So this is the outline for today.

85
00:04:10,360 --> 00:04:13,280
I will quickly cover the pre-requirements.

86
00:04:13,280 --> 00:04:15,320
Then we will run a demo app

87
00:04:15,320 --> 00:04:17,800
built around a Macro Services architecture

88
00:04:17,800 --> 00:04:20,440
on a single machine using Docker Compose.

89
00:04:20,440 --> 00:04:24,160
And then we will want to run that app

90
00:04:24,160 --> 00:04:25,620
on a Kubernetes cluster.

91
00:04:25,620 --> 00:04:28,880
So I will present Kubernetes concepts,

92
00:04:28,880 --> 00:04:33,160
including the emphasis on the declarative model

93
00:04:33,160 --> 00:04:35,520
of Kubernetes and reconciliation loops.

94
00:04:35,520 --> 00:04:38,860
I will quickly explain how networking works.

95
00:04:38,860 --> 00:04:40,940
And then once we're equipped with the theory,

96
00:04:40,940 --> 00:04:42,900
we will jump into the practice.

97
00:04:42,900 --> 00:04:45,860
We will do some things with kubectl,

98
00:04:45,860 --> 00:04:49,580
run a few containers on a Kubernetes cluster,

99
00:04:49,580 --> 00:04:53,100
see how to interconnect containers together.

100
00:04:53,100 --> 00:04:56,680
Then we will run the demo app on the Kubernetes cluster,

101
00:04:56,680 --> 00:05:00,660
and we will see two different ways to scale services

102
00:05:00,660 --> 00:05:02,780
using deployments and replica sets,

103
00:05:02,780 --> 00:05:04,380
and then using daemon sets.

104
00:05:04,380 --> 00:05:05,820
This will give us an opportunity

105
00:05:05,820 --> 00:05:08,780
to see how to write YAML descriptions

106
00:05:08,780 --> 00:05:10,540
for Kubernetes resources.

107
00:05:10,540 --> 00:05:13,980
Then we will wrap up with labels and selectors,

108
00:05:13,980 --> 00:05:17,260
which are a pretty useful concept in Kubernetes,

109
00:05:17,260 --> 00:05:20,100
and a quick demo of running updates.

110
00:05:20,100 --> 00:05:23,900
And then I will give you a few extra links and next steps,

111
00:05:23,900 --> 00:05:27,400
like what to do next after what we will have seen.

112
00:05:28,420 --> 00:05:30,920
And these are the famous full screen container pictures

113
00:05:30,920 --> 00:05:32,980
I was mentioning earlier.

114
00:05:32,980 --> 00:05:37,860
So do you have any question about this?

115
00:05:37,860 --> 00:05:39,180
Can everybody hear me okay?

116
00:05:39,180 --> 00:05:41,180
Can everybody see the slides okay?

117
00:05:42,620 --> 00:05:43,460
All right.

118
00:05:44,340 --> 00:05:45,660
So prerequirements.

119
00:05:46,820 --> 00:05:49,500
The prerequirements are fairly minimal

120
00:05:49,500 --> 00:05:52,900
in the sense that I wanted anyone who wants to learn

121
00:05:52,900 --> 00:05:56,200
about containers and Kubernetes to be able to attend.

122
00:05:56,200 --> 00:05:58,040
So you just need to be comfortable

123
00:05:58,040 --> 00:05:59,460
with the Unix command line,

124
00:05:59,460 --> 00:06:02,560
because we're going to SSH into remote servers

125
00:06:02,560 --> 00:06:05,080
to do some things there.

126
00:06:05,080 --> 00:06:07,160
You need to be able to clone a repo,

127
00:06:07,160 --> 00:06:10,400
navigate directories, edit files with VI

128
00:06:10,400 --> 00:06:12,200
or your favorite command line editor.

129
00:06:12,200 --> 00:06:15,880
And maybe we will go as far as setting our own variables

130
00:06:15,880 --> 00:06:19,080
and doing a few shell loops, but that's pretty much it.

131
00:06:19,080 --> 00:06:21,640
You need a little bit of container knowledge,

132
00:06:21,640 --> 00:06:25,320
but a very small, like bare minimum.

133
00:06:25,320 --> 00:06:28,600
If you know how to build a container image

134
00:06:28,600 --> 00:06:30,860
and how to run it, that's perfect.

135
00:06:30,860 --> 00:06:32,160
That's all you will need to know.

136
00:06:32,160 --> 00:06:33,320
It could be with Docker,

137
00:06:33,320 --> 00:06:36,280
it could be with another container platform, that's fine.

138
00:06:37,440 --> 00:06:40,600
There is some theory about education

139
00:06:40,600 --> 00:06:45,600
telling that we all have different ways to memorize things.

140
00:06:45,920 --> 00:06:48,980
Some people are able to just sit here and listen,

141
00:06:48,980 --> 00:06:52,680
and then they understand and they can do things.

142
00:06:52,680 --> 00:06:56,120
Some of us need to see someone do things,

143
00:06:56,120 --> 00:06:58,520
and then they can repeat that.

144
00:06:58,520 --> 00:07:01,400
Some people need to do things themselves

145
00:07:01,400 --> 00:07:03,360
and then muscle memory kicks in.

146
00:07:03,360 --> 00:07:06,240
And some people need to explain to others

147
00:07:06,240 --> 00:07:08,040
and the act of explaining to others

148
00:07:08,040 --> 00:07:10,120
helps them to actually remember.

149
00:07:10,120 --> 00:07:12,720
I'm more in the last category,

150
00:07:12,720 --> 00:07:15,600
but to help everyone to be able to understand

151
00:07:15,600 --> 00:07:18,520
and memorize what we're going to do this morning,

152
00:07:18,520 --> 00:07:21,760
each chapter I'm going to explain concepts,

153
00:07:21,760 --> 00:07:24,600
then run some demos and then invite you

154
00:07:24,600 --> 00:07:28,340
to reproduce the demos in your own environment

155
00:07:28,340 --> 00:07:29,520
if you want to.

156
00:07:30,000 --> 00:07:32,440
You don't have to, for some people,

157
00:07:32,440 --> 00:07:34,280
running commands and listening, et cetera,

158
00:07:34,280 --> 00:07:35,520
can be a distraction.

159
00:07:35,520 --> 00:07:38,200
But if you're the kind of person who likes to do things,

160
00:07:38,200 --> 00:07:41,480
who likes to follow how-tos and things like that,

161
00:07:41,480 --> 00:07:45,080
everybody will be given a private environment,

162
00:07:45,080 --> 00:07:49,420
a Kubernetes cluster, to run all the demos and labs.

163
00:07:49,420 --> 00:07:51,480
So precisely, these demos and labs

164
00:07:51,480 --> 00:07:55,080
are going to be identified by these gray rectangles

165
00:07:55,080 --> 00:08:00,080
like this and they will show the labs and exercises.

166
00:08:00,080 --> 00:08:03,960
So the first thing that I invite you to do

167
00:08:03,960 --> 00:08:06,420
is to open the slides.

168
00:08:06,420 --> 00:08:08,720
So if you haven't done so already,

169
00:08:08,720 --> 00:08:12,480
so that's PyCon2019.container.training.

170
00:08:12,480 --> 00:08:14,640
That way, you will have the slides.

171
00:08:14,640 --> 00:08:18,480
So if you are either very much on the back of the room

172
00:08:18,480 --> 00:08:20,000
and you can't see them really well,

173
00:08:20,000 --> 00:08:23,400
or if I'm jumping to the terminal to do some demos

174
00:08:23,400 --> 00:08:25,240
and you are wondering what was on the slide,

175
00:08:25,240 --> 00:08:27,960
et cetera, et cetera, you will be able to always

176
00:08:27,960 --> 00:08:30,520
have the slides handy.

177
00:08:30,520 --> 00:08:33,760
You can see the slide number in the top right corner

178
00:08:33,760 --> 00:08:37,560
and a pro tip, if you type a slide number,

179
00:08:37,560 --> 00:08:39,600
it will jump to that slide number.

180
00:08:39,600 --> 00:08:44,600
Like if I type 42, enter, it takes me to slide 42

181
00:08:45,440 --> 00:08:49,900
and then I can go back to wherever I was.

182
00:08:51,040 --> 00:08:52,720
The other thing I invite you to do

183
00:08:52,720 --> 00:08:56,960
is once you have opened the slides and jumped to slide 14

184
00:08:56,960 --> 00:09:01,240
is to join the chat room by clicking on that link

185
00:09:01,240 --> 00:09:02,780
over there, the Gitter link.

186
00:09:03,720 --> 00:09:06,300
So this is going to open Gitter.

187
00:09:07,420 --> 00:09:10,500
And Gitter is, if you're not familiar with it,

188
00:09:10,500 --> 00:09:12,900
it's a little bit like Slack or IRC.

189
00:09:13,760 --> 00:09:17,160
It's pretty much like a low key, no frills,

190
00:09:17,160 --> 00:09:21,060
no super fancy features, but it's super easy

191
00:09:21,060 --> 00:09:22,980
to join a chat room.

192
00:09:22,980 --> 00:09:27,140
If you have a GitHub or GitLab or Twitter account,

193
00:09:27,140 --> 00:09:29,920
it should be just one click and you're in.

194
00:09:29,920 --> 00:09:32,700
So we pick that for simplicity.

195
00:09:32,700 --> 00:09:35,140
Again, you don't have to join the chat room

196
00:09:35,140 --> 00:09:38,260
but it can be super useful if you have questions,

197
00:09:38,260 --> 00:09:40,680
comments, et cetera, et cetera.

198
00:09:40,680 --> 00:09:43,740
So I was mentioning that everybody would get

199
00:09:43,740 --> 00:09:46,260
a private Kubernetes cluster.

200
00:09:46,260 --> 00:09:48,420
So everybody gets a cluster indeed

201
00:09:48,420 --> 00:09:51,980
and that's going to be with these little cards

202
00:09:51,980 --> 00:09:53,900
that should be on your tables.

203
00:09:53,900 --> 00:09:56,860
Just making sure everybody got one of these.

204
00:09:56,860 --> 00:10:00,860
If you can't find that little card, please raise your hand

205
00:10:00,860 --> 00:10:04,380
and we will magically get one to you.

206
00:10:04,380 --> 00:10:06,940
No, everybody has one, awesome.

207
00:10:06,940 --> 00:10:11,580
So these are VMs that were deployed just this morning

208
00:10:11,580 --> 00:10:13,340
and set up with Kubernetes.

209
00:10:13,340 --> 00:10:16,540
Each of you gets four VMs pre-set up

210
00:10:16,540 --> 00:10:18,860
as a Kubernetes cluster.

211
00:10:18,860 --> 00:10:21,240
Unfortunately, since VMs cost money

212
00:10:21,240 --> 00:10:24,700
because cloud providers are trying to make profit these days,

213
00:10:24,700 --> 00:10:27,260
I will not be able to leave them online

214
00:10:27,260 --> 00:10:30,420
for a very long time after the workshop

215
00:10:30,420 --> 00:10:32,540
but they will be online for today.

216
00:10:33,620 --> 00:10:36,900
So why are we not just running containers

217
00:10:36,900 --> 00:10:38,700
and Kubernetes locally?

218
00:10:38,700 --> 00:10:41,160
Because not all of us have machines

219
00:10:41,160 --> 00:10:43,260
that would be able to do that.

220
00:10:43,260 --> 00:10:46,160
Sometimes we don't have like the virtualization extensions

221
00:10:47,080 --> 00:10:50,920
enabled or we might have some really restrictive IT policies

222
00:10:50,920 --> 00:10:54,760
in place preventing us from installing all that.

223
00:10:54,760 --> 00:10:58,640
And furthermore, most of the installation options

224
00:10:58,640 --> 00:11:01,000
to install Kubernetes locally

225
00:11:01,000 --> 00:11:03,380
will give you a one node cluster.

226
00:11:03,380 --> 00:11:05,840
And for some labs and some demos,

227
00:11:05,840 --> 00:11:08,500
we want an actual cluster with multiple nodes.

228
00:11:08,500 --> 00:11:10,760
So to completely sidestep the issues,

229
00:11:10,760 --> 00:11:13,600
we're going to SSH into remote VMs.

230
00:11:13,600 --> 00:11:16,920
Oh, and one last thing, if it's like 70 people,

231
00:11:16,920 --> 00:11:19,600
all of us pulling images and building things

232
00:11:19,600 --> 00:11:21,880
at the same time over conference Wi-Fi,

233
00:11:21,880 --> 00:11:24,280
that's not going to be a pleasant experience.

234
00:11:24,280 --> 00:11:27,600
So instead let's SSH into remote machines

235
00:11:27,600 --> 00:11:28,900
because on the remote machines,

236
00:11:28,900 --> 00:11:32,000
the bandwidth is infinite, almost.

237
00:11:33,040 --> 00:11:35,080
If you don't have an SSH client,

238
00:11:35,080 --> 00:11:37,120
because for instance, if you're running on Windows,

239
00:11:37,120 --> 00:11:38,880
on older versions of Windows,

240
00:11:38,880 --> 00:11:41,680
I've put some links in the slides here

241
00:11:41,960 --> 00:11:44,440
where you can get like putty or something like that

242
00:11:44,440 --> 00:11:46,360
that will give you a way to connect

243
00:11:46,360 --> 00:11:47,820
into these remote machines.

244
00:11:49,080 --> 00:11:51,480
So the first thing that we're gonna do

245
00:11:51,480 --> 00:11:53,960
is connect into the lab environment

246
00:11:53,960 --> 00:11:56,600
to make sure that everything works fine.

247
00:11:56,600 --> 00:11:58,560
And so in my terminal here,

248
00:11:58,560 --> 00:12:01,960
I'm going to do SSH Docker at.

249
00:12:01,960 --> 00:12:05,680
Here I'm going to put the IP address of node one.

250
00:12:05,680 --> 00:12:08,560
Of course, that's my node one, not yours.

251
00:12:08,560 --> 00:12:11,320
Please put the IP address of your node, not mine.

252
00:12:11,320 --> 00:12:14,840
Otherwise we're going to step on each other's feet.

253
00:12:16,440 --> 00:12:19,520
And once I'm logged in,

254
00:12:19,520 --> 00:12:22,060
I will just do one command

255
00:12:22,060 --> 00:12:24,820
to make sure that my cluster is fine.

256
00:12:24,820 --> 00:12:29,640
I'm going to do kubectl get nodes.

257
00:12:29,640 --> 00:12:32,540
And this is going to show me the list of nodes

258
00:12:32,540 --> 00:12:34,400
on my cluster.

259
00:12:34,400 --> 00:12:38,640
And I should see four nodes named node one, two, three, four.

260
00:12:38,640 --> 00:12:40,460
They should all be ready.

261
00:12:41,480 --> 00:12:46,480
And if by any chance you can't log into your environment

262
00:12:47,360 --> 00:12:51,000
or when you do that command, it shows something else,

263
00:12:51,000 --> 00:12:53,440
please raise your hand or let us know

264
00:12:53,440 --> 00:12:55,560
so that we can address that now

265
00:12:55,560 --> 00:12:59,280
and to make sure that you have a working environment

266
00:12:59,280 --> 00:13:00,360
for the labs.

267
00:13:01,420 --> 00:13:05,580
Everybody managed to log into their environment?

268
00:13:08,400 --> 00:13:09,820
Anybody had problems?

269
00:13:10,820 --> 00:13:11,880
Nope, okay.

270
00:13:13,660 --> 00:13:17,340
If you are redoing parts of that later

271
00:13:17,340 --> 00:13:19,020
and you would like an environment,

272
00:13:19,020 --> 00:13:22,420
you can use something called Play with Kubernetes

273
00:13:22,420 --> 00:13:24,240
or Catacoda for instance,

274
00:13:24,240 --> 00:13:27,780
that will give you a cluster in the browser.

275
00:13:27,780 --> 00:13:31,120
Obviously the cluster won't be running in the browser,

276
00:13:31,120 --> 00:13:34,180
it will be somewhere on some cloud instances,

277
00:13:34,180 --> 00:13:36,260
but you will get the terminal in the browser

278
00:13:36,260 --> 00:13:39,020
letting you connect to a remote cluster.

279
00:13:39,020 --> 00:13:41,260
The only downside of these environments

280
00:13:41,260 --> 00:13:43,540
is that they are short-lived.

281
00:13:43,540 --> 00:13:45,820
For instance, Play with Kubernetes,

282
00:13:45,820 --> 00:13:49,460
the environments are destroyed after four hours of use.

283
00:13:50,660 --> 00:13:53,460
We will do almost everything with node one.

284
00:13:53,460 --> 00:13:57,680
We won't really need to SSH into the other nodes.

285
00:13:57,680 --> 00:13:59,680
Node one will be our control plane,

286
00:13:59,680 --> 00:14:01,900
it will be our bastion host,

287
00:14:01,900 --> 00:14:04,300
our control center if you will.

288
00:14:05,260 --> 00:14:06,780
Once in a while I'm going to say,

289
00:14:06,780 --> 00:14:09,980
hey, let's open a new terminal and do this and that

290
00:14:09,980 --> 00:14:13,300
because for instance, we will want to see logs

291
00:14:13,300 --> 00:14:15,540
with tail-f or something similar

292
00:14:15,540 --> 00:14:19,140
and run some command and see the impact

293
00:14:19,140 --> 00:14:20,660
of that command in the logs.

294
00:14:20,660 --> 00:14:24,000
You can do either like have multiple windows

295
00:14:24,000 --> 00:14:27,420
and multiple SSH sessions to the servers

296
00:14:27,420 --> 00:14:30,740
or in my case, I'm going to use Tmux,

297
00:14:30,740 --> 00:14:33,820
which is a terminal multiplexer like screen

298
00:14:33,820 --> 00:14:36,780
just because as a presenter, it's an easier option

299
00:14:36,780 --> 00:14:39,500
but pick whatever you're the most comfortable with.

300
00:14:39,500 --> 00:14:40,960
It could be like multiple windows,

301
00:14:40,960 --> 00:14:43,920
it could be screen, it could be Tmux, everything is fine.

302
00:14:46,860 --> 00:14:49,260
Do you have questions on the environment?

303
00:14:51,540 --> 00:14:52,380
Yes.

304
00:14:59,360 --> 00:15:01,260
Oh, so if you get an error message,

305
00:15:01,260 --> 00:15:03,340
like if you do kubectl get nodes

306
00:15:03,340 --> 00:15:07,060
and you see like a connection to localhost 8080 was refused,

307
00:15:07,060 --> 00:15:09,980
this is probably because you're not on node one

308
00:15:09,980 --> 00:15:11,960
but on node two, three or four.

309
00:15:14,620 --> 00:15:15,460
Is that the case?

310
00:15:15,460 --> 00:15:16,700
It only works on node one.

311
00:15:16,700 --> 00:15:20,700
Yes, the kubectl command is only going to work on node one

312
00:15:20,700 --> 00:15:23,860
because the configuration file that kubectl requires

313
00:15:23,860 --> 00:15:26,540
is only deployed on node one.

314
00:15:26,540 --> 00:15:27,380
Yeah.

315
00:15:29,180 --> 00:15:31,380
Okay, so without further ado,

316
00:15:31,380 --> 00:15:33,860
we are going to see the demo app

317
00:15:33,860 --> 00:15:37,020
that we're going to use for the whole morning.

318
00:15:37,020 --> 00:15:42,020
So for starters, we're going to run that demo app with Compose

319
00:15:42,140 --> 00:15:44,660
just to bring it up, see how it works

320
00:15:44,660 --> 00:15:46,120
and then you are going to tear it down

321
00:15:46,120 --> 00:15:49,020
and see how to deploy it on Kubernetes.

322
00:15:49,020 --> 00:15:51,860
This demo app is in on the same repo

323
00:15:51,860 --> 00:15:55,380
that has the slides and scripts and everything else.

324
00:15:55,380 --> 00:15:58,220
And so I'm going to clone that repo,

325
00:15:58,260 --> 00:16:03,260
git clone, HTTPS, git up.com, jp.zo, that's me,

326
00:16:06,100 --> 00:16:07,980
slash container.training.

327
00:16:11,460 --> 00:16:15,940
And in that repo, I have a bunch of things,

328
00:16:15,940 --> 00:16:18,220
for instance, the slides directory,

329
00:16:18,220 --> 00:16:20,900
as you might guess, has the slides,

330
00:16:20,900 --> 00:16:25,460
but the demo app that we want to run is called Docker Coins

331
00:16:25,460 --> 00:16:28,660
and it's in the subdirectory Docker Coins.

332
00:16:28,660 --> 00:16:30,980
When I go to that subdirectory,

333
00:16:30,980 --> 00:16:33,240
I see a Docker Compose ML file.

334
00:16:34,100 --> 00:16:35,840
Without diving into too much details,

335
00:16:35,840 --> 00:16:38,620
when you see a Docker Compose ML file,

336
00:16:38,620 --> 00:16:40,900
it means we can run this with Compose.

337
00:16:40,900 --> 00:16:42,940
It's a little bit like when you see a make file,

338
00:16:42,940 --> 00:16:44,280
you know you can run make.

339
00:16:44,280 --> 00:16:47,540
When you see requirements.txt or setup.py,

340
00:16:47,540 --> 00:16:50,900
you know you can use pip or python-sitter py

341
00:16:50,900 --> 00:16:52,820
to install dependencies.

342
00:16:52,820 --> 00:16:55,100
If you see a gem file, if you also do Ruby,

343
00:16:55,340 --> 00:16:58,100
you can use bundler to install that.

344
00:16:58,100 --> 00:17:02,620
You can also see package JSON for Node.js applications

345
00:17:02,620 --> 00:17:06,740
or POMXML for Java, Deploy with Maven, et cetera, et cetera.

346
00:17:06,740 --> 00:17:09,100
So when I see DockerCompose.yml,

347
00:17:09,100 --> 00:17:11,820
I know I can just do DockerCompose up,

348
00:17:12,940 --> 00:17:16,240
and this is going to read that YAML file,

349
00:17:16,240 --> 00:17:20,320
analyze it, and build and run the application.

350
00:17:20,320 --> 00:17:24,940
While it's building, I'm going to have a quick look

351
00:17:24,940 --> 00:17:29,940
at the Compose.yml file, and in that Compose.yml file,

352
00:17:30,220 --> 00:17:32,700
I see that they have a bunch of services,

353
00:17:32,700 --> 00:17:35,940
each of them corresponding to one container.

354
00:17:35,940 --> 00:17:40,940
And for each service, I can either have an image statement,

355
00:17:41,120 --> 00:17:43,860
which means just get an existing image

356
00:17:43,860 --> 00:17:46,260
from a container registry.

357
00:17:46,260 --> 00:17:48,900
So in that case, I have a Redis service,

358
00:17:48,900 --> 00:17:51,340
and I say just use image Redis.

359
00:17:51,340 --> 00:17:54,140
And then I have a bunch of services

360
00:17:54,140 --> 00:17:57,740
for which I have a build statement,

361
00:17:57,740 --> 00:18:01,860
and there is one free space over there,

362
00:18:01,860 --> 00:18:04,900
and if you have a free space next to you,

363
00:18:04,900 --> 00:18:07,180
can you raise your hand as well?

364
00:18:07,180 --> 00:18:09,780
Okay, there are a few more spaces over there as well.

365
00:18:12,620 --> 00:18:17,620
I counted 69 seats, and I think there were 67 people

366
00:18:17,860 --> 00:18:20,500
registered, so there should be one seat for everyone.

367
00:18:21,260 --> 00:18:25,420
Okay, so when you see a build statement,

368
00:18:25,420 --> 00:18:28,920
that means that we're going to build a container image

369
00:18:28,920 --> 00:18:32,420
using a Dockerfile contained in the directory

370
00:18:32,420 --> 00:18:34,420
indicated by the build statement.

371
00:18:34,420 --> 00:18:39,420
So for instance, here I have RNG, build RNG,

372
00:18:40,060 --> 00:18:42,680
that means there is an RNG directory,

373
00:18:42,680 --> 00:18:46,220
and in that RNG directory, there is a Dockerfile,

374
00:18:46,220 --> 00:18:48,740
and a little RNG.py file.

375
00:18:49,540 --> 00:18:52,780
Okay, while I was explaining all that,

376
00:18:52,780 --> 00:18:56,340
the app should now be running,

377
00:18:56,340 --> 00:18:59,060
and Compose has created all the containers

378
00:18:59,060 --> 00:19:02,020
for the application, and is now showing me the logs

379
00:19:02,020 --> 00:19:03,740
for the application.

380
00:19:03,740 --> 00:19:07,220
If I do a little freeze, like just for a second,

381
00:19:07,220 --> 00:19:10,780
we can see that we have what looks like HTTP logs,

382
00:19:10,780 --> 00:19:14,580
have some get on RNG and some post on Hasher,

383
00:19:14,580 --> 00:19:18,360
and once in a while, I have worker saying

384
00:19:18,360 --> 00:19:21,200
something about X units of work done,

385
00:19:21,200 --> 00:19:22,620
updating hash counter.

386
00:19:23,560 --> 00:19:27,780
So what exactly is this application?

387
00:19:27,780 --> 00:19:31,440
It's a Docker coin miner, fortunately for the environment,

388
00:19:31,440 --> 00:19:34,120
but unfortunately for my retirement plans,

389
00:19:34,120 --> 00:19:37,200
Docker coins is not an actual cryptocurrency,

390
00:19:37,200 --> 00:19:39,660
you can't buy anything with it.

391
00:19:39,660 --> 00:19:42,920
What Docker coins is doing is that it's an infinite loop

392
00:19:42,920 --> 00:19:46,800
where we are generating a little bit of random data,

393
00:19:46,800 --> 00:19:49,920
hashing that random data, and incrementing a counter,

394
00:19:49,920 --> 00:19:51,840
and we do that again and again and again.

395
00:19:51,840 --> 00:19:54,400
So in Python, we just like random.random,

396
00:19:54,400 --> 00:19:59,000
and then we use like SHA-256 to compute that hash,

397
00:19:59,000 --> 00:20:00,820
and then we increment a counter.

398
00:20:00,820 --> 00:20:04,000
But since that would not be a very interesting example,

399
00:20:04,000 --> 00:20:07,520
we are kind of spicing things up a little bit

400
00:20:07,520 --> 00:20:11,240
by having the random number generation part

401
00:20:11,240 --> 00:20:14,160
being in the web service, so that's the RNG service,

402
00:20:14,160 --> 00:20:16,720
random number generator, and so when we need

403
00:20:17,720 --> 00:20:22,720
to generate random data, we do a get on the RNG service.

404
00:20:23,640 --> 00:20:25,360
And then when we want to hash the data,

405
00:20:25,360 --> 00:20:29,600
instead of just calling like a built-in crypto package,

406
00:20:29,600 --> 00:20:32,560
we post the data to the hasher service,

407
00:20:33,640 --> 00:20:35,880
and when we need to increment the counter,

408
00:20:35,880 --> 00:20:39,480
instead of doing like counter plus equal one,

409
00:20:39,480 --> 00:20:41,800
we store the counter in Redis,

410
00:20:41,800 --> 00:20:44,760
and we increment that counter in Redis.

411
00:20:44,800 --> 00:20:49,560
And finally, we have a nice little web UI

412
00:20:49,560 --> 00:20:52,360
that will allow us to see the speed

413
00:20:52,360 --> 00:20:54,960
at which the whole thing is working.

414
00:20:56,720 --> 00:20:59,240
So if you like diagrams,

415
00:20:59,240 --> 00:21:01,240
that's a diagram showing how that works.

416
00:21:01,240 --> 00:21:05,320
Worker, doing get requests to RNG, posting to hasher,

417
00:21:05,320 --> 00:21:09,500
incrementing things in Redis, and then the web UI.

418
00:21:09,500 --> 00:21:14,500
Speaking of, so how does these things connect together?

419
00:21:15,440 --> 00:21:19,440
So we don't need to hard code IP addresses

420
00:21:19,440 --> 00:21:21,520
or service names in the code.

421
00:21:21,520 --> 00:21:24,320
We just put the name of the service itself.

422
00:21:24,320 --> 00:21:28,040
So when I want to connect to Redis, I put the name Redis.

423
00:21:28,040 --> 00:21:30,720
When I want to make a web request to RNG,

424
00:21:30,720 --> 00:21:34,320
I do a request to HTTP colon slash slash RNG,

425
00:21:34,320 --> 00:21:35,640
and so on and so on.

426
00:21:35,640 --> 00:21:38,120
So it's kind of one step beyond 12 factors,

427
00:21:38,120 --> 00:21:42,640
because I don't even need to change on variable variables

428
00:21:42,640 --> 00:21:44,760
when I deploy my application

429
00:21:44,760 --> 00:21:47,200
from one environment to another.

430
00:21:47,200 --> 00:21:48,820
How does that work?

431
00:21:48,820 --> 00:21:51,440
The container engine is going to expose

432
00:21:51,440 --> 00:21:56,200
a dynamic DNS service where the name Redis

433
00:21:56,200 --> 00:21:59,200
will correspond to the IP address of the Redis container,

434
00:21:59,200 --> 00:22:01,400
the name RNG corresponds to the IP address

435
00:22:01,400 --> 00:22:04,080
of the RNG container, and so on and so forth.

436
00:22:04,080 --> 00:22:06,800
And once we will deploy on a cluster,

437
00:22:06,800 --> 00:22:08,480
we will have a global DNS service

438
00:22:08,480 --> 00:22:11,120
that will feel exactly the same role

439
00:22:12,080 --> 00:22:14,800
so that the code keeps working the same way.

440
00:22:15,840 --> 00:22:16,680
Okay.

441
00:22:17,920 --> 00:22:19,800
So logs.

442
00:22:19,800 --> 00:22:21,120
I mentioned the web UI,

443
00:22:21,120 --> 00:22:23,360
so we're going to have a look at that.

444
00:22:23,360 --> 00:22:27,060
The web UI is on port 8000.

445
00:22:27,060 --> 00:22:29,280
We know that port number either

446
00:22:29,280 --> 00:22:31,760
from the Docker Compose YAML file.

447
00:22:32,640 --> 00:22:36,420
We see here ports 8000 colon 80.

448
00:22:37,380 --> 00:22:41,340
And I'm going to connect to that port 8000.

449
00:22:41,340 --> 00:22:45,140
So I'm going to put the IP address of node one.

450
00:22:48,780 --> 00:22:51,200
And then port 8000,

451
00:22:51,200 --> 00:22:55,300
and I should get a magnificent web interface

452
00:22:55,300 --> 00:22:57,340
with a blue graph here showing me

453
00:22:57,340 --> 00:23:00,740
how fast my DockerCoin app is running.

454
00:23:00,740 --> 00:23:04,440
So our goal will be to run that on Kubernetes,

455
00:23:04,440 --> 00:23:07,120
and then find ways to scale that up

456
00:23:07,120 --> 00:23:11,400
so that this counter goes higher than just four.

457
00:23:12,440 --> 00:23:13,280
Okay.

458
00:23:16,200 --> 00:23:19,040
So before moving on, we're going to shut down the app

459
00:23:19,040 --> 00:23:21,520
and clean up what we've done

460
00:23:21,520 --> 00:23:23,680
before we move on to Kubernetes.

461
00:23:23,680 --> 00:23:28,200
So I'm going to stop the app by hitting Control C.

462
00:23:30,160 --> 00:23:31,000
Okay.

463
00:23:34,840 --> 00:23:36,880
Let's see.

464
00:23:36,880 --> 00:23:37,920
There we go.

465
00:23:37,920 --> 00:23:40,680
Okay, I did hit Control C twice in a row,

466
00:23:40,680 --> 00:23:41,520
a little bit too fast,

467
00:23:41,520 --> 00:23:44,860
so all my containers did exit very quickly.

468
00:23:44,860 --> 00:23:47,800
But if you hit just Control C once,

469
00:23:47,800 --> 00:23:52,240
what you will see is that two containers stop immediately,

470
00:23:52,240 --> 00:23:54,800
and the three others take a little bit longer.

471
00:23:54,800 --> 00:23:56,580
I'm going to try to show that.

472
00:23:56,580 --> 00:23:58,400
Let me restart the application.

473
00:24:00,740 --> 00:24:03,600
I'm going to give it 10 seconds to start.

474
00:24:03,640 --> 00:24:04,680
Oh, that's great.

475
00:24:04,680 --> 00:24:07,020
I can introduce that as well.

476
00:24:07,020 --> 00:24:12,020
The worker is running in a kind of a crash loop mode.

477
00:24:12,200 --> 00:24:14,240
If there is any problem, any exception,

478
00:24:14,240 --> 00:24:16,800
any problem whatsoever, it just displays the exception,

479
00:24:16,800 --> 00:24:19,360
and then it waits 10 seconds and starts over.

480
00:24:19,360 --> 00:24:22,140
So that's why we saw this trace back in the beginning,

481
00:24:22,140 --> 00:24:25,080
because it tried to connect to the other services.

482
00:24:25,080 --> 00:24:28,400
The other services were not up yet, so it crashed.

483
00:24:28,400 --> 00:24:30,320
But when it tried again 10 seconds later,

484
00:24:30,320 --> 00:24:32,880
everything was up and running, and everything worked fine.

485
00:24:32,880 --> 00:24:35,120
So I hit Control C.

486
00:24:35,120 --> 00:24:37,960
I see, okay, that's a glitch.

487
00:24:37,960 --> 00:24:40,600
I probably hit Control C twice again.

488
00:24:40,600 --> 00:24:41,440
Please ignore that.

489
00:24:41,440 --> 00:24:46,320
But two services are handling the term signal properly.

490
00:24:46,320 --> 00:24:48,040
That should be Hasher and Redis,

491
00:24:48,040 --> 00:24:50,080
and so they exit right away.

492
00:24:50,080 --> 00:24:54,400
And the three other services are ignoring the signal,

493
00:24:54,400 --> 00:24:55,920
so they will keep running.

494
00:24:55,920 --> 00:24:57,080
And 10 seconds later,

495
00:24:57,080 --> 00:24:59,620
the container engine is going to kill them.

496
00:24:59,620 --> 00:25:02,440
This will become more relevant later

497
00:25:02,440 --> 00:25:04,840
when we will do rolling updates.

498
00:25:04,840 --> 00:25:05,680
Why?

499
00:25:05,680 --> 00:25:08,120
Because if I just have a couple of services,

500
00:25:09,040 --> 00:25:12,520
and I'm just like, for now, the only downside of this

501
00:25:12,520 --> 00:25:14,140
is that when I stop my app,

502
00:25:14,140 --> 00:25:16,600
I have to wait 10 seconds for the app to be done.

503
00:25:16,600 --> 00:25:18,040
So no big deal.

504
00:25:18,040 --> 00:25:20,120
Now imagine that I'm doing a rolling update

505
00:25:20,120 --> 00:25:22,420
where I have 100 containers,

506
00:25:22,420 --> 00:25:25,640
and I want to upgrade these containers,

507
00:25:25,640 --> 00:25:27,340
and I want to do that smoothly,

508
00:25:27,340 --> 00:25:29,140
so I don't want to shut down everything

509
00:25:29,140 --> 00:25:30,080
and restart everything.

510
00:25:30,080 --> 00:25:32,520
I want to take each container one by one,

511
00:25:32,520 --> 00:25:34,920
shut down, bring the new version up.

512
00:25:34,920 --> 00:25:36,360
Once the new version is up,

513
00:25:36,360 --> 00:25:38,480
I shut down the next one,

514
00:25:38,480 --> 00:25:40,880
bring up the new version, et cetera, et cetera, et cetera.

515
00:25:40,880 --> 00:25:43,200
And now imagine that this timeout,

516
00:25:43,200 --> 00:25:46,400
instead of being 10 seconds, it's one minute.

517
00:25:46,400 --> 00:25:50,160
So now instead of just waiting 10 seconds

518
00:25:50,160 --> 00:25:51,920
for my app to quit,

519
00:25:51,920 --> 00:25:55,880
I'm going to wait one minute for each container

520
00:25:55,880 --> 00:25:56,960
to be upgraded.

521
00:25:56,960 --> 00:25:58,680
If I have 100 containers,

522
00:25:58,680 --> 00:26:00,960
that means waiting 100 minutes,

523
00:26:00,960 --> 00:26:05,840
which means one and a half hours just for an upgrade,

524
00:26:05,840 --> 00:26:07,980
which would take just a couple of minutes

525
00:26:07,980 --> 00:26:11,480
if I had handled signals properly.

526
00:26:11,480 --> 00:26:12,320
Right.

527
00:26:12,320 --> 00:26:17,320
So, and the last thing we will do before moving on

528
00:26:17,920 --> 00:26:21,360
is Docker compose down.

529
00:26:21,360 --> 00:26:23,760
That's the opposite action of Docker compose up.

530
00:26:23,760 --> 00:26:25,520
So it's going to shut down things,

531
00:26:25,520 --> 00:26:27,240
but also remove the containers

532
00:26:27,240 --> 00:26:29,520
and clean up all the resources

533
00:26:29,520 --> 00:26:33,360
that were allocated for this demo app.

534
00:26:33,360 --> 00:26:34,200
Okay.

535
00:26:34,200 --> 00:26:37,760
We're back to a pristine condition

536
00:26:37,760 --> 00:26:40,920
and we can move to Kubernetes.

537
00:26:43,480 --> 00:26:46,160
Do you have questions on that app

538
00:26:46,160 --> 00:26:48,680
and on what we've done so far?

539
00:26:48,680 --> 00:26:49,520
Yes.

540
00:26:49,520 --> 00:26:51,840
What is the difference between the down command

541
00:26:51,840 --> 00:26:53,320
and the control C?

542
00:26:53,320 --> 00:26:56,080
What's the difference between control C and down?

543
00:26:56,080 --> 00:26:59,680
When I do control C, I'm just sending like the,

544
00:26:59,680 --> 00:27:03,840
control C is the equivalent of Docker compose stop.

545
00:27:03,840 --> 00:27:05,680
And then there is Docker compose kill,

546
00:27:05,680 --> 00:27:07,280
which goes like one step further

547
00:27:07,280 --> 00:27:10,080
and forcibly shut downs the container.

548
00:27:10,080 --> 00:27:12,320
And then I have Docker compose down,

549
00:27:12,320 --> 00:27:14,880
which is going to remove the container.

550
00:27:14,880 --> 00:27:16,640
So instead of just being stopped,

551
00:27:16,640 --> 00:27:18,840
they're going to be deleted.

552
00:27:18,840 --> 00:27:21,560
And resources like the network

553
00:27:21,560 --> 00:27:22,960
that was created for the application

554
00:27:23,600 --> 00:27:25,120
is going to be removed as well.

555
00:27:25,120 --> 00:27:28,920
And if I had volumes, databases and things like that,

556
00:27:28,920 --> 00:27:32,360
I would even be able to do Docker compose down dash V

557
00:27:32,360 --> 00:27:34,600
to say, and also remove the volumes,

558
00:27:34,600 --> 00:27:36,000
which is not done by default

559
00:27:36,000 --> 00:27:39,760
because don't want people to lose their databases

560
00:27:39,760 --> 00:27:41,360
with a little mistake like that.

561
00:27:43,080 --> 00:27:43,920
Okay.

562
00:27:45,760 --> 00:27:50,760
So I'm going to present the concepts of Kubernetes,

563
00:27:51,160 --> 00:27:52,760
just like a high level,

564
00:27:52,760 --> 00:27:54,840
what we need to know before we can use it.

565
00:27:54,840 --> 00:27:59,000
And then we are going to roll up our sleeves

566
00:27:59,000 --> 00:28:01,880
and actually use Kubernetes clusters.

567
00:28:02,760 --> 00:28:06,680
So if we look at a kind of Wikipedia style definition

568
00:28:06,680 --> 00:28:09,360
of Kubernetes, we learned that it's a container

569
00:28:09,360 --> 00:28:12,560
management system, which doesn't really teach us

570
00:28:12,560 --> 00:28:14,040
anything useful.

571
00:28:14,040 --> 00:28:19,040
So instead of going to give you a more functional definition,

572
00:28:20,040 --> 00:28:21,520
you know, like in Python,

573
00:28:21,520 --> 00:28:24,600
we like to do a lot of duck typing.

574
00:28:24,600 --> 00:28:28,000
So we define what a thing can do so that we know what it is.

575
00:28:28,000 --> 00:28:32,640
So in Kubernetes, that's a list of the things I can do.

576
00:28:32,640 --> 00:28:35,600
This slide is going to show the things we can do

577
00:28:35,600 --> 00:28:36,640
and will do today.

578
00:28:36,640 --> 00:28:39,480
And on the next slide, I have some more advanced concepts.

579
00:28:39,480 --> 00:28:42,440
So let's pretend for this example

580
00:28:42,440 --> 00:28:45,360
that I have an e-commerce website,

581
00:28:45,360 --> 00:28:48,600
and I decided to break that down into multiple tiers.

582
00:28:48,600 --> 00:28:52,320
So I have a web front ends, and then I have an API server,

583
00:28:52,320 --> 00:28:54,520
and then I have my database somewhere.

584
00:28:54,520 --> 00:28:57,440
So I want to deploy multiple containers

585
00:28:57,440 --> 00:28:59,920
running my API backend.

586
00:28:59,920 --> 00:29:02,400
And I want to set up an internal load balancer

587
00:29:02,400 --> 00:29:04,800
for these API containers.

588
00:29:04,800 --> 00:29:07,640
Then I want to deploy a bunch of containers

589
00:29:07,640 --> 00:29:09,000
for the web front end.

590
00:29:09,000 --> 00:29:11,160
And I want to add a load balancer,

591
00:29:11,160 --> 00:29:13,400
but a public one this time.

592
00:29:13,400 --> 00:29:16,320
And then since I'm doing e-commerce

593
00:29:16,320 --> 00:29:18,480
and it's either Black Friday or Christmas

594
00:29:18,480 --> 00:29:21,560
or whatever, I need much more containers

595
00:29:21,560 --> 00:29:23,400
to deal with the spike of traffic.

596
00:29:23,400 --> 00:29:26,360
So I want to be able really easily to scale up

597
00:29:26,360 --> 00:29:30,200
and add more containers, add more nodes to the cluster,

598
00:29:30,200 --> 00:29:31,560
add more containers.

599
00:29:31,560 --> 00:29:34,520
And then, of course, since my code was written by humans,

600
00:29:34,520 --> 00:29:37,040
and humans make mistakes sometimes,

601
00:29:37,040 --> 00:29:40,400
I notice I have bugs, and I need to roll out a new version.

602
00:29:40,400 --> 00:29:43,840
But of course, it happens right in the middle of peak traffic.

603
00:29:43,840 --> 00:29:46,920
And I don't want to put the whole website down

604
00:29:46,920 --> 00:29:48,160
while I do a maintenance.

605
00:29:48,160 --> 00:29:52,480
So I would like to do a smooth rolling update,

606
00:29:52,480 --> 00:29:55,360
so upgrade to a new version.

607
00:29:55,360 --> 00:29:58,680
But just like I was describing earlier,

608
00:29:58,680 --> 00:30:03,080
one container at a time without stopping everything else.

609
00:30:03,080 --> 00:30:07,040
So that's the kind of things that we're going to do today.

610
00:30:07,040 --> 00:30:09,880
More advanced features include auto scaling.

611
00:30:09,880 --> 00:30:14,040
So instead of me manually eyeballing traffic and metrics,

612
00:30:14,040 --> 00:30:16,920
I will want Kubernetes to decide by itself

613
00:30:16,920 --> 00:30:19,000
how much containers there should be

614
00:30:19,000 --> 00:30:23,000
so that I can attain my SLOs.

615
00:30:23,000 --> 00:30:25,160
I can do blue, green, and canary deployments,

616
00:30:25,160 --> 00:30:27,840
which are like several ways of deploying things.

617
00:30:27,840 --> 00:30:31,640
I can not only run web services and workers,

618
00:30:31,640 --> 00:30:33,920
but also batch jobs.

619
00:30:33,920 --> 00:30:38,000
I can also overcommit my cluster and use priorities

620
00:30:38,000 --> 00:30:40,560
so that if I have too much things running,

621
00:30:40,560 --> 00:30:43,760
it's going to be able to evict low priority workloads,

622
00:30:43,760 --> 00:30:46,920
so to keep the main production things running.

623
00:30:46,920 --> 00:30:50,880
I can run not only web services and stateless services,

624
00:30:50,880 --> 00:30:54,120
but also stateful services like databases,

625
00:30:54,120 --> 00:30:56,960
message queues, et cetera, and much, much more.

626
00:30:59,360 --> 00:31:01,480
So that was a kind of functional definition,

627
00:31:01,480 --> 00:31:03,240
what we can do with it.

628
00:31:03,240 --> 00:31:06,960
Now next, I want to show a little architecture diagram.

629
00:31:06,960 --> 00:31:11,160
So you probably have heard that Kubernetes was complicated

630
00:31:11,160 --> 00:31:13,400
and there were many concepts and objects.

631
00:31:13,440 --> 00:31:15,440
Don't worry, as you can see on this diagram,

632
00:31:15,440 --> 00:31:16,960
it's pretty simple.

633
00:31:16,960 --> 00:31:20,320
Okay, no, that one is just to check if you are awake.

634
00:31:20,320 --> 00:31:21,880
Thanks for laughing at my joke.

635
00:31:21,880 --> 00:31:24,360
This is the actual diagram,

636
00:31:24,360 --> 00:31:26,960
and I'm going to break that down for you.

637
00:31:26,960 --> 00:31:30,520
On the left-hand side here, the dark blue rectangle,

638
00:31:30,520 --> 00:31:31,880
that's the control plane.

639
00:31:31,880 --> 00:31:34,560
That's the brains of the cluster.

640
00:31:34,560 --> 00:31:37,360
Each time we want to interact with Kubernetes,

641
00:31:37,360 --> 00:31:38,880
that's going to be through that.

642
00:31:38,880 --> 00:31:41,280
We're going to talk to this control plane,

643
00:31:41,280 --> 00:31:43,520
and then it's going to figure it out

644
00:31:43,520 --> 00:31:45,560
with the rest of the cluster.

645
00:31:45,560 --> 00:31:47,400
Then we have the nodes,

646
00:31:47,400 --> 00:31:50,440
which will run our actual containers.

647
00:31:50,440 --> 00:31:51,800
So the actual containers

648
00:31:51,800 --> 00:31:55,480
are the little colored rectangles here.

649
00:31:55,480 --> 00:31:58,560
They run on a container runtime,

650
00:31:58,560 --> 00:32:02,280
which often will be darker, but could be anything else,

651
00:32:02,280 --> 00:32:04,320
because now there are many other options.

652
00:32:04,320 --> 00:32:06,920
That container runtime is going to be running

653
00:32:06,920 --> 00:32:09,840
on an operating system, which often will be Linux,

654
00:32:09,840 --> 00:32:12,560
but now you can also run Kubernetes on Windows,

655
00:32:12,560 --> 00:32:14,680
and that operating system is going to run

656
00:32:14,680 --> 00:32:17,480
on some infrastructure, which means servers,

657
00:32:17,480 --> 00:32:21,040
physical machines, virtual machines, anything goes.

658
00:32:21,040 --> 00:32:25,040
On each node, we have a couple of components

659
00:32:25,040 --> 00:32:29,640
that will kind of connect that node to the cluster,

660
00:32:29,640 --> 00:32:31,680
kubelet and kube-proxy.

661
00:32:31,680 --> 00:32:35,080
Kubelet is a little agent that sits here

662
00:32:35,080 --> 00:32:37,000
and connects to the control plane to say,

663
00:32:37,040 --> 00:32:39,840
hello, I'm node number 44.

664
00:32:39,840 --> 00:32:42,000
Do you have any containers I should be running?

665
00:32:42,000 --> 00:32:44,560
The control plane will say, oh, welcome, node 44.

666
00:32:44,560 --> 00:32:46,880
Please run this and this and this container.

667
00:32:46,880 --> 00:32:49,640
And then kubelet will start these containers

668
00:32:49,640 --> 00:32:53,240
and monitor them and continuously update the control plane

669
00:32:53,240 --> 00:32:55,040
with the status of these containers.

670
00:32:55,040 --> 00:32:56,480
So when you read about kubelet,

671
00:32:56,480 --> 00:32:59,120
that's the little agent that sits on each node

672
00:32:59,120 --> 00:33:01,560
and keeps the node in the cluster.

673
00:33:01,560 --> 00:33:03,040
There is also kube-proxy,

674
00:33:03,040 --> 00:33:05,480
which is one part of the network story,

675
00:33:05,920 --> 00:33:08,520
and I will tell more about that in a bit.

676
00:33:10,640 --> 00:33:14,840
The scary diagram was an actual Kubernetes diagram,

677
00:33:14,840 --> 00:33:19,840
but using like, there is a sign in the middle

678
00:33:21,280 --> 00:33:25,440
accessed with Linux LVM over multi-path

679
00:33:25,440 --> 00:33:26,640
and a few other features.

680
00:33:26,640 --> 00:33:28,520
So it's an actual Kubernetes diagram,

681
00:33:28,520 --> 00:33:30,280
but a fairly complex one indeed.

682
00:33:31,280 --> 00:33:34,280
Okay, so nodes.

683
00:33:34,280 --> 00:33:35,800
Now the control plane.

684
00:33:35,800 --> 00:33:39,400
In the control plane, we have a handful of components.

685
00:33:39,400 --> 00:33:42,440
The API server is a notable one

686
00:33:42,440 --> 00:33:46,400
because everything will go through the API server.

687
00:33:46,400 --> 00:33:48,120
When we will control the cluster,

688
00:33:48,120 --> 00:33:49,960
we will talk to the API server.

689
00:33:49,960 --> 00:33:52,440
Kubelet connects to the API server.

690
00:33:52,440 --> 00:33:54,360
The other components of the control plane

691
00:33:54,360 --> 00:33:56,720
also connect through the API server.

692
00:33:56,720 --> 00:33:59,560
So it's a little bit like a big connection point,

693
00:33:59,560 --> 00:34:02,840
a hub if you will, for everything else.

694
00:34:02,840 --> 00:34:06,720
Then we have a few other components in the control plane.

695
00:34:06,720 --> 00:34:09,840
We have each CD, which is the database

696
00:34:09,840 --> 00:34:13,520
storing all the important stuff of the cluster.

697
00:34:13,520 --> 00:34:16,960
And then we have controller manager and scheduler.

698
00:34:16,960 --> 00:34:19,680
What we should retain is that each CD

699
00:34:19,680 --> 00:34:22,800
is the only stateful thing in that control plane.

700
00:34:22,800 --> 00:34:25,480
It's the only thing storing information.

701
00:34:25,480 --> 00:34:27,680
The other components are stateless,

702
00:34:27,680 --> 00:34:31,800
meaning that I could destroy them and recreate them

703
00:34:31,800 --> 00:34:33,000
and everything would be fine.

704
00:34:33,000 --> 00:34:36,440
However, if I lose each CD, I'm toast.

705
00:34:36,440 --> 00:34:38,680
As for the other components,

706
00:34:38,680 --> 00:34:41,680
we can ignore their exact rule for now.

707
00:34:41,680 --> 00:34:42,520
It's fine.

708
00:34:43,360 --> 00:34:44,200
Okay.

709
00:34:45,840 --> 00:34:49,120
So when we interact with the cluster,

710
00:34:49,120 --> 00:34:51,360
we do that through the Kubernetes API,

711
00:34:51,360 --> 00:34:54,360
which is a mostly REST API.

712
00:34:54,360 --> 00:34:58,520
And it allows us to do the classic

713
00:34:58,520 --> 00:35:02,640
CRUD, create, read, update, delete on resources.

714
00:35:02,640 --> 00:35:06,400
The Kubernetes resources are things like nodes,

715
00:35:06,400 --> 00:35:08,320
like the machines of the cluster,

716
00:35:08,320 --> 00:35:10,640
pods, which are groups of containers,

717
00:35:10,640 --> 00:35:12,440
more on that in just a minute,

718
00:35:12,440 --> 00:35:15,080
services, which are network constructs,

719
00:35:15,080 --> 00:35:16,960
we'll also talk more about this,

720
00:35:16,960 --> 00:35:18,800
and many, many, many others.

721
00:35:18,800 --> 00:35:23,040
So in a way, the Kubernetes API server

722
00:35:23,040 --> 00:35:27,480
acts as an intermediary between us

723
00:35:27,480 --> 00:35:31,320
and a bunch of objects stored,

724
00:35:31,320 --> 00:35:36,200
persisted in each CD and with strong typing.

725
00:35:36,200 --> 00:35:38,920
So a node is fairly easy to understand.

726
00:35:38,920 --> 00:35:41,840
It's just a machine part of the cluster.

727
00:35:41,840 --> 00:35:44,200
Now a pod is a little bit trickier.

728
00:35:44,200 --> 00:35:48,760
A pod is a group of containers that work together

729
00:35:48,760 --> 00:35:53,440
and are always scheduled, running on the same node.

730
00:35:53,440 --> 00:35:57,520
You can't have a pod straddling across multiple nodes.

731
00:35:57,520 --> 00:36:01,360
And when we scale things up, we scale pods.

732
00:36:01,360 --> 00:36:03,840
We don't scale inside pods.

733
00:36:03,840 --> 00:36:06,560
So what I mean by that is look at this example

734
00:36:06,560 --> 00:36:09,160
where I have Nginx and a logger.

735
00:36:09,160 --> 00:36:11,640
So the assumption here is that there is Nginx

736
00:36:11,640 --> 00:36:14,840
serving web traffic, writing log files,

737
00:36:14,840 --> 00:36:17,320
and next to it, there is a logger

738
00:36:17,320 --> 00:36:21,760
that gets these log files and sends them to maybe Splunk

739
00:36:21,760 --> 00:36:24,640
or Stackdriver or CloudWatch or Elasticsearch

740
00:36:24,640 --> 00:36:27,680
or whatever I have to manage my logs.

741
00:36:27,680 --> 00:36:29,720
If I need to scale up,

742
00:36:29,720 --> 00:36:34,320
I'm not going to add more Nginx inside that pod.

743
00:36:34,320 --> 00:36:37,720
I'm going to add more pods like this,

744
00:36:37,720 --> 00:36:42,640
because if I put multiple Nginx containers inside that pod,

745
00:36:42,640 --> 00:36:46,240
since the pod is always on one single node,

746
00:36:46,240 --> 00:36:50,240
I'm just ending up with maybe five Nginx on the same machine.

747
00:36:50,240 --> 00:36:52,280
And usually that doesn't really help me.

748
00:36:52,280 --> 00:36:57,280
So what I want is multiple pods with the same purpose.

749
00:36:58,280 --> 00:37:00,040
All of the content, yeah?

750
00:37:00,040 --> 00:37:02,880
So it's always the node to pod relationship

751
00:37:02,880 --> 00:37:04,240
is always one to one?

752
00:37:04,240 --> 00:37:06,880
Is the node to pod relationship one to one?

753
00:37:06,880 --> 00:37:10,920
No, the one node can have as many pods as we want.

754
00:37:11,840 --> 00:37:16,840
A pod is a little collection of processes of containers

755
00:37:17,640 --> 00:37:19,800
that work together.

756
00:37:19,800 --> 00:37:24,120
So for instance, if I'm using a WSGI app,

757
00:37:24,120 --> 00:37:27,200
but I also want to have maybe Apache or Nginx

758
00:37:27,200 --> 00:37:28,960
to serve static content,

759
00:37:28,960 --> 00:37:31,800
I might decide to have in a single pod,

760
00:37:31,800 --> 00:37:36,800
let's say Nginx and UWSGI together in the same pod.

761
00:37:36,960 --> 00:37:38,080
And then when I scale up,

762
00:37:38,080 --> 00:37:39,920
I end up with multiple pods like that.

763
00:37:39,920 --> 00:37:41,000
But on a given machine,

764
00:37:41,000 --> 00:37:44,920
I might have this Nginx UWSGI pod

765
00:37:44,920 --> 00:37:47,240
and also a PostgreSQL pod

766
00:37:47,240 --> 00:37:49,800
and also a pod for a completely different application,

767
00:37:49,800 --> 00:37:51,400
et cetera, et cetera, et cetera.

768
00:37:52,800 --> 00:37:57,400
All the containers in a given pod have the same IP address.

769
00:37:57,400 --> 00:37:59,960
So if you've done some networking,

770
00:37:59,960 --> 00:38:02,800
at first this might seem extremely upsetting

771
00:38:02,800 --> 00:38:04,920
because usually when we have multiple things

772
00:38:04,920 --> 00:38:06,440
with the same IP address,

773
00:38:06,440 --> 00:38:09,800
we get IP address conflict and one thing works,

774
00:38:10,640 --> 00:38:11,480
the other doesn't,

775
00:38:11,480 --> 00:38:14,040
and sometimes it randomly flips from one to the other.

776
00:38:14,040 --> 00:38:16,600
So here you can imagine if it helps

777
00:38:16,600 --> 00:38:21,040
that each pod is a little bit like a micro virtual machine

778
00:38:21,040 --> 00:38:23,200
in which we have multiple things running.

779
00:38:23,200 --> 00:38:25,120
So the pod has an IP address

780
00:38:25,120 --> 00:38:29,200
and everything in that pod has the same IP address

781
00:38:29,200 --> 00:38:32,440
and everything in that pod can communicate

782
00:38:32,440 --> 00:38:34,980
over local host, for instance.

783
00:38:35,960 --> 00:38:36,800
Right.

784
00:38:36,800 --> 00:38:38,400
So what about side cars?

785
00:38:38,400 --> 00:38:41,400
Yeah, you will often see the side cars in Kubernetes.

786
00:38:41,400 --> 00:38:45,400
A side car is a container in a pod

787
00:38:45,400 --> 00:38:48,920
that provides some extra feature, some extra little thing.

788
00:38:48,920 --> 00:38:52,160
For instance, here we would probably call that logger,

789
00:38:52,160 --> 00:38:55,000
a logger side car or logging side car.

790
00:38:55,000 --> 00:38:57,960
When we use service meshes

791
00:38:57,960 --> 00:38:59,600
without diving into too much details,

792
00:38:59,600 --> 00:39:03,720
but we end up with a side car whose purpose is to

793
00:39:04,640 --> 00:39:09,640
intercept the network connections that your code is making

794
00:39:09,640 --> 00:39:12,560
and route them to the right service,

795
00:39:12,560 --> 00:39:15,440
add some instrumentation, et cetera, et cetera.

796
00:39:15,440 --> 00:39:18,840
So we could have side cars for pretty much anything.

797
00:39:18,840 --> 00:39:22,360
It just means a container that we add into a pod

798
00:39:22,360 --> 00:39:25,480
to add some extra features in that pod.

799
00:39:26,800 --> 00:39:27,640
Right.

800
00:39:29,400 --> 00:39:32,160
Other questions on these concepts?

801
00:39:32,160 --> 00:39:33,000
Yes.

802
00:39:33,720 --> 00:39:35,920
What would happen if master dies?

803
00:39:35,920 --> 00:39:38,540
What happens if the control plane dies?

804
00:39:39,800 --> 00:39:43,640
So when the control plane dies,

805
00:39:43,640 --> 00:39:44,920
everything else keeps running.

806
00:39:44,920 --> 00:39:47,460
So all my containers, my pods, my applications,

807
00:39:47,460 --> 00:39:49,860
everything keeps running and everything is fine.

808
00:39:50,760 --> 00:39:53,440
And everything is designed to kind of

809
00:39:53,440 --> 00:39:55,480
keep trying and reconnecting.

810
00:39:55,480 --> 00:40:00,480
So for instance, if I disconnect or reboot my control plane,

811
00:40:00,920 --> 00:40:02,960
the cubelets on each node are just going,

812
00:40:03,120 --> 00:40:04,640
they're going to notice of course,

813
00:40:04,640 --> 00:40:07,440
but they're not going to terminate containers or anything.

814
00:40:07,440 --> 00:40:10,320
They're just going to try again and try again and try again.

815
00:40:10,320 --> 00:40:12,640
And once the connection is up again,

816
00:40:12,640 --> 00:40:14,380
they're going to resynchronize.

817
00:40:14,380 --> 00:40:16,720
So they're going to say, I'm not 44.

818
00:40:16,720 --> 00:40:18,320
I'm currently running these containers.

819
00:40:18,320 --> 00:40:19,720
What should I be doing now?

820
00:40:19,720 --> 00:40:24,080
And if my each CD hasn't been corrupted

821
00:40:24,080 --> 00:40:25,960
or wiped out or et cetera,

822
00:40:25,960 --> 00:40:28,920
the state on the control plane is still the same.

823
00:40:28,920 --> 00:40:32,560
So the resynchronization is fine.

824
00:40:32,560 --> 00:40:35,880
Now on the other hand, if we wipe out each CD for instance,

825
00:40:35,880 --> 00:40:37,520
and the control plane comes back,

826
00:40:37,520 --> 00:40:40,400
when the node reconnects is going to be extremely confused

827
00:40:40,400 --> 00:40:44,240
and undefined things, bad things might happen.

828
00:40:44,240 --> 00:40:46,320
It will depend on the details, but yeah.

829
00:40:48,400 --> 00:40:49,920
So that's, yeah.

830
00:40:49,920 --> 00:40:52,600
Of course there can be some specific things

831
00:40:52,600 --> 00:40:54,240
that will rely on the control plane.

832
00:40:54,240 --> 00:40:55,640
Let me give you an example.

833
00:40:55,640 --> 00:40:58,040
Let's say I have a bunch of workers

834
00:40:58,040 --> 00:41:01,720
that are pulling messages from a Kafka queue

835
00:41:01,760 --> 00:41:06,560
and I want to dynamically add workers

836
00:41:06,560 --> 00:41:08,400
if the Kafka queue grows too big.

837
00:41:08,400 --> 00:41:10,660
Like I say, oh, if I have more than 1000 messages

838
00:41:10,660 --> 00:41:12,840
in the Kafka queue, I want to add workers.

839
00:41:12,840 --> 00:41:16,400
And I add workers by talking to the control plane.

840
00:41:16,400 --> 00:41:19,000
So of course in that case, if the control plane is down,

841
00:41:19,000 --> 00:41:22,800
I'm not able to add workers until it's back up.

842
00:41:29,640 --> 00:41:30,780
Does it work as a cluster?

843
00:41:30,780 --> 00:41:32,700
So is the question like, how do we ensure

844
00:41:32,700 --> 00:41:34,820
the availability of the control plane?

845
00:41:34,820 --> 00:41:38,120
Right, so if we look at this diagram over there,

846
00:41:39,060 --> 00:41:43,080
the each CD itself is designed to be highly available

847
00:41:43,080 --> 00:41:46,100
in the sense that we can set it up as a cluster

848
00:41:46,100 --> 00:41:48,740
with typically three or five nodes.

849
00:41:48,740 --> 00:41:50,700
And when we say highly available,

850
00:41:50,700 --> 00:41:54,420
it means that, you know, it's not like, for instance,

851
00:41:54,420 --> 00:41:57,620
a replicated SQL database where I have a primary

852
00:41:57,620 --> 00:42:00,540
and a replica and when the primary goes down,

853
00:42:01,300 --> 00:42:04,100
I still have to do some operations to switch traffic

854
00:42:04,100 --> 00:42:07,780
to the replica, which can be almost automated sometimes

855
00:42:07,780 --> 00:42:10,420
with something like PG bouncer or MySQL proxy,

856
00:42:10,420 --> 00:42:11,580
et cetera, et cetera.

857
00:42:11,580 --> 00:42:16,260
In the case of each CD, high availability was built in

858
00:42:16,260 --> 00:42:18,340
like designed from the very beginning.

859
00:42:18,340 --> 00:42:21,820
So I have like my three or five each CD nodes

860
00:42:21,820 --> 00:42:25,760
and I can issue requests versus any of them.

861
00:42:26,640 --> 00:42:31,640
And if any node goes down, everything else keeps working

862
00:42:31,720 --> 00:42:34,140
like without a single interruption.

863
00:42:35,400 --> 00:42:38,280
And if I'm making a request on a node

864
00:42:38,280 --> 00:42:40,920
and at that very moment the node blows up,

865
00:42:40,920 --> 00:42:43,580
I will get an error and I know I just need to repeat

866
00:42:43,580 --> 00:42:46,080
my request on another node and it will be fine.

867
00:42:46,080 --> 00:42:49,640
So that's how it's designed for high availability.

868
00:42:49,640 --> 00:42:52,560
The other components are stateless

869
00:42:52,560 --> 00:42:55,880
and are also designed to be replicated.

870
00:42:55,880 --> 00:43:00,880
So the API server can be scaled without anything special

871
00:43:01,000 --> 00:43:03,360
and then I can talk to any API server

872
00:43:03,360 --> 00:43:08,280
and since everything goes to each CD behind the API server,

873
00:43:08,280 --> 00:43:09,120
it will be fine.

874
00:43:09,120 --> 00:43:11,200
And as for the two other components,

875
00:43:11,200 --> 00:43:13,040
controller manager and scheduler,

876
00:43:13,040 --> 00:43:16,400
they are using an active passive replication mode

877
00:43:16,400 --> 00:43:20,400
where one of them is going to acquire a lease

878
00:43:20,400 --> 00:43:23,640
on the API server and say, hey, I'm the active scheduler now

879
00:43:23,640 --> 00:43:26,560
and the other schedulers are going to stand by

880
00:43:26,560 --> 00:43:30,080
and until that lease expires and then are you like,

881
00:43:30,080 --> 00:43:33,520
oh, the lease expired, which means the active

882
00:43:33,520 --> 00:43:37,320
went down offline so I'm going to take over.

883
00:43:37,320 --> 00:43:39,640
So that's how we achieve high availability.

884
00:43:41,060 --> 00:43:41,900
Yeah.

885
00:43:46,800 --> 00:43:49,240
Who's responsible for making sure that each CD

886
00:43:49,240 --> 00:43:51,560
stays available, you mean?

887
00:43:51,560 --> 00:43:54,120
Like if I'm running my own cluster,

888
00:43:54,120 --> 00:43:59,120
like so there are a couple of ways

889
00:43:59,960 --> 00:44:01,240
I could answer that question.

890
00:44:01,240 --> 00:44:04,000
Like if we're talking about, hey, I'm a developer

891
00:44:04,000 --> 00:44:05,960
and I'm using a Kubernetes cluster,

892
00:44:05,960 --> 00:44:09,420
whose responsibility is it to maintain all that stuff?

893
00:44:09,420 --> 00:44:11,960
Then the answer is not us.

894
00:44:11,960 --> 00:44:16,660
Generally, we will either use managed Kubernetes

895
00:44:16,660 --> 00:44:19,700
or we will have folks responsible for setting that up

896
00:44:19,700 --> 00:44:20,720
on our machine.

897
00:44:20,720 --> 00:44:23,580
So as a user of Kubernetes,

898
00:44:23,580 --> 00:44:25,940
like deploying my apps on Kubernetes,

899
00:44:25,940 --> 00:44:29,100
it's great that I know what's behind the scenes

900
00:44:29,100 --> 00:44:33,460
but I won't have to operate and maintain and et cetera,

901
00:44:33,460 --> 00:44:36,520
each CD to deploy my applications.

902
00:44:41,760 --> 00:44:42,600
Right.

903
00:44:44,100 --> 00:44:45,100
More questions?

904
00:44:47,060 --> 00:44:48,340
No, okay.

905
00:44:48,340 --> 00:44:53,340
So Kubernetes puts on phases on being declarative.

906
00:44:53,880 --> 00:44:58,880
So this means that I'm giving an example with tea,

907
00:44:59,040 --> 00:45:02,840
for instance, where the declarative mode is when I say,

908
00:45:02,840 --> 00:45:06,220
I would like some tea and then some things happen

909
00:45:06,220 --> 00:45:08,900
and boom, tea happens.

910
00:45:08,900 --> 00:45:11,100
The imperative mode is I give a recipe,

911
00:45:11,100 --> 00:45:13,020
which is like step one, boil water,

912
00:45:13,020 --> 00:45:15,840
step two, add tea leaves, step three,

913
00:45:17,020 --> 00:45:19,500
power the tea in a cup.

914
00:45:19,500 --> 00:45:22,400
So at first, we could think that the declarative mode

915
00:45:22,400 --> 00:45:25,500
is simpler because I just say I want some tea

916
00:45:25,500 --> 00:45:28,620
but it requires to know how to brew tea.

917
00:45:28,620 --> 00:45:30,860
And then I need to explicit like, okay,

918
00:45:30,860 --> 00:45:34,580
so a cup of tea is an infusion of tea leaves

919
00:45:34,580 --> 00:45:35,740
that you put in a cup.

920
00:45:35,740 --> 00:45:36,760
What's an infusion?

921
00:45:36,760 --> 00:45:40,120
Well, an infusion is when you put the thing

922
00:45:40,120 --> 00:45:42,440
that you want to infuse like the tea leaves

923
00:45:42,440 --> 00:45:44,360
in hot water for a moment.

924
00:45:44,360 --> 00:45:45,880
How do you obtain hot water?

925
00:45:45,880 --> 00:45:49,960
By warming up cold water, et cetera, et cetera.

926
00:45:49,960 --> 00:45:53,240
So now, once we explicit that,

927
00:45:53,240 --> 00:45:56,280
the imperative version is way simpler

928
00:45:56,280 --> 00:46:00,160
because it's just like step one, step two, step three, done.

929
00:46:00,160 --> 00:46:02,920
However, if we have an interruption,

930
00:46:02,920 --> 00:46:05,640
if like in the middle of brewing tea,

931
00:46:05,640 --> 00:46:07,520
I'm getting a phone call or somebody asks me

932
00:46:07,520 --> 00:46:11,640
to do something else, with the imperative version,

933
00:46:11,640 --> 00:46:15,120
I have to start over because maybe I was interrupted

934
00:46:15,120 --> 00:46:16,800
for 10 seconds and it's fine.

935
00:46:16,800 --> 00:46:19,040
Maybe I was interrupted for one hour

936
00:46:19,040 --> 00:46:22,520
and if my water was boiling, now it's completely evaporated

937
00:46:22,520 --> 00:46:26,440
or maybe it's cold again or maybe I had put the water

938
00:46:26,440 --> 00:46:28,600
on the tea leaves and now after one hour,

939
00:46:28,600 --> 00:46:31,680
my tea is completely undrinkable, et cetera, et cetera.

940
00:46:31,680 --> 00:46:33,980
So my only option is to start over.

941
00:46:33,980 --> 00:46:36,740
However, with the declarative system,

942
00:46:36,740 --> 00:46:40,340
if I'm smart enough, if I realize like when walking

943
00:46:40,340 --> 00:46:43,780
into the kitchen, okay, I already have hot water

944
00:46:43,780 --> 00:46:48,620
so I can skip that step or okay, we already have tea leaves

945
00:46:48,620 --> 00:46:52,460
in some hot water so I don't need to do that again.

946
00:46:52,460 --> 00:46:57,380
So the declarative system will be able to recover

947
00:46:57,380 --> 00:47:02,300
from problems more easily, but it implies

948
00:47:02,300 --> 00:47:04,660
that I'm able to observe the system.

949
00:47:04,660 --> 00:47:06,940
I need to be able to figure out what I have

950
00:47:06,940 --> 00:47:11,940
and to compute a diff between my recipe, my instructions

951
00:47:12,060 --> 00:47:14,260
and what I already have.

952
00:47:14,260 --> 00:47:17,140
So Kubernetes is a declarative system,

953
00:47:17,140 --> 00:47:21,440
which means in concrete terms that I cannot tell Kubernetes

954
00:47:21,440 --> 00:47:22,780
run that container.

955
00:47:22,780 --> 00:47:25,980
What I can do is write a spec that says,

956
00:47:25,980 --> 00:47:28,180
I would like that container to be running.

957
00:47:28,180 --> 00:47:31,100
Then I send the spec to the API server

958
00:47:31,100 --> 00:47:34,060
and the API server then works on it

959
00:47:35,100 --> 00:47:37,780
and makes that thing happen.

960
00:47:39,300 --> 00:47:42,320
So when I do an API request to Kubernetes,

961
00:47:42,320 --> 00:47:46,860
what I'm really doing is storing something in each CD

962
00:47:46,860 --> 00:47:51,020
and then I will have a bunch of control loops,

963
00:47:51,020 --> 00:47:53,980
the controller manager I was mentioning earlier

964
00:47:53,980 --> 00:47:57,680
who are going to wake up and work on these specs.

965
00:47:59,160 --> 00:48:00,860
So that's how it works.

966
00:48:01,500 --> 00:48:06,500
I have a little diagram I would like to show.

967
00:48:07,860 --> 00:48:09,460
That's in the slide themselves

968
00:48:09,460 --> 00:48:11,500
because I'm still working on that.

969
00:48:11,500 --> 00:48:16,500
But will that work?

970
00:48:17,220 --> 00:48:18,220
Yes, wonderful.

971
00:48:19,420 --> 00:48:24,420
So this comes from the Learn Kubernetes website,

972
00:48:25,740 --> 00:48:30,700
but I've slightly adapted that to add some extra steps.

973
00:48:31,700 --> 00:48:34,980
So this is like the state of my cluster at rest,

974
00:48:34,980 --> 00:48:36,820
no pun intended.

975
00:48:36,820 --> 00:48:39,940
And the first thing I do is I connect to the cluster

976
00:48:39,940 --> 00:48:43,460
and I say, hey, I would like to create a bunch of pods

977
00:48:43,460 --> 00:48:44,980
running my application.

978
00:48:44,980 --> 00:48:48,420
So by doing that, I'm going to store,

979
00:48:48,420 --> 00:48:50,180
for instance, a replica set.

980
00:48:50,180 --> 00:48:51,740
We're going to talk about that in a while,

981
00:48:51,740 --> 00:48:56,740
but I'm storing a replica set specification in each CD.

982
00:48:57,260 --> 00:49:02,100
And then I can go away and I'm done.

983
00:49:02,100 --> 00:49:05,340
Like the API tells me I got it,

984
00:49:05,340 --> 00:49:07,860
which doesn't mean that my containers are running,

985
00:49:07,860 --> 00:49:09,040
but it means I got it,

986
00:49:09,040 --> 00:49:12,860
now things are going to happen in the background.

987
00:49:12,860 --> 00:49:14,420
Then there is a controller manager

988
00:49:14,420 --> 00:49:16,900
that is observing resources and it's like,

989
00:49:16,900 --> 00:49:18,140
oh, there is a new replica set,

990
00:49:18,140 --> 00:49:19,860
we need to do something about that.

991
00:49:19,860 --> 00:49:21,520
It's like, hmm, what is this replica set?

992
00:49:21,520 --> 00:49:24,580
Oh, you want three pods like this?

993
00:49:24,580 --> 00:49:28,100
Fine, I'm going to create three pods

994
00:49:28,100 --> 00:49:30,860
in the storage backend.

995
00:49:30,860 --> 00:49:34,060
These pods will be in pending state,

996
00:49:34,060 --> 00:49:36,860
which means that we haven't decided yet

997
00:49:36,860 --> 00:49:38,740
where they should run.

998
00:49:38,740 --> 00:49:41,060
So at this point, the scheduler,

999
00:49:41,060 --> 00:49:44,020
which I also like to call the grand master of Tetris,

1000
00:49:44,020 --> 00:49:48,700
is going to wake up and see, okay, we have pending pods,

1001
00:49:48,700 --> 00:49:51,300
we need to decide where to run them.

1002
00:49:51,300 --> 00:49:53,020
In that case, we have only one node,

1003
00:49:53,100 --> 00:49:55,660
so it's an extremely difficult decision.

1004
00:49:55,660 --> 00:49:59,740
But after assigning the pods to the node,

1005
00:49:59,740 --> 00:50:01,540
they are now scheduled,

1006
00:50:01,540 --> 00:50:03,580
and then the scheduler goes back to sleep.

1007
00:50:03,580 --> 00:50:06,940
And then after that, the sync loop

1008
00:50:06,940 --> 00:50:09,740
between the control plane and Kubelet kicks in

1009
00:50:09,740 --> 00:50:13,460
and says, hey, node number 44,

1010
00:50:13,460 --> 00:50:15,340
because I like node 44,

1011
00:50:15,340 --> 00:50:16,940
you should be running these pods.

1012
00:50:16,940 --> 00:50:21,220
So Kubelet is going to pull the image from that container

1013
00:50:21,220 --> 00:50:24,820
and run these pods, and at this point,

1014
00:50:24,820 --> 00:50:27,900
everything is up and running, and I'm done.

1015
00:50:29,300 --> 00:50:32,140
So that's how it works behind the scenes.

1016
00:50:33,340 --> 00:50:34,180
Questions?

1017
00:50:34,180 --> 00:50:35,020
Yeah?

1018
00:50:35,020 --> 00:50:37,060
So once it's running, it goes out of schedule state

1019
00:50:37,060 --> 00:50:38,940
into a running state?

1020
00:50:38,940 --> 00:50:40,900
Oh, yes, once it's running,

1021
00:50:40,900 --> 00:50:45,340
we will see the lifecycle of the pod

1022
00:50:45,340 --> 00:50:47,380
going from like pending,

1023
00:50:47,380 --> 00:50:49,940
then it will go to like container creating,

1024
00:50:49,940 --> 00:50:52,740
which really means it's pulling the image

1025
00:50:52,740 --> 00:50:54,860
and creating the network, et cetera,

1026
00:50:54,860 --> 00:50:56,820
and then it will go to running,

1027
00:50:56,820 --> 00:51:00,860
and then at some, either it stays running forever,

1028
00:51:00,860 --> 00:51:04,660
or it will go to completed or failed state,

1029
00:51:04,660 --> 00:51:08,420
depending on the spec and how it exits.

1030
00:51:10,860 --> 00:51:12,020
And if you like details,

1031
00:51:12,020 --> 00:51:15,980
once a pod is completed or failed, it can't come back up.

1032
00:51:15,980 --> 00:51:19,660
So it's like a one-way thing, it's not a cycle.

1033
00:51:20,340 --> 00:51:23,020
So if the pod dies and we need a new pod,

1034
00:51:23,020 --> 00:51:25,980
then we get a new pod, we don't resurrect that pod.

1035
00:51:25,980 --> 00:51:26,820
Yeah?

1036
00:51:26,820 --> 00:51:28,820
Oh, yeah, yeah.

1037
00:51:28,820 --> 00:51:31,660
So which thing come first, readiness or...

1038
00:51:34,700 --> 00:51:36,500
Readiness and liveness?

1039
00:51:36,500 --> 00:51:39,500
Oh, which one comes first between readiness and liveness?

1040
00:51:39,500 --> 00:51:42,660
So readiness and liveness are two health checks

1041
00:51:42,660 --> 00:51:43,820
in Kubernetes.

1042
00:51:43,820 --> 00:51:45,660
I'm going to give you a really short answer

1043
00:51:45,660 --> 00:51:47,380
because it's a little bit more advanced,

1044
00:51:47,380 --> 00:51:49,980
but there are two health checks that we can run

1045
00:51:49,980 --> 00:51:54,580
on containers within the pod, liveness and readiness.

1046
00:51:54,580 --> 00:51:56,340
And to remember which is which,

1047
00:51:56,340 --> 00:51:59,340
I think about the fact that for liveness,

1048
00:51:59,340 --> 00:52:03,300
once something is dead, you can't bring it back to life.

1049
00:52:03,300 --> 00:52:05,700
So there is no undead pods or whatever.

1050
00:52:05,700 --> 00:52:08,380
So the liveness check, once it fails,

1051
00:52:08,380 --> 00:52:13,380
that means, okay, that container is like broken beyond repair

1052
00:52:13,620 --> 00:52:16,900
and we need to restart a new thing.

1053
00:52:16,900 --> 00:52:19,940
However, the readiness means like, are you ready or not?

1054
00:52:19,940 --> 00:52:20,900
And if you're not ready,

1055
00:52:20,900 --> 00:52:23,140
then eventually you might become ready again.

1056
00:52:23,140 --> 00:52:26,200
So the readiness check can go like,

1057
00:52:26,200 --> 00:52:28,700
ready, not ready, ready, not ready.

1058
00:52:28,700 --> 00:52:33,700
So the liveness probe is going to be used to assess,

1059
00:52:34,740 --> 00:52:39,460
okay, that container has a problem, we need to restart it.

1060
00:52:39,460 --> 00:52:41,780
While the readiness check will be used to say,

1061
00:52:41,780 --> 00:52:45,420
okay, that container can't handle traffic yet,

1062
00:52:45,420 --> 00:52:47,340
so we are going to remove it

1063
00:52:47,340 --> 00:52:50,320
from the load balancer rotation, for instance.

1064
00:52:51,660 --> 00:52:52,500
Yeah.

1065
00:52:54,500 --> 00:52:55,780
Oh, what's a replica set?

1066
00:52:55,780 --> 00:52:58,140
I'm going to talk more with the replica set

1067
00:52:58,140 --> 00:53:01,620
in just a moment, but for now we can say,

1068
00:53:01,620 --> 00:53:04,380
okay, it's a thing to create a bunch of pods,

1069
00:53:04,380 --> 00:53:06,460
but I will give more details shortly.

1070
00:53:07,780 --> 00:53:08,620
All right.

1071
00:53:09,940 --> 00:53:13,280
So one more thing, the network model.

1072
00:53:14,240 --> 00:53:16,480
So this is just going to,

1073
00:53:16,480 --> 00:53:18,620
I'm going to be relatively quick on this.

1074
00:53:19,760 --> 00:53:22,000
The short version is that on Kubernetes,

1075
00:53:22,000 --> 00:53:24,680
we have one big flat network.

1076
00:53:24,680 --> 00:53:28,300
So we have nodes to the machines and the pods,

1077
00:53:28,300 --> 00:53:31,700
the containers, everything is on one big flat network,

1078
00:53:31,700 --> 00:53:35,920
and everything can ping everything directly.

1079
00:53:35,920 --> 00:53:38,680
We don't need to map ports, we don't need to,

1080
00:53:40,680 --> 00:53:42,020
it's pretty simple.

1081
00:53:43,840 --> 00:53:45,840
And there is no NAT.

1082
00:53:45,840 --> 00:53:48,080
And one little implementation detail is that

1083
00:53:48,080 --> 00:53:50,760
the IP addresses of the pods

1084
00:53:50,760 --> 00:53:53,760
are assigned by the network layer.

1085
00:53:53,760 --> 00:53:56,220
I'm not going to say I want this IP address

1086
00:53:56,220 --> 00:53:58,560
or this IP address, this is going to be assigned

1087
00:53:58,560 --> 00:54:00,120
by the network implementation.

1088
00:54:00,120 --> 00:54:03,880
And that's all there is in the spec,

1089
00:54:03,880 --> 00:54:07,100
then everything else, all the little details

1090
00:54:07,100 --> 00:54:08,980
can be implementation dependent.

1091
00:54:08,980 --> 00:54:12,280
So it doesn't mandate to use any particular protocol

1092
00:54:12,280 --> 00:54:15,560
or a capsulation or et cetera, et cetera,

1093
00:54:15,560 --> 00:54:19,600
which means that on the upside,

1094
00:54:19,600 --> 00:54:21,520
that's pretty simple to reason with.

1095
00:54:21,520 --> 00:54:25,860
I don't have to think about, again, mapping ports.

1096
00:54:25,860 --> 00:54:29,600
I don't have to think about introducing a new protocol

1097
00:54:29,600 --> 00:54:33,420
like VXLAN or et cetera in my network.

1098
00:54:35,160 --> 00:54:39,080
I don't need to worry about porting IP addresses

1099
00:54:39,080 --> 00:54:41,680
from one node to another.

1100
00:54:41,680 --> 00:54:46,120
The network layer can decide all these details by itself.

1101
00:54:46,120 --> 00:54:47,280
So that's great.

1102
00:54:47,280 --> 00:54:50,720
So that means that we can have many implementations

1103
00:54:50,720 --> 00:54:52,920
of the network on Kubernetes,

1104
00:54:52,920 --> 00:54:56,800
which is both great and not so great when you get started,

1105
00:54:56,800 --> 00:54:58,800
because when you set up your cluster,

1106
00:54:58,800 --> 00:55:01,240
you reach the point in documentation that says,

1107
00:55:01,240 --> 00:55:04,760
now pick one of these 15 network implementations

1108
00:55:04,760 --> 00:55:05,600
before you move on.

1109
00:55:05,600 --> 00:55:07,160
You're like, ah, thanks.

1110
00:55:07,160 --> 00:55:10,920
So it's great because for virtually every situation,

1111
00:55:10,920 --> 00:55:14,440
cloud on-prem, IOT, et cetera, et cetera,

1112
00:55:14,440 --> 00:55:17,140
there will be something to help us,

1113
00:55:17,140 --> 00:55:20,440
but it means that to pick the right one,

1114
00:55:20,440 --> 00:55:23,260
it will incur a bunch of research

1115
00:55:23,260 --> 00:55:26,320
to make sure we're making the right choice basically.

1116
00:55:26,320 --> 00:55:29,640
It also means that from a security standpoint,

1117
00:55:29,640 --> 00:55:32,100
since everything is one big network,

1118
00:55:33,160 --> 00:55:35,920
we have to think about that

1119
00:55:35,920 --> 00:55:40,000
and we have to add another layer of isolation.

1120
00:55:40,240 --> 00:55:41,720
If on the same cluster,

1121
00:55:41,720 --> 00:55:43,840
if I deploy multiple applications,

1122
00:55:43,840 --> 00:55:45,540
by default, these applications

1123
00:55:45,540 --> 00:55:47,480
will be able to communicate together

1124
00:55:47,480 --> 00:55:49,600
and if I want to isolate them,

1125
00:55:49,600 --> 00:55:51,880
I will have to learn about network policies

1126
00:55:51,880 --> 00:55:53,140
and implement that.

1127
00:55:54,760 --> 00:55:55,600
Okay.

1128
00:55:56,760 --> 00:56:00,000
In practice, on the clusters that we're going to use today,

1129
00:56:00,000 --> 00:56:04,040
we've used a network implementation called Weave.

1130
00:56:04,040 --> 00:56:07,280
Weave is the kind of all wheels drive

1131
00:56:07,280 --> 00:56:08,700
network layer of Kubernetes

1132
00:56:08,700 --> 00:56:11,620
because it works everywhere on all terrains,

1133
00:56:11,620 --> 00:56:13,860
cloud, on-prem, et cetera.

1134
00:56:13,860 --> 00:56:15,820
So for that, it's pretty convenient.

1135
00:56:15,820 --> 00:56:17,420
It's super easy to set up.

1136
00:56:17,420 --> 00:56:20,700
I'm not saying that it's the best one for every situation,

1137
00:56:20,700 --> 00:56:23,640
but in doubt when getting started, it's a pretty safe bet.

1138
00:56:25,380 --> 00:56:27,700
Okay, I have some extra details here

1139
00:56:27,700 --> 00:56:29,900
if you're the kind of person who really wants to know

1140
00:56:29,900 --> 00:56:31,860
what's under the hood for the network,

1141
00:56:31,860 --> 00:56:34,260
but I'm setting that aside.

1142
00:56:35,180 --> 00:56:37,460
All right, now we know what we need to know

1143
00:56:37,460 --> 00:56:41,580
to get started with networking on Kubernetes.

1144
00:56:41,580 --> 00:56:43,460
Do you have questions on that part?

1145
00:56:46,380 --> 00:56:47,220
Yes.

1146
00:56:48,300 --> 00:56:50,180
Is it possible to actually have

1147
00:56:50,180 --> 00:56:52,780
separated environments on the support

1148
00:56:52,780 --> 00:56:55,540
and really separate those or isolated networks?

1149
00:56:55,540 --> 00:56:58,180
Is it possible to have multiple environments

1150
00:56:58,180 --> 00:57:01,080
on the same cluster that are isolated?

1151
00:57:01,080 --> 00:57:04,180
Yes, there are many ways to do that,

1152
00:57:04,180 --> 00:57:07,140
but the easiest way is going to be using something

1153
00:57:07,140 --> 00:57:10,900
called namespaces, and namespaces are a little bit

1154
00:57:10,900 --> 00:57:14,220
like folders in which we can put pods

1155
00:57:14,220 --> 00:57:16,500
and services and other things,

1156
00:57:16,500 --> 00:57:20,500
and then when we define policies,

1157
00:57:20,500 --> 00:57:23,500
whether it's network policies for firewalling

1158
00:57:23,500 --> 00:57:27,380
or whether it's RBAC to define access control rights,

1159
00:57:27,380 --> 00:57:30,180
we can do that on a namespace level.

1160
00:57:30,180 --> 00:57:32,660
So for instance, if I have, let's say,

1161
00:57:32,660 --> 00:57:35,100
two copies of my Docker Coins app,

1162
00:57:35,100 --> 00:57:38,140
and so in one copy I have a bunch of things

1163
00:57:38,140 --> 00:57:39,560
that need to talk together,

1164
00:57:39,560 --> 00:57:41,380
then I have the other copy with, again,

1165
00:57:41,380 --> 00:57:43,420
same bunch of things that need to talk together,

1166
00:57:43,420 --> 00:57:47,120
but I don't want traffic to go between them,

1167
00:57:47,120 --> 00:57:49,820
I could deploy everything in one namespace

1168
00:57:49,820 --> 00:57:52,700
and then write a bunch of network policies to say,

1169
00:57:52,700 --> 00:57:54,180
this is allowed, this is allowed,

1170
00:57:54,180 --> 00:57:55,900
this is allowed, et cetera, et cetera,

1171
00:57:55,900 --> 00:57:59,020
or I could put each of them in one namespace

1172
00:57:59,020 --> 00:58:02,700
and then have a kind of, let's call that a blanket statement

1173
00:58:02,700 --> 00:58:05,060
saying all these things in that namespace

1174
00:58:05,060 --> 00:58:09,380
can communicate together, but not the outside.

1175
00:58:09,380 --> 00:58:13,580
So that would be a relatively easy way to do that,

1176
00:58:13,580 --> 00:58:16,340
easier at least than defining one by one

1177
00:58:16,340 --> 00:58:18,880
all the network flows that we want to allow.

1178
00:58:18,880 --> 00:58:21,400
Same thing for access control.

1179
00:58:21,400 --> 00:58:24,660
Let's say I want to give to someone access

1180
00:58:24,660 --> 00:58:28,160
to the objects of that application.

1181
00:58:28,160 --> 00:58:31,660
I could do that one by one, like, okay,

1182
00:58:31,660 --> 00:58:34,460
Alice can access to this pod and this pod and this pod

1183
00:58:34,460 --> 00:58:36,380
and this load balancer and this and that,

1184
00:58:36,380 --> 00:58:38,180
or I could say, Alice can access

1185
00:58:38,180 --> 00:58:40,180
all the objects in that namespace.

1186
00:58:40,180 --> 00:58:43,060
So when I want to have multiple applications,

1187
00:58:44,020 --> 00:58:45,780
typically, most of the times,

1188
00:58:45,780 --> 00:58:49,540
I will want to use namespaces because it's so much easier.

1189
00:58:52,300 --> 00:58:53,220
Other questions?

1190
00:58:55,700 --> 00:58:56,660
All right.

1191
00:58:57,660 --> 00:59:02,380
All right, time to get started with kubectl.

1192
00:59:02,380 --> 00:59:05,820
So kubectl is the kubernetes CLI.

1193
00:59:09,260 --> 00:59:12,700
It's just a glorified version of curl, if you will,

1194
00:59:12,700 --> 00:59:15,540
because each time we do something with kubectl,

1195
00:59:15,540 --> 00:59:19,460
it's going to craft an API request,

1196
00:59:19,460 --> 00:59:22,280
send that to the kubernetes API server

1197
00:59:22,280 --> 00:59:24,580
and display the result.

1198
00:59:24,580 --> 00:59:26,940
So everything we do with kubectl,

1199
00:59:26,940 --> 00:59:31,240
we could do with curl if we were slightly masochist.

1200
00:59:32,260 --> 00:59:36,780
So kubectl is using a configuration file

1201
00:59:36,780 --> 00:59:39,460
to know where is the API server

1202
00:59:39,460 --> 00:59:41,700
and what kind of credentials we should use.

1203
00:59:41,700 --> 00:59:43,420
So that's kube config.

1204
00:59:43,420 --> 00:59:48,420
So for instance, if I look here in home

1205
00:59:48,500 --> 00:59:51,720
slash .kube slash config,

1206
00:59:51,720 --> 00:59:54,500
I will see that config file

1207
00:59:55,420 --> 00:59:56,700
and in that config file, among other things,

1208
00:59:56,700 --> 01:00:01,660
I will see the address of the API server

1209
01:00:01,660 --> 01:00:06,660
and I'm going also to see TLS certificate and key

1210
01:00:07,020 --> 01:00:10,900
that are used to communicate with the API server.

1211
01:00:12,740 --> 01:00:14,940
All right, oh, one little thing.

1212
01:00:15,860 --> 01:00:18,700
Since I'm French, as you probably noticed from my accent,

1213
01:00:18,700 --> 01:00:21,660
in French, when we have acronyms,

1214
01:00:21,660 --> 01:00:23,300
we typically spell them out.

1215
01:00:23,300 --> 01:00:26,740
So I'm almost always gonna say kubectl,

1216
01:00:26,740 --> 01:00:31,140
but other English speakers will often say kubectl,

1217
01:00:31,140 --> 01:00:35,840
kubectl, kubectl, kubectl or even other things.

1218
01:00:35,840 --> 01:00:39,440
I will stick to kubectl, but no judgment whatsoever.

1219
01:00:40,860 --> 01:00:42,780
The first comment that we're going to use

1220
01:00:42,780 --> 01:00:45,540
is going to be kubectl get

1221
01:00:45,540 --> 01:00:50,540
and kubectl get will list all the resources

1222
01:00:50,980 --> 01:00:52,140
of a given type.

1223
01:00:52,140 --> 01:00:56,140
So for instance, if I do kubectl get nodes,

1224
01:00:56,140 --> 01:00:59,580
which we did like almost one hour ago when I said,

1225
01:00:59,580 --> 01:01:01,940
hey, let's check everything works correctly.

1226
01:01:01,940 --> 01:01:04,040
That means list all the nodes.

1227
01:01:05,260 --> 01:01:10,260
And for each resource type, we have the plural form,

1228
01:01:10,340 --> 01:01:13,260
the singular form, get node,

1229
01:01:13,260 --> 01:01:15,860
and the short form for lazy people like me,

1230
01:01:15,860 --> 01:01:17,480
which is kubectl get node.

1231
01:01:18,480 --> 01:01:22,240
The point of having kubectl get node singular

1232
01:01:22,240 --> 01:01:26,360
is because we can also give a node name behind that.

1233
01:01:26,360 --> 01:01:30,400
So I can say kubectl get node node one,

1234
01:01:30,400 --> 01:01:33,740
and then it's going to show me only node one for instance.

1235
01:01:33,740 --> 01:01:36,480
The point of the short form kubectl get node

1236
01:01:36,480 --> 01:01:38,120
is to save a few keystrokes.

1237
01:01:38,120 --> 01:01:41,400
And also if we had a name called satisfaction,

1238
01:01:41,400 --> 01:01:44,820
we could do kubectl get no satisfaction, but anyway.

1239
01:01:45,380 --> 01:01:48,700
We can also change the output format

1240
01:01:48,700 --> 01:01:50,960
to get a little bit more information.

1241
01:01:50,960 --> 01:01:55,960
So for instance, I can do kubectl get nodes dash o wide.

1242
01:01:57,900 --> 01:02:01,720
So wide is going to add a bit of information.

1243
01:02:01,720 --> 01:02:06,600
So just to contrast, this is adding a bunch of columns

1244
01:02:06,600 --> 01:02:09,340
showing me the internal IP address,

1245
01:02:09,340 --> 01:02:11,580
which OS we're running, the kernel version,

1246
01:02:11,580 --> 01:02:13,760
container runtime, and this will work

1247
01:02:13,760 --> 01:02:16,720
with all resource types when I will show pods

1248
01:02:16,720 --> 01:02:18,480
or this or that or whatever.

1249
01:02:18,480 --> 01:02:22,700
Each time I can use wide format to get some extra stuff.

1250
01:02:24,640 --> 01:02:25,480
Okay.

1251
01:02:27,840 --> 01:02:30,080
We have other output formats.

1252
01:02:30,080 --> 01:02:33,200
For instance, I can do get nodes,

1253
01:02:34,920 --> 01:02:38,080
let's use the singular form because it will make more sense.

1254
01:02:38,080 --> 01:02:42,660
Get node node one dash o JSON.

1255
01:02:42,660 --> 01:02:46,260
And that gives me JSON output with a lot of information

1256
01:02:46,260 --> 01:02:47,580
about that node.

1257
01:02:49,620 --> 01:02:54,080
And JSON is great for machines, but I'm a human,

1258
01:02:54,080 --> 01:02:55,780
so I'm going to ask for YAML,

1259
01:02:55,780 --> 01:03:00,140
which is slightly more readable, only slightly.

1260
01:03:00,140 --> 01:03:03,140
And here you will see like in the output,

1261
01:03:03,140 --> 01:03:07,940
notice how we have API version, kind and metadata.

1262
01:03:07,940 --> 01:03:11,300
Every single API response from Kubernetes,

1263
01:03:11,300 --> 01:03:14,060
like every single one of them is going to have these fields,

1264
01:03:14,060 --> 01:03:16,220
always, always, always.

1265
01:03:16,220 --> 01:03:18,540
So in that case, this is telling me,

1266
01:03:18,540 --> 01:03:21,660
hey, this thing is a node.

1267
01:03:21,660 --> 01:03:25,060
So I could go to the API specification.

1268
01:03:25,060 --> 01:03:28,060
It would tell me, okay, if the type is a node,

1269
01:03:28,060 --> 01:03:31,020
then it will have this and this and this and this field.

1270
01:03:31,020 --> 01:03:33,140
Like it would tell me I have a spec

1271
01:03:33,140 --> 01:03:34,620
and I have status, et cetera.

1272
01:03:34,620 --> 01:03:36,620
And it would tell me about all the fields

1273
01:03:36,620 --> 01:03:38,100
I can expect there.

1274
01:03:39,060 --> 01:03:43,220
Now, for instance, if I do get nodes

1275
01:03:43,220 --> 01:03:46,540
without indicating which one I want,

1276
01:03:46,540 --> 01:03:50,700
the output is going to be still API version.

1277
01:03:50,700 --> 01:03:52,620
I don't see a kind here,

1278
01:03:52,620 --> 01:03:56,620
but if I look for it in the output, there it is.

1279
01:03:56,620 --> 01:03:59,420
It says kind list.

1280
01:03:59,420 --> 01:04:01,020
There is also a metadata.

1281
01:04:01,020 --> 01:04:03,020
It doesn't carry anything useful,

1282
01:04:03,020 --> 01:04:04,660
but I still have metadata.

1283
01:04:04,660 --> 01:04:07,100
And then if I look into the API specification,

1284
01:04:07,100 --> 01:04:10,700
it tells me the objects of kind list

1285
01:04:10,700 --> 01:04:13,060
have a field called items.

1286
01:04:14,060 --> 01:04:15,780
Indeed, I have items.

1287
01:04:15,780 --> 01:04:19,260
And the value of items should be a list of things.

1288
01:04:19,260 --> 01:04:22,060
And each thing, again, will have API version,

1289
01:04:22,060 --> 01:04:24,660
kind metadata, and a bunch of things.

1290
01:04:25,700 --> 01:04:28,500
Okay, so the YAML can be useful

1291
01:04:28,500 --> 01:04:30,860
because it's a little bit more readable than JSON.

1292
01:04:30,860 --> 01:04:35,380
And JSON can be useful because you can parse it,

1293
01:04:35,380 --> 01:04:36,740
of course, with code.

1294
01:04:36,740 --> 01:04:39,620
But if you don't want to break out the interpreter

1295
01:04:39,620 --> 01:04:42,380
or IPython to do some little things,

1296
01:04:42,380 --> 01:04:45,980
you could use jq, for instance, which I really, really like.

1297
01:04:48,060 --> 01:04:50,580
And with, like for instance,

1298
01:04:50,580 --> 01:04:53,140
if I want to get some information about node one,

1299
01:04:55,540 --> 01:04:58,020
I can do things like that, for instance.

1300
01:04:58,020 --> 01:04:59,540
If you don't know about jq,

1301
01:04:59,540 --> 01:05:01,060
I recommend that you make a note

1302
01:05:01,060 --> 01:05:03,420
to look on that a little bit later,

1303
01:05:03,460 --> 01:05:06,820
because it's awesome if you want to process

1304
01:05:06,820 --> 01:05:08,540
some JSON from the shell.

1305
01:05:09,580 --> 01:05:10,420
All right.

1306
01:05:12,660 --> 01:05:13,500
Okay.

1307
01:05:15,580 --> 01:05:19,900
So, yep, I told about the fact that we had plural form,

1308
01:05:19,900 --> 01:05:22,260
singular form, short form, that's great.

1309
01:05:23,140 --> 01:05:24,940
We can get some details.

1310
01:05:24,940 --> 01:05:28,580
And I said, okay, so we have JSON, YAML,

1311
01:05:28,580 --> 01:05:31,620
but we also have another form,

1312
01:05:31,620 --> 01:05:34,180
which is kubectl describe.

1313
01:05:34,180 --> 01:05:36,940
It's a different verb because it's going to

1314
01:05:36,940 --> 01:05:40,500
kind of consolidate information coming from multiple places.

1315
01:05:40,500 --> 01:05:45,500
But for instance, if I do kubectl describe node one,

1316
01:05:47,820 --> 01:05:50,460
it's going to show me a bunch of things coming

1317
01:05:50,460 --> 01:05:54,100
from kubectl get,

1318
01:05:54,100 --> 01:05:56,620
but also collate with other information.

1319
01:05:56,620 --> 01:05:59,980
For instance, here, it's showing me the pods

1320
01:05:59,980 --> 01:06:02,780
currently running on node one.

1321
01:06:02,780 --> 01:06:04,620
This is a little bit smaller on the screen,

1322
01:06:04,620 --> 01:06:08,300
so I guess you can't really see exactly what's here,

1323
01:06:08,300 --> 01:06:09,700
but I'm going to tell it for you.

1324
01:06:09,700 --> 01:06:11,580
This is telling me that on node one,

1325
01:06:11,580 --> 01:06:15,820
I have core DNS, I have each CD, kubepi server,

1326
01:06:15,820 --> 01:06:19,860
kube control manager, kube proxy, kube scheduler, we've net.

1327
01:06:19,860 --> 01:06:23,340
So these names will remind you the names

1328
01:06:23,340 --> 01:06:25,220
of the control plane that I gave earlier.

1329
01:06:25,220 --> 01:06:27,820
This is showing me that on node one,

1330
01:06:27,820 --> 01:06:32,100
I have the control plane of my cluster running in pods.

1331
01:06:32,100 --> 01:06:32,940
Okay.

1332
01:06:34,740 --> 01:06:36,180
Right.

1333
01:06:36,180 --> 01:06:39,900
Next object types, I have services.

1334
01:06:39,900 --> 01:06:41,140
So services,

1335
01:06:43,680 --> 01:06:46,840
services or SVC for lazy people like me.

1336
01:06:47,860 --> 01:06:52,080
Services are an interesting network construct.

1337
01:06:52,080 --> 01:06:56,740
We can think about them as either static IP addresses

1338
01:06:56,740 --> 01:06:59,460
for pods or also load balancers,

1339
01:06:59,460 --> 01:07:04,460
or also things allowing you to reach something else.

1340
01:07:04,660 --> 01:07:06,740
In the early versions of Kubernetes,

1341
01:07:06,740 --> 01:07:09,100
services were called portals.

1342
01:07:09,100 --> 01:07:12,900
And to be fair, I preferred that name way better

1343
01:07:12,900 --> 01:07:15,820
because it represents more like what it really is.

1344
01:07:15,820 --> 01:07:19,420
A service is a stable network endpoint.

1345
01:07:21,260 --> 01:07:26,260
What that really means is that when we have a pod running,

1346
01:07:27,540 --> 01:07:30,260
as I said earlier, the IP address of the pod

1347
01:07:30,260 --> 01:07:32,980
is assigned by the network implementation,

1348
01:07:32,980 --> 01:07:35,660
which means that if my pod has a problem

1349
01:07:35,660 --> 01:07:39,180
and another pod needs to be created to replace it,

1350
01:07:39,180 --> 01:07:41,080
that other pod will almost certainly

1351
01:07:41,080 --> 01:07:42,860
have a different IP address.

1352
01:07:42,860 --> 01:07:45,740
So if I need to connect to that pod,

1353
01:07:45,740 --> 01:07:49,100
I will need to update my IP addresses and that sucks.

1354
01:07:49,100 --> 01:07:51,500
So instead, I can use a service,

1355
01:07:51,500 --> 01:07:54,460
which is going to give me a stable IP address,

1356
01:07:54,460 --> 01:07:56,900
and then I can connect to that address,

1357
01:07:56,900 --> 01:08:00,980
and that address will then redirect me or route me

1358
01:08:01,860 --> 01:08:04,220
to the actual pod.

1359
01:08:04,220 --> 01:08:06,060
So for instance, in that case,

1360
01:08:06,060 --> 01:08:09,340
I see that I have a service called Kubernetes

1361
01:08:09,340 --> 01:08:13,040
with a cluster IP of 1096.01.

1362
01:08:13,040 --> 01:08:14,460
If you've done some networking,

1363
01:08:14,460 --> 01:08:16,880
you can also think about that cluster IP

1364
01:08:16,880 --> 01:08:20,780
like a service IP or virtual IP.

1365
01:08:20,780 --> 01:08:25,780
And so this IP, like I can, hey, can I ping it?

1366
01:08:26,340 --> 01:08:30,720
No, I can't ping it because implementation details,

1367
01:08:30,720 --> 01:08:34,340
and because a service is a layer four construct.

1368
01:08:34,340 --> 01:08:37,380
So if you studied like the seven layers networking model

1369
01:08:37,380 --> 01:08:39,500
that maybe rings a bell, if you haven't,

1370
01:08:39,500 --> 01:08:40,500
don't worry about it.

1371
01:08:40,500 --> 01:08:43,560
What it really means is that the service,

1372
01:08:43,560 --> 01:08:46,540
I have to specify the part of the service

1373
01:08:46,540 --> 01:08:48,220
if I want to connect to it.

1374
01:08:48,220 --> 01:08:51,540
Here in the output, we see that the Kubernetes service

1375
01:08:51,540 --> 01:08:53,420
is on port 443.

1376
01:08:53,420 --> 01:08:58,420
That means I have to connect to that specific port.

1377
01:08:59,220 --> 01:09:02,480
So I know that 443 is HTTPS,

1378
01:09:02,480 --> 01:09:07,000
so I'm going to hit that IP address with curl and HTTPS.

1379
01:09:07,000 --> 01:09:10,380
And if I hit that service on any other port,

1380
01:09:10,380 --> 01:09:11,660
it's just going to time out.

1381
01:09:11,660 --> 01:09:14,100
Like my packets are going to devnurl

1382
01:09:14,100 --> 01:09:16,220
to the great void and never come back.

1383
01:09:16,220 --> 01:09:18,860
But if I connect to port 443,

1384
01:09:18,860 --> 01:09:22,220
then first I get a certificate warning, of course,

1385
01:09:22,220 --> 01:09:27,220
because doing HTTPS on IP addresses like that doesn't work.

1386
01:09:27,340 --> 01:09:30,600
But if I tell curl, okay, don't mind the certificate,

1387
01:09:31,740 --> 01:09:35,860
I get something that looks like a Kubernetes API reply.

1388
01:09:35,860 --> 01:09:40,420
You know, like there is a API version, kind and metadata.

1389
01:09:40,420 --> 01:09:44,060
It looks like an API response

1390
01:09:44,060 --> 01:09:46,860
because it is an API response.

1391
01:09:46,860 --> 01:09:50,820
That Kubernetes service, that's the Kubernetes API,

1392
01:09:50,820 --> 01:09:54,420
and I can access it from within the cluster.

1393
01:09:54,420 --> 01:09:57,140
This is giving me a kind of introspection,

1394
01:09:57,140 --> 01:09:59,660
a kind of entry point into the API

1395
01:09:59,660 --> 01:10:02,220
that I can hit if I need to.

1396
01:10:02,220 --> 01:10:03,620
In that case, it's telling me,

1397
01:10:03,620 --> 01:10:06,900
failure, you are system anonymous

1398
01:10:06,900 --> 01:10:09,480
because I did not provide any certificate

1399
01:10:09,480 --> 01:10:11,780
or any token or any whatever.

1400
01:10:11,780 --> 01:10:15,540
So the API could not find out who I was,

1401
01:10:15,540 --> 01:10:18,220
so it thinks I'm anonymous.

1402
01:10:18,220 --> 01:10:21,300
And by default, anonymous can't do anything

1403
01:10:21,300 --> 01:10:24,260
on the Kubernetes API.

1404
01:10:24,260 --> 01:10:25,980
Right, okay.

1405
01:10:28,640 --> 01:10:31,140
Next object type, pods.

1406
01:10:31,140 --> 01:10:33,780
So I can do kubectl get pods,

1407
01:10:33,780 --> 01:10:36,100
and this is going to list me my pods.

1408
01:10:36,100 --> 01:10:38,580
And it's telling me no resources found,

1409
01:10:38,580 --> 01:10:40,460
and that's slightly surprising

1410
01:10:40,460 --> 01:10:43,460
because just a few minutes ago,

1411
01:10:43,460 --> 01:10:45,660
we did a describe on node one,

1412
01:10:45,660 --> 01:10:47,300
and we saw a bunch of pods.

1413
01:10:47,300 --> 01:10:48,580
So we know that this is a lie.

1414
01:10:48,580 --> 01:10:52,140
We know there are pods and something is hiding them.

1415
01:10:52,140 --> 01:10:55,380
The answer to that is namespaces,

1416
01:10:55,380 --> 01:11:00,380
which I mentioned just a minutes ago to answer a question.

1417
01:11:00,540 --> 01:11:05,540
We have namespaces that allow us to organize resources.

1418
01:11:05,540 --> 01:11:09,260
And so namespaces are a resource like any other,

1419
01:11:09,260 --> 01:11:13,740
which means I can do kubectl get namespaces,

1420
01:11:13,740 --> 01:11:18,380
or for lazy people like me, kubectl get ns.

1421
01:11:18,380 --> 01:11:21,820
And then I see I have default kube-node-list,

1422
01:11:21,820 --> 01:11:23,540
kube-public-kube-system.

1423
01:11:24,500 --> 01:11:27,540
You might guess that by default,

1424
01:11:27,540 --> 01:11:29,940
when we do kubectl get pods,

1425
01:11:29,940 --> 01:11:32,300
we see the pods in the default namespace,

1426
01:11:32,300 --> 01:11:33,660
and you would be right.

1427
01:11:33,660 --> 01:11:37,700
And you might remember if you did like squint your eyes

1428
01:11:37,700 --> 01:11:41,500
a little bit when I was showing the kubectl describe

1429
01:11:41,500 --> 01:11:45,060
on node one, you might remember that here,

1430
01:11:45,060 --> 01:11:47,420
it showed kube-system.

1431
01:11:47,420 --> 01:11:50,020
So the pods of the control plane

1432
01:11:50,020 --> 01:11:52,700
are running in the kube-system namespace,

1433
01:11:52,700 --> 01:11:56,020
and that's why when I do kubectl get pods,

1434
01:11:56,020 --> 01:11:57,780
I'm not seeing them.

1435
01:11:57,780 --> 01:12:01,580
If I want to see them anyway, I have a handful of options.

1436
01:12:01,580 --> 01:12:06,460
One of them is to say, okay, show me all the namespaces,

1437
01:12:06,460 --> 01:12:08,640
get pods all namespaces.

1438
01:12:09,660 --> 01:12:13,780
And in that case, I get the list of all pods

1439
01:12:13,780 --> 01:12:15,540
across all namespaces.

1440
01:12:16,580 --> 01:12:20,940
Since kubernetes 1.14, we can even abbreviate

1441
01:12:20,940 --> 01:12:23,940
all namespaces in dash uppercase A,

1442
01:12:23,940 --> 01:12:26,660
and these features just by itself justifies upgrading

1443
01:12:26,660 --> 01:12:27,940
to kubernetes 1.14.

1444
01:12:29,180 --> 01:12:33,780
And I can also ask to scope to a specific namespace.

1445
01:12:33,780 --> 01:12:38,140
So I can say dash dash namespace kube-system,

1446
01:12:38,140 --> 01:12:40,100
and then it's going to show me only what's

1447
01:12:40,100 --> 01:12:42,980
in the kube-system namespace.

1448
01:12:42,980 --> 01:12:45,460
Okay, so we have pods.

1449
01:12:46,520 --> 01:12:50,660
We see here the control plane pods.

1450
01:12:50,660 --> 01:12:53,580
We see also a few interesting things,

1451
01:12:53,580 --> 01:12:58,120
like for instance, I have four pods, kube-proxy, yada yada,

1452
01:12:58,120 --> 01:13:00,220
and four pods, weave-net, yada yada.

1453
01:13:00,220 --> 01:13:04,660
Like why would I need multiple identical

1454
01:13:04,660 --> 01:13:07,140
or seemingly identical pods?

1455
01:13:07,140 --> 01:13:12,140
Because these pods are, okay, let me,

1456
01:13:12,820 --> 01:13:16,160
these pods are running like one per node.

1457
01:13:16,980 --> 01:13:20,340
For instance, if I look at the four kube-proxy here,

1458
01:13:20,340 --> 01:13:25,340
and I switch to dash O-wide, so give me extra information,

1459
01:13:25,980 --> 01:13:28,980
I get the IP addresses of the pods,

1460
01:13:29,020 --> 01:13:31,460
and I get a node column that tells me

1461
01:13:31,460 --> 01:13:36,460
that I have one on node one, node two, node three, node four.

1462
01:13:36,540 --> 01:13:39,420
So one copy of kube-proxy per node.

1463
01:13:39,420 --> 01:13:42,000
Same thing if I look at weave here.

1464
01:13:43,620 --> 01:13:45,740
Reminder, weave is the thing giving me

1465
01:13:45,740 --> 01:13:48,020
the network connectivity between my nodes.

1466
01:13:48,020 --> 01:13:52,200
And again, I have one copy of weave running on each node.

1467
01:13:53,400 --> 01:13:55,580
Okay, yeah.

1468
01:13:56,580 --> 01:13:58,340
Let's just go back to the FCD thing,

1469
01:13:58,340 --> 01:14:01,220
but it looks like FCD is running on node one,

1470
01:14:01,220 --> 01:14:03,460
so like things we do to the nodes

1471
01:14:03,460 --> 01:14:06,220
can also impact our FCD service.

1472
01:14:06,220 --> 01:14:08,740
So it looks like it's CD is running on node one,

1473
01:14:08,740 --> 01:14:13,100
so if we do something bad with node one,

1474
01:14:13,100 --> 01:14:15,460
we might do something bad with it CD.

1475
01:14:15,460 --> 01:14:17,460
The experimental model, I'm just trying to understand

1476
01:14:17,460 --> 01:14:20,100
what responsibilities we might have about it CD,

1477
01:14:20,100 --> 01:14:21,260
and then how.

1478
01:14:21,260 --> 01:14:23,700
What will be my responsibility with it CD?

1479
01:14:23,700 --> 01:14:24,540
Right.

1480
01:14:24,540 --> 01:14:28,780
I was surprised to see FCD invested under node one.

1481
01:14:28,780 --> 01:14:29,700
Sure.

1482
01:14:29,700 --> 01:14:33,500
So there is a handful of slides that I skipped on

1483
01:14:33,500 --> 01:14:35,300
like in the extra details about like

1484
01:14:35,300 --> 01:14:37,860
where should the control plane be running,

1485
01:14:37,860 --> 01:14:39,980
and we have actually many options.

1486
01:14:39,980 --> 01:14:42,500
The control plane, like if I want to give you

1487
01:14:42,500 --> 01:14:45,880
the kind of dictionary style, pedantic definition,

1488
01:14:45,880 --> 01:14:48,660
the control plane is a bunch of processes

1489
01:14:48,660 --> 01:14:51,260
that implement the Kubernetes API,

1490
01:14:51,260 --> 01:14:54,260
but that control plane could be running anywhere.

1491
01:14:54,260 --> 01:14:59,260
In that case, we are, I would say, partially self-hosted.

1492
01:15:00,540 --> 01:15:04,220
So self-hosted meaning the control plane for Kubernetes

1493
01:15:04,220 --> 01:15:08,380
runs on Kubernetes itself, and I say partially

1494
01:15:08,380 --> 01:15:11,660
because it's little details.

1495
01:15:11,660 --> 01:15:14,820
Like for instance, I can't update it using

1496
01:15:14,820 --> 01:15:17,380
Kubernetes primitives and commands,

1497
01:15:18,220 --> 01:15:21,000
but that control plane could be completely outside.

1498
01:15:21,000 --> 01:15:24,280
For instance, if I deploy a Kubernetes cluster

1499
01:15:24,280 --> 01:15:29,280
on Amazon EKS or Google GKE or Azure AKS or Digital Ocean

1500
01:15:31,300 --> 01:15:36,300
or OVH or pretty much all the managed Kubernetes offerings

1501
01:15:36,440 --> 01:15:39,960
of cloud and hosting providers out there,

1502
01:15:39,960 --> 01:15:43,440
they are not going to run the control plane on my cluster.

1503
01:15:43,440 --> 01:15:46,800
They are going to run the control plane somewhere.

1504
01:15:46,800 --> 01:15:48,840
I don't get to know where.

1505
01:15:48,840 --> 01:15:49,680
They don't tell me.

1506
01:15:49,680 --> 01:15:52,380
Well, sometimes they tell you because they make like

1507
01:15:52,380 --> 01:15:54,700
talks about it and et cetera, but for me,

1508
01:15:54,700 --> 01:15:56,720
that's going to be an implementation detail

1509
01:15:56,720 --> 01:16:01,120
because all I will be able to see is an API endpoint.

1510
01:16:01,120 --> 01:16:02,880
I connect to that API endpoint,

1511
01:16:02,880 --> 01:16:04,520
and for all practical purposes,

1512
01:16:04,520 --> 01:16:06,360
all the commands I was running here

1513
01:16:06,360 --> 01:16:08,200
will work exactly the same way,

1514
01:16:08,200 --> 01:16:10,800
except all these processes like etcd,

1515
01:16:10,800 --> 01:16:13,180
API server, controller manager, scheduler,

1516
01:16:13,180 --> 01:16:16,960
they will all be running somewhere else.

1517
01:16:16,960 --> 01:16:19,580
And so in that case, it's not my responsibility

1518
01:16:20,380 --> 01:16:24,740
to maintain and backup and operate that control plane.

1519
01:16:24,740 --> 01:16:27,100
How often do we expect the needs to be there

1520
01:16:27,100 --> 01:16:30,100
about the things that are running in the kube system?

1521
01:16:30,100 --> 01:16:32,860
How often do we expect to care about what's running

1522
01:16:32,860 --> 01:16:34,660
in the kube system namespace?

1523
01:16:34,660 --> 01:16:39,660
Only if you are operating the Kubernetes cluster yourself.

1524
01:16:41,000 --> 01:16:44,940
For many scenarios, I often do the joke

1525
01:16:44,940 --> 01:16:47,540
that the easiest way to install Kubernetes

1526
01:16:47,660 --> 01:16:50,060
is to get somebody else to do it for you,

1527
01:16:50,060 --> 01:16:51,900
but it's also very true.

1528
01:16:51,900 --> 01:16:56,900
It's Kubernetes is a pretty complex beast to operate.

1529
01:16:57,180 --> 01:17:00,380
My goal this morning is to show you that using Kubernetes

1530
01:17:00,380 --> 01:17:02,460
is going to be relatively straightforward

1531
01:17:02,460 --> 01:17:04,780
once we've wrapped our head around the concepts,

1532
01:17:04,780 --> 01:17:06,620
we'll be able to run apps on it.

1533
01:17:06,620 --> 01:17:09,020
However, operating Kubernetes itself

1534
01:17:09,020 --> 01:17:10,460
is a whole different story.

1535
01:17:10,460 --> 01:17:12,060
It's a little bit the same difference

1536
01:17:12,060 --> 01:17:15,140
between I can write a Python app

1537
01:17:15,140 --> 01:17:17,180
and I can compile my own kernel.

1538
01:17:17,540 --> 01:17:19,460
I don't need to compile my own Linux kernel

1539
01:17:19,460 --> 01:17:22,900
if I just want to run a Python app.

1540
01:17:22,900 --> 01:17:25,980
So likewise, I don't need to be able to know

1541
01:17:25,980 --> 01:17:30,180
how to deploy and upgrade and maintain Kubernetes

1542
01:17:30,180 --> 01:17:32,220
to deploy apps on it.

1543
01:17:32,220 --> 01:17:36,900
So most of us hopefully won't have to ever worry

1544
01:17:36,900 --> 01:17:40,100
about how do I maintain that control plane, et cetera.

1545
01:17:40,100 --> 01:17:43,820
And in some scenarios, for instance,

1546
01:17:44,500 --> 01:17:49,260
if I work for the military or nation state

1547
01:17:49,260 --> 01:17:51,860
or something like that, and I can't use the cloud

1548
01:17:51,860 --> 01:17:53,460
for regulation purposes,

1549
01:17:53,460 --> 01:17:55,780
then I might have to learn how to operate

1550
01:17:55,780 --> 01:17:57,140
my Kubernetes cluster,

1551
01:17:57,140 --> 01:17:59,140
but most of the times I won't have to.

1552
01:18:01,540 --> 01:18:02,620
I think so, yeah.

1553
01:18:03,620 --> 01:18:06,580
What is, is weaves the network implementation

1554
01:18:06,580 --> 01:18:08,700
that dictates the IPs for each pod?

1555
01:18:08,700 --> 01:18:10,220
Or what is weave?

1556
01:18:10,220 --> 01:18:11,060
What is weave?

1557
01:18:11,060 --> 01:18:14,260
So weave here is the network implementation

1558
01:18:14,260 --> 01:18:18,820
that we are using to provide what we call the pod network.

1559
01:18:18,820 --> 01:18:20,780
So the connectivity between pods

1560
01:18:20,780 --> 01:18:23,620
is going to allocate IP addresses to the pods

1561
01:18:23,620 --> 01:18:27,980
and plump them into some kind of network fabric

1562
01:18:27,980 --> 01:18:30,540
so that all the pods can ping each other,

1563
01:18:30,540 --> 01:18:34,020
so that the nodes can ping the pods and vice versa.

1564
01:18:34,020 --> 01:18:36,980
And you have to select this type of network implementation.

1565
01:18:36,980 --> 01:18:40,780
And we have to select a network implementation indeed.

1566
01:18:40,780 --> 01:18:43,420
Weave is a kind of easy choice, but there are many others.

1567
01:18:43,420 --> 01:18:45,140
Is this part of Kubernetes?

1568
01:18:45,140 --> 01:18:46,900
Is this part of Kubernetes?

1569
01:18:46,900 --> 01:18:50,220
Kind of, I mean, weave doesn't come with Kubernetes.

1570
01:18:50,220 --> 01:18:52,460
It's an external product,

1571
01:18:53,380 --> 01:18:57,620
but the constructs that it uses, et cetera, et cetera,

1572
01:18:57,620 --> 01:19:02,180
are very well integrated with the rest of the ecosystem.

1573
01:19:02,180 --> 01:19:03,940
Is there a default?

1574
01:19:03,940 --> 01:19:06,540
Is there a default network implementation?

1575
01:19:06,540 --> 01:19:08,140
Not really.

1576
01:19:08,140 --> 01:19:09,580
There is a kind of a default,

1577
01:19:09,580 --> 01:19:13,780
but which is not usable for practical purposes.

1578
01:19:13,780 --> 01:19:16,140
Let's have the coffee break.

1579
01:19:16,140 --> 01:19:20,100
Let's reconvene in 20 minutes at 10.40.

1580
01:19:20,100 --> 01:19:21,500
If you have more questions than that,

1581
01:19:21,500 --> 01:19:24,700
I'm going to stay here, maybe walk to coffee,

1582
01:19:24,700 --> 01:19:29,020
but feel free to ask me more questions about that if you need.

1583
01:19:29,020 --> 01:19:30,020
Yeah.

1584
01:20:10,540 --> 01:20:12,340
Yeah, we should be able to share.

1585
01:20:12,340 --> 01:20:15,700
Because it's one big HTML file,

1586
01:20:15,700 --> 01:20:17,660
so if you save, I have two more.

1587
01:20:17,660 --> 01:20:18,660
Hi.

1588
01:20:18,660 --> 01:20:21,500
We're going to talk about load balancer.

1589
01:20:21,500 --> 01:20:22,500
Soon.

1590
01:20:22,500 --> 01:20:23,500
Yeah.

1591
01:20:23,500 --> 01:20:26,540
If we could speak about,

1592
01:20:26,540 --> 01:20:32,020
if we need some special scheduling for load balancer logic.

1593
01:20:32,020 --> 01:20:34,060
I'd say I have a use case when, let's say,

1594
01:20:34,060 --> 01:20:37,020
I'm going to use several ports.

1595
01:20:37,020 --> 01:20:39,380
But when I say, if you take class for the spots,

1596
01:20:39,380 --> 01:20:43,580
I want to take into place the schedule of policy

1597
01:20:43,580 --> 01:20:47,940
to reach products that have a separate class.

1598
01:20:47,940 --> 01:20:50,260
I'm not sure if you want this, is it right for this,

1599
01:20:50,260 --> 01:20:53,500
but just to think it's classically.

1600
01:20:53,500 --> 01:20:58,580
So in that case, you want to load balancer HTTP requests,

1601
01:20:58,580 --> 01:20:59,580
or more like?

1602
01:20:59,580 --> 01:21:01,180
HTTP requests.

1603
01:21:01,180 --> 01:21:02,740
OK, so you have a PC,

1604
01:21:02,740 --> 01:21:06,420
and you want to route them to the best place?

1605
01:21:06,420 --> 01:21:09,660
Maybe I can ask Kubernetes to tell me where the ports are landing,

1606
01:21:09,660 --> 01:21:11,100
and how many of them fell a bit,

1607
01:21:11,100 --> 01:21:15,500
and then to do additional service to those parts.

1608
01:21:15,500 --> 01:21:20,340
That might be a good use case for a service mesh,

1609
01:21:20,340 --> 01:21:25,820
but I don't want to give a blanket statement like that.

1610
01:21:25,820 --> 01:21:29,860
But basically, so you would make a choice

1611
01:21:29,860 --> 01:21:33,340
depending on whether the backend is loaded or not.

1612
01:21:33,340 --> 01:21:36,100
Exactly, it's a stack for backend.

1613
01:21:36,100 --> 01:21:38,100
You can't give the stack's answer on the backend.

1614
01:21:38,100 --> 01:21:39,820
It's a lot of basics.

1615
01:21:39,820 --> 01:21:42,820
So, yeah, service mesh could be a good option to look at,

1616
01:21:42,820 --> 01:21:44,900
at least to get some inspiration.

1617
01:21:44,900 --> 01:21:47,020
If you look at Istio and Linkerd,

1618
01:21:47,020 --> 01:21:50,260
they have things like that where,

1619
01:21:50,260 --> 01:21:54,740
instead of using the load balancing mechanism of Kubernetes,

1620
01:21:54,740 --> 01:21:56,620
which is very simple,

1621
01:21:56,620 --> 01:22:00,180
there would be this sidecar that we were talking about earlier

1622
01:22:00,180 --> 01:22:03,220
that's going to intercept your connection to the service.

1623
01:22:03,300 --> 01:22:06,500
And then it can do some pretty advanced logic,

1624
01:22:06,500 --> 01:22:08,500
like, okay, I'm going to...

1625
01:22:08,500 --> 01:22:10,940
It's kind of maintaining metrics on the backends,

1626
01:22:10,940 --> 01:22:13,420
and you can decide I'm going to send to the backend

1627
01:22:13,420 --> 01:22:15,100
that has the least connections,

1628
01:22:15,100 --> 01:22:18,980
or the one that was responding the faster in the last few minutes,

1629
01:22:18,980 --> 01:22:20,980
or it's trying to do it.

1630
01:22:21,420 --> 01:22:22,380
Sales mesh.

1631
01:22:22,380 --> 01:22:23,500
Yeah.

1632
01:22:23,500 --> 01:22:28,380
And some service meshes are independent of Kubernetes,

1633
01:22:28,380 --> 01:22:29,900
but now they kind of work together,

1634
01:22:29,900 --> 01:22:32,580
but you can also often deploy a service mesh

1635
01:22:32,620 --> 01:22:34,220
without Kubernetes.

1636
01:22:34,220 --> 01:22:35,220
Yeah.

1637
01:22:35,220 --> 01:22:36,220
Thank you.

1638
01:22:36,220 --> 01:22:37,220
I have a question.

1639
01:22:37,220 --> 01:22:40,220
I get the feeling that the admin user...

1640
01:22:40,220 --> 01:22:42,580
I tried to look into the dashboard,

1641
01:22:42,580 --> 01:22:44,580
but I cannot access anything else.

1642
01:22:46,100 --> 01:22:47,100
Right.

1643
01:22:47,100 --> 01:22:49,100
How would I get the feeling of the admin user

1644
01:22:49,100 --> 01:22:52,100
when you have a kube-ctl file?

1645
01:22:52,100 --> 01:22:56,100
So the admin user doesn't...

1646
01:22:56,100 --> 01:22:58,100
Oh, you mean the user?

1647
01:22:58,100 --> 01:23:00,100
Yeah.

1648
01:23:00,900 --> 01:23:03,900
So now a few ways to do that.

1649
01:23:05,900 --> 01:23:11,900
One way is to give permission to that user over there.

1650
01:23:11,900 --> 01:23:13,900
Like, give system...

1651
01:23:19,900 --> 01:23:21,900
Okay, I see.

1652
01:23:24,900 --> 01:23:26,900
So...

1653
01:23:30,900 --> 01:23:31,900
Yeah.

1654
01:23:31,900 --> 01:23:34,900
You could create another service account.

1655
01:23:34,900 --> 01:23:37,900
So you could create a service account...

1656
01:23:37,900 --> 01:23:39,900
Yeah.

1657
01:23:39,900 --> 01:23:41,900
...admin or whatever,

1658
01:23:41,900 --> 01:23:46,900
and then you would grant privileges to that service account.

1659
01:23:46,900 --> 01:23:47,900
Okay.

1660
01:23:47,900 --> 01:23:49,900
In the slides normally there is...

1661
01:23:51,900 --> 01:23:53,900
I don't know if it's in the demo,

1662
01:23:53,900 --> 01:23:56,900
but you could run a command like...

1663
01:23:57,700 --> 01:23:59,700
Like...

1664
01:24:01,700 --> 01:24:04,700
Create cluster role binding.

1665
01:24:04,700 --> 01:24:06,700
So a cluster role binding is an association

1666
01:24:06,700 --> 01:24:09,700
between a user and a set of permissions.

1667
01:24:11,700 --> 01:24:13,700
And then you would do like...

1668
01:24:13,700 --> 01:24:16,700
service account equal admin

1669
01:24:16,700 --> 01:24:19,700
or whatever the name of the service account you created.

1670
01:24:22,700 --> 01:24:25,700
And then this would...

1671
01:24:25,700 --> 01:24:28,700
Because system masters is the equivalent of root

1672
01:24:28,700 --> 01:24:30,700
on a Kubernetes cluster.

1673
01:24:30,700 --> 01:24:33,700
How do I get the token of that specific user?

1674
01:24:35,700 --> 01:24:37,700
So you would do that.

1675
01:24:37,700 --> 01:24:40,700
Okay, we need to specify a name.

1676
01:24:40,700 --> 01:24:42,700
So the name would be like...

1677
01:24:42,700 --> 01:24:44,700
admin can do everything.

1678
01:24:44,700 --> 01:24:47,700
It's just like an arbitrary name for the binding.

1679
01:24:48,700 --> 01:24:50,700
What did I forget?

1680
01:24:50,700 --> 01:24:53,700
Sorry, my bad.

1681
01:24:53,700 --> 01:24:56,700
The cluster role is system...

1682
01:24:56,700 --> 01:24:58,700
It's cluster admin.

1683
01:24:58,700 --> 01:25:00,700
Yeah.

1684
01:25:00,700 --> 01:25:03,700
System masters is the equivalent of root,

1685
01:25:03,700 --> 01:25:05,700
but the permissions corresponding to that

1686
01:25:05,700 --> 01:25:08,700
is the cluster role, cluster admin.

1687
01:25:08,700 --> 01:25:13,700
And service account, I need to put the namespace name.

1688
01:25:13,700 --> 01:25:15,700
So this.

1689
01:25:15,700 --> 01:25:18,700
Then kubectl create service account.

1690
01:25:18,700 --> 01:25:21,700
kubectl create service account

1691
01:25:23,700 --> 01:25:25,700
in the namespace kubesystem.

1692
01:25:25,700 --> 01:25:27,700
I call it admin.

1693
01:25:27,700 --> 01:25:30,700
And then if I want to get the token for that,

1694
01:25:30,700 --> 01:25:34,700
I would do kubectl get secrets in kubesystem.

1695
01:25:37,700 --> 01:25:40,700
And I would have one for...

1696
01:25:40,700 --> 01:25:43,700
Yeah, that would be admin token.

1697
01:25:43,700 --> 01:25:46,700
Here, that has been automatically created.

1698
01:25:48,700 --> 01:25:50,700
Sure.

1699
01:26:02,700 --> 01:26:04,700
So I just thought I'd ask you a little more

1700
01:26:04,700 --> 01:26:08,700
about my last question, which was about weave.

1701
01:26:08,700 --> 01:26:10,700
So it's actually kind of surprising to me

1702
01:26:10,700 --> 01:26:13,700
that there's no default network implementation.

1703
01:26:13,700 --> 01:26:16,700
You said that there might be one, but it's not that good.

1704
01:26:16,700 --> 01:26:19,700
So there is...

1705
01:26:19,700 --> 01:26:23,700
There is a default implementation.

1706
01:26:23,700 --> 01:26:26,700
There are three plugins for network.

1707
01:26:26,700 --> 01:26:29,700
The first one is kind of the no plugin.

1708
01:26:29,700 --> 01:26:32,700
In that case, when kubectl creates a container,

1709
01:26:32,700 --> 01:26:34,700
it just creates the container,

1710
01:26:34,700 --> 01:26:36,700
and it doesn't care about the network.

1711
01:26:36,700 --> 01:26:39,700
So in most cases, this will be okay,

1712
01:26:39,700 --> 01:26:41,700
but if you only have one node,

1713
01:26:41,700 --> 01:26:43,700
because it will create the container

1714
01:26:43,700 --> 01:26:45,700
using the container engine,

1715
01:26:45,700 --> 01:26:48,700
and so when you create a bunch of containers,

1716
01:26:48,700 --> 01:26:50,700
generally they can communicate together,

1717
01:26:50,700 --> 01:26:52,700
but if you have another node somewhere else,

1718
01:26:52,700 --> 01:26:54,700
they will just be able to communicate.

1719
01:26:54,700 --> 01:26:57,700
The second plugin is called kubenet.

1720
01:26:57,700 --> 01:27:00,700
When you use kubenet,

1721
01:27:00,700 --> 01:27:03,700
you have to associate to each node a subnet,

1722
01:27:03,700 --> 01:27:07,700
and then it's going to allocate addresses in that subnet.

1723
01:27:07,700 --> 01:27:11,700
So you can manually say, okay, this node has this subnet,

1724
01:27:11,700 --> 01:27:13,700
this node this subnet, this node this subnet.

1725
01:27:13,700 --> 01:27:15,700
So you get a bunch of different subnets,

1726
01:27:15,700 --> 01:27:19,700
and then you need to add bridging or routing

1727
01:27:19,700 --> 01:27:23,700
or something to interconnect these things together.

1728
01:27:23,700 --> 01:27:26,700
And then the third option is to use CNI,

1729
01:27:26,700 --> 01:27:28,700
the Container Networking interface,

1730
01:27:28,700 --> 01:27:34,700
and with CNI, basically each time that the kubelet needs

1731
01:27:34,700 --> 01:27:36,700
to allocate an IP address

1732
01:27:36,700 --> 01:27:40,700
or to configure the actual networking,

1733
01:27:41,700 --> 01:27:44,700
then it's going to call a CNI plugin.

1734
01:27:44,700 --> 01:27:47,700
CNI plugins can do whatever they want.

1735
01:27:47,700 --> 01:27:51,700
Virtually every Kubernetes cluster nowadays is using CNI,

1736
01:27:51,700 --> 01:27:54,700
but that's where you have many, many, many different

1737
01:27:54,700 --> 01:27:56,700
CNI plugins available.

1738
01:27:56,700 --> 01:27:59,700
So what's the benefit of Weave? I'm just kind of curious now.

1739
01:27:59,700 --> 01:28:02,700
So the thing is that if you're on-prem, for instance,

1740
01:28:02,700 --> 01:28:05,700
if you just have a few machines in a rack

1741
01:28:05,700 --> 01:28:07,700
or, you know, like you're setting up a lab

1742
01:28:07,700 --> 01:28:11,700
with actual switches, then you could use bridging,

1743
01:28:11,700 --> 01:28:13,700
and that would be super simple.

1744
01:28:13,700 --> 01:28:15,700
You could have like a DHCP server for the pods

1745
01:28:15,700 --> 01:28:17,700
and bridging, and that's it.

1746
01:28:17,700 --> 01:28:20,700
That would be extremely straightforward.

1747
01:28:20,700 --> 01:28:23,700
However, you can't do that on the cloud

1748
01:28:23,700 --> 01:28:27,700
because you can't do DHCP for containers

1749
01:28:27,700 --> 01:28:29,700
in a cloud environment,

1750
01:28:29,700 --> 01:28:32,700
and you don't have direct level 2 connectivity,

1751
01:28:32,700 --> 01:28:34,700
so you can't do bridging.

1752
01:28:34,700 --> 01:28:38,700
What you can do on some cloud providers

1753
01:28:38,700 --> 01:28:41,700
is that you can tell them,

1754
01:28:41,700 --> 01:28:47,700
okay, let arbitrary IP addresses circulate on the network fabric,

1755
01:28:47,700 --> 01:28:49,700
but by default you can't even do that.

1756
01:28:49,700 --> 01:28:52,700
Like, you know, if I have a cloud instance

1757
01:28:52,700 --> 01:28:55,700
and I change the IP address like with ifconfig,

1758
01:28:55,700 --> 01:28:57,700
it won't work.

1759
01:28:57,700 --> 01:28:59,700
I mean, I can change the IP address,

1760
01:28:59,700 --> 01:29:05,700
and the packets will never be forwarded by the cloud provider.

1761
01:29:05,700 --> 01:29:09,700
So that's why very quickly,

1762
01:29:09,700 --> 01:29:14,700
we need something so that we can encapsulate

1763
01:29:14,700 --> 01:29:17,700
the traffic between the pods

1764
01:29:17,700 --> 01:29:21,700
inside packets bearing the normal IP addresses of the instances,

1765
01:29:21,700 --> 01:29:23,700
and that's exactly what Weave is doing.

1766
01:29:23,700 --> 01:29:25,700
It's using encapsulation.

1767
01:29:25,700 --> 01:29:27,700
It's a little bit like a VPN if you want,

1768
01:29:27,700 --> 01:29:29,700
but instead of being between two machines,

1769
01:29:29,700 --> 01:29:31,700
it's between a bunch of machines,

1770
01:29:31,700 --> 01:29:34,700
and instead of having like one VPN server,

1771
01:29:34,700 --> 01:29:36,700
it's kind of smart.

1772
01:29:36,700 --> 01:29:39,700
So let's say we both are nodes running containers,

1773
01:29:39,700 --> 01:29:42,700
and I have a container wanting to send a packet here.

1774
01:29:42,700 --> 01:29:44,700
It's going to be like, okay,

1775
01:29:44,700 --> 01:29:46,700
the container is going to just send a packet.

1776
01:29:46,700 --> 01:29:48,700
The host will be like, okay,

1777
01:29:48,700 --> 01:29:50,700
this is a packet coming from a container

1778
01:29:50,700 --> 01:29:52,700
going to another container.

1779
01:29:52,700 --> 01:29:54,700
I'm going to look up where is that container.

1780
01:29:54,700 --> 01:29:56,700
Oh, on this machine.

1781
01:29:56,700 --> 01:30:01,700
So it's going to encapsulate that in a UDP packet,

1782
01:30:01,700 --> 01:30:05,700
send that UDP packet like node to node.

1783
01:30:05,700 --> 01:30:07,700
So you, the node, you receive that packet,

1784
01:30:07,700 --> 01:30:12,700
you can encapsulate that and forward it to the container.

1785
01:30:12,700 --> 01:30:14,700
Okay.

1786
01:30:16,700 --> 01:30:18,700
I had turned it off.

1787
01:30:18,700 --> 01:30:20,700
Okay.

1788
01:30:26,700 --> 01:30:28,700
Thanks.

1789
01:30:56,700 --> 01:30:59,700
Okay.

1790
01:31:26,700 --> 01:31:28,700
Okay.

1791
01:31:56,700 --> 01:31:59,700
Okay.

1792
01:32:26,700 --> 01:32:29,700
Okay.

1793
01:32:56,700 --> 01:32:59,700
Okay.

1794
01:33:26,700 --> 01:33:29,700
Okay.

1795
01:33:56,700 --> 01:33:59,700
Okay.

1796
01:34:26,700 --> 01:34:29,700
Okay.

1797
01:34:56,700 --> 01:34:59,700
Okay.

1798
01:35:26,700 --> 01:35:29,700
Okay.

1799
01:35:56,700 --> 01:35:59,700
Okay.

1800
01:36:26,700 --> 01:36:29,700
Okay.

1801
01:36:56,700 --> 01:36:59,700
Okay.

1802
01:37:26,700 --> 01:37:29,700
Okay.

1803
01:37:56,700 --> 01:37:58,700
Okay.

1804
01:38:26,700 --> 01:38:29,700
Okay.

1805
01:38:56,700 --> 01:38:59,700
Okay.

1806
01:39:26,700 --> 01:39:28,700
Okay.

1807
01:39:56,700 --> 01:39:58,700
Okay.

1808
01:40:26,700 --> 01:40:28,700
Okay.

1809
01:40:47,700 --> 01:40:49,700
All right.

1810
01:40:49,700 --> 01:40:54,700
Let's resume for the second part.

1811
01:40:57,700 --> 01:40:59,700
So I quickly looked at the questions

1812
01:40:59,700 --> 01:41:03,700
that had been asked in the Gitter chat room.

1813
01:41:03,700 --> 01:41:05,700
First, kind of logistics things,

1814
01:41:05,700 --> 01:41:07,700
so about size of text here.

1815
01:41:07,700 --> 01:41:09,700
I'm going to try to zoom more

1816
01:41:09,700 --> 01:41:11,700
because indeed from the back of the room

1817
01:41:11,700 --> 01:41:14,700
it's pretty hardcore to see what I'm typing.

1818
01:41:14,700 --> 01:41:17,700
Almost all the commands that I type

1819
01:41:17,700 --> 01:41:19,700
are in the slides in the gray rectangles.

1820
01:41:19,700 --> 01:41:23,700
We have almost continuous integration on the slides

1821
01:41:23,700 --> 01:41:27,700
that extracts the commands from the gray rectangles

1822
01:41:27,700 --> 01:41:29,700
and copy-pastes them into terminals

1823
01:41:29,700 --> 01:41:31,700
to make sure everything works.

1824
01:41:31,700 --> 01:41:33,700
So sometimes I deviate a little bit

1825
01:41:33,700 --> 01:41:36,700
because I want to show alternate ways of doing things,

1826
01:41:36,700 --> 01:41:39,700
but if at some point you can't see a command that I typed,

1827
01:41:39,700 --> 01:41:42,700
you will see a command that works in the slide themselves.

1828
01:41:42,700 --> 01:41:44,700
But I'm going to try to zoom that enough

1829
01:41:44,700 --> 01:41:47,700
so that you can see what's going on here.

1830
01:41:47,700 --> 01:41:50,700
There was another question about clarification

1831
01:41:50,700 --> 01:41:52,700
of what exactly our services are.

1832
01:41:52,700 --> 01:41:56,700
Are there a little bit like Elastic IPs on Amazon, for instance?

1833
01:41:56,700 --> 01:42:00,700
I'm going to say kind of, but it's more like that.

1834
01:42:00,700 --> 01:42:04,700
It's extremely hard to give a definition of services

1835
01:42:04,700 --> 01:42:07,700
that would be simultaneously correct

1836
01:42:07,700 --> 01:42:10,700
and also not kind of eye-rolling.

1837
01:42:10,700 --> 01:42:13,700
We could say, oh, it's a static IP address for a pod.

1838
01:42:13,700 --> 01:42:15,700
It's kind of easy to understand,

1839
01:42:15,700 --> 01:42:20,700
but it's gross simplification because it's not just that.

1840
01:42:20,700 --> 01:42:23,700
A service can also give us load balancing,

1841
01:42:23,700 --> 01:42:25,700
but sometimes there is just one pod,

1842
01:42:25,700 --> 01:42:27,700
so there is nothing to balance.

1843
01:42:27,700 --> 01:42:29,700
And it's not always about pods.

1844
01:42:29,700 --> 01:42:32,700
Sometimes a service can be used to access something

1845
01:42:32,700 --> 01:42:36,700
that is not on a pod or that is even outside of the cluster.

1846
01:42:36,700 --> 01:42:39,700
So that's why the only correct definition

1847
01:42:39,700 --> 01:42:42,700
is stable endpoint for a network thing,

1848
01:42:42,700 --> 01:42:45,700
but that's the kind of definition that lets us,

1849
01:42:45,700 --> 01:42:47,700
like, okay, what does that really mean?

1850
01:42:47,700 --> 01:42:50,700
We're going, of course, to see examples of services

1851
01:42:50,700 --> 01:42:52,700
and create our own,

1852
01:42:52,700 --> 01:42:57,700
and I hope it will clear up any questions you might have.

1853
01:42:57,700 --> 01:43:00,700
So it's time to run our first containers,

1854
01:43:00,700 --> 01:43:05,700
now that we've seen a lot of theory and concepts and et cetera.

1855
01:43:05,700 --> 01:43:08,700
And first things first, remember what we said earlier.

1856
01:43:08,700 --> 01:43:12,700
So first, we can't run really a container.

1857
01:43:12,700 --> 01:43:14,700
What we can run is a pod,

1858
01:43:14,700 --> 01:43:16,700
and in that pod there will be a container.

1859
01:43:16,700 --> 01:43:19,700
And again, we can't really run,

1860
01:43:19,700 --> 01:43:22,700
we can't tell Kubernetes to run something.

1861
01:43:22,700 --> 01:43:25,700
All we can do is write a nice spec saying,

1862
01:43:25,700 --> 01:43:27,700
I would like this thing to be running,

1863
01:43:27,700 --> 01:43:29,700
send that to the API server,

1864
01:43:29,700 --> 01:43:32,700
and then the API is going to store that in its CD,

1865
01:43:32,700 --> 01:43:35,700
and then a bunch of controllers are going to wake up

1866
01:43:35,700 --> 01:43:37,700
and make that happen.

1867
01:43:37,700 --> 01:43:39,700
Okay, for the first example,

1868
01:43:39,700 --> 01:43:41,700
we're going to do something fairly simple.

1869
01:43:41,700 --> 01:43:44,700
I just want to run ping.

1870
01:43:44,700 --> 01:43:47,700
So I want to have a ping in a container,

1871
01:43:47,700 --> 01:43:49,700
like pinging an external address.

1872
01:43:49,700 --> 01:43:54,700
And the syntax for that is going to be kubectl run.

1873
01:43:54,700 --> 01:43:56,700
Then I have to give a name.

1874
01:43:56,700 --> 01:43:58,700
So the name I'm going to give is ping pong,

1875
01:43:58,700 --> 01:44:02,700
but it could be foo or abc, like it doesn't matter.

1876
01:44:02,700 --> 01:44:07,700
Then I indicate the image, the container image that I want to use.

1877
01:44:07,700 --> 01:44:09,700
So here I'm using alpine,

1878
01:44:09,700 --> 01:44:13,700
which is a tiny distro that's often super convenient

1879
01:44:13,700 --> 01:44:16,700
for container workloads because it's so small,

1880
01:44:16,700 --> 01:44:19,700
so it's fast to pull, et cetera, et cetera.

1881
01:44:19,700 --> 01:44:25,700
And then I put the command I want to run, ping 1.1.1.1.

1882
01:44:27,700 --> 01:44:30,700
So at first I get a deprecation warning.

1883
01:44:30,700 --> 01:44:32,700
Let's pretend that this did not happen.

1884
01:44:32,700 --> 01:44:37,700
I'm going to explain in a few minutes why it's here and what it means.

1885
01:44:37,700 --> 01:44:41,700
And then I see deployment ping pong created.

1886
01:44:41,700 --> 01:44:44,700
And remember, when we see that,

1887
01:44:44,700 --> 01:44:47,700
it doesn't mean that our thing is running.

1888
01:44:47,700 --> 01:44:51,700
It just means I've written the spec in each CD,

1889
01:44:51,700 --> 01:44:54,700
and now I'm going to work on that.

1890
01:44:54,700 --> 01:44:58,700
Now if I do kubectl get pods,

1891
01:44:58,700 --> 01:45:03,700
I see that I have indeed a pod called ping pong, yada yada,

1892
01:45:03,700 --> 01:45:05,700
and it's running.

1893
01:45:05,700 --> 01:45:09,700
It's 30 seconds old, so it seems to be up and running.

1894
01:45:09,700 --> 01:45:14,700
However, it's interesting because that pod is not named ping pong.

1895
01:45:14,700 --> 01:45:20,700
It's named ping pong, and then there is a string here and another string there.

1896
01:45:20,700 --> 01:45:24,700
And here it did not tell me pod ping pong created.

1897
01:45:24,700 --> 01:45:28,700
It told me deployment ping pong created.

1898
01:45:28,700 --> 01:45:32,700
So to figure out what exactly happened,

1899
01:45:32,700 --> 01:45:38,700
I'm going to do kubectl get all, like show me everything.

1900
01:45:38,700 --> 01:45:45,700
It's not exactly everything because a bunch of objects here are not shown,

1901
01:45:45,700 --> 01:45:51,700
but at least I see that I have indeed a deployment named ping pong,

1902
01:45:51,700 --> 01:45:57,700
and then I have a replica set named ping pong something,

1903
01:45:57,700 --> 01:46:03,700
and then I have a pod named ping pong something and something more.

1904
01:46:03,700 --> 01:46:06,700
So I created a deployment.

1905
01:46:06,700 --> 01:46:08,700
That deployment created a replica set.

1906
01:46:08,700 --> 01:46:11,700
That replica set created a pod.

1907
01:46:11,700 --> 01:46:13,700
So at this point, we can wonder like why?

1908
01:46:13,700 --> 01:46:16,700
Why all these extra layers?

1909
01:46:16,700 --> 01:46:18,700
Do we really need all that?

1910
01:46:18,700 --> 01:46:22,700
So yeah, why do we need all that?

1911
01:46:22,700 --> 01:46:27,700
Because imagine that later I want to scale my things.

1912
01:46:27,700 --> 01:46:29,700
I don't want just like one ping.

1913
01:46:29,700 --> 01:46:30,700
I want multiple pings.

1914
01:46:30,700 --> 01:46:34,700
Okay, for ping, it doesn't make sense, but for a web service, that would make sense.

1915
01:46:34,700 --> 01:46:39,700
So if I just have a pod definition in that pod definition,

1916
01:46:39,700 --> 01:46:43,700
I don't have any number or any scaling notion with that.

1917
01:46:43,700 --> 01:46:53,700
So what I want is an object that is I want that many pods like this specification.

1918
01:46:53,700 --> 01:46:58,700
So I have a pod specification, and then I have something bigger than that,

1919
01:46:58,700 --> 01:47:01,700
and that thing bigger than that is a replica set.

1920
01:47:01,700 --> 01:47:07,700
A replica set is a pod specification and a number.

1921
01:47:07,700 --> 01:47:12,700
So the replica set will help me to scale things fairly easily.

1922
01:47:12,700 --> 01:47:17,700
But if I just have a replica set, to simplify things a little bit,

1923
01:47:17,700 --> 01:47:25,700
we could say the mission of the replica set is to make sure that I have that many pods with that specification.

1924
01:47:26,700 --> 01:47:33,700
So let's say I create a replica set saying I want five pods pinging 1.1.1.1.

1925
01:47:33,700 --> 01:47:37,700
And then I change my mind. I say I want to ping 2.2.2.2.

1926
01:47:37,700 --> 01:47:40,700
So I update the definition of the replica set.

1927
01:47:40,700 --> 01:47:48,700
So suddenly the mission of the replica set is to make sure I have five pods pinging 2.2.2.2.

1928
01:47:48,700 --> 01:47:57,700
And at that point, the replica set had its eyes on the pods pinging 1.1.1.1

1929
01:47:57,700 --> 01:48:01,700
and making sure like, OK, if a pod goes away, I'm going to restart it.

1930
01:48:01,700 --> 01:48:05,700
If I have too many pods, I'm going to remove the extra ones, et cetera.

1931
01:48:05,700 --> 01:48:11,700
So this replica set was seeing five pods running the right thing and everything was fine.

1932
01:48:11,700 --> 01:48:14,700
Suddenly I changed the replica set.

1933
01:48:14,700 --> 01:48:19,700
Suddenly the replica set doesn't see five pods. It sees zero pods.

1934
01:48:19,700 --> 01:48:24,700
And it's like, oh, panic. And it's creating five new pods with the new definition.

1935
01:48:24,700 --> 01:48:31,700
If you find the ping example a little bit silly, imagine that we're talking about the version of the code that we've deployed.

1936
01:48:31,700 --> 01:48:35,700
So I'm deploying V5, and then I want to deploy V6.

1937
01:48:35,700 --> 01:48:39,700
And so when I update my replica set, suddenly the replica set is kind of lost.

1938
01:48:39,700 --> 01:48:42,700
It's like, OK, I need to recreate all the pods.

1939
01:48:42,700 --> 01:48:44,700
That's typically not what we want.

1940
01:48:44,700 --> 01:48:51,700
We want to do a progressive and smooth rollout where we update the pods one by one,

1941
01:48:51,700 --> 01:48:55,700
or at least like by groups in a progressive way.

1942
01:48:55,700 --> 01:49:04,700
So to do that, instead of just updating the replica set and having everything happen in chaos,

1943
01:49:04,700 --> 01:49:08,700
what we will do is that we will have two replica sets,

1944
01:49:08,700 --> 01:49:14,700
one corresponding to the old version, one corresponding to the new version.

1945
01:49:14,700 --> 01:49:17,700
And we are going to do a kind of fade in, fade out.

1946
01:49:17,700 --> 01:49:22,700
So I'm going to, OK, let's say this is old version. This is new version.

1947
01:49:22,700 --> 01:49:28,700
So I'm going to scale down the old version while I scale up the new version.

1948
01:49:28,700 --> 01:49:31,700
A little bit like a DJ mixing two tracks, basically.

1949
01:49:31,700 --> 01:49:37,700
And so to coordinate that, I'm going to have another object sitting on top.

1950
01:49:37,700 --> 01:49:39,700
And that object is the deployment.

1951
01:49:39,700 --> 01:49:43,700
So what really happens is that when I create a deployment,

1952
01:49:43,700 --> 01:49:49,700
it's going to create a replica set corresponding to the version that I want to deploy.

1953
01:49:49,700 --> 01:49:52,700
And that replica set is going to create a pod.

1954
01:49:52,700 --> 01:49:58,700
Now later when I want to update these pods, I update the deployment object.

1955
01:49:58,700 --> 01:50:06,700
And what it's going to do is create a new replica set and then do that scale down, scale up in parallel.

1956
01:50:07,700 --> 01:50:13,700
So that's why we have these three layers, pod, replica set and deployment.

1957
01:50:13,700 --> 01:50:16,700
Each of them adds something.

1958
01:50:16,700 --> 01:50:18,700
Right.

1959
01:50:18,700 --> 01:50:24,700
So supposedly we are running a pod that's pinging some IP address.

1960
01:50:24,700 --> 01:50:28,700
How do we confirm that this is working correctly?

1961
01:50:28,700 --> 01:50:31,700
I'm going to do kubectl logs.

1962
01:50:31,700 --> 01:50:35,700
And then I give the name of a pod.

1963
01:50:35,700 --> 01:50:40,700
I can't do just kubectl logs ping pong because ping pong is not a pod.

1964
01:50:40,700 --> 01:50:41,700
It's a deployment.

1965
01:50:41,700 --> 01:50:44,700
So if I do that, it's telling me pod ping pong not found.

1966
01:50:44,700 --> 01:50:52,700
I have to put the name of the pod, which in my case happens to be this.

1967
01:50:52,700 --> 01:50:54,700
Here I use the completion.

1968
01:50:54,700 --> 01:50:58,700
I just hit tab and it completed to the only pod matching that.

1969
01:50:58,700 --> 01:51:02,700
But I could also copy paste the pod ID.

1970
01:51:02,700 --> 01:51:07,700
So when I do that, I get the output of my ping command.

1971
01:51:07,700 --> 01:51:09,700
So far so good.

1972
01:51:09,700 --> 01:51:16,700
And another option I have is to do kubectl logs deploy slash ping pong.

1973
01:51:16,700 --> 01:51:24,700
In that case, it's going to be like, OK, let's look up that deployment ping pong, deploy ping pong or deployment ping pong.

1974
01:51:24,700 --> 01:51:28,700
That's the same thing. Deploy is just a short version of deployment.

1975
01:51:28,700 --> 01:51:37,700
So it's going to figure out, OK, what are the pods corresponding to that deployment and show me the same output.

1976
01:51:37,700 --> 01:51:40,700
OK.

1977
01:51:40,700 --> 01:51:42,700
Right.

1978
01:51:42,700 --> 01:51:49,700
If I want, I can also use dash dash tail and indicate how many lines I want to see.

1979
01:51:49,700 --> 01:51:51,700
Three lines is good enough.

1980
01:51:51,700 --> 01:51:57,700
I can also add dash dash follow and have the equivalent of a tail dash f.

1981
01:51:57,700 --> 01:52:02,700
In that case, it's going to stream the logs of that pod for me.

1982
01:52:02,700 --> 01:52:05,700
Right.

1983
01:52:05,700 --> 01:52:09,700
Now, the next step is that I want to scale this deployment.

1984
01:52:09,700 --> 01:52:11,700
I don't want just one pod doing that thing.

1985
01:52:11,700 --> 01:52:13,700
I want multiple pods.

1986
01:52:13,700 --> 01:52:19,700
So I do kubectl scale.

1987
01:52:19,700 --> 01:52:21,700
What do I want to scale a deployment?

1988
01:52:21,700 --> 01:52:23,700
Which one? Ping pong.

1989
01:52:23,700 --> 01:52:25,700
How? With replicas.

1990
01:52:25,700 --> 01:52:28,700
I want three replicas.

1991
01:52:28,700 --> 01:52:30,700
It's telling me immediately, yep, done.

1992
01:52:30,700 --> 01:52:35,700
But again, that just means that it has stored the new spec in its CD.

1993
01:52:35,700 --> 01:52:41,700
And then the controllers wake up and create the pods and push them to the cubelets.

1994
01:52:41,700 --> 01:52:54,700
And now here, look, if I run the kubectl logs command again, I'm going to have a little line of information on top telling me I found three pods.

1995
01:52:54,700 --> 01:52:58,700
And I'm going to show you the output of that specific one.

1996
01:52:58,700 --> 01:53:05,700
So it's not showing me the logs of multiple pods at the same time.

1997
01:53:05,700 --> 01:53:08,700
OK.

1998
01:53:08,700 --> 01:53:14,700
The replicas set is making sure that my pods keep running.

1999
01:53:14,700 --> 01:53:23,700
So for instance, if I do kubectl get pods and I decide to destroy one of these pods.

2000
01:53:23,700 --> 01:53:26,700
So I'm going to do something a little bit fancy here.

2001
01:53:26,700 --> 01:53:28,700
I'm going to split my screen.

2002
01:53:28,700 --> 01:53:32,700
And here I'm going to do kubectl get pods.

2003
01:53:32,700 --> 01:53:46,700
And I'm going to add dash dash watch or dash w in short, which means show me the output of that command and then keep looking at these objects and show me if anything moves.

2004
01:53:46,700 --> 01:53:52,700
Now I'm going to do kubectl delete pod.

2005
01:53:52,700 --> 01:53:57,700
And I'm going to put the name of one of these pods here.

2006
01:53:57,700 --> 01:54:09,700
When I do that, I see that my pod will be terminating like immediately and instantly the replicas set whose job is to make sure I have exactly three pods doing that ping.

2007
01:54:09,700 --> 01:54:11,700
The replicas set is like, oh, problem.

2008
01:54:11,700 --> 01:54:14,700
We only have two pods matching this spec.

2009
01:54:14,700 --> 01:54:20,700
So I'm creating a third pod right away to replace the one that was destroyed.

2010
01:54:20,700 --> 01:54:24,700
And three seconds later, the replacement pod is up and running.

2011
01:54:24,700 --> 01:54:31,700
This is the sometimes we call it like the self healing features of Kubernetes.

2012
01:54:31,700 --> 01:54:46,700
One of one of the points of using Kubernetes is to make sure that we have that many replicas up and running at any given point and automatically restart pods or containers when needed.

2013
01:54:46,700 --> 01:54:55,700
That's the that's the kind of thing we can also achieve with auto scaling groups, for instance, on Amazon.

2014
01:54:55,700 --> 01:55:04,700
That's the equivalent of using, for instance, supervisor to make sure that one process is always up and running, but on the cluster.

2015
01:55:04,700 --> 01:55:17,700
So it's not just making sure that one process is running on the machine, but making sure that three copies of this container are running across the cluster.

2016
01:55:17,700 --> 01:55:22,700
So now let's go back. You know, I said, oh, there is a deprecation warning when we did kubectl run.

2017
01:55:22,700 --> 01:55:24,700
Let's revisit that.

2018
01:55:24,700 --> 01:55:28,700
If I wanted, I could add some flags to kubectl run.

2019
01:55:28,700 --> 01:55:33,700
For instance, I could do kubectl run dash dash restart never.

2020
01:55:33,700 --> 01:55:37,700
And in that case, it would not create a deployment.

2021
01:55:37,700 --> 01:55:48,700
In that case, it would really give me just a pod, because if I say dash dash restart never, it means I don't care about automatically restarting and scaling, etc.

2022
01:55:48,700 --> 01:55:51,700
I just want a one shot container.

2023
01:55:51,700 --> 01:55:56,700
In that case, kubectl run is like, OK, I'm going to give you just a straight pod.

2024
01:55:56,700 --> 01:56:04,700
I can also use kubectl run dash dash schedule, and then I can pass a cron style schedule.

2025
01:56:04,700 --> 01:56:11,700
So that would be something like this dash dash schedule.

2026
01:56:11,700 --> 01:56:14,700
You know, it's the five fields of a cron tab.

2027
01:56:14,700 --> 01:56:19,700
And this would mean and then I would put like whatever here.

2028
01:56:19,700 --> 01:56:24,700
The thing I would put behind would be run every minute on the cluster.

2029
01:56:24,700 --> 01:56:32,700
So that means that depending on the exact flags and combination of flags, I'm passing to kubectl run.

2030
01:56:32,700 --> 01:56:38,700
Sometimes it would create a deployment. Sometimes it would create a pod.

2031
01:56:38,700 --> 01:56:42,700
Sometimes it would create a cron job, etc.

2032
01:56:42,700 --> 01:56:45,700
And this can be kind of confusing.

2033
01:56:45,700 --> 01:56:53,700
And so there was the decision a while ago to say, OK, let's let's stop with this kind of pseudo magic where you have to know which combination of flags do what.

2034
01:56:53,700 --> 01:56:59,700
And instead, let's be explicit about things. Explicit is better than implicit.

2035
01:56:59,700 --> 01:57:03,700
So if we want to create a deployment, we use kubectl create deployment.

2036
01:57:03,700 --> 01:57:07,700
If we want to create a job, we do kubectl create job.

2037
01:57:07,700 --> 01:57:10,700
If we want to, etc, etc, etc.

2038
01:57:10,700 --> 01:57:20,700
And eventually, at some point, kubectl run would be repurposed so that it just runs a one off job, a little bit like Docker run.

2039
01:57:20,700 --> 01:57:23,700
So that's why we got that deprecation warning.

2040
01:57:23,700 --> 01:57:31,700
I'm still using kubectl run like for the first thing that we want to run because it's simpler.

2041
01:57:31,700 --> 01:57:38,700
But in the future, we will almost never use kubectl run again.

2042
01:57:38,700 --> 01:57:41,700
So what are the various options we have to run things?

2043
01:57:41,700 --> 01:57:52,700
We have kubectl run, which is great when getting started because it's pretty close to Docker run, for instance, and it can create a bunch of different things.

2044
01:57:52,700 --> 01:57:59,700
Then we have kubectl create and then the resource type like kubectl create deployment, for instance.

2045
01:57:59,700 --> 01:58:02,700
That one is fairly easy to understand.

2046
01:58:02,700 --> 01:58:07,700
Like when you look at the script doing that, it's fairly easy to understand what's happening.

2047
01:58:07,700 --> 01:58:10,700
But it won't let you create everything.

2048
01:58:10,700 --> 01:58:16,700
Like there are some resources that you won't be able to create with kubectl create.

2049
01:58:16,700 --> 01:58:29,700
And then finally, we have kubectl create dash f or kubectl apply dash f that both take a YAML file and the YAML file contains a resource specification.

2050
01:58:29,700 --> 01:58:35,700
And then we can pass absolutely whatever we want because that YAML spec can be arbitrary.

2051
01:58:35,700 --> 01:58:46,700
So if there is a new type of object tomorrow and I want to create that, I don't need to wait for a special command like kubectl create elastic search cluster.

2052
01:58:46,700 --> 01:58:55,700
I could write the YAML specification for that and then use kubectl create dash f my YAML file.

2053
01:58:55,700 --> 01:58:57,700
Right.

2054
01:58:57,700 --> 01:59:00,700
OK. Back to logs.

2055
01:59:00,700 --> 01:59:17,700
We have on our pods, if I use, for instance, kubectl describe pods on one of my ping pong pods, I see that on my pods I have labels.

2056
01:59:17,700 --> 01:59:22,700
In particular, I have a label run equal ping pong.

2057
01:59:22,700 --> 01:59:29,700
When I use kubectl run to create something, there will be a run label with the name of the object.

2058
01:59:29,700 --> 01:59:39,700
So if I do kubectl run abc123, then there will be a label run equal abc123.

2059
01:59:39,700 --> 01:59:44,700
So I can use that label when I use commands.

2060
01:59:44,700 --> 01:59:54,700
So, for instance, I can do kubectl get pods dash l like labels run equal ping pong.

2061
01:59:54,700 --> 02:00:01,700
And that shows me only the pods with that specific label.

2062
02:00:01,700 --> 02:00:07,700
Virtually every command that can manipulate objects can use selectors.

2063
02:00:07,700 --> 02:00:17,700
So, for instance, earlier when I used delete, I could do kubectl delete pods with that label and it would delete all of them.

2064
02:00:17,700 --> 02:00:22,700
Later we will see other commands like kubectl label to manipulate the labels themselves.

2065
02:00:22,700 --> 02:00:26,700
Same thing. We can use these commands.

2066
02:00:26,700 --> 02:00:37,700
Same thing with kubectl logs. I can do kubectl logs and I say, OK, show me the logs of all the pods that have this label.

2067
02:00:37,700 --> 02:00:45,700
And now I'm seeing not just the logs of one pod, but all the pods of this deployment.

2068
02:00:45,700 --> 02:00:54,700
So it's not really a nice way to consume this information because here I don't see which pod sent which log line.

2069
02:00:54,700 --> 02:00:59,700
So it's not so great. But it's a good starting point.

2070
02:00:59,700 --> 02:01:07,700
I can use, for instance, dash dash tail one. So show me the last line of log of each pod.

2071
02:01:07,700 --> 02:01:09,700
So since I have three pods, I get three lines.

2072
02:01:09,700 --> 02:01:18,700
And I can also use dash dash follow. And then it's going to stream in parallel all the logs of these pods.

2073
02:01:18,700 --> 02:01:25,700
This, like combining streaming and a selector only works since very recent versions of Kubernetes.

2074
02:01:25,700 --> 02:01:32,700
Why? Let me show you why. I'm going to scale up my ping a little bit more.

2075
02:01:32,700 --> 02:01:44,700
So I'm going to do kubectl scale deployment ping pong. How many replicas? Let's say 10.

2076
02:01:44,700 --> 02:01:48,700
And then I'm going to run the kubectl logs command again.

2077
02:01:48,700 --> 02:01:55,700
And now you're seeing me know I can do that because there are 10 log streams and I will only do five. Why?

2078
02:01:55,700 --> 02:02:02,700
Because when we are streaming logs for each log stream, we are opening a separate connection to the API server.

2079
02:02:02,700 --> 02:02:09,700
And then the API server in turn is opening a connection to the kubectl that is responsible for that pod.

2080
02:02:09,700 --> 02:02:14,700
So now if I do that with 10 pods, it's opening like 10 connections to the API server.

2081
02:02:14,700 --> 02:02:20,700
And for each of these connections, like one more connections to the to the node running that pod.

2082
02:02:20,700 --> 02:02:24,700
So this is why it's only a recent option.

2083
02:02:24,700 --> 02:02:28,700
Like in older versions of kubectl, it was like, no, we're not going to do anything fancy like that.

2084
02:02:28,700 --> 02:02:43,700
It's like one stream at a time. And then when the option was added, we're like, huh, if I have a big cluster with like thousands of pods and I fumbled a little bit and I end up passing a selector that selects 1000 pods,

2085
02:02:43,700 --> 02:02:51,700
I'm going to open 1000 connections to the API server and the API server is probably going to not feel great about that.

2086
02:02:51,700 --> 02:02:56,700
So that's why there is this limit of five by default.

2087
02:02:56,700 --> 02:03:02,700
Now, if I know what I'm doing, I can say, hey, max log requests equal 100.

2088
02:03:02,700 --> 02:03:06,700
And now it's going to stream me all these things at the same time.

2089
02:03:06,700 --> 02:03:18,700
Still, even with recent versions of kubectl, that's I mean, that's not great because I'm not seeing which pod is sending which line of output.

2090
02:03:18,700 --> 02:03:28,700
So I'm going to give you an extra tool that is great to have like streams of logs that's Stern.

2091
02:03:28,700 --> 02:03:33,700
So that's not part of Kubernetes. It's a third party external thing.

2092
02:03:33,700 --> 02:03:41,700
It's a go program, which means that you just have to download that binary and it's available for Linux, Mac, Windows, and you just download the binary and that's it.

2093
02:03:41,700 --> 02:03:47,700
And when you do Stern, you can pass it either a selector.

2094
02:03:47,700 --> 02:03:52,700
So, again, like dash L run equal ping pong.

2095
02:03:52,700 --> 02:03:58,700
And it's going to stream all these logs.

2096
02:03:58,700 --> 02:04:01,700
Okay, it's catching up.

2097
02:04:01,700 --> 02:04:07,700
And now when it stream the logs, we get something that reminds us a little bit of compose.

2098
02:04:07,700 --> 02:04:24,700
So I have the name of the pod and then the name of the container within the pod and then the output and a nice set of colors here, which will turns out to be convenient in a little moment.

2099
02:04:24,700 --> 02:04:35,700
All right. Another advantage of Stern is that if I scale up or down, it's going to keep track of new pods coming up and coming down.

2100
02:04:35,700 --> 02:04:50,700
So, for instance, if I do Cube CTL scale deployment, ping pong dash dash replicas 12, you're going to see here.

2101
02:04:50,700 --> 02:04:53,700
Okay, that was really, really fast.

2102
02:04:53,700 --> 02:04:58,700
And but you see like plus here.

2103
02:04:58,700 --> 02:05:00,700
So we detected how we have new pods matching that.

2104
02:05:00,700 --> 02:05:06,700
So we're going to dynamically attach and show the stream.

2105
02:05:06,700 --> 02:05:14,700
And one last thing about Stern is that if I find super lazy, I don't even want to find out the selector of my pods.

2106
02:05:14,700 --> 02:05:20,700
I can pass a string and it's going to be handled as a reg ex against pod names.

2107
02:05:20,700 --> 02:05:31,700
So, for instance, if I do Stern ping, it's going to match all the pods with ping in the name and show them to me.

2108
02:05:31,700 --> 02:05:38,700
Of course, that's fairly dangerous because if I do Stern dot, it's going to show me the logs of all the pods.

2109
02:05:38,700 --> 02:05:45,700
So we have to keep in mind each time it's opening one connection to the API servers on a big production cluster.

2110
02:05:45,700 --> 02:05:56,700
If you do that, the people responsible for the clusters might feel might have feelings about it, but it's it's available if necessary.

2111
02:05:56,700 --> 02:06:04,700
Okay, streaming logs, shortcomings of cubes logs before moving up.

2112
02:06:04,700 --> 02:06:11,700
Yeah. What's the namespace where we created these pods by default?

2113
02:06:11,700 --> 02:06:13,700
The namespace is default.

2114
02:06:13,700 --> 02:06:29,700
So, for instance, if I if I were to use the cube CTL get pods dash dash all namespaces, then I would see all my cube system pods and then the ping pong pods are in default.

2115
02:06:29,700 --> 02:06:46,700
Okay. And while I'm here, just to simplify things a little bit, I'm going to scale down ping pong to two pods so that when I display resources, et cetera, we're not getting pages and pages of useless ping pong pods.

2116
02:06:47,700 --> 02:06:55,700
If you're worried about are we basically flooding one point one point one point one, don't worry.

2117
02:06:55,700 --> 02:07:04,700
It's designed well, not specifically for that purpose, but it's been designed to withstand random streams of traffic.

2118
02:07:04,700 --> 02:07:16,700
So there nobody is waking up right now in the middle of the night somewhere wondering why they are like 70 people sending a bunch of ping packets to that address.

2119
02:07:16,700 --> 02:07:19,700
Right. Questions.

2120
02:07:19,700 --> 02:07:24,700
Yes.

2121
02:07:24,700 --> 02:07:41,700
So the command I use for Stern to so I could use just like a substring and then we do a match or I can do dash L and then the label expression like dash ping pong.

2122
02:07:41,700 --> 02:07:46,700
Note that dash L can also be dash dash selector.

2123
02:07:46,700 --> 02:07:51,700
That's the same thing.

2124
02:07:51,700 --> 02:07:59,700
Okay. Yes.

2125
02:07:59,700 --> 02:08:05,700
Is that label available only for Cube CTL run.

2126
02:08:05,700 --> 02:08:22,700
Right. So in that case when I do Cube CTL run foo, it's going to put a label run equal foo. If I use Cube CTL create deployment foo, the label is going to be app equal foo.

2127
02:08:22,700 --> 02:08:25,700
It's just a convention in the Cube CTL commands.

2128
02:08:25,700 --> 02:08:30,700
Yeah.

2129
02:08:30,700 --> 02:08:34,700
If I if I create a job or.

2130
02:08:34,700 --> 02:08:35,700
Right.

2131
02:08:35,700 --> 02:08:51,700
If I if I do Cube CTL create job, the job object or if I do Cube CTL run dash dash restart on failure, the job object is going to create a pod and it's going to let's say monitor that pod.

2132
02:08:51,700 --> 02:09:00,700
If the pod exits with a zero status code, meaning all good, then the job object is going to be like, okay, perfect. Great.

2133
02:09:00,700 --> 02:09:18,700
But if the pod exits with a non zero status code or if it gets killed by a signal or if it goes out of memory or if the node goes away and the pod goes away, then the job is going to recreate that pod until it completes successfully.

2134
02:09:18,700 --> 02:09:23,700
That's that will be the difference between a job and a pod.

2135
02:09:23,700 --> 02:09:25,700
Right.

2136
02:09:25,700 --> 02:09:42,700
Okay, so we've been sending things around. That's fun, but we would like to have something that has a more of a purpose. We would like something that can serve network traffic, and that will be the opportunity to revisit services and and hopefully really figure them out this time.

2137
02:09:42,700 --> 02:09:54,700
So we want to we are going to to create a little web service and then we will expose it so that we can connect to that service.

2138
02:09:54,700 --> 02:10:02,700
We have different types of services, depending on basically where the traffic comes from.

2139
02:10:02,700 --> 02:10:13,700
So we have cluster IP services, which are purely internal services, just like the Kubernetes service that we saw a little bit earlier.

2140
02:10:13,700 --> 02:10:19,700
It just has like a cluster IP and that's it. So this is purely internal communication.

2141
02:10:19,700 --> 02:10:35,700
Then we have node port node port is a kind of basic building block for external communication. When I create a node port service, I still have a cluster IP that I can use internally, but I also get a node port.

2142
02:10:35,700 --> 02:10:51,700
So it's a port between 30,000 and 32 768 allocated for my service. And then when I connect to any of my four nodes, it will on that node port number, it will send me to my service.

2143
02:10:51,700 --> 02:11:03,700
So for instance, for the Web UI, when we will want to connect to that, we will create a node port. A port number will be allocated at random, maybe like 30,000 123.

2144
02:11:03,700 --> 02:11:10,700
And then if I connect to one node on 30,000 123, I will get the Web UI.

2145
02:11:10,700 --> 02:11:22,700
Then we have two more service types load balancer and external name load balancer is used when we want a cloud load balancer like an ELB or GLB.

2146
02:11:22,700 --> 02:11:31,700
Just to dispel a doubt, if I want to load balance traffic, I don't need a load balancer service.

2147
02:11:31,700 --> 02:11:39,700
Any service will do load balancing if I have multiple pods behind it. So load balancer is only for a cloud load balancer.

2148
02:11:39,700 --> 02:11:54,700
The point of this is if I want to expose a service on a specific port, not something like 30,000 whatever, but like port 80 or port 443 or port 22 for SSH or Git,

2149
02:11:54,700 --> 02:12:06,700
then I will create a load balancer service. Then Kubernetes is going to make some API calls to my cloud infrastructure that will create a cloud load balancer.

2150
02:12:06,700 --> 02:12:11,700
And that cloud load balancer will send traffic to the pods behind it.

2151
02:12:11,700 --> 02:12:20,700
And finally, we have external name, which just sets a DNS alias so we can safely ignore it for now.

2152
02:12:21,700 --> 02:12:30,700
OK, so for our first example of a service, we are going to run an image called JPTadzo slash HTTP Env.

2153
02:12:30,700 --> 02:12:38,700
You will see many examples like on Kubernetes using just NGINX because it gives us a nice and easy and simple web server.

2154
02:12:38,700 --> 02:12:43,700
The only downside of NGINX is that it will give us a welcome to NGINX page.

2155
02:12:43,700 --> 02:12:49,700
And if we put load balancing, all the backends give us the same welcome to NGINX page.

2156
02:12:49,700 --> 02:12:57,700
So this HTTP Env little service will show its environment, including the hostname.

2157
02:12:57,700 --> 02:13:03,700
And so that will show us which pod served the response.

2158
02:13:03,700 --> 02:13:10,700
So I'm going to do kubectl create deployment.

2159
02:13:10,700 --> 02:13:14,700
I will call that HTTP Env.

2160
02:13:14,700 --> 02:13:18,700
And then I have to indicate which image I want to use.

2161
02:13:18,700 --> 02:13:22,700
So image JPTadzo HTTP Env.

2162
02:13:22,700 --> 02:13:24,700
That's it.

2163
02:13:24,700 --> 02:13:29,700
And then I'm going to expose that deployment.

2164
02:13:29,700 --> 02:13:32,700
So basically create a service.

2165
02:13:32,700 --> 02:13:34,700
So kubectl expose.

2166
02:13:34,700 --> 02:13:36,700
What do I want to expose?

2167
02:13:36,700 --> 02:13:37,700
A deployment.

2168
02:13:37,700 --> 02:13:38,700
Which one?

2169
02:13:38,700 --> 02:13:39,700
HTTP Env.

2170
02:13:39,700 --> 02:13:44,700
And if I do that, it's going to tell me, hey, what's the port number that you want to use?

2171
02:13:44,700 --> 02:13:47,700
And so I have to tell which port.

2172
02:13:47,700 --> 02:13:54,700
And in that case, my HTTP Env service is listening on port 8888.

2173
02:13:54,700 --> 02:14:07,700
So when I do that, the same way that kubectl run and kubectl create deployment are basically writing a YAML specification for me and sending the specification to the API server.

2174
02:14:07,700 --> 02:14:15,700
kubectl expose is writing a YAML specification for a service and sending that specification to the API server.

2175
02:14:15,700 --> 02:14:25,700
So now that I've done that, if I do kubectl get services in addition to Kubernetes, I see HTTP Env.

2176
02:14:25,700 --> 02:14:29,700
And you see there is like a cluster IP that has been allocated.

2177
02:14:29,700 --> 02:14:34,700
So I'm going to curl that.

2178
02:14:34,700 --> 02:14:39,700
If I curl it like this, it's going to hit port 80.

2179
02:14:39,700 --> 02:14:45,700
So it's going to just time out because, again, my service is on port 8888.

2180
02:14:45,700 --> 02:14:48,700
So let's add the port number.

2181
02:14:48,700 --> 02:14:55,700
And it's sending me the environment of the server as a JSON map.

2182
02:14:55,700 --> 02:15:03,700
The only field that is of interest to me is the hostname because it's telling me the hostname of the pod.

2183
02:15:03,700 --> 02:15:09,700
See, like hostname, HTTP Env, 599, etc. 9p8db.

2184
02:15:09,700 --> 02:15:16,700
If I do kubectl get pods, I have indeed a pod with that very name.

2185
02:15:16,700 --> 02:15:21,700
OK, so I can repeatedly curl that.

2186
02:15:21,700 --> 02:15:28,700
And I see that since I only have one pod, all my responses come from that.

2187
02:15:28,700 --> 02:15:33,700
So next, I want to scale up this deployment.

2188
02:15:33,700 --> 02:15:42,700
So kubectl scale deployment, HTTP Env, dash dash replicas.

2189
02:15:42,700 --> 02:15:48,700
And I'm going to put like three replicas.

2190
02:15:48,700 --> 02:15:56,700
And then I'm going to do my curl command again.

2191
02:15:56,700 --> 02:16:02,700
So we see now the hostname environment variable will change over time.

2192
02:16:02,700 --> 02:16:13,700
Since that's JSON, I can use jq, which I was mentioning earlier, to extract just the field I want, like give me the field hostname.

2193
02:16:13,700 --> 02:16:17,700
So I do curl pipe jq dot hostname.

2194
02:16:17,700 --> 02:16:25,700
Since I don't want to have like the extra output of curl, I'm going to add curl dash s, silent.

2195
02:16:25,700 --> 02:16:34,700
So now I see like which backend is serving the request.

2196
02:16:34,700 --> 02:16:37,700
So what kind of load balancing are we using?

2197
02:16:37,700 --> 02:16:40,700
Well, to find out, I'm going to send a bunch of requests.

2198
02:16:40,700 --> 02:16:51,700
So let's do a little for loop, like for i in sec 20, do this.

2199
02:16:51,700 --> 02:16:58,700
And then I look at the output and I don't know if you see a pattern, but it doesn't seem to be round robin.

2200
02:16:58,700 --> 02:17:06,700
It's not like it's not one, two, three, one, two, three, one, two, three, like nine, J, G, nine, J, J, G, nine, nine.

2201
02:17:06,700 --> 02:17:08,700
There is no pattern in it.

2202
02:17:08,700 --> 02:17:13,700
There is no pattern in it because at this point it's using random load balancing.

2203
02:17:13,700 --> 02:17:17,700
If you're curious about what's going on, you can dive into IP tables.

2204
02:17:17,700 --> 02:17:21,700
I won't do it now because I won't need to stay on track.

2205
02:17:21,700 --> 02:17:38,700
But what's happening here is that the kube proxy component, which I briefly mentioned this morning, is responsible for watching the services and the pods and creating a bunch of IP tables rules to intercept the traffic going to a cluster IP.

2206
02:17:38,700 --> 02:17:51,700
And each time I create a connection to a cluster IP, it's going to pick a back end at random and rewrite the destination IP address using the IP address of one of the back end points.

2207
02:17:51,700 --> 02:17:53,700
This is an implementation detail.

2208
02:17:53,700 --> 02:17:54,700
You don't have to worry about it.

2209
02:17:54,700 --> 02:17:57,700
But if you wonder what's going on, that's what's going on.

2210
02:17:57,700 --> 02:18:00,700
Question?

2211
02:18:00,700 --> 02:18:04,700
Sorry, I really can't hear you.

2212
02:18:04,700 --> 02:18:10,700
Is there a way to get some metrics on that?

2213
02:18:10,700 --> 02:18:17,700
Are you thinking about like network metrics or CPU metrics or all of the above?

2214
02:18:17,700 --> 02:18:18,700
Everything.

2215
02:18:18,700 --> 02:18:22,700
So there are some metrics that we can get in.

2216
02:18:22,700 --> 02:18:31,700
There is something called the core metrics pipeline, which is something that is not required on all Kubernetes clusters.

2217
02:18:31,700 --> 02:18:39,700
So it will work on some, not on others, but it's basically kubectl top and you can do kubectl top nodes.

2218
02:18:39,700 --> 02:18:45,700
It's going to show you CPU and memory usage across the nodes or kubectl top pods.

2219
02:18:45,700 --> 02:18:49,700
And it's going to show you like same thing but for pods.

2220
02:18:49,700 --> 02:18:58,700
But again, this requires the metrics server to be installed, which has been done on these clusters, but maybe not on all clusters.

2221
02:18:58,700 --> 02:19:07,700
Now, if you want like more insights, you will need to install, for instance, Prometheus.

2222
02:19:07,700 --> 02:19:18,700
And then Prometheus will be able to scrape metrics for all the nodes and also the pods and at the container level, et cetera, et cetera, and aggregate all that.

2223
02:19:19,700 --> 02:19:29,700
If I'm not going to talk about Prometheus this morning, but if you look on container training in longer workshops, there will be a chapter on Prometheus.

2224
02:19:29,700 --> 02:19:32,700
Yeah.

2225
02:19:32,700 --> 02:19:47,700
Oh, and just to wrap up, if you want metrics on the network level, often people will advise, hey, look at service meshes because that way you will be able to monitor like latency to services and that kind of thing.

2226
02:19:47,700 --> 02:19:53,700
It's not the only way to do it, but it's a really popular way to do that on Kubernetes clusters.

2227
02:19:53,700 --> 02:19:55,700
Okay.

2228
02:19:55,700 --> 02:20:00,700
So we run a simple HTTP service on the cluster.

2229
02:20:00,700 --> 02:20:03,700
We exposed it.

2230
02:20:03,700 --> 02:20:16,700
Here I'm repeating that services are layer for constructs, which is really a fancy way to say that, yeah, I can curl that thing, but I can't ping it and I can't curl a different port.

2231
02:20:16,700 --> 02:20:20,700
Right.

2232
02:20:20,700 --> 02:20:23,700
So that was our first exposed service.

2233
02:20:23,700 --> 02:20:29,700
Other questions before I move to the next thing.

2234
02:20:29,700 --> 02:20:41,700
So you see in that case that the service is a load balancer, but I don't want to say a service is a load balancer because if I only have one pod, then it's not load balancing anything.

2235
02:20:41,700 --> 02:20:52,700
And I don't want to say that it's a load balancer for pods because a service could also send traffic somewhere else like outside of the server.

2236
02:20:52,700 --> 02:20:54,700
It provides an abstraction.

2237
02:20:54,700 --> 02:21:00,700
So for instance, right now, when I connect to this IP address over there, it's load balancing traffic to pods.

2238
02:21:00,700 --> 02:21:09,700
But if I wanted, I could handcraft a service that would send connections to something else completely outside of the cluster.

2239
02:21:09,700 --> 02:21:16,700
So again, a service is a stable endpoint for things.

2240
02:21:16,700 --> 02:21:20,700
Okay, so now we want to run Docker coins on the cluster.

2241
02:21:20,700 --> 02:21:23,700
For that we will need to ship images.

2242
02:21:23,700 --> 02:21:25,700
Why do we need to ship images?

2243
02:21:25,700 --> 02:21:31,700
Maybe you've heard like the Docker tagline like build, ship and run any app anywhere.

2244
02:21:31,700 --> 02:21:39,700
So early this morning we did build and run, like we built container images and we ran them.

2245
02:21:39,700 --> 02:21:43,700
We didn't need to ship anything because we built and ran on the same machine.

2246
02:21:43,700 --> 02:21:46,700
So it's like ship to the same place.

2247
02:21:46,700 --> 02:21:51,700
But now we have multiple nodes, so we need to ship things around.

2248
02:21:51,700 --> 02:21:56,700
We need to have the same images on all the nodes of the cluster.

2249
02:21:56,700 --> 02:22:02,700
So we need to ship these images and the easiest way is to use container registry.

2250
02:22:02,700 --> 02:22:08,700
So what we're going to do is that we're going to reference images for Docker coins.

2251
02:22:08,700 --> 02:22:18,700
We're going to use images named Docker coins slash Walker, Docker coins slash Hasher, Docker coins slash RNG, et cetera, et cetera.

2252
02:22:18,700 --> 02:22:25,700
Now a little bit of details of how do container registry work exactly.

2253
02:22:25,700 --> 02:22:38,700
When we run an image like Alpine, for instance, like we did earlier, or Redis, this is going to expand into library slash Alpine.

2254
02:22:38,700 --> 02:22:41,700
Library is a special user on the registry.

2255
02:22:41,700 --> 02:22:50,700
Now if I just have library slash Alpine or JPTadzo slash HTTP Env, this means an image that is on the Docker Hub.

2256
02:22:50,700 --> 02:22:57,700
And so it's going to expand into index.docker.io slash library slash Alpine.

2257
02:22:57,700 --> 02:23:02,700
And so likewise, we could access images on different registries.

2258
02:23:02,700 --> 02:23:12,700
For instance, if I want the Alpine image, but with bash instead of a really minimal shell, I could use gcr.io.

2259
02:23:12,700 --> 02:23:18,700
So that's the Google Container Registry slash Google Containers slash Alpine with bash.

2260
02:23:19,700 --> 02:23:32,700
OK. So the plan to run Docker coins on Kubernetes is first, I'm sidestepping the build issue.

2261
02:23:32,700 --> 02:23:36,700
I assume that somebody has built the Docker coins image for me.

2262
02:23:36,700 --> 02:23:45,700
Somebody that could be for someone else who built and then pushed the image or maybe I've set up some automated build.

2263
02:23:45,700 --> 02:23:50,700
We consider that's a solved thing. I have images on the registry.

2264
02:23:50,700 --> 02:23:56,700
So I'm going to create one deployment for each container.

2265
02:23:56,700 --> 02:23:59,700
So one deployment for Hasher, Redis, RNG, etc.

2266
02:23:59,700 --> 02:24:05,700
And then I'm going to expose the ones that need to receive traffic.

2267
02:24:05,700 --> 02:24:09,700
So for the Redis container, I can use the Redis image.

2268
02:24:09,700 --> 02:24:20,700
And for the other containers, the images that I have are all called Docker coins slash the name of the service column v0.1.

2269
02:24:20,700 --> 02:24:31,700
So that means Docker coins slash Hasher column v0.1 Docker coins slash Walker column v0.1, etc.

2270
02:24:31,700 --> 02:24:35,700
Right. So we're going to do that.

2271
02:24:35,700 --> 02:24:47,700
That's the recap for the four image names in the slide here that follows.

2272
02:24:47,700 --> 02:24:57,700
I'm using a little for loop and that for loop assumes that you have set the registry and tag on variables.

2273
02:24:57,700 --> 02:25:00,700
Sorry, that's maybe a little small from the back of the room.

2274
02:25:00,700 --> 02:25:06,700
So you can just follow the comments that I'm going to run on screen.

2275
02:25:06,700 --> 02:25:11,700
First, let's do the easy one. The Redis deployment.

2276
02:25:11,700 --> 02:25:23,700
So I do kubectl create deployment Redis and I indicate the image I want to use, which is dash dash image Redis.

2277
02:25:23,700 --> 02:25:30,700
Done. Then I want to create the four other containers.

2278
02:25:30,700 --> 02:25:44,700
So create deployment Walker dash dash image equal Docker coins slash Walker column v0.1.

2279
02:25:44,700 --> 02:25:51,700
And then I do the same for the three others.

2280
02:25:51,700 --> 02:26:04,700
So Hasher. Web UI.

2281
02:26:04,700 --> 02:26:08,700
And RNG.

2282
02:26:08,700 --> 02:26:13,700
Oops.

2283
02:26:13,700 --> 02:26:19,700
And I'm going to give everyone a minute to run all that.

2284
02:26:19,700 --> 02:26:25,700
Yeah.

2285
02:26:25,700 --> 02:26:30,700
Is the order important? No, at this point, it's not.

2286
02:26:30,700 --> 02:26:36,700
Because the the Walker is going to try to connect right away to the other ones.

2287
02:26:36,700 --> 02:26:51,700
But the order is not important because as long as we haven't created the services, the worker won't be able to connect to anything.

2288
02:26:51,700 --> 02:26:54,700
So at this point, what's going on?

2289
02:26:54,700 --> 02:26:58,700
Well, let's have a look.

2290
02:26:58,700 --> 02:27:06,700
I'm going to do kubectl logs deploy slash RNG, for instance.

2291
02:27:06,700 --> 02:27:12,700
And this is going to tell me like that's just the output of my RNG container.

2292
02:27:12,700 --> 02:27:15,700
It say, yep, I'm up and running and listening on port 80.

2293
02:27:15,700 --> 02:27:18,700
Same thing for Hasher.

2294
02:27:18,700 --> 02:27:21,700
Yep, I'm up and running and listening on port 80.

2295
02:27:21,700 --> 02:27:34,700
However, if I check the logs of the Walker, spoiler alert, I get pages and pages of tracebacks telling me error connecting to Redis name does not resolve.

2296
02:27:34,700 --> 02:27:39,700
That's because I haven't exposed yet Redis.

2297
02:27:39,700 --> 02:27:43,700
So if I want that to work, I need to expose Redis.

2298
02:27:43,700 --> 02:27:46,700
For that I do kubectl expose.

2299
02:27:46,700 --> 02:27:49,700
What do I want to expose a deployment?

2300
02:27:49,700 --> 02:27:51,700
Which one? Redis.

2301
02:27:51,700 --> 02:27:54,700
When I do that, it's going to tell me, hey, I need the port number.

2302
02:27:54,700 --> 02:27:56,700
OK, what's the port number?

2303
02:27:56,700 --> 02:28:05,700
Even if I don't remember which port is used by Redis, I get a nice convenient reminder just here by my Python traceback.

2304
02:28:05,700 --> 02:28:12,700
So I need to expose Redis on port 6379.

2305
02:28:12,700 --> 02:28:17,700
OK, so it says service Redis exposed.

2306
02:28:17,700 --> 02:28:21,700
Let's have a look now at the logs of the Walker.

2307
02:28:21,700 --> 02:28:30,700
And in fact, I'm going to put these logs in a separate pane here.

2308
02:28:30,700 --> 02:28:32,700
OK, now I get a slightly different error.

2309
02:28:32,700 --> 02:28:35,700
It's telling me name does not resolve.

2310
02:28:35,700 --> 02:28:39,700
But now that's for host equal RNG.

2311
02:28:39,700 --> 02:28:43,700
So yeah, I haven't exposed RNG yet, so I need to do the same thing.

2312
02:28:43,700 --> 02:28:47,700
I need to do kubectl expose deployment RNG.

2313
02:28:47,700 --> 02:28:53,700
On which port? Well, that one is on port 80.

2314
02:28:53,700 --> 02:28:55,700
So I do that.

2315
02:28:55,700 --> 02:29:01,700
And now it's telling me that it can't connect to Hasher.

2316
02:29:01,700 --> 02:29:07,700
So I need to expose Hasher as well.

2317
02:29:07,700 --> 02:29:19,700
And for each service that I create, I get a DNS entry mapping that service to its cluster IP.

2318
02:29:19,700 --> 02:29:22,700
That's why the code works.

2319
02:29:22,700 --> 02:29:28,700
When it's trying to connect to Redis, when I create service Redis,

2320
02:29:28,700 --> 02:29:37,700
it's adding a DNS entry mapping Redis to 10.105.158.51.

2321
02:29:37,700 --> 02:29:43,700
So doing the same dynamic DNS thing that we had early this morning with Compose.

2322
02:29:43,700 --> 02:29:47,700
And now that I have created all the relevant services,

2323
02:29:47,700 --> 02:29:57,700
my worker code is happily doing its infinite loop, generating random data, hashing it, etc.

2324
02:29:57,700 --> 02:30:03,700
Can I avoid using fixed port?

2325
02:30:03,700 --> 02:30:09,700
Do you have a specific example in mind?

2326
02:30:09,700 --> 02:30:14,700
Oh, OK. So if I'm, yeah, what if I'm already using port 80 or port?

2327
02:30:14,700 --> 02:30:22,700
So here it's important to know that I'm not using port 6379 on the whole cluster.

2328
02:30:22,700 --> 02:30:26,700
I'm just using that on this cluster IP.

2329
02:30:26,700 --> 02:30:34,700
So it means I can have as many services I want using the same port because each of them gets its own cluster IP.

2330
02:30:34,700 --> 02:30:39,700
For instance, here I have both Hasher and RNG using port 80.

2331
02:30:39,700 --> 02:30:45,700
But since they each get their own cluster IP, that's fine.

2332
02:30:45,700 --> 02:30:55,700
I don't get a conflict. I can see here like if I do get SVC, I see like Hasher port 80, RNG port 80.

2333
02:30:55,700 --> 02:31:01,700
But they both use different cluster IPs. So I'm good.

2334
02:31:01,700 --> 02:31:08,700
OK, now I would like to see the Web UI. So I need to also expose the Web UI.

2335
02:31:08,700 --> 02:31:14,700
But when exposing the Web UI, I want to make it available from outside.

2336
02:31:14,700 --> 02:31:22,700
So I'm going to add a dash dash type node port.

2337
02:31:22,700 --> 02:31:29,700
So now if I do kubectl get SVC Web UI, please.

2338
02:31:29,700 --> 02:31:37,700
There we go. Now in the ports column, I see port 80 has been exposed as 32190.

2339
02:31:37,700 --> 02:31:45,700
So now I can go back to my web browser and I can connect to node one on 32.

2340
02:31:45,700 --> 02:31:50,700
Sorry, I forgot the port number already 190.

2341
02:31:50,700 --> 02:31:55,700
And that gives me the Web UI that we had this morning.

2342
02:31:55,700 --> 02:31:59,700
And I can connect to any of the four nodes on that port number.

2343
02:31:59,700 --> 02:32:07,700
And it's going to route the connection to the pod that is running the Web UI.

2344
02:32:07,700 --> 02:32:10,700
Right. So we're back to square one.

2345
02:32:10,700 --> 02:32:32,700
But now we're running on a cluster, so hopefully we can scale things and have fun.

2346
02:32:32,700 --> 02:32:39,700
So scaling that with what we've seen earlier, it should be fairly straightforward.

2347
02:32:39,700 --> 02:32:44,700
So as a reminder, it could be useful to understand how we really want to scale the app.

2348
02:32:44,700 --> 02:32:50,700
Because the command itself, we've seen it already, but we need to understand how are we going to scale Docker coins.

2349
02:32:50,700 --> 02:32:57,700
If we look at the architecture of Docker coins, we have this worker doing an infinite loop and then these web services around it.

2350
02:32:57,700 --> 02:33:06,700
So if I start additional copies of the worker, I get like multiple things doing the infinite loop in parallel.

2351
02:33:06,700 --> 02:33:12,700
So I should get more performance. Let's see if it adds up.

2352
02:33:12,700 --> 02:33:17,700
I'm going to scale additional copies of the worker.

2353
02:33:17,700 --> 02:33:32,700
So I will do kubectl scale deployment worker dash dash replicas two.

2354
02:33:32,700 --> 02:33:46,700
So a few seconds later, we on the pane above see we saw like now we have a brownish pods and proper pods.

2355
02:33:46,700 --> 02:33:56,700
And if I look in the Web UI at the same moment, I see that instead of having about four hashes per second, now I have about eight hashes per second.

2356
02:33:56,700 --> 02:34:02,700
So by doubling the number of workers, I doubled the speed of Docker coins.

2357
02:34:02,700 --> 02:34:11,700
I'm going to repeat the same thing to get like three replicas of same command like kubectl scale deployment worker dash dash replicas three.

2358
02:34:11,700 --> 02:34:19,700
And.

2359
02:34:19,700 --> 02:34:25,700
OK, it picked another shade of purple, so it's not super visible above.

2360
02:34:25,700 --> 02:34:33,700
But if I look here in the Web UI, I see that indeed we seem to be going faster.

2361
02:34:33,700 --> 02:34:39,700
So we might think, OK, I'm just going to put like 10 replicas.

2362
02:34:39,700 --> 02:34:47,700
And I'm going to get like many, many Docker coins and we can all go home and say, well, we learn to scale things.

2363
02:34:47,700 --> 02:34:54,700
Unfortunately, if I look at the Web UI now, it looks like it's slowing down.

2364
02:34:54,700 --> 02:35:00,700
It's going back to 10 hashes per second without diving into too much details.

2365
02:35:00,700 --> 02:35:04,700
But here it was not really going to 12 per second.

2366
02:35:04,700 --> 02:35:11,700
On average, it was at 10, but it was doing like 12 and then eight and then 12 and then eight.

2367
02:35:11,700 --> 02:35:14,700
That's why you can see like peaks and throughs.

2368
02:35:14,700 --> 02:35:25,700
But really, on average, we were at 10 per second and we cannot go higher than 10 per seconds because we have a bottleneck somewhere.

2369
02:35:25,700 --> 02:35:31,700
So let's try to find out where that bottleneck is.

2370
02:35:31,700 --> 02:35:42,700
And to do that, if we if we were, you know, like if this were like actual production code, we would have some instrumentation and we would be able to see like latency and whatnot.

2371
02:35:42,700 --> 02:35:50,700
But if we were to look at the code, we would see that it's like.

2372
02:35:50,700 --> 02:35:56,700
Two Python scripts in the trench coat, so we have to to use other tools to find out what's going on.

2373
02:35:56,700 --> 02:35:58,700
So I'm going to use HTTP ping.

2374
02:35:58,700 --> 02:36:02,700
HTTP ping is just like ping, but for HTTP services.

2375
02:36:02,700 --> 02:36:08,700
So it sends a really simple get request and sees how long it takes.

2376
02:36:08,700 --> 02:36:17,700
So I'm going to I'm going to go to the next pane zoom a little bit so that everybody can see what's going on.

2377
02:36:17,700 --> 02:36:19,700
CTO get services.

2378
02:36:19,700 --> 02:36:24,700
I want Hasher and I want RNG.

2379
02:36:24,700 --> 02:36:35,700
So Hasher is on that cluster IP, so I'm going to do HTTP on that cluster IP.

2380
02:36:35,700 --> 02:36:39,700
And I see that Hasher takes like a couple of milliseconds to reply.

2381
02:36:39,700 --> 02:36:42,700
OK, that seems normal.

2382
02:36:42,700 --> 02:36:44,700
Now let's look at RNG.

2383
02:36:44,700 --> 02:36:49,700
I'm going to do HTTP on RNG.

2384
02:36:49,700 --> 02:36:54,700
800 milliseconds for a local web service that's slow.

2385
02:36:54,700 --> 02:36:59,700
So immediately we can guess something's wrong with the RNG service.

2386
02:36:59,700 --> 02:37:05,700
So we can think about OK, why is RNG slower?

2387
02:37:05,700 --> 02:37:10,700
Like what does justify that for a local web service?

2388
02:37:10,700 --> 02:37:12,700
It takes almost one second to reply.

2389
02:37:12,700 --> 02:37:18,700
So by chatting with some coworkers on the coffee machine, somebody will say, oh, that's probably because of entropy.

2390
02:37:18,700 --> 02:37:25,700
Entropy is that thing that is used to generate random numbers on modern computers.

2391
02:37:25,700 --> 02:37:30,700
We need an actual source of randomness.

2392
02:37:30,700 --> 02:37:43,700
So if that you can imagine, it's a little bit as if somebody was throwing dices like in the kernel constantly and writing down the numbers on papers.

2393
02:37:43,700 --> 02:37:51,700
And when you have something that needs strong random numbers, they just say, hey, give me some random numbers and they get a bunch of pages with random numbers.

2394
02:37:51,700 --> 02:38:04,700
But if you are out of random numbers, because the person writing down like throwing dice and writing down the results, if that thing is too slow, then you have to wait for your random numbers to come in.

2395
02:38:04,700 --> 02:38:15,700
This is why if you're using something like PGP or GPG, when you generate strong crypto keys, it can ask you now, please move the mouse.

2396
02:38:15,700 --> 02:38:26,700
You're like what? Because while you move the mouse, you're generating some randomness that gets collected by the system and that helps to generate your your random key.

2397
02:38:26,700 --> 02:38:32,700
So here we're going to guess RNG is slow because of that entropy thing.

2398
02:38:32,700 --> 02:38:42,700
And so the solution to make it go faster is to scale it, but not just scale it like give me I have four nodes.

2399
02:38:42,700 --> 02:38:50,700
So not just like give me four replicas because nothing guarantees that my replicas will be nicely spread out on the cluster.

2400
02:38:50,700 --> 02:38:57,700
I want something that will guarantee that I have exactly one copy per node.

2401
02:38:57,700 --> 02:39:06,700
By the way, that explication is complete. Excuse my French bullshit, but it's a good pretext to introduce another way to scale things on Kubernetes.

2402
02:39:06,700 --> 02:39:09,700
The problem is not entropy.

2403
02:39:09,700 --> 02:39:16,700
So we're going to introduce them on sets and in the process see a lot of interesting things in Kubernetes.

2404
02:39:16,700 --> 02:39:23,700
Just to make it clear in the real world, it's extremely rare to use them on sets for application workloads.

2405
02:39:23,700 --> 02:39:27,700
But more on that in a moment.

2406
02:39:27,700 --> 02:39:33,700
OK, so we want to make sure that we get one copy of RNG on each node of the cluster.

2407
02:39:33,700 --> 02:39:36,700
That's exactly what the demon set will give us.

2408
02:39:36,700 --> 02:39:40,700
And in fact, we have met demon sets before.

2409
02:39:40,700 --> 02:39:50,700
Remember when we were looking at the list of pods in kube system, we had four kube proxy pods and four weave pods.

2410
02:39:50,700 --> 02:40:03,700
And when I use dash O wide, we were able to see that the kube proxy pods were one on each node.

2411
02:40:03,700 --> 02:40:06,700
One, two, three, four. Same thing for weave net.

2412
02:40:06,700 --> 02:40:12,700
These things are created by a demon set.

2413
02:40:12,700 --> 02:40:21,700
If I do kubectl get demon sets, I see that indeed I have a kube proxy demon set and a weave net demon set.

2414
02:40:21,700 --> 02:40:31,700
This is extremely convenient because it means that when a node joins the cluster, automatically it will receive a kube proxy and a weave net pod.

2415
02:40:31,700 --> 02:40:41,700
So automatically these network components will be scheduled and started on the node without me, cluster administrator, having to do anything with it.

2416
02:40:41,700 --> 02:40:51,700
OK, unfortunately, we don't have a kubectl create demon set command, at least not in kube 114.

2417
02:40:51,700 --> 02:40:57,700
So we cannot create a demon set in a nice way.

2418
02:40:57,700 --> 02:41:05,700
So we have to fall back to plan B, which is write custom email and send that email to the API server.

2419
02:41:05,700 --> 02:41:15,700
So how do we do that? Well, we read the documentation and we write the email, except I don't feel like reading documentation this morning.

2420
02:41:15,700 --> 02:41:21,700
So instead, we're going to try to find our way around that.

2421
02:41:21,700 --> 02:41:31,700
We're going to get a email that hopefully is close enough and see if we can twist it a little bit to get what we want.

2422
02:41:31,700 --> 02:41:44,700
I'm going to get the email of the RNG deployment since it's already creating pods for RNG and see if I can turn that into a demon set.

2423
02:41:44,700 --> 02:41:48,700
So how do I get the email for an object? Well, we saw this already.

2424
02:41:48,700 --> 02:41:52,700
It's with kubectl get dash OEML.

2425
02:41:52,700 --> 02:42:06,700
So I'm going to do kubectl get deployment RNG dash OEML and send that to RNG dot YML, for instance.

2426
02:42:06,700 --> 02:42:20,700
And then I'm going to edit that email file and see what it takes to transform a deployment into a demon set.

2427
02:42:20,700 --> 02:42:29,700
So first line API version, second line kind deployment.

2428
02:42:29,700 --> 02:42:34,700
Well, we're going to transform that deployment in demon set.

2429
02:42:34,700 --> 02:42:37,700
Pay attention to the capitalization. It matters.

2430
02:42:37,700 --> 02:42:42,700
And then there are other things, but I don't feel like reading more email.

2431
02:42:42,700 --> 02:42:55,700
So I'm just going to save and try to load that email with kubectl create dash F RNG dot email.

2432
02:42:55,700 --> 02:43:06,700
And it's going to give me exactly one screen full of errors, error validating data, unknown field progress deadline seconds, et cetera, et cetera.

2433
02:43:06,700 --> 02:43:14,700
Of course, it couldn't be that easy. I can't just cast resource type into another and hope that it would work.

2434
02:43:14,700 --> 02:43:19,700
But if I look a little bit more in the error message, I see like unknown field replicas.

2435
02:43:19,700 --> 02:43:24,700
OK, that makes sense, because in the deployment, I say how many replicas I want.

2436
02:43:24,700 --> 02:43:27,700
In a demon set, I don't say how many replicas.

2437
02:43:27,700 --> 02:43:30,700
The number of replicas is exactly one per node.

2438
02:43:30,700 --> 02:43:34,700
So that it makes sense that I wouldn't have a replicas field.

2439
02:43:34,700 --> 02:43:43,700
I have another bunch of errors like unknown field last update time missing require field that.

2440
02:43:43,700 --> 02:43:51,700
And at the end, it says if you choose to ignore these errors, turn validation off with dash dash validate equal false.

2441
02:43:51,700 --> 02:43:54,700
That seems like a good idea.

2442
02:43:54,700 --> 02:43:59,700
And so far, nothing bad happened.

2443
02:43:59,700 --> 02:44:04,700
So by turning on validation, did I just break my cluster or do anything bad?

2444
02:44:04,700 --> 02:44:16,700
No. Good news that the job of the API server is also to make sure that it does not accept things that would completely corrupt or or do horrible things to the cluster.

2445
02:44:16,700 --> 02:44:19,700
I mean, don't get me wrong. I can still do horrible things.

2446
02:44:19,700 --> 02:44:22,700
Like I could say, oh, give me one million replicas.

2447
02:44:22,700 --> 02:44:25,700
That may be something bad could happen if I were to do that.

2448
02:44:25,700 --> 02:44:31,700
But I can't corrupt my cluster by sending invalid email.

2449
02:44:31,700 --> 02:44:39,700
In that case, when I turned off validation, what it did was drop all the extra fields, all the fields that it doesn't know.

2450
02:44:39,700 --> 02:44:43,700
Goodbye. And then there are a bunch of required fields.

2451
02:44:43,700 --> 02:44:46,700
Well, it tried to use default values.

2452
02:44:46,700 --> 02:44:51,700
And if at the end the result is a valid email, then that's fine.

2453
02:44:51,700 --> 02:44:59,700
Turns out that it's the case after removing the extra news fields, providing default values for the missing ones.

2454
02:44:59,700 --> 02:45:03,700
We get the perfectly good email file and we can use it.

2455
02:45:03,700 --> 02:45:09,700
So before going farther, I'm going to have a look at the Web UI.

2456
02:45:09,700 --> 02:45:17,700
And interestingly, now, instead of being stuck at 10 hashes per second, we are at 20, 30 hashes per second.

2457
02:45:17,700 --> 02:45:21,700
So what we did definitely had an effect.

2458
02:45:21,700 --> 02:45:25,700
We maybe we have more pods or something like something happened.

2459
02:45:25,700 --> 02:45:32,700
And now the app is faster, which is really interesting, because if we think about it, OK, maybe I created more pods.

2460
02:45:32,700 --> 02:45:35,700
Let's check. Cube CTL get pods.

2461
02:45:35,700 --> 02:45:39,700
Lots and lots and lots of things here.

2462
02:45:39,700 --> 02:45:43,700
But you can see in the middle, I have multiple RNG pods.

2463
02:45:43,700 --> 02:45:49,700
So I created additional pods and you see they are slightly different.

2464
02:45:49,700 --> 02:45:54,700
This one has a longer name. That's the pod created by the deployment.

2465
02:45:54,700 --> 02:45:59,700
And these ones have shorter names. They are the ones created by the demon set.

2466
02:45:59,700 --> 02:46:02,700
So we created pods. That's great.

2467
02:46:02,700 --> 02:46:14,700
But somehow by some magic, the pods were added to the load balancer, because if the app is going faster, it's because like these pods are in use right now.

2468
02:46:14,700 --> 02:46:19,700
In fact, I would like to see the logs of these pods.

2469
02:46:19,700 --> 02:46:27,700
So I'm going to do Stern RNG and Stern RNG is going to.

2470
02:46:27,700 --> 02:46:37,700
So, yeah, you can you can see like if I freeze that for a second, the red one here, that's the pod belonging to the deployment.

2471
02:46:37,700 --> 02:46:43,700
And the short ones are the pods belonging to the demon set. And everybody is serving traffic.

2472
02:46:43,700 --> 02:46:47,700
Interesting. I kind of wonder why.

2473
02:46:47,700 --> 02:46:51,700
And here to try and find out.

2474
02:46:51,700 --> 02:46:55,700
Well, first, I don't know what you think, but this list is really long.

2475
02:46:55,700 --> 02:47:00,700
So I would like to see only the pods belonging to RNG.

2476
02:47:00,700 --> 02:47:06,700
So one option would be CubeCTL get pods by grep RNG.

2477
02:47:06,700 --> 02:47:18,700
Or another option would be to use a selector. Remember, like we said, I can do CubeCTL get pods dash dash selector or dash L.

2478
02:47:19,700 --> 02:47:34,700
Remember when I said if we do CubeCTL create deployment, it's going to put a label called app so I can do CubeCTL get pods dash dash selector equal app equal RNG.

2479
02:47:34,700 --> 02:47:39,700
And that shows me all the pods with app equal RNG.

2480
02:47:39,700 --> 02:47:47,700
And that gives us a hint about how do these pods receive traffic to to get another hint.

2481
02:47:47,700 --> 02:47:57,700
I'm going to look at the service RNG. I'm going to do CubeCTL describe service RNG.

2482
02:47:57,700 --> 02:48:08,700
In the output, I see that my service has a label app equal RNG, and it also has a selector app equal RNG.

2483
02:48:08,700 --> 02:48:19,700
That selector here, you can think of that as a live query on my pods, indicating which pods should be targeted by the service.

2484
02:48:19,700 --> 02:48:24,700
That means which pods should receive traffic for this service.

2485
02:48:24,700 --> 02:48:29,700
And here in endpoints, I get the result of that query, if you will.

2486
02:48:29,700 --> 02:48:41,700
It's a little bit as if this were a software defined load balancer where the backends, the endpoints are select host port from pods where app equal RNG.

2487
02:48:41,700 --> 02:48:48,700
Right. So now the last missing piece of the puzzle.

2488
02:48:48,700 --> 02:48:55,700
OK, so the the the graph is going higher because I have more backends in the RNG service.

2489
02:48:55,700 --> 02:49:11,700
I have more backends in the RNG service because the RNG service is sending traffic to all the pods that have app equal RNG and all the pods that were created by the demo set have app equal RNG.

2490
02:49:11,700 --> 02:49:18,700
But the last piece of the puzzle is why do they have app equal RNG? I did not do like CubeCTL create with them.

2491
02:49:18,700 --> 02:49:23,700
I just took the YAML of the deployment and tweaked it.

2492
02:49:23,700 --> 02:49:34,700
And that's the reason why, because in the YAML of the deployment, if I scroll down more and more and more, I have the template.

2493
02:49:34,700 --> 02:49:37,700
So that's the template of the pods created.

2494
02:49:37,700 --> 02:49:40,700
And I have labels app RNG.

2495
02:49:40,700 --> 02:49:50,700
So all the pods created by the deployment and also all the pods created by the demo set have app equal RNG.

2496
02:49:50,700 --> 02:49:54,700
Right.

2497
02:49:54,700 --> 02:50:03,700
So that's that's why this kind of automatically worked without us consciously adding these things to the load balancer.

2498
02:50:03,700 --> 02:50:09,700
Right.

2499
02:50:09,700 --> 02:50:22,700
One other little detail is that if I look at my resources, like if I do CubeCTL get, let's say, deployment and demo sets like this,

2500
02:50:22,700 --> 02:50:29,700
I see that I indeed have a demo set called RNG and I still have a deployment called RNG.

2501
02:50:29,700 --> 02:50:39,700
When I when I used my YAML to create the demo set, this did not destroy the RNG deployment.

2502
02:50:39,700 --> 02:50:49,700
The RNG deployment still exists, which means that I can have multiple objects with the same name RNG, but different types.

2503
02:50:49,700 --> 02:50:55,700
And that was already the case before, because I have a deployment hasher and a service hasher.

2504
02:50:55,700 --> 02:51:02,700
I have a deployment RNG and a service RNG and a demo set RNG, et cetera.

2505
02:51:02,700 --> 02:51:05,700
Right.

2506
02:51:05,700 --> 02:51:15,700
Another thing is that, OK, so the goal of the demo set is to create one pod per node, one pod per node.

2507
02:51:15,700 --> 02:51:18,700
Let's look again at the RNG pods.

2508
02:51:18,700 --> 02:51:24,700
I have four pods and I have four nodes.

2509
02:51:24,700 --> 02:51:31,700
But if I look again, I'm going to use dash O wide to see on which node they are running.

2510
02:51:31,700 --> 02:51:37,700
I could naively expect to have one pod on node one, node two, node three, node four.

2511
02:51:37,700 --> 02:51:41,700
And yet I have node two, node two, node three, node four.

2512
02:51:41,700 --> 02:51:43,700
So what's going on here?

2513
02:51:43,700 --> 02:51:45,700
There are two things at play here.

2514
02:51:45,700 --> 02:51:49,700
The first one is that node one is special.

2515
02:51:49,700 --> 02:52:04,700
Node one is where the control plane is running and it's using a special mechanism to say, OK, node one is the master node and it should not receive pods like normal application pods.

2516
02:52:04,700 --> 02:52:14,700
That way, even if I do a if I make some mistake and I create one million pods, they won't go to node one and node one will keep running.

2517
02:52:14,700 --> 02:52:19,700
So I can still access the control plane and I can still fix my mistakes.

2518
02:52:19,700 --> 02:52:23,700
So, OK, that's why I don't have a pod on node one.

2519
02:52:23,700 --> 02:52:28,700
Now, why do I have two pods on node two?

2520
02:52:28,700 --> 02:52:36,700
That's because the demo set is creating one pod per node except node one.

2521
02:52:36,700 --> 02:52:41,700
But it's also not looking at the pods created by the deployment.

2522
02:52:41,700 --> 02:52:47,700
So when it counts like one pod per node, it's not counting the pod created by the deployment.

2523
02:52:47,700 --> 02:52:54,700
The kind of under the hood reason for that is explained a little bit farther.

2524
02:52:54,700 --> 02:53:02,700
There will be one of the extra detail slide explaining why the deployment and demo sets are using distinct sets of pods.

2525
02:53:02,700 --> 02:53:07,700
If that's the kind of question that would prevent you from sleeping at night.

2526
02:53:07,700 --> 02:53:09,700
Questions?

2527
02:53:09,700 --> 02:53:10,700
Yeah.

2528
02:53:10,700 --> 02:53:15,700
So for the registries, how do we set up a private registry?

2529
02:53:15,700 --> 02:53:19,700
How do we set up a private registry?

2530
02:53:19,700 --> 02:53:22,700
There are there are short and long answers.

2531
02:53:22,700 --> 02:53:24,700
Let me try to condense things.

2532
02:53:24,700 --> 02:53:36,700
If you want an example that you could kind of work like step by step, if you if you look at container training on the self paced version, like basically the super long version of this,

2533
02:53:36,700 --> 02:53:43,700
there is a lab where we are going to use a private registry running on the cluster and use that private registry.

2534
02:53:43,700 --> 02:53:52,700
But it's also not a really realistic example because it's just using the basic registry without authentication, without anything, et cetera.

2535
02:53:52,700 --> 02:53:59,700
So if we want a realistic example, we could have like two ways.

2536
02:53:59,700 --> 02:54:05,700
One way is when I want a private registry because I don't want my images to be public.

2537
02:54:05,700 --> 02:54:12,700
In that case, I could still use a third party registry like Quay or Docker Hub or something like that.

2538
02:54:12,700 --> 02:54:19,700
So I would create an account there, maybe give them some money because for private images, usually they require that.

2539
02:54:19,700 --> 02:54:28,700
Then I would create a special object called a secret and that secret would hold the credentials to access that registry.

2540
02:54:28,700 --> 02:54:37,700
And then when I would create the deployment, I would say, hey, to to get the image from the registry, I need to use this secret that I just created.

2541
02:54:37,700 --> 02:54:46,700
You can see that when I do QCTL create secret, there is a sub command here for Docker registries.

2542
02:54:46,700 --> 02:54:55,700
And when we do QCTL create deployment in the many, many flake flags that we have.

2543
02:54:55,700 --> 02:55:05,700
OK, I'm not seeing it here, but somewhere in there I have a flag to say use these credentials to pull the image.

2544
02:55:05,700 --> 02:55:15,700
So that that's one option. The other option is, well, I want private registry as in the image stays entirely within my control.

2545
02:55:15,700 --> 02:55:19,700
In that case, I again like choose your adventure. I have two branches.

2546
02:55:19,700 --> 02:55:24,700
If I'm on cloud, I will typically use a registry backed by my cloud provider.

2547
02:55:24,700 --> 02:55:32,700
So each big cloud provider will have one like ECR on Amazon, GCR on Google Cloud, etc, etc.

2548
02:55:32,700 --> 02:55:35,700
And then I will be able to use that registry.

2549
02:55:35,700 --> 02:55:48,700
So I will create a registry instance using the API of the cloud provider and then I will be able to push images there or even have the cloud use some automated build from my repo straight to the registry.

2550
02:55:48,700 --> 02:56:01,700
So that's one option. And finally, if I'm like, well, I'm on my own machines, I'm not even using a cloud provider, then that's the most complex option, because then I will have to run my own registry.

2551
02:56:01,700 --> 02:56:11,700
And so either I go the commercial route and I buy something from Quay or I get like Tectonic or Docker Enterprise, etc.

2552
02:56:11,700 --> 02:56:14,700
Or I build it myself with open source components.

2553
02:56:14,700 --> 02:56:24,700
So in that case, I typically get the registry container image, which provides like the just the REST API, but no authentication, etc.

2554
02:56:24,700 --> 02:56:31,700
And then I add other components on top of that to provide replication and caching and authentication, etc, etc.

2555
02:56:31,700 --> 02:56:36,700
Yeah.

2556
02:56:36,700 --> 02:56:54,700
I didn't hear entirely something about not running on the master node.

2557
02:56:54,700 --> 02:57:00,700
Right.

2558
02:57:00,700 --> 02:57:07,700
Good questions. Since since we don't run workloads on the master node, does that mean that we need to have at least two machines?

2559
02:57:07,700 --> 02:57:10,700
The answer is it depends.

2560
02:57:10,700 --> 02:57:25,700
If, for instance, if we are using something like Minikube or Docker desktop or Micro K8s, which are like local Kubernetes deployment options, they won't have this this mechanism in place.

2561
02:57:25,700 --> 02:57:29,700
And so in that case, since I only have one node, I want to run everything together.

2562
02:57:29,700 --> 02:57:40,700
Now, in a more production like scenario, very often the don't run things on master nodes won't matter because I will not have a master node.

2563
02:57:40,700 --> 02:57:47,700
What I mean is that if I'm using a managed Kubernetes cluster, the control plane is running somewhere else.

2564
02:57:47,700 --> 02:57:54,700
I don't have access to it. I just have like the API endpoint and somebody else is running the control plane for me.

2565
02:57:54,700 --> 02:58:01,700
And then I I can't run pods on that because it's it's something else.

2566
02:58:01,700 --> 02:58:08,700
For instance, there are many providers who are going to say, OK, I'm going to sell you like managed Kubernetes as a service.

2567
02:58:08,700 --> 02:58:17,700
And what I'm going to do is that I'm going to set up one special Kubernetes cluster for me internally.

2568
02:58:17,700 --> 02:58:20,700
And I'm going to run your Kubernetes control plane.

2569
02:58:20,700 --> 02:58:24,700
I mean, of all my customers on this Kubernetes cluster.

2570
02:58:24,700 --> 02:58:28,700
So it's kind of running Kubernetes and Kubernetes in a way.

2571
02:58:28,700 --> 02:58:33,700
And in that case, there is a bunch of nodes which are kind of your master nodes.

2572
02:58:33,700 --> 02:58:41,700
But it's really a different cluster to which you have zero access and which is entirely managed by the provider.

2573
02:58:41,700 --> 02:58:50,700
Yeah.

2574
02:58:50,700 --> 02:58:57,700
Is it active active for the control plane or for the workload nodes?

2575
02:58:57,700 --> 02:59:04,700
So for the workload, all the nodes are active at the same time and all the nodes can run pods at the same time.

2576
02:59:04,700 --> 02:59:10,700
And so when one node goes down, there's going to have like two two steps.

2577
02:59:10,700 --> 02:59:17,700
The first step is that very quickly, like within one minute, the node will be marked as not ready.

2578
02:59:17,700 --> 02:59:21,700
So we find out which is master.

2579
02:59:21,700 --> 02:59:26,700
Well, how do we find out which role is which node is a master node?

2580
02:59:26,700 --> 02:59:28,700
There is it.

2581
02:59:28,700 --> 02:59:35,700
There is no absolute answer to that in this specific case, because we deployed the clusters in a certain way.

2582
02:59:35,700 --> 02:59:40,700
Then we do have a node that is dedicated to the control plane.

2583
02:59:40,700 --> 02:59:45,700
But there are clusters where they won't be such a node because the control plane is somewhere else.

2584
02:59:45,700 --> 02:59:51,700
There are nodes where I will have three nodes or even five nodes for the control plane.

2585
02:59:51,700 --> 02:59:55,700
It really depends.

2586
02:59:55,700 --> 02:59:59,700
Does this mean there's no master node? Have we noticed a master?

2587
02:59:59,700 --> 03:00:04,700
It depends how the cluster was deployed really.

2588
03:00:04,700 --> 03:00:22,700
So I tried to do this. I tried to dump the YAML for the DemonSet and then delete the RNG deployment and the DemonSet and then reapply the DemonSet. But it didn't work. Can you do that directly?

2589
03:00:22,700 --> 03:00:30,700
OK, good question. So this is if I understood correctly, you try to dump the YAML for the DemonSet.

2590
03:00:30,700 --> 03:00:37,700
So dash O YAML. And I'm going to put that into RNG DS dot YAML.

2591
03:00:37,700 --> 03:00:42,700
Then you destroyed the deployment RNG.

2592
03:00:42,700 --> 03:00:47,700
And then you destroyed the DemonSet RNG.

2593
03:00:47,700 --> 03:00:56,700
And then you try to recreate the DemonSet using the YAML that we just dumped earlier.

2594
03:00:56,700 --> 03:01:00,700
And then you said it didn't work.

2595
03:01:00,700 --> 03:01:05,700
So when we do that, let's look at the pods.

2596
03:01:05,700 --> 03:01:09,700
Normally I should see three. OK, I see a little bit more.

2597
03:01:09,700 --> 03:01:16,700
I see the old pods being terminated and I see three new pods like freshly created and just like 10 seconds old.

2598
03:01:16,700 --> 03:01:23,700
And if I look at the Web UI, I see a big crash and it's going to take some time to recover.

2599
03:01:23,700 --> 03:01:29,700
Why? Because first I had like these workers making requests like continuously.

2600
03:01:29,700 --> 03:01:32,700
And at some point I had no backends at all.

2601
03:01:32,700 --> 03:01:38,700
So what's going on right now is that the requests to RNG are going to time out.

2602
03:01:38,700 --> 03:01:44,700
But it's going to take a while because when I don't have any available back end,

2603
03:01:44,700 --> 03:01:49,700
instead of, you know, immediately telling like connection refuse or no route to destination or et cetera,

2604
03:01:49,700 --> 03:01:57,700
it's just going to time out as if like I was trying to reach an IP address like that doesn't exist.

2605
03:01:57,700 --> 03:02:01,700
Yeah. And if we give it a few minutes, it's going to come back up.

2606
03:02:01,700 --> 03:02:09,700
And if I feel really impatient, I could like give a kick into the the worker pods by doing like CubeCTL,

2607
03:02:09,700 --> 03:02:15,700
delete pods with the label app equal worker.

2608
03:02:15,700 --> 03:02:19,700
And that would recreate the worker pods immediately.

2609
03:02:19,700 --> 03:02:27,700
And so very quickly I should see the load coming back up.

2610
03:02:27,700 --> 03:02:31,700
Right.

2611
03:02:31,700 --> 03:02:36,700
How do we scale the nodes themselves?

2612
03:02:36,700 --> 03:02:41,700
So this will be strongly dependent on how we set up the cluster.

2613
03:02:41,700 --> 03:02:47,700
If we're using a managed Kubernetes cluster, very often there will be direct API calls to do that.

2614
03:02:47,700 --> 03:02:52,700
Like, for instance, if we are on Google Cloud, it will be something like with the CLI,

2615
03:02:52,700 --> 03:02:57,700
like G Cloud container cluster, dash, dash scale.

2616
03:02:57,700 --> 03:03:00,700
And that's going to automatically add nodes to the cluster.

2617
03:03:00,700 --> 03:03:06,700
And within a few minutes when I do my CubeCTL get nodes, I see nodes up and running.

2618
03:03:06,700 --> 03:03:14,700
If I'm managing my cluster myself, it depends on how I deployed my cluster here.

2619
03:03:14,700 --> 03:03:19,700
For instance, what we did is that we created a bunch of Ubuntu instances,

2620
03:03:19,700 --> 03:03:24,700
like using the basic Ubuntu LTS image, like nothing fancy.

2621
03:03:24,700 --> 03:03:29,700
These are running on EC2 and I'm just using like the default Ubuntu image.

2622
03:03:29,700 --> 03:03:36,700
And when I want to add a node, I install a container engine, in that case Docker.

2623
03:03:36,700 --> 03:03:39,700
I install the Kubernetes packages.

2624
03:03:39,700 --> 03:03:43,700
And then specifically because I deployed this cluster with QBDM,

2625
03:03:43,700 --> 03:03:46,700
which is one of the many ways to install them,

2626
03:03:46,700 --> 03:03:55,700
I have a command that I can run to say basically QBDM join this node with this secret token

2627
03:03:55,700 --> 03:03:58,700
and join this control plane.

2628
03:03:58,700 --> 03:04:01,700
And then the node is going to connect to the control plane,

2629
03:04:01,700 --> 03:04:07,700
retrieve like certificates, like the kind of basic marbles to get started.

2630
03:04:07,700 --> 03:04:14,700
And then it's going to use the secret token that I passed to authenticate itself with that cluster

2631
03:04:14,700 --> 03:04:16,700
and then dynamically join the cluster.

2632
03:04:16,700 --> 03:04:20,700
But this is specific to when you use QBDM.

2633
03:04:20,700 --> 03:04:24,700
There are many other ways to do that.

2634
03:04:24,700 --> 03:04:27,700
Given the questions that you ask, I guess that you might think,

2635
03:04:27,700 --> 03:04:32,700
okay, I'm going to have to run Kubernetes clusters and I need to learn not only about how to use them,

2636
03:04:32,700 --> 03:04:34,700
but also how to operate them.

2637
03:04:34,700 --> 03:04:38,700
If you're interested, if you look on container.training,

2638
03:04:39,700 --> 03:04:47,700
there is a deck about how to operate and like basically Kubernetes for ops and admins,

2639
03:04:47,700 --> 03:04:52,700
which deals with these things more in depth.

2640
03:04:52,700 --> 03:04:55,700
Yeah.

2641
03:04:55,700 --> 03:04:58,700
Other questions?

2642
03:04:58,700 --> 03:05:03,700
Okay, so there are two more things that I would like to cover today,

2643
03:05:03,700 --> 03:05:06,700
which are labels and selector and rolling updates.

2644
03:05:06,700 --> 03:05:14,700
I'm going to be mindful of time, so I'm going to take some shortcuts compared to what's in the slides.

2645
03:05:14,700 --> 03:05:20,700
So for labels and selectors, we started to touch that a little bit when I explained,

2646
03:05:20,700 --> 03:05:27,700
okay, how is it possible that we are sending traffic to these pods of the daemon set

2647
03:05:27,700 --> 03:05:35,700
because of the labels and because the service is defined as send traffic to all the pods that have this specific label.

2648
03:05:35,700 --> 03:05:43,700
So this means that we can use labels to configure load balancing.

2649
03:05:43,700 --> 03:05:45,700
How do we do that?

2650
03:05:45,700 --> 03:05:51,700
So here, for instance, I would like to remove a pod from the load balancer.

2651
03:05:51,700 --> 03:06:00,700
So currently, let me see like this here on top, that's the logs for all the RNG pods.

2652
03:06:00,700 --> 03:06:07,700
And maybe you can see on the left, there is a red pod, a dark blue pod, and a light blue pod.

2653
03:06:07,700 --> 03:06:11,700
I would like to remove the red pod from the rotation.

2654
03:06:11,700 --> 03:06:18,700
So if I freeze frame for a second to get the name of that pod, it's RNG ASCII88F.

2655
03:06:18,700 --> 03:06:23,700
Okay, so I would like to remove that pod from rotation.

2656
03:06:23,700 --> 03:06:30,700
But so I could remove the label, like the label app equal RNG.

2657
03:06:30,700 --> 03:06:38,700
But if I do that, what's going to happen is that the daemon set is going to recreate a replacement pod immediately.

2658
03:06:38,700 --> 03:06:43,700
Because if really if we look at the definition of the daemon set and how it works,

2659
03:06:43,700 --> 03:06:50,700
the job of the daemon set is to make sure that we have one pod matching this per node.

2660
03:06:50,700 --> 03:06:55,700
And matching this includes with label app equal RNG.

2661
03:06:55,700 --> 03:07:01,700
So if I touch to this RNG label, automatically another replacement pod will be created.

2662
03:07:01,700 --> 03:07:12,700
Okay, so if I want to fiddle with the load balancing, I need to add another label and I need to use that label.

2663
03:07:12,700 --> 03:07:16,700
Okay, so we're going to do that.

2664
03:07:16,700 --> 03:07:20,700
I'm going to do kubectl.

2665
03:07:20,700 --> 03:07:24,700
Okay, let me do that in that screen because it will be bigger.

2666
03:07:24,700 --> 03:07:33,700
So I'm going to use another label called enabled and with value yes for the pods that should be receiving traffic.

2667
03:07:33,700 --> 03:07:37,700
So first, I'm going to add that label to all my pods.

2668
03:07:37,700 --> 03:07:45,700
So kubectl label pods enabled equal yes.

2669
03:07:45,700 --> 03:07:56,700
And I want to put that on all the pods that have app equal RNG.

2670
03:07:56,700 --> 03:07:58,700
Okay, done.

2671
03:07:58,700 --> 03:08:05,700
Now I will edit the service definition so that it uses this label.

2672
03:08:05,700 --> 03:08:13,700
So there are many ways to edit an object, but one that is quite convenient is kubectl edit.

2673
03:08:13,700 --> 03:08:17,700
So kubectl edit service RNG.

2674
03:08:17,700 --> 03:08:25,700
This is going to download the YAML file, open it in the editor, then I can make changes, save, exit.

2675
03:08:25,700 --> 03:08:32,700
And when I exit, it's going to send back the modified file to the API server.

2676
03:08:32,700 --> 03:08:35,700
So that's the definition.

2677
03:08:35,700 --> 03:08:40,700
Sorry.

2678
03:08:40,700 --> 03:08:42,700
Do we need to list the services?

2679
03:08:42,700 --> 03:08:46,700
No.

2680
03:08:46,700 --> 03:08:48,700
Oh, do we need to restart the service?

2681
03:08:48,700 --> 03:08:51,700
No, this will be entirely dynamic.

2682
03:08:51,700 --> 03:08:52,700
That's the whole point.

2683
03:08:52,700 --> 03:09:01,700
So with Kubernetes, I can't really restart a service because there is no process associated with the service.

2684
03:09:01,700 --> 03:09:06,700
The service, that's why I said earlier, I liked the previous name before, a portal,

2685
03:09:06,700 --> 03:09:11,700
because a portal is like, okay, this is this door that takes me to something else.

2686
03:09:11,700 --> 03:09:18,700
So a service in Kubernetes is a network construct and there is no associated process to it.

2687
03:09:18,700 --> 03:09:26,700
It's just in this cluster, it's a bunch of IP tables rules, but in other clusters, it might be something else.

2688
03:09:26,700 --> 03:09:40,700
So here I'm going to go to the selector here and I'm going to add a second rule in the selector, enabled, colon, yes.

2689
03:09:40,700 --> 03:09:46,700
Here to save us some time and frustration, we have to use a string.

2690
03:09:46,700 --> 03:09:55,700
If I just put enable yes like that, we're going to get a super weird error message because in YAML,

2691
03:09:55,700 --> 03:10:02,700
yes becomes true, like the Boolean value, and a Boolean value can't be a label.

2692
03:10:02,700 --> 03:10:06,700
So that's a little gotcha in YAML and Kubernetes.

2693
03:10:06,700 --> 03:10:15,700
So I have to put enabled yes, like with quotes to say, yeah, this is really a string, not the Boolean value.

2694
03:10:15,700 --> 03:10:16,700
I do that.

2695
03:10:16,700 --> 03:10:21,700
So this is kind of pushing back the updated service definition.

2696
03:10:21,700 --> 03:10:26,700
No change because the set of pods is still the same.

2697
03:10:26,700 --> 03:10:30,700
I changed the query, if you will, but the result of the query is the same.

2698
03:10:30,700 --> 03:10:33,700
So that didn't affect anything.

2699
03:10:33,700 --> 03:10:36,700
But now I can touch this enabled label.

2700
03:10:36,700 --> 03:10:40,700
I can change it to add and remove services.

2701
03:10:40,700 --> 03:10:46,700
So for instance, remember I said I want to remove the red service.

2702
03:10:46,700 --> 03:10:51,700
So I do kubectl label pod.

2703
03:10:51,700 --> 03:10:59,700
So that's this one RNG, esky88f.

2704
03:10:59,700 --> 03:11:02,700
And I want to remove the enabled label.

2705
03:11:02,700 --> 03:11:06,700
So to remove a label, the syntax is a little bit different.

2706
03:11:06,700 --> 03:11:10,700
You put the name of the label and then you put minus.

2707
03:11:10,700 --> 03:11:14,700
That means drop that label.

2708
03:11:14,700 --> 03:11:16,700
I do that immediately.

2709
03:11:16,700 --> 03:11:23,700
You see on top, I'm sorry if you can't see colors, but the red pod is not visible anymore.

2710
03:11:23,700 --> 03:11:25,700
It stops showing up above.

2711
03:11:25,700 --> 03:11:33,700
And if I look in the Web UI, the performance is now going to drop to 20 per second.

2712
03:11:33,700 --> 03:11:36,700
It's not going to be visible immediately.

2713
03:11:36,700 --> 03:11:40,700
But and I can repeat that operation.

2714
03:11:40,700 --> 03:11:47,700
You can say, OK, now drop the label from RNG, gz9n5.

2715
03:11:47,700 --> 03:11:50,700
So that's the light blue pod.

2716
03:11:50,700 --> 03:11:55,700
Sorry, gz9n5.

2717
03:11:55,700 --> 03:11:57,700
So that's the light blue pod.

2718
03:11:57,700 --> 03:12:02,700
So here we're going to only keep the dark blue pod.

2719
03:12:02,700 --> 03:12:04,700
That's it.

2720
03:12:04,700 --> 03:12:10,700
So that's great because it means that without knowing exactly how the load balancing works under the hood,

2721
03:12:10,700 --> 03:12:14,700
without having to worry about kube proxy, IP tables, whatever,

2722
03:12:14,700 --> 03:12:19,700
I am able to add and remove backends to a load balancer.

2723
03:12:19,700 --> 03:12:22,700
It takes effect instantly.

2724
03:12:22,700 --> 03:12:30,700
And I can manipulate my load balancer configuration live without requiring root access to load balancer,

2725
03:12:30,700 --> 03:12:36,700
without requiring me to open a ticket with the networking team, et cetera, et cetera, et cetera, et cetera.

2726
03:12:36,700 --> 03:12:43,700
OK, so that's what we did.

2727
03:12:43,700 --> 03:12:45,700
So this can be used for debugging.

2728
03:12:45,700 --> 03:12:51,700
Like if I need to remove something from the load balancer, I remove the label, do some tests.

2729
03:12:51,700 --> 03:12:54,700
Yeah.

2730
03:12:54,700 --> 03:12:57,700
Is there a?

2731
03:12:57,700 --> 03:13:03,700
I'm sorry, I can't hear you.

2732
03:13:03,700 --> 03:13:11,700
I'm really sorry, but I really can't hear you from the back of the room.

2733
03:13:11,700 --> 03:13:15,700
Oh, is there a way to capture the state information of the load balancer?

2734
03:13:15,700 --> 03:13:25,700
You mean states like to make sure that a given client always will go to the same back end or?

2735
03:13:25,700 --> 03:13:28,700
Like to see the back ends or?

2736
03:13:28,700 --> 03:13:37,700
So if I do kubectl describe on describe service RNG, I see the list of endpoints.

2737
03:13:37,700 --> 03:13:43,700
And that tells me where the traffic is going.

2738
03:13:43,700 --> 03:13:45,700
Oh, is there a push notification?

2739
03:13:45,700 --> 03:13:50,700
OK, like for instance, I want to know when a service is OK, I see what you mean.

2740
03:13:50,700 --> 03:13:51,700
Right.

2741
03:13:51,700 --> 03:13:52,700
That's an excellent question.

2742
03:13:52,700 --> 03:13:53,700
And yes, we can do that.

2743
03:13:53,700 --> 03:14:00,700
For instance, I can do OK, I want to be aware when something changes in my load balancer.

2744
03:14:00,700 --> 03:14:14,700
So I'm going to split this again and I'm going to do kubectl get service RNG dash W, the same watch option that we used for pods earlier.

2745
03:14:14,700 --> 03:14:17,700
And then I'm going to change something.

2746
03:14:17,700 --> 03:14:20,700
So for instance, I'm going to read a label to a pod.

2747
03:14:20,700 --> 03:14:30,700
So kubectl label pod RNG something like SK, etc.

2748
03:14:30,700 --> 03:14:31,700
Enabled equal.

2749
03:14:31,700 --> 03:14:32,700
Yes.

2750
03:14:32,700 --> 03:14:38,700
So when I do that, I will see that pod reappear up there and here I will see an update as well.

2751
03:14:38,700 --> 03:14:40,700
Oh, no.

2752
03:14:40,700 --> 03:14:46,700
OK, maybe that's not the right object that I need to monitor.

2753
03:14:46,700 --> 03:14:49,700
I think it's the endpoints object that I need to monitor.

2754
03:14:49,700 --> 03:14:52,700
Let's try again.

2755
03:14:52,700 --> 03:14:54,700
OK, so rewind.

2756
03:14:54,700 --> 03:14:58,700
Let's pretend nothing happened and let's try again with the endpoints object.

2757
03:14:58,700 --> 03:15:01,700
So I'm going to.

2758
03:15:01,700 --> 03:15:04,700
Yes, that's it.

2759
03:15:04,700 --> 03:15:05,700
Yeah, that's what I did.

2760
03:15:05,700 --> 03:15:09,700
I did kubectl get endpoints RNG dash W.

2761
03:15:09,700 --> 03:15:13,700
And now I'm streaming the endpoints.

2762
03:15:13,700 --> 03:15:27,700
So if I want to be aware each time something changes, I can do it that way.

2763
03:15:27,700 --> 03:15:29,700
Oh, if I could I streaming to something else?

2764
03:15:29,700 --> 03:15:32,700
Yeah. So in that case, I'm just streaming with the CLI.

2765
03:15:32,700 --> 03:15:34,700
So it's just re-showing the object.

2766
03:15:34,700 --> 03:15:38,700
But on the API level, I can do the equivalent of a watch.

2767
03:15:38,700 --> 03:15:44,700
And then I get the updated object and I can.

2768
03:15:44,700 --> 03:15:51,700
OK, so I think the plug on which I had connected my laptop was actually not connected to power.

2769
03:15:51,700 --> 03:15:54,700
And so my laptop just lost power.

2770
03:15:54,700 --> 03:16:00,700
So let's see.

2771
03:16:00,700 --> 03:16:05,700
OK.

2772
03:16:05,700 --> 03:16:07,700
Now I have power.

2773
03:16:07,700 --> 03:16:12,700
I'm going to report that really quick.

2774
03:16:12,700 --> 03:16:14,700
Do we have sorry?

2775
03:16:14,700 --> 03:16:15,700
Yeah, yeah, I know.

2776
03:16:15,700 --> 03:16:16,700
I know, I know.

2777
03:16:16,700 --> 03:16:18,700
I just wanted to.

2778
03:16:18,700 --> 03:16:22,700
So we've seen selectors and labels.

2779
03:16:22,700 --> 03:16:25,700
The last chapter was about rollouts.

2780
03:16:25,700 --> 03:16:29,700
And I'm just going to give you the high level picture.

2781
03:16:29,700 --> 03:16:38,700
When we want to push a new version of a service, we do that by changing.

2782
03:16:38,700 --> 03:16:45,700
Yeah, yeah, yeah, I'm just going to let me just restart my graphic environment and then.

2783
03:16:45,700 --> 03:16:48,700
Let's see.

2784
03:16:48,700 --> 03:16:51,700
There we go.

2785
03:16:51,700 --> 03:16:56,700
And then slides.

2786
03:16:56,700 --> 03:17:02,700
And then this goes here.

2787
03:17:02,700 --> 03:17:07,700
And then we were around here.

2788
03:17:07,700 --> 03:17:16,700
So when we do a rolling update, so we want to update one of the services, we do that by changing the deployment object.

2789
03:17:16,700 --> 03:17:23,700
And I alluded to that earlier today when we change the deployment object, it's going to create a new replica set.

2790
03:17:23,700 --> 03:17:30,700
And then it's going to scale up that replica set and scale down the old replica set.

2791
03:17:30,700 --> 03:17:40,700
And there are a few parameters that we can set here to say how many new pods we want to create and how many old pods we want to remove.

2792
03:17:40,700 --> 03:17:50,700
Just to show you in concrete terms what this looks like, I'm going to reconnect to my cluster.

2793
03:17:50,700 --> 03:17:52,700
Reattach to T-Mux.

2794
03:17:52,700 --> 03:18:05,700
And so, for instance, if I want to upgrade the container image used by Walker, I can do kubectl set image.

2795
03:18:05,700 --> 03:18:08,700
Deployment Walker.

2796
03:18:08,700 --> 03:18:10,700
I think.

2797
03:18:10,700 --> 03:18:13,700
Just making sure.

2798
03:18:13,700 --> 03:18:15,700
Yeah.

2799
03:18:15,700 --> 03:18:20,700
Walker equal Docker coin slash Walker, colon V 0.2.

2800
03:18:20,700 --> 03:18:23,700
So that means set image is it's a helper.

2801
03:18:23,700 --> 03:18:30,700
It's going to get the email of the deployment, patch it, like change something in it, and then push it back.

2802
03:18:30,700 --> 03:18:43,700
So that means get the deployment Walker and change the image for the container Walker inside that deployment and set it to this image.

2803
03:18:43,700 --> 03:18:44,700
So we do that.

2804
03:18:44,700 --> 03:18:45,700
And that's all we have to do.

2805
03:18:45,700 --> 03:18:49,700
And then it's going to proceed with the rolling updates.

2806
03:18:49,700 --> 03:18:50,700
OK.

2807
03:18:50,700 --> 03:18:53,700
You can try that like this afternoon if you want.

2808
03:18:53,700 --> 03:18:57,700
I just would like to give the conclusion quickly.

2809
03:18:57,700 --> 03:19:00,700
So how do we get started to containerize applications?

2810
03:19:00,700 --> 03:19:07,700
So very often what I tell to people who get started with Kubernetes is first make sure that you got the basics.

2811
03:19:07,700 --> 03:19:18,700
So make sure that you are already using containers in Dev, because if you're not using containers in Dev and you want to jump directly to using Kubernetes, you're going to have a bad time.

2812
03:19:18,700 --> 03:19:21,700
So make sure that you're using containers.

2813
03:19:21,700 --> 03:19:25,700
It could be with Docker files and compose files, just like I showed this morning.

2814
03:19:25,700 --> 03:19:27,700
It could be something else.

2815
03:19:27,700 --> 03:19:34,700
I think that compose is a really great development tool because remember what we did this morning.

2816
03:19:34,700 --> 03:19:35,700
We cloned the repo.

2817
03:19:35,700 --> 03:19:40,700
We did compose up and couple of minutes later we had the app up and running.

2818
03:19:40,700 --> 03:19:49,700
Fun fact that app was written almost four years ago and the code hasn't changed by a single line since then.

2819
03:19:49,700 --> 03:19:55,700
So code that has been written for Docker demos four years ago still works today with Kubernetes.

2820
03:19:55,700 --> 03:20:00,700
The images that we've built four years ago still work today with Kubernetes.

2821
03:20:00,700 --> 03:20:02,700
So that's that's pretty nice.

2822
03:20:02,700 --> 03:20:14,700
And once you have containers in Dev, then you can start thinking about CI, maybe doing some CD to staging, like have a specific branch going to staging.

2823
03:20:14,700 --> 03:20:17,700
And that does not require clusters and orchestration.

2824
03:20:17,700 --> 03:20:19,700
You can do that on single nodes.

2825
03:20:19,700 --> 03:20:25,700
And then after that, you can start thinking about orchestration and things like Kubernetes.

2826
03:20:25,700 --> 03:20:28,700
Start with a managed Kubernetes cluster.

2827
03:20:28,700 --> 03:20:38,700
You might end up having to run it yourself, but start with a managed cluster because you will save yourself a lot of problems and extra hassle.

2828
03:20:39,700 --> 03:20:48,700
Prefer multiple small clusters rather than one big cluster, because if there is a problem, you will have one small problem instead of a big one.

2829
03:20:48,700 --> 03:20:53,700
Namespaces we mentioned earlier this morning, so I can skip that.

2830
03:20:53,700 --> 03:21:00,700
I have a few links here to chapters about namespaces, network policies and role-based access control.

2831
03:21:00,700 --> 03:21:06,700
Stateful services are if you want to run databases, message queues and things like that.

2832
03:21:06,700 --> 03:21:08,700
You don't have to very often.

2833
03:21:08,700 --> 03:21:11,700
People ask me, hey, should I run my database on Kubernetes?

2834
03:21:11,700 --> 03:21:12,700
Then I tell them no.

2835
03:21:12,700 --> 03:21:17,700
But if they ask me, should I run my databases like Perl on Kubernetes?

2836
03:21:17,700 --> 03:21:18,700
And I say, maybe.

2837
03:21:18,700 --> 03:21:26,700
What I mean by that is that running a stateful service on Kubernetes is a lot of extra work, especially if you're getting started with Kubernetes.

2838
03:21:26,700 --> 03:21:30,700
So there is no shame in keeping your databases outside.

2839
03:21:30,700 --> 03:21:50,700
Now, if you are running lots of databases like tens, hundreds, thousands, it might be a good idea to leverage Kubernetes because you will leverage the automation and the features we saw for pods that repair themselves, et cetera, for databases as well.

2840
03:21:51,700 --> 03:22:03,700
There is another thing that if we had more time, I wish I could have shown you, which is ingresses, which lets us handle HTTP traffic in special ways.

2841
03:22:03,700 --> 03:22:08,700
Logins and metrics, I alluded to that earlier today.

2842
03:22:08,700 --> 03:22:11,700
Configuration of applications.

2843
03:22:11,700 --> 03:22:12,700
Same thing.

2844
03:22:12,700 --> 03:22:22,700
I have links here to a section showing how to inject on variables and how to use configuration maps to manage application configuration.

2845
03:22:22,700 --> 03:22:32,700
And for stack deployment, today we've used mostly kubectl and we created resources almost manually.

2846
03:22:32,700 --> 03:22:45,700
The next step to that is to get the YAML, clean it up a little bit because the YAML has lots of information, but you can drop a lot of that information and kind of boil it down to the essential.

2847
03:22:45,700 --> 03:22:50,700
And then you can version that YAML in your repo and use that to deploy.

2848
03:22:50,700 --> 03:23:02,700
So instead of telling someone, oh, you need to kubectl create this, kubectl run that, kubectl expose, you just give them a nice little heap of YAML and they just create that YAML and that's it.

2849
03:23:02,700 --> 03:23:22,700
And then you have other tools like customize and helm that will let you template that YAML so that it can be customized and tailored for specific scenarios so that you have one core YAML and you specialize it for dev, for preprod, for prod, etc.

2850
03:23:22,700 --> 03:23:28,700
Last step, I've put it last because I want to make sure that you remember about that.

2851
03:23:28,700 --> 03:23:39,700
When you start learning about Kubernetes, really very quickly, you will be ahead of the rest of your team in terms of what you know about Kubernetes.

2852
03:23:39,700 --> 03:23:46,700
And there are many concepts that are new and sometimes weird or bizarre.

2853
03:23:46,700 --> 03:23:53,700
And mind the gap. Remember that now you know more about containers and Kubernetes than the rest of your team.

2854
03:23:53,700 --> 03:24:04,700
And that once you start talking about pods and services and selectors and etc., a lot of people are going to give you a blank stare if you don't take the time to spread that knowledge as well.

2855
03:24:04,700 --> 03:24:09,700
I've put a few questions here. It's a kind of good litmus test.

2856
03:24:09,700 --> 03:24:14,700
Do I have good answers to these questions when I start implementing containers and Kubernetes in my team?

2857
03:24:14,700 --> 03:24:29,700
If I do have good answers to that, great. If I don't, maybe I should stop there to make sure that I won't end up with a small team of Kubernetes experts and everybody else kind of like, how do we grasp that thing?

2858
03:24:29,700 --> 03:24:34,700
That's it. I have a bunch of extra links and resources.

2859
03:24:34,700 --> 03:24:43,700
Most importantly, the PyCon organizer would like me to tell you that you should rate this workshop using that link that I will keep on the screen.

2860
03:24:43,700 --> 03:24:54,700
If you want to know and learn more about Kubernetes, if you go and content other training, you will find slides decks that I use for one day, two day, even longer workshops.

2861
03:24:54,700 --> 03:25:02,700
And that's it. And the last words as a shameless self-promotion. I also do that as a professional trainer.

2862
03:25:02,700 --> 03:25:07,700
So if you liked this and you want more of it, talk to me. I'll be happy to come and train your team.

2863
03:25:07,700 --> 03:25:11,700
Thank you. And I'll be here to answer questions and during the whole conference as well.

2864
03:25:13,700 --> 03:25:18,700
Thank you.

