1
00:00:00,000 --> 00:00:14,000
Parece que es un ingeniero junior, no sé cómo traducirlo pero creo que no tenemos traducción en español de junior, es nada más alguien que no tiene tanta experiencia.

2
00:00:14,000 --> 00:00:40,000
Y vaya, en general, también ETL en español, no sé si exista una traducción de ETL porque es tal cual como un concepto, pero vaya, nos va a platicar un poquito de su experiencia con ETLs y trabajando con Python.

3
00:00:40,000 --> 00:00:42,000
Bueno, un aplauso por favor.

4
00:00:47,000 --> 00:00:55,000
Hola a todos, muy buenos días o casi tardes ya. Me alegra mucho veros aquí y me alegra que podamos estar aquí con todos.

5
00:00:55,000 --> 00:01:02,000
Entonces, voy a contar rápidamente cómo fue mi experiencia y qué fue lo que hicimos en mi empresa cuando quisimos implementar un ETL usando Python.

6
00:01:02,000 --> 00:01:08,000
Entonces, mi nombre es David, estoy ingeniero de sistemas y me gusta resolver problemas del mundo real.

7
00:01:08,000 --> 00:01:19,000
Somos una empresa de matemáticas aplicadas, entonces utilizamos tecnología combinada con economía, física y otro tipo de áreas de la ciencia para resolver problemas para el sector público y privado del mundo real.

8
00:01:19,000 --> 00:01:24,000
Desde salud hasta cómo hacer que las plantas de energía no se caigan y ahorren mucha energía.

9
00:01:24,000 --> 00:01:32,000
Trabajo en Quantil, que es en Colombia y si quieren conocerme con un poquito más de mis posturas tecnológicas, políticas y demás, síganme en Twitter, ahí coloco un montón de cosas.

10
00:01:32,000 --> 00:01:38,000
Y pues prefiero, igual que me hablen en persona y nos volvamos amigos de verdad.

11
00:01:38,000 --> 00:01:41,000
Entonces, el inicio, ¿sí? ¿Por qué construimos un pipeline de ETL?

12
00:01:41,000 --> 00:01:52,000
Porque como primera cosa es que yo tengo esta cosa que tenga en la cabeza y es que en mi universidad nos repetían esto todo el tiempo y es que una solución de tecnología solo tiene valor si agrega valor al negocio de cierta forma.

13
00:01:52,000 --> 00:02:00,000
Si apunta a un SLA, por ejemplo, o si apunta a que la empresa ahorre costos o algo que a la gente negociara feliz.

14
00:02:00,000 --> 00:02:06,000
Si no, por más de que sea muy chévere y tenga Python y tenga las cosas más nuevas, pues no te van a poder ir cuidados porque no aporta nada de plata.

15
00:02:06,000 --> 00:02:08,000
Es triste, pero es así.

16
00:02:08,000 --> 00:02:09,000
Entonces, tengan eso en cuenta.

17
00:02:09,000 --> 00:02:17,000
Para el caso de nuestro cliente que teníamos el año pasado, que era una cooperativa, una cooperativa en Colombia, es como una organización que presta beneficios a sus afiliados, ¿sí?

18
00:02:17,000 --> 00:02:22,000
De salud, de recreación, bancaria para préstamos de casas, para educación y demás, ¿cierto?

19
00:02:22,000 --> 00:02:28,000
Y también de turismo porque muchas veces uno quiere salir y despejarse del mundo y irse a la playa, ¿cierto?

20
00:02:28,000 --> 00:02:37,000
Entonces, los afiliados pagan una mensualidad, una cantidad presta de dinero, la idea es que no sea tan alta y pues poco a poco pueden obtener sus beneficios eventualmente.

21
00:02:37,000 --> 00:02:44,000
La cuestión es que los afiliados de esta cooperativa colombiana estaban cancelando su afiliación porque no se va bien bien.

22
00:02:44,000 --> 00:02:56,000
Y la idea o lo que notamos es que hay una probabilidad bastante alta, más o menos de un 89%, de que un afiliado que hubiera entrado a la cooperativa en los últimos cinco años, al pasar dos años se saliera y dejará de pagar.

23
00:02:56,000 --> 00:03:04,000
Entonces, pues dijimos, esto va a ser que eventualmente la cooperativa, cuando su gente que está desde hace mucho tiempo muera o se salga, ¿cierto?

24
00:03:04,000 --> 00:03:06,000
Tengo una tasa de...

25
00:03:06,000 --> 00:03:17,000
Y no llegue nunca más gente nueva, pues la gente se va a empezar a salir y no va a tener ingresos para poder seguir pesando los beneficios, entonces la cooperativa no va a seguir funcionando, ¿no?

26
00:03:17,000 --> 00:03:20,000
Y pues la idea es también saber qué podemos hacer para prevenirlo.

27
00:03:20,000 --> 00:03:23,000
Entonces, nosotros somos una compañía de ciencia de datos, entre comillas, ¿no?

28
00:03:23,000 --> 00:03:26,000
Porque hacemos matemáticas combinadas con todo esto.

29
00:03:26,000 --> 00:03:33,000
Entonces, dijimos, bueno, vamos a intentar construir un modelo predictivo utilizando los datos que hay, que nos diga la probabilidad de que un afiliado se vaya a ir, ¿cierto?

30
00:03:33,000 --> 00:03:42,000
Y luego, a los que tengan alta probabilidad de irse, los atacamos junto con el equipo de marketing y diseño y más, con beneficios, descuentos, etcétera, etcétera, para que se queden, ¿no?

31
00:03:42,000 --> 00:03:48,000
Y sigan recibiendo los beneficios, que igual son buenos y hacen que la cooperativa, que es una entidad, de hecho, sin ánimo de lucro, ayude a la gente.

32
00:03:50,000 --> 00:03:58,000
Sin embargo, nos encontramos con un problema y era que al momento de preguntarles un poco más sobre la infraestructura y sobre los datos que tenían, bueno, que todos estaban esparcidos en cientos de tablas.

33
00:03:59,000 --> 00:04:07,000
La cooperativa llevaba 60 años en el mercado colombiano y obviamente en los 60 años se habían capturado información de todo tipo y de muchas fuentes con diferentes formatos, colores y sabores.

34
00:04:07,000 --> 00:04:09,000
Entonces, era súper difícil entenderlo.

35
00:04:10,000 --> 00:04:17,000
Y a su vez también, los datos estaban almacenados en un servidor monolítico que recibió su última migración, creo que en el año 2007, 2008, ¿sí?

36
00:04:18,000 --> 00:04:19,000
Y ya está a salvo de su capacidad.

37
00:04:20,000 --> 00:04:31,000
Intentamos intentar explorar un poco utilizando los sentimientos de SQL como para preguntarnos un poco qué formatos había, qué datos había y realmente nos dijeron, por favor cáncelen esas búsquedas porque no están saturando el servidor.

38
00:04:31,000 --> 00:04:33,000
No podemos estar haciendo eso mientras que estamos en producción.

39
00:04:34,000 --> 00:04:39,000
Entonces dijimos, bueno, ah bueno, lo que les decía, muchos de múltiples formatos y colores diferentes, ¿no?

40
00:04:40,000 --> 00:04:48,000
Entonces dijimos, bueno, vamos a construir un pipeline para poder transformar todo ese desorden de información en algo que podamos utilizar para contestar las preguntas que la empresa quiere que contestemos, ¿no?

41
00:04:49,000 --> 00:04:52,000
Entonces dijimos, bueno, utilizamos algo chevere, algo que conocemos, que hemos hecho antes.

42
00:04:53,000 --> 00:04:56,000
Entonces, primero y por todo, utilizamos Google Cloud porque a la empresa le gusta Google Cloud, ¿sí?

43
00:04:57,000 --> 00:05:02,000
Segundo, utilizamos Django para poder construir modelos para poder recibir y transformar la información.

44
00:05:03,000 --> 00:05:09,000
Y a su vez exponerla y a una API para que los científicos de datos de la empresa que ya estaban montando, se vea apareciendo, puedan consumirla y utilizarla.

45
00:05:10,000 --> 00:05:14,000
Y finalmente utilizar Cassandra, que es una base de datos distribuida que, eso es open source, ¿no?

46
00:05:14,000 --> 00:05:17,000
Pero Facebook la apoyo un montón porque la utilizan en producción.

47
00:05:17,000 --> 00:05:22,000
Y la idea es que permitiera la escritura muy rápida y a su vez almacenamiento distribuida en varias partes, ¿sí?

48
00:05:23,000 --> 00:05:31,000
Entonces dijimos, esa arquitectura y esa arquitectura nos permitió, uno, que los datos estuvieran consolidados en una infraestructura nueva y organizada, ¿no?

49
00:05:31,000 --> 00:05:33,000
No en un servidor monolídico que pueda escalar, ¿no?

50
00:05:34,000 --> 00:05:40,000
Segundo, exponerlos a través de una API REST que es súper bueno para los científicos de datos y para cualquiera del negocio que quiera conectar.

51
00:05:40,000 --> 00:05:42,000
Un dashboard, por ejemplo, para poder consumirlos.

52
00:05:43,000 --> 00:05:47,000
Tercero, no afectamos la infraestructura anterior, que es súper importante.

53
00:05:47,000 --> 00:05:50,000
Nos dijeron, por favor, no la toquen, es súper importante que no la molesten.

54
00:05:51,000 --> 00:05:52,000
Y cuatro, pues escala muy fácil.

55
00:05:52,000 --> 00:05:57,000
En realidad la compañía tenía bajo presupuesto para el proyecto, pero tiene mucho plata por detrás, muchos millones.

56
00:05:57,000 --> 00:06:06,000
Entonces, si querían crecer esto y se volvió un proyecto serio, podían pagar más Google Cloud y aumentarlos el número de nodos de la base de datos o de los servidores de procesamiento o lo que quisieran.

57
00:06:07,000 --> 00:06:14,000
Sin embargo, en ese momento estábamos emocionados y dijimos vamos a construirlos sin tanto miedo, sin tanto problema, pero nos encontramos con un problema.

58
00:06:14,000 --> 00:06:19,000
Y era que en realidad no era tan fácil definir qué es lo que teníamos que hacer.

59
00:06:19,000 --> 00:06:24,000
La empresa nunca había hecho nada de ciencia de datos, entonces nos dijo pues explora un poquito para ver qué sucede.

60
00:06:24,000 --> 00:06:30,000
Nuestro equipo para este proyecto éramos dos personas en ingeniería, en sistemas, por decirlo así, en tecnología, y tres personas en ciencia de datos, ¿no?

61
00:06:30,000 --> 00:06:33,000
La cuestión es que empezamos cada uno a explorar por aparte, separados.

62
00:06:33,000 --> 00:06:38,000
Y los de ingeniería nos dimos cuenta que, primero de todo, era imposible entender esa cantidad de datos.

63
00:06:38,000 --> 00:06:45,000
Eran muchas tablas con cientos de columnas con nombres no tan informativos y sin el conocimiento del negocio, pues era súper difícil entenderlo, ¿no?

64
00:06:46,000 --> 00:06:53,000
A su vez había muchas restricciones donde nos decían no, esa tabla fue de un año que pasó algo especial y por favor no la consideren.

65
00:06:53,000 --> 00:06:58,000
Y no, o sea, muchas restricciones súper complicadas que impedían avanzar rápidamente.

66
00:06:58,000 --> 00:07:00,000
Y a su vez había muchas variables inútiles.

67
00:07:00,000 --> 00:07:07,000
Ellos pensaban que cada columna que ellos tenían era útil para un modelo posible, pues obviamente eso no es cierto, hay más variables que importan más que otras.

68
00:07:07,000 --> 00:07:12,000
Entonces, pues, tocaba eliminarlas y poder conversarlos, decirles que porque eran no eran tan buenas.

69
00:07:12,000 --> 00:07:17,000
Entonces, pues, la falta de esa buena documentación estaba matando nuestra productividad y nos estaba complicando la vida.

70
00:07:17,000 --> 00:07:24,000
Lanto a nosotros, como el equipo de ciencia de datos, que sin hablarnos mucho, empezaron a escoger modelos y a escoger ciertas variables que pues no nos informaron, ¿no?

71
00:07:24,000 --> 00:07:28,000
Entonces, nos dimos, venga, no podemos seguir haciendo eso, tenemos que cambiar la manera como lo estamos haciendo.

72
00:07:28,000 --> 00:07:36,000
Así que decimos aplicar un framework de ciencia de datos que yo aprendí en la universidad y que creo que está en un libro de visualización de Tamara Mosner.

73
00:07:36,000 --> 00:07:39,000
No sé si la conocen a ella, es una persona muy buena en visualización.

74
00:07:39,000 --> 00:07:40,000
Y son tres preguntas muy básicas.

75
00:07:40,000 --> 00:07:43,000
La primera, ¿qué quieres saber o qué quieres hacer?

76
00:07:43,000 --> 00:07:46,000
En este caso, ¿qué quiere nuestro cliente a saber o hacer?

77
00:07:46,000 --> 00:07:48,000
Y en este caso era, ¿por qué los afiliados están desafiliando?

78
00:07:48,000 --> 00:07:49,000
Es súper sencillo.

79
00:07:49,000 --> 00:07:53,000
Una pregunta que tiene más dimensiones específicas, pero a grandes rasgos, es esa, ¿no?

80
00:07:53,000 --> 00:07:59,000
A su vez, y a su vez, creo que si es conector wifi, vamos a conectar la PDF.

81
00:07:59,000 --> 00:08:00,000
¿Qué pena?

82
00:08:03,000 --> 00:08:04,000
Disculpen.

83
00:08:09,000 --> 00:08:10,000
Eso siempre pasa.

84
00:08:19,000 --> 00:08:20,000
Sorry.

85
00:08:20,000 --> 00:08:21,000
Disculpen.

86
00:08:22,000 --> 00:08:23,000
Eso siempre pasa.

87
00:08:24,000 --> 00:08:25,000
Listo.

88
00:08:25,000 --> 00:08:28,000
La segunda pregunta es, ¿quién está haciendo la pregunta?

89
00:08:28,000 --> 00:08:33,000
Porque si se dan cuenta, es muy diferente si un ingeniero o un científico de datos hace la pregunta de, ¿por qué están desafiliando a una persona de negocio?

90
00:08:33,000 --> 00:08:38,000
A una persona de negocio le interesa saber motivos, causas habladas en un lenguaje que es natural.

91
00:08:38,000 --> 00:08:43,000
Al científico de datos le interesa saber qué población con tales características, con qué edad tiene que estar característica.

92
00:08:43,000 --> 00:08:46,000
Están entriándose para poder atacarlos.

93
00:08:46,000 --> 00:08:49,000
Al de marketing le interesa más cosas particulares.

94
00:08:49,000 --> 00:08:51,000
Y finalmente, ¿qué datos tenemos para contestarlas?

95
00:08:51,000 --> 00:08:59,000
Porque una empresa puede querer saber muchas cosas, pero si nunca ha tenido datos o no tiene sus bases organizadas, pues tal vez sea imposible contestar la pregunta.

96
00:08:59,000 --> 00:09:03,000
Entonces, dado esto, nos planteamos estas preguntas y definimos muy bien el problema.

97
00:09:03,000 --> 00:09:06,000
Y nos planteamos cinco preguntas de analítica que ellos querían contestar específicamente.

98
00:09:06,000 --> 00:09:09,000
Una de ellas siendo la de por qué se estaban desafiliando.

99
00:09:09,000 --> 00:09:12,000
Pero había otras más y pudimos reducir el problema.

100
00:09:12,000 --> 00:09:16,000
De cientos de tablas, por ejemplo, ya pasamos a solo 58.

101
00:09:16,000 --> 00:09:22,000
Inclusive, desde 58 solamente necesitamos algunas columnas, no todas las columnas, lo cual reduce el problema un montón.

102
00:09:22,000 --> 00:09:26,000
Y una vez tenemos estas preguntas resueltas, podemos seguir adelante.

103
00:09:26,000 --> 00:09:33,000
Sin embargo, al momento ya enfrentarnos a la implementación tecnológica, nos dimos cuenta que Python incrementó el tamaño de los datos.

104
00:09:33,000 --> 00:09:37,000
Una cosa es tenerlo en base de datos con cierto tamaño y otra cosa es cargar la memoria y procesarlo.

105
00:09:37,000 --> 00:09:40,000
Y con esto pasamos a la parte interesante.

106
00:09:40,000 --> 00:09:41,000
Entonces, ¿cómo funcionaba?

107
00:09:41,000 --> 00:09:46,000
Su base de datos era SQL Server, un poco como analítica y de los años 2000 y pico.

108
00:09:46,000 --> 00:09:49,000
Utilizamos el driver de SQL, operación regular en Python.

109
00:09:49,000 --> 00:09:55,000
Nos conectamos al driver, extraemos la información y lo cargamos en memoria para poder procesarlo.

110
00:09:55,000 --> 00:09:58,000
Yo, para mirar qué tan grande era, lo puse en formato PKL.

111
00:09:58,000 --> 00:10:00,000
No sé si conocen el formato.

112
00:10:00,000 --> 00:10:04,000
Es un formato comprimido para guardar información en Python. Muy chévere.

113
00:10:04,000 --> 00:10:09,000
Y nos dimos cuenta que todas las tablas, para el momento que empezamos el proyecto, pesaban 1.3 GB.

114
00:10:09,000 --> 00:10:12,000
¿Cierto? Usted dirá, pues 1.3 GB no es tanto.

115
00:10:12,000 --> 00:10:16,000
Sin embargo, cuando se carga su memoria, eso puede saturar un servidor de 16 GB y lo acaba completamente.

116
00:10:16,000 --> 00:10:18,000
Ni siquiera cargando un tercio de las tablas.

117
00:10:18,000 --> 00:10:23,000
Nos dimos cuenta que iba a ser un reto procesarlo, dado los recursos computacionales que nos habían asignado.

118
00:10:23,000 --> 00:10:25,000
Dado el presupuesto al proyecto.

119
00:10:25,000 --> 00:10:30,000
Dado esto, pues, le quiero recordar, estamos con una cooperativa que tiene servicios de todas esas casas.

120
00:10:30,000 --> 00:10:33,000
Bancarios, recreación, turismo, salud.

121
00:10:33,000 --> 00:10:37,000
Dado esto, una de las preguntas que hemos definido, esas cinco que mencioné anteriormente, era

122
00:10:37,000 --> 00:10:41,000
Podemos tener toda la información analítica en un solo lugar, para un solo afiliado, ¿cierto?

123
00:10:41,000 --> 00:10:43,000
Sin necesidad de hacer más de una petición.

124
00:10:43,000 --> 00:10:45,000
Sin hacer joins, merges, etc.

125
00:10:45,000 --> 00:10:49,000
Que era una pregunta, una cosa que un científico de datos y la gente de negocio quería saber.

126
00:10:49,000 --> 00:10:53,000
¿Por qué Pepito Pérez, por qué Guido Van Rosum se fue de la cooperativa?

127
00:10:53,000 --> 00:10:58,000
Pues, queremos saber todas las interacciones que tenga esa persona cooperativa para poder tomar decisiones.

128
00:10:58,000 --> 00:11:01,000
Entonces, si se ponen a pensar, ¿esto cómo se hace?

129
00:11:01,000 --> 00:11:04,000
Pues, sabemos que la persona tiene un montón de interacciones, ¿cierto?

130
00:11:04,000 --> 00:11:06,000
Guardadas en múltiples bases de datos y en múltiples tablas.

131
00:11:06,000 --> 00:11:11,000
Entonces, necesitamos, como que todo, remover filas duplicadas porque puede que la persona haya cometido errores de que

132
00:11:11,000 --> 00:11:14,000
fui al banco, empezó la operación y la canceló.

133
00:11:14,000 --> 00:11:19,000
Pero, sin embargo, quedó el registro de ingreso de que tuvo un turno y el banco para poder ser atendido, ¿cierto?

134
00:11:19,000 --> 00:11:24,000
Entonces, esos tipos de cosas que son duplicadas o simplemente no aportan a tomar decisiones,

135
00:11:24,000 --> 00:11:26,000
o a un modelo predictivo eventualmente, fueron retiradas.

136
00:11:26,000 --> 00:11:28,000
Segundo, generamos agregados.

137
00:11:28,000 --> 00:11:34,000
Porque, como le dije, la cooperativa ofrece servicios de recreación, por ejemplo, ir al cine.

138
00:11:34,000 --> 00:11:40,000
Entonces, si tú compras, por ejemplo, vas al cine diez veces al mes, que uno nunca va al cine diez veces al mes,

139
00:11:40,000 --> 00:11:45,000
pero vas al cine diez veces al año, y pagas cinco dólares por boleta cada vez,

140
00:11:45,000 --> 00:11:49,000
pues de nada te sirve tener diez entradas cada una con cinco dólares en una columna.

141
00:11:49,000 --> 00:11:54,000
Te sirve más bien tener en cine cinco dólares y un contador de diez.

142
00:11:54,000 --> 00:11:57,000
Entonces, tú agregas los datos y así como haces agregaciones para ir al cine,

143
00:11:57,000 --> 00:12:00,000
haces agregaciones para el número de veces que sucede otro evento particular,

144
00:12:00,000 --> 00:12:06,000
y así reduces el número de orales para el modelo y lo simplificas un poco.

145
00:12:06,000 --> 00:12:10,000
Y finalmente, tenía que hacer los joins que no se querían hacer después en el momento de hacer la petición,

146
00:12:10,000 --> 00:12:12,000
para que el científico de datos se haga trabajo.

147
00:12:12,000 --> 00:12:18,000
Esto implicaba recorrer cada una de las diferentes tablas y bases de datos que había en el servidor,

148
00:12:18,000 --> 00:12:22,000
extraer información relevante para este afiliado particular,

149
00:12:22,000 --> 00:12:24,000
porque un afiliado puede que vaya al cine, pero otro no.

150
00:12:24,000 --> 00:12:29,000
Entonces, tocaba saber a qué servicios consumía y pues ir extrayendo y poco a poco construyendo

151
00:12:29,000 --> 00:12:32,000
un registro consolidado de todo lo que hacía un afiliado.

152
00:12:32,000 --> 00:12:34,000
Entonces, pues sí puede imaginar qué es lo que hicimos.

153
00:12:34,000 --> 00:12:37,000
En este caso, esta cooperativa entonces ya tuvo 800 mil clientes,

154
00:12:37,000 --> 00:12:41,000
entonces para cada uno de los 800 mil, teníamos que buscar su ID particular,

155
00:12:41,000 --> 00:12:44,000
que pues casi siempre da su documento de identidad nacional,

156
00:12:44,000 --> 00:12:49,000
y empezamos a ir por todas las bases de datos buscando qué filas de las bases de datos correspondían a este cliente.

157
00:12:49,000 --> 00:12:52,000
Y empezamos a construir los registros individuales para cada uno de estos.

158
00:12:52,000 --> 00:12:56,000
La cuestión es que al momento de intentar esto en una máquina de 12 GB

159
00:12:56,000 --> 00:13:00,000
y cargar todas las tablas de memoria, pues no funcionó.

160
00:13:00,000 --> 00:13:04,000
Intentamos esto porque pues, como decimos en Colombia, la peor vuelta es la que no se hace,

161
00:13:04,000 --> 00:13:06,000
vamos a intentarlo, tal vez sirva, ¿no?

162
00:13:06,000 --> 00:13:09,000
Entonces, vamos a ver, no sirvió claramente, muy poquita memoria.

163
00:13:09,000 --> 00:13:11,000
Entonces decimos, vamos a intentar atacarlo, ¿no?

164
00:13:11,000 --> 00:13:13,000
Pues eso fue el error que salió muchas veces.

165
00:13:15,000 --> 00:13:16,000
Y esta ocasión nunca pasa en Python, ¿no?

166
00:13:16,000 --> 00:13:19,000
Pues a menos que hagas como algo muy pesado, eso no sucede mucho.

167
00:13:19,000 --> 00:13:21,000
Entonces, lo primero que todo, en cuanto al cuello botella.

168
00:13:21,000 --> 00:13:25,000
Como se puede imaginar, hacer merge y joins en Python toma tiempo, ¿cierto?

169
00:13:25,000 --> 00:13:27,000
Entonces, ese claramente era un cuello botella.

170
00:13:27,000 --> 00:13:30,000
Sin embargo, cargar la información y dejarla en memoria, mientras que...

171
00:13:32,000 --> 00:13:34,000
Por ejemplo, digamos que había una tabla de salud.

172
00:13:34,000 --> 00:13:37,000
Y yo la cargaba en memoria y, pues en Python no acostumbramos a bajar la memoria.

173
00:13:37,000 --> 00:13:42,000
Nunca bajamos la información y dijamos que el colector de basura, el garbage collector, haga eso.

174
00:13:42,000 --> 00:13:44,000
Dejar la memoria, eso también consume mucha rana.

175
00:13:44,000 --> 00:13:46,000
Entonces, dimos cuenta que el cuello botella estaba por ahí.

176
00:13:46,000 --> 00:13:50,000
Entonces, dijimos, venga, podemos hacer eso, ensuciar el código, desordenarlo,

177
00:13:50,000 --> 00:13:54,000
vamos a ir poco a poquito a poquito a ver si con alguna solución parcial se resuelve.

178
00:13:54,000 --> 00:13:57,000
Lo primero que hicimos fue, uno, remover cualquier approach no vectorizado de las operaciones que estábamos haciendo,

179
00:13:57,000 --> 00:14:01,000
porque uno en Pandas puede hacer todo con Pandas o con Lambdas o con un Forkiter eso de las filas.

180
00:14:01,000 --> 00:14:06,000
Pues, remover todo lo que no fuera poco eficiente y usamos solamente lo bueno.

181
00:14:06,000 --> 00:14:11,000
A su vez, si tocaba hacer algo raro, utilizar Python en Lambdas,

182
00:14:11,000 --> 00:14:14,000
que es la siguiente mejor opción a utilizar Pandas solamente.

183
00:14:14,000 --> 00:14:17,000
Tercero, cargar una tabla a la vez.

184
00:14:17,000 --> 00:14:21,000
Entonces, en vez de cargar todas las tablas de una, ir cargando las tablas una por una en memoria.

185
00:14:21,000 --> 00:14:26,000
Y, pues, te cargo la tabla de salud, la proceso, agrego el proceso y luego la vuelvo a bajar.

186
00:14:26,000 --> 00:14:31,000
Cuarto, eso es una cosa que nos utiliza mucho porque, pues, no se nos pensará que es muy rápido,

187
00:14:31,000 --> 00:14:34,000
pero de hecho cuando le metes índices como si fuera un índice de base de datos a Pandas,

188
00:14:34,000 --> 00:14:38,000
se reduce un montón el tiempo para ciertas tablas, para otras no tanto.

189
00:14:38,000 --> 00:14:42,000
Igual que base de datos depende de la forma y las características del dataset.

190
00:14:42,000 --> 00:14:44,000
Matar cualquier otro proceso.

191
00:14:44,000 --> 00:14:47,000
En este caso, pues, tenemos una máquina virtual, entonces, había un montón de ecuas corriendo,

192
00:14:47,000 --> 00:14:50,000
y, pues, lo que hicimos fue, dijimos, matemos todo lo posible mientras que estamos procesando

193
00:14:50,000 --> 00:14:52,000
y que solamente se esté corriendo Python.

194
00:14:52,000 --> 00:14:55,000
Y, pues, finalmente, algo que yo jamás había hecho es, literalmente,

195
00:14:55,000 --> 00:14:57,000
colectar la memoria con GCCollect, ¿sí?

196
00:14:57,000 --> 00:15:02,000
Algo que Python se pone atrás, pero la ocasión es que a veces Python no quita memoria lo suficientemente rápido.

197
00:15:02,000 --> 00:15:09,000
Entonces, tocaba reducir la memoria, perdón, tocaba aumentar la memoria en algún momento.

198
00:15:09,000 --> 00:15:12,000
Entonces, en ese momento, paramos la ejecución de todo el pipeline,

199
00:15:12,000 --> 00:15:15,000
bajamos todo lo que no fuera necesario a disco y seguíamos, ¿cierto?

200
00:15:15,000 --> 00:15:17,000
Conintiéndonos de todo, sin embargo, nada de esto sirvió.

201
00:15:17,000 --> 00:15:19,000
Y nos preguntamos, ¿pero por qué?

202
00:15:19,000 --> 00:15:22,000
Realmente nos dimos cuenta que la única opción era, en vez de procesar a los 800.000 clientes de totazo,

203
00:15:22,000 --> 00:15:25,000
teníamos que partirlos en un proceso más chiquito, ¿no?

204
00:15:25,000 --> 00:15:29,000
Lo que obviamente complicó el proyecto más, porque la idea es que fue un proyecto corto,

205
00:15:29,000 --> 00:15:32,000
un proyecto no tan pesado, pero ya va a tomar un poco más de complejidad.

206
00:15:32,000 --> 00:15:35,000
Y, a su vez, decimos, no podemos trabajar con 12 GB de RAM, es demasiado poco,

207
00:15:35,000 --> 00:15:39,000
mi computador tiene más RAM que ese servidor que nos diera, es preciso que nos den más memoria.

208
00:15:39,000 --> 00:15:43,000
Entonces, partiendo de la solución entre el OTS y, a su vez, aumentando la RAM, pudimos hacerlo.

209
00:15:43,000 --> 00:15:46,000
Entonces, como se pueden dar cuenta, lo que hicimos fue procesar cada lote de forma separada.

210
00:15:46,000 --> 00:15:51,000
Entonces, empezamos que, digamos que en este caso, el tamaño en cada lote de batch es de tres clientes.

211
00:15:51,000 --> 00:15:56,000
Entonces, empezamos de tres en tres, moviéndonos y procesando cada uno de los batches de forma separada.

212
00:15:56,000 --> 00:15:59,000
Haciendo toda la operación de agregación, meternos en las bases de datos, cargar las de memoria

213
00:15:59,000 --> 00:16:02,000
y sacar el registro consolidado para cada usuario.

214
00:16:02,000 --> 00:16:08,000
Hicimos esto, pues, claramente 800.000 veces dividido entre el N, que es el número del lote,

215
00:16:08,000 --> 00:16:11,000
y creo que eventualmente el número del lote llegó a ser de 25.000 clientes.

216
00:16:11,000 --> 00:16:14,000
Entonces, se puede mostrar en memoria bien con 25.000 clientes.

217
00:16:14,000 --> 00:16:19,000
Entonces, vamos de 25.000 en 25.000, procesando todo el lote.

218
00:16:19,000 --> 00:16:23,000
El resultado, pues, es una gráfica del uso de RAM del servidor.

219
00:16:23,000 --> 00:16:29,000
La parte inicial que pueden ver aquí es como una parte de, como limpiece el procesamiento de todo el asunto.

220
00:16:29,000 --> 00:16:32,000
Y estas puntitas que ven acá son los lotes o chunks siendo procesados.

221
00:16:32,000 --> 00:16:38,000
Claramente, había lotes que son más grandes que otros porque, pues, tienen más, por más de que son el mismo número de clientes,

222
00:16:38,000 --> 00:16:40,000
tienen más o menos interacciones con la cooperativa.

223
00:16:40,000 --> 00:16:47,000
Entonces, pues, ya que este pico alto, puede que este pico de aquí, fuera de clientes que van muchos veces al cine,

224
00:16:47,000 --> 00:16:49,000
entonces, hay muchas más filas que tocaba procesar.

225
00:16:49,000 --> 00:16:51,000
Entonces, hay un poquito más de uso de procesamiento.

226
00:16:51,000 --> 00:16:57,000
Como se dan cuenta, el proceso duraba, pues, no tantas horas, algunas, sí, duraba desde la noche hasta un poco más de las 8 de la mañana,

227
00:16:57,000 --> 00:16:59,000
o sea, un poco más de 10 horas.

228
00:16:59,000 --> 00:17:05,000
Y, pues, eventualmente, que cuando pase el tiempo, el proceso se empezó a devorar más porque se tiene en cuenta más información.

229
00:17:05,000 --> 00:17:08,000
Entonces, dentro de 10 años, pues, ahí más se va a morar, más tiempo.

230
00:17:08,000 --> 00:17:12,000
Entonces, eso es un pecanse, pero, pues, se sale el alcance del proyecto, el presupuesto.

231
00:17:14,000 --> 00:17:19,000
Dado esto, ya les quiero contar un poco más cómo funciona, cómo fué que hicimos internamente ya con código

232
00:17:19,000 --> 00:17:24,000
algunas de las operaciones y cosas lindas que descubrí el lenguaje mientras que estábamos procesando todo con Python.

233
00:17:24,000 --> 00:17:28,000
Lo primero que todo, creo que todos saben que es un diccionario, o al menos lo han escuchado.

234
00:17:28,000 --> 00:17:32,000
Un diccionario, pues, es una estructura de datos de Python muy bonita que no tiene otros lenguajes.

235
00:17:32,000 --> 00:17:34,000
Bueno, sí, algunos sí, pero Python no tiene.

236
00:17:34,000 --> 00:17:39,000
Y es simplemente una asociación ya de valor, donde da una llave particular que puede ser cualquier objeto.

237
00:17:39,000 --> 00:17:43,000
Está asociado a un valor particular. Entonces, es una función uno a uno, no puede haber repetidos.

238
00:17:43,000 --> 00:17:50,000
Y simplemente, por ejemplo, en este caso, si colocamos la llave A, pues nos llega el valor alfa, la llave O omega y la llave G gamma.

239
00:17:50,000 --> 00:17:52,000
Y así sucesivamente, ¿cierto?

240
00:17:52,000 --> 00:17:58,000
Esta búsqueda es súper rápida, súper rápida, funciona muy parecido a una hash table con una complejidad algorítmica de ODE1 amortizado un poquito.

241
00:17:58,000 --> 00:18:03,000
Y, pues, es una estructura agrocenía porque permite guardar información ya de valor súper rápido.

242
00:18:03,000 --> 00:18:07,000
En otros lenguajes hubiera sido necesario crear clases y demás y, pues, esto no es necesario en Python.

243
00:18:07,000 --> 00:18:13,000
Entonces, como se dan cuenta, lo que hicimos fue, os quiero que se imaginaran cómo construir,

244
00:18:13,000 --> 00:18:18,000
o sea, la gran estructura de datos que contenía la información de un solo afiliado era un gran diccionario, ¿sí?

245
00:18:18,000 --> 00:18:21,000
Que tenía un montón de diccionarios internos consoliados.

246
00:18:21,000 --> 00:18:27,000
Entonces, si se ponen a pensar, cuando estamos recorriendo una por una, entre comillas, no porque es con pandas,

247
00:18:27,000 --> 00:18:32,000
las filas de una tabla de las bases de datos, teníamos que, para un cliente particular,

248
00:18:32,000 --> 00:18:35,000
si encontramos una interacción, ir hasta el diccionario, ¿cierto?

249
00:18:35,000 --> 00:18:42,000
Encontrarlo e ir insertando y agregando las cosas necesarias.

250
00:18:44,000 --> 00:18:49,000
Sin embargo, como eran tantos clientes, pues, obtener la información de ese cliente particular pues puede llegar a tomar tiempo.

251
00:18:49,000 --> 00:18:57,000
Entonces, utilizamos claramente esta figura del lenguaje que, pues, permite obtener un cliente particular

252
00:18:57,000 --> 00:18:59,000
de el diccionario nómico de clientes que teníamos.

253
00:18:59,000 --> 00:19:05,000
Si ustedes quieren obtener el número de clientes, la forma normal es utilizando esta figura que ustedes pueden ver acá,

254
00:19:05,000 --> 00:19:10,000
que es D, corchetes cuadrados y, pues, esta figura claramente es la normal, la que todos usamos.

255
00:19:10,000 --> 00:19:14,000
Sin embargo, hay otra figura que a mí me resultó muy útil que es D.getY.

256
00:19:14,000 --> 00:19:17,000
Entonces, ustedes dirán, ¿eso existe y por qué existe y para qué sirve?

257
00:19:17,000 --> 00:19:23,000
Pues, resulta que D.getY les permite a ustedes definir un caso por defecto

258
00:19:23,000 --> 00:19:28,000
para que el diccionario retorne ese caso por defecto en caso tal de que no encuentre ningún valor asociado a esa llave.

259
00:19:28,000 --> 00:19:31,000
Entonces, como no lo encontró, retorna algo particular.

260
00:19:31,000 --> 00:19:38,000
Eso fue particularmente útil en los contadores, por ejemplo, de las veces que uno va al cine o de las veces que se sucede algún evento.

261
00:19:38,000 --> 00:19:46,000
¿Por qué? Porque uno puede quedar en un diccionario, luego asociar la siguiente línea para tener un contador de lo que está sucediendo.

262
00:19:47,000 --> 00:19:54,000
Entonces, por ejemplo, mi diccionario, una llave particular es igual a mi diccionario.get, some key.

263
00:19:54,000 --> 00:20:03,000
Entonces, si some key existe en el diccionario, esta operación de aquí, este bloque de aquí, retorna el valor actual para el diccionario.

264
00:20:03,000 --> 00:20:09,000
Si no existe, retorna 0 y, por ejemplo, si 0 más 1 da 1, entonces agrega inicialmente el contador para que se empiece a contar en el diccionario.

265
00:20:09,000 --> 00:20:13,000
Entonces, esto es algo que se puede hacer sin necesidad de esta línea,

266
00:20:13,000 --> 00:20:16,000
pero me parece un bonito poder hacerlo con Python directamente.

267
00:20:16,000 --> 00:20:27,000
A su vez, una cosa que no es tan trivial, por ejemplo, con Java, que hace unos años era necesario crear un montón de capas de abstracción hasta poder convertir a JSON.

268
00:20:27,000 --> 00:20:34,000
Ahora es super trivial. Uno simplemente importa a JSON, coge el diccionario y utilizando JSON.Adams, le mandas el diccionario como parámetro,

269
00:20:34,000 --> 00:20:42,000
generas el diccionario y lo puedes utilizar como string dentro de una petición HTTP o donde tú quieras.

270
00:20:42,000 --> 00:20:47,000
Es super fácil. Evidentemente, si se puede utilizar como JSON, pues se puede enviar fácilmente.

271
00:20:47,000 --> 00:20:54,000
En este caso utilizamos una versión más avanzada de la librería de request de KennethWrites, que permite hacer el procesamiento en paralelo,

272
00:20:54,000 --> 00:21:03,000
que además GRequest es un poco pequeña, no tiene mucho uso, pero ahí se está usando y la usamos para mandar las peticiones en paralelo.

273
00:21:04,000 --> 00:21:12,000
Finalmente, JSON en la versión por defecto de lenguaje a veces tiene algunos percances que algunos formatos especiales no los reconocen.

274
00:21:12,000 --> 00:21:20,000
Entonces, si no tenemos cómo procesar eso con JSON, pues utilizamos SimpleJSON, que es como un wrapper alrededor de JSON,

275
00:21:20,000 --> 00:21:26,000
que tiene un montón de hacks que resuelven esos problemas comunes de los formatos y sigue siendo muy rápido.

276
00:21:26,000 --> 00:21:29,000
Entonces, en este caso utilizamos lo mismo, solo que con SimpleJSON.

277
00:21:29,000 --> 00:21:36,000
Entonces, en este caso particular sucedía que, por ejemplo, teníamos unos NANDs en Python y JSON no lo puede procesar.

278
00:21:36,000 --> 00:21:43,000
Entonces, utilizamos SimpleJSON para que fuera procesado y eventualmente eso se volviera null en JSON, que tiene significado en JSON.

279
00:21:45,000 --> 00:21:51,000
Y finalmente, se irá a contar cómo medir la duración de la ETL, porque pues normalmente uno cuando ejecuta codas,

280
00:21:51,000 --> 00:21:55,000
pues esas peticiones se demoran milisegundos y uno no piensa mucho en cuánto está durando.

281
00:21:55,000 --> 00:22:00,000
Sin embargo, este proceso, como se vieron cuenta, podía durar 10, 12 y eventualmente durar más tiempo.

282
00:22:00,000 --> 00:22:03,000
Entonces, cómo medir cuándo tiempo la ETL va a estar corriendo.

283
00:22:03,000 --> 00:22:05,000
Hay varias formas de tomar el tiempo en Python.

284
00:22:05,000 --> 00:22:08,000
La más básica y que no menciono aquí es utilizar los dates.

285
00:22:08,000 --> 00:22:14,000
Entonces, uno captura la fecha en algún momento particular que se va a estar al sistema operativo, obtiene la fecha web de operativo,

286
00:22:14,000 --> 00:22:21,000
que si es un servidor Linux sería la Unix date de la consola, se extrae y se puede calcular la diferencia.

287
00:22:21,000 --> 00:22:27,000
Sin embargo, yo quería mirarse a una manera de medir el tiempo utilizando Python para no tener que depender del sistema operativo,

288
00:22:27,000 --> 00:22:30,000
sino que fuera agnóstico de dónde estuviera corriendo.

289
00:22:30,000 --> 00:22:37,000
En este caso particular utilizamos para la primera prueba timeit que se utiliza de esa forma.

290
00:22:37,000 --> 00:22:44,000
Entonces simplemente es importarlo en la primera línea y colocas los timers entre las funciones que van a tomar un tiempo en correr,

291
00:22:44,000 --> 00:22:48,000
calculas el delta y aquí te da la respuesta en milisegundos de cuánto va a correr.

292
00:22:48,000 --> 00:22:50,000
Es realmente sencillo de usar, ¿cierto?

293
00:22:50,000 --> 00:22:55,000
Pero hay otra forma. Otra forma es time.clock, que funciona exactamente igual.

294
00:22:55,000 --> 00:23:00,000
Obtienes time.clock en un t1, t2 en time.clock y luego acuálice el delta.

295
00:23:00,000 --> 00:23:05,000
Normalmente los resultados en time.clock y de timers son diferentes.

296
00:23:05,000 --> 00:23:07,000
Entonces ustedes dirán por qué.

297
00:23:07,000 --> 00:23:13,000
Resulta que timeit se ha usado para medir chunks de código y no bloques grandes de ejecución.

298
00:23:13,000 --> 00:23:18,000
Esto es porque, uno, repite las tareas de ejecutar para eliminar la influencia de otras tareas en tu máquina.

299
00:23:18,000 --> 00:23:21,000
Entonces uno quiere correr, por ejemplo, un for loop, alguna cosa pequeñita,

300
00:23:21,000 --> 00:23:25,000
vale las veces saber si ese bloque de código es suficiente o no, se utiliza timeit.

301
00:23:25,000 --> 00:23:30,000
Segundo, él inhabilita el garbage collector para prevenir que, justamente cuando uno esté mediendo el tiempo,

302
00:23:30,000 --> 00:23:35,000
se ejecute el garbage collector y obviamente dañe el resultado de la duración del tiempo.

303
00:23:35,000 --> 00:23:39,000
Y finalmente utiliza el timer más suboperativo para el sistema operativo.

304
00:23:39,000 --> 00:23:45,000
En el caso particular, time.clock en Python 2 y time.perfcounter en Python 3.

305
00:23:45,000 --> 00:23:51,000
Entonces, dependiendo de la instalación de Python, él agarra el timer más apropiado y te dice cuánto dura.

306
00:23:51,000 --> 00:23:56,000
Sin embargo, como recuerdan, yo sí he estado utilizando garbage collector en la ecuación de ltl.

307
00:23:56,000 --> 00:24:01,000
Entonces yo no pude darme el lujo de medir algo que me bloqueaba el garbage collector porque iba a ser una misión errónea.

308
00:24:01,000 --> 00:24:08,000
Entonces, claramente para mi caso particular, yo decidí utilizar time porque no me bloqueaba el garbage collector.

309
00:24:08,000 --> 00:24:13,000
Y eso es porque lo medí utilizando tanto Python date como un medidor literalmente físico para medir con tiempo durado.

310
00:24:13,000 --> 00:24:22,000
Y efectivamente, time fue mucho mejor para medir la duración del ltl dado que era un proceso muy largo.

311
00:24:22,000 --> 00:24:28,000
Sin embargo, para medir bloques pequeñitos de código, el timer fue súper bueno y sugiero que lo utilicen.

312
00:24:28,000 --> 00:24:33,000
Por ejemplo, para claves de algoritmica se puede utilizar un montón dado que mide bloques de código pequeñitos.

313
00:24:34,000 --> 00:24:39,000
Mi consejo es conocer el programa con el cual ustedes estén lidiando y probar ambas alternativas.

314
00:24:39,000 --> 00:24:45,000
Porque si ustedes puedan ambas y se dan cuenta que una es mejor que otra, deben ya escoger esa.

315
00:24:45,000 --> 00:24:47,000
Y también intenten medirlo con otra cosa particular.

316
00:24:47,000 --> 00:24:55,000
Puede ser un programa que esté corriendo aparte en el computador o incluso físicamente con el celular si el proceso va a durar muy poquito y pueden estar pendientes.

317
00:24:55,000 --> 00:24:59,000
Si el código solamente se demora mil segundos, por favor utiliza time.

318
00:24:59,000 --> 00:25:03,000
Entonces damos la leja de todo esto. Es uno, que aprendimos de todo el proceso.

319
00:25:03,000 --> 00:25:07,000
Uno, que hay que saber exactamente las mismas preguntas.

320
00:25:07,000 --> 00:25:13,000
El proyecto no tenía mucho presupuesto y tampoco sabían bien qué preguntas querían contestar.

321
00:25:13,000 --> 00:25:18,000
No sabían exactamente qué razones había para que el afiliado no se desafiliara de la cooperativa.

322
00:25:18,000 --> 00:25:23,000
Entonces fue preciso hacerse las preguntas adecuadas para poder saber qué estábamos buscando y qué teníamos que progresar.

323
00:25:23,000 --> 00:25:30,000
En vez de intentar hacer un ETL de todas las tablas de la empresa, perdiendo el tiempo y perdiendo plata y dándonos dolores de cabeza.

324
00:25:30,000 --> 00:25:36,000
Segundo, a veces la mejor solución simple y costo efectivo es dividir el problema.

325
00:25:36,000 --> 00:25:42,000
No intentar optimizar el código un montón porque tal vez eso no va a ser suficiente para resolver el problema.

326
00:25:42,000 --> 00:25:45,000
Entonces dividan los datos en pequeñas tandas.

327
00:25:45,000 --> 00:25:51,000
Sin embargo no lo hagan primero. Intenten, como yo dije, el approach que no es muy inteligente para ver si es suficiente.

328
00:25:51,000 --> 00:25:55,000
Y si es suficiente, pues mejoren un poco, pero no se pongan a complicarse la vida.

329
00:25:55,000 --> 00:25:59,000
Como dice Donald Knott, la optimización prematura es la raíz de todo mal.

330
00:25:59,000 --> 00:26:06,000
Tercer, las audiciones de Python son una cosa genial. Me parece maravillosa y permiten el procesamiento de datos de una forma increíble.

331
00:26:06,000 --> 00:26:12,000
No es necesaria tener clases para representar objetos del mundo real como un cliente o una interacción con banco o con recreación.

332
00:26:12,000 --> 00:26:15,000
Y a su vez, permiten manejar muy poco verget.

333
00:26:15,000 --> 00:26:23,000
En este caso la tele era muy funcional y podíamos pasar de una base de datos a procesamiento a volver a mandar a una API o a otra base de datos súper fácil.

334
00:26:23,000 --> 00:26:27,000
Y a su vez, convertir a JSON no puede ser más sencillo.

335
00:26:27,000 --> 00:26:31,000
Y media del tiempo es una cosa curiosa en todo el lenguaje de programación y claramente en Python.

336
00:26:31,000 --> 00:26:34,000
Y hay que tener en cuenta si tu proceso requiere garbage collection o no.

337
00:26:34,000 --> 00:26:38,000
Porque los timers tienen eso en cuenta y no queremos tener una medición errónea.

338
00:26:38,000 --> 00:26:44,000
Porque si uno va a presentar un reino de negocio un tiempo que no es real, pues eso puede traer consecuencias tanto legales como malas para la empresa.

339
00:26:44,000 --> 00:26:48,000
Mi consejo es que hagan pruebas con su propio código y deciden.

340
00:26:48,000 --> 00:26:50,000
Muchas gracias.

