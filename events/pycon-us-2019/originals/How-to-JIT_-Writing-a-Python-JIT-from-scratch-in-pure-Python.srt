1
00:00:00,000 --> 00:00:08,840
All right. This talk is how to JIT, building a simple JIT compiler for Python from principles

2
00:00:08,840 --> 00:00:16,800
by Matt Page. Now, Matt has requested that you hold off all your questions until the

3
00:00:16,800 --> 00:00:24,120
end of the talk, and Matt will be outside and be more than happy to answer any questions

4
00:00:24,120 --> 00:00:31,120
that you may have. So, with that, here's Matt.

5
00:00:31,120 --> 00:00:42,080
Hey, everybody. I'm Matt Page. So, we're going to talk today about building a simple JIT

6
00:00:42,080 --> 00:00:47,320
compiler for Python from first principles, but we're going to do it in Python.

7
00:00:48,200 --> 00:00:54,200
Okay. So, a quick outline of how the talk is going to progress. First, I'll give, I'll

8
00:00:54,200 --> 00:00:57,600
introduce myself and then go over some of the motivation for why we want to do something

9
00:00:57,600 --> 00:01:03,720
so crazy. Then we'll go through a high-level overview of what we're going to build. Then

10
00:01:03,720 --> 00:01:09,160
we'll transition into talking about some supporting runtime infrastructure we need, and finally,

11
00:01:09,160 --> 00:01:12,120
we'll close out with building a bare bones compiler.

12
00:01:13,000 --> 00:01:19,560
Okay. So, who am I? Well, I'm a husband and a dad of two amazing daughters, one and four.

13
00:01:19,560 --> 00:01:26,080
They keep me super busy. In my free time, I'm Moonlight as a software engineer at Instagram,

14
00:01:26,080 --> 00:01:33,880
and I've been there since 2017. And I've worked mostly on Python-related projects. So, initially,

15
00:01:33,880 --> 00:01:39,240
I worked on bootstrapping types into our code base. We have about two million lines of Python

16
00:01:39,240 --> 00:01:44,120
code that were initially untyped. So, we built this open source project called monkey type

17
00:01:44,120 --> 00:01:49,520
that collects types at runtime, and you can use those to generate static type annotations.

18
00:01:49,520 --> 00:01:55,040
And then for about the past year, I've been focused on Python performance work. So, specifically,

19
00:01:55,040 --> 00:02:03,560
how do we make Python go faster for Instagram? All right. So, why would we want to build

20
00:02:03,560 --> 00:02:07,800
a JIT? Why would we want to do something crazy like this? So, first off, it's a learning

21
00:02:07,800 --> 00:02:13,280
opportunity. For me, the best way to really understand something and to really understand

22
00:02:13,280 --> 00:02:18,200
how it works is to build it. I'm not a compiler expert, so I'll probably say some wrong things

23
00:02:18,200 --> 00:02:23,960
here. But I've always been curious about how JITs work, and even more so with the recent focus on

24
00:02:23,960 --> 00:02:30,080
performance work. And so, in learning about that stuff, trying to dive into production quality JIT

25
00:02:30,080 --> 00:02:36,960
compiler is pretty hard. They're large, complicated pieces of software, and that's not intended as a

26
00:02:36,960 --> 00:02:41,280
criticism. They're kind of that way out of necessity. So, it can be easier to start from

27
00:02:41,280 --> 00:02:47,320
something small. I also think it's really fun. I like understanding how things work, and I also

28
00:02:47,320 --> 00:02:51,480
like being close to the metal, and you can't really get much closer to the metal than this.

29
00:02:51,480 --> 00:02:59,480
And finally, you actually start to see opportunities for special purpose JITs once you kind of

30
00:02:59,480 --> 00:03:04,360
understand at a baseline level how they work. Depending on your workload, you can actually

31
00:03:04,520 --> 00:03:11,040
see large constant factor speed ups with pretty low effort. So, a couple examples of this are

32
00:03:11,040 --> 00:03:17,080
like the PCRE regex library actually has a JIT compiler, so it can compile your regexes into

33
00:03:17,080 --> 00:03:22,280
machine code. Graphic shaders are another great example of this. So, you kind of start seeing

34
00:03:22,280 --> 00:03:27,600
places where if you have access to data at runtime, you might be able to generate more

35
00:03:27,600 --> 00:03:34,560
efficient code. All right. So, first off, kind of let's go over at a high level what we're going

36
00:03:34,560 --> 00:03:42,640
to build. You know, like, what even is a JIT compiler, right? Let's break down what it's doing.

37
00:03:42,640 --> 00:03:50,880
So, for the purposes of this talk, it's something that translates Python code into machine code at

38
00:03:50,880 --> 00:03:56,880
runtime. So, it's compiling while your program is running. This is where the JIT comes from. That

39
00:03:56,880 --> 00:04:01,920
stands for just in time. So, you take a Python function, you stuff it into this magical black

40
00:04:01,920 --> 00:04:07,600
box and out pop some native code. There are lots of different kinds of JIT compilers, but we're

41
00:04:07,600 --> 00:04:12,960
going to focus on a simple method at a time template JIT. We'll see what that means as the

42
00:04:12,960 --> 00:04:16,720
talk progresses. Hopefully, you'll leave this and have a good understanding of what that means.

43
00:04:18,000 --> 00:04:26,000
So, breaking this down into subcomponents. So, we have this JIT. First, we said that it needs

44
00:04:26,000 --> 00:04:32,960
translate Python code into native machine code. So, we're going to focus on x8664, on UNIX-like

45
00:04:32,960 --> 00:04:37,520
systems, so, like, Mac OS and Linux. So, we're going to need a compiler. This is the thing that's

46
00:04:37,520 --> 00:04:45,440
going to take Python and spit out machine code. So, we have some machine code. We're going to need

47
00:04:45,440 --> 00:04:48,880
a place to put it and it's going to need to be executable. So, we need a memory allocator.

48
00:04:50,480 --> 00:04:55,040
And, finally, we need to be able to integrate it with CPython so that we can call it. So,

49
00:04:55,040 --> 00:05:00,480
we're going to need a foreign function interface or an FFI that will let us call native code from

50
00:05:00,480 --> 00:05:07,520
Python. And we're going to say that this sort of foundational layer, that's our runtime.

51
00:05:07,520 --> 00:05:12,640
That doesn't deal directly with translating Python code into native code, but it's required to sort

52
00:05:12,640 --> 00:05:19,200
of wire everything together and make it all work. Okay. So, let's look into how we might implement

53
00:05:19,280 --> 00:05:24,640
a runtime. So, we're going to do that. I like to learn by doing. So, we're going to do that

54
00:05:24,640 --> 00:05:30,240
by implementing the world's simplest JIT compiler. This function, we're going to JIT a function that

55
00:05:30,240 --> 00:05:37,120
computes the meaning of life. So, it's just going to return 42. Okay. So, diving in, first off,

56
00:05:37,120 --> 00:05:41,840
we import the C types module. And that's part of the standard library and that gives us a way to

57
00:05:41,840 --> 00:05:49,280
call native code from Python. So, that's our FFI piece. Next, we're going to import the M map module

58
00:05:49,280 --> 00:05:54,480
and that gives us access to the M map system call. And we can use M map to allocate executable memory

59
00:05:54,480 --> 00:05:59,360
from the operating system. So, this is our memory allocator. If we were going to do this for realsies,

60
00:06:00,160 --> 00:06:05,760
we would need to worry about limiting the amount of memory we've allocated and freeing it. But for

61
00:06:05,760 --> 00:06:11,760
now, since this is slideware, we'll just free it when the process executes. Or, sorry, exits.

62
00:06:13,200 --> 00:06:18,880
Okay. So, then we call M map and that gives us 4K of readable, writeable, executable memory.

63
00:06:19,680 --> 00:06:25,680
And the map private just says, hey, operating system, if a child process tries to write to

64
00:06:25,680 --> 00:06:30,800
these bits, create its own mapping. And the map anonymous just says, hey, operating system,

65
00:06:30,800 --> 00:06:37,040
we don't have a file backing this mapping. So, then we write out some code into the memory we've

66
00:06:37,040 --> 00:06:44,560
just allocated. We expose it to Python. So, here we're saying, well, using C types, here we're

67
00:06:44,560 --> 00:06:50,160
saying, hey, Python, treat this address as a function that returns an integer. And finally,

68
00:06:50,160 --> 00:06:57,200
we call it. And it prints out 42. Awesome. But taking a step back, we didn't talk too much

69
00:06:57,200 --> 00:07:03,840
about this magical binary string. So, this is the X8664 encoding of the assembly shown here.

70
00:07:04,800 --> 00:07:12,080
Move 42 into the register racks and then return. So, racks is special. That's where return values

71
00:07:12,080 --> 00:07:18,640
go. And then we had to translate that into some machine code for the CPU. So, this means we're

72
00:07:18,640 --> 00:07:22,320
actually going to have to do instruction encoding. And this is something we'd rather not have to

73
00:07:22,320 --> 00:07:29,280
focus on. We'd rather focus on the high level task of translating Python into some higher level

74
00:07:29,280 --> 00:07:37,200
representation. Say assembly code, for instance. Okay. So, we don't want to deal with manual

75
00:07:37,200 --> 00:07:41,200
instruction encoding. And to avoid that, we're going to use a library called PeachPy.

76
00:07:43,040 --> 00:07:48,320
So, PeachPy is an awesome open source library that was originally designed for writing

77
00:07:48,320 --> 00:07:54,240
hand-tuned highly optimized computational kernels for numerical computing applications. So, think of

78
00:07:54,240 --> 00:08:00,320
things like math applications where you as a human might actually have a higher chance of writing

79
00:08:00,320 --> 00:08:05,040
something more efficient than a compiler would produce. It handles all of what we just went over.

80
00:08:05,840 --> 00:08:11,760
So, instruction encoding, allocating executable memory, exposing it to Python and more. And it

81
00:08:11,760 --> 00:08:17,600
exposes a high level DSL that lets us write that assembly what we wanted to in Python.

82
00:08:18,560 --> 00:08:25,760
So, this is great. So, let's revisit the world's simplest JIT using PeachPy. So, this looks already

83
00:08:25,760 --> 00:08:31,440
kind of a lot shorter and hopefully a lot more clear. So, walking through what this is doing,

84
00:08:31,440 --> 00:08:36,240
first we're wild card importing everything we need from PeachPy. Normally, we'd probably want to

85
00:08:36,240 --> 00:08:41,040
avoid doing this. But this is what lets us use it like a DSL. We'll see that in a second.

86
00:08:42,720 --> 00:08:47,840
Then we declare our function. So, we say, hey, we have this function. It takes no arguments. And

87
00:08:47,840 --> 00:08:54,320
it returns a 64-bit integer. And then we get to use the nice PeachPy DSL. So, we're actually

88
00:08:54,320 --> 00:08:59,120
writing assembly code, but we're doing it in Python. So, we're moving 42 into the register

89
00:08:59,120 --> 00:09:05,920
axe. And then we're returning. And this is where PeachPy I think really shines. So, we can actually

90
00:09:05,920 --> 00:09:14,080
just dump all of that C types instruction encoding work off into PeachPy. So, there's a lot going on

91
00:09:14,960 --> 00:09:21,360
in this line. But PeachPy is taking that assembly code we wrote, lowering it down into machine code,

92
00:09:21,360 --> 00:09:26,480
allocating executable memory, and then exposing it into Python for us. Under the hood, it's doing

93
00:09:26,480 --> 00:09:31,440
this using C types and M map. So, we at least kind of know how that works. But we don't have to think

94
00:09:31,440 --> 00:09:39,680
about it anymore, which is great. Then we call the function. And boom, we get 42. Awesome. So,

95
00:09:39,680 --> 00:09:49,440
we've now figured out the runtime bits. Let's dive into the compiler. But before we get too deep,

96
00:09:49,440 --> 00:09:55,600
we're going to go over a bit about how the CPython VM executes Python code that we write. So,

97
00:09:55,600 --> 00:10:00,640
here we have a simple function called add. That just adds its arguments. And on the right,

98
00:10:00,640 --> 00:10:06,720
so that's what we write. That actually gets compiled by CPython into byte code that you see

99
00:10:06,720 --> 00:10:14,080
on the right here. So, this is byte code for a stack-based virtual machine. If that doesn't make

100
00:10:14,080 --> 00:10:18,160
any sense to you, don't worry. We're going to walk through this byte code on the next slide.

101
00:10:20,160 --> 00:10:25,760
So, here we have the byte code we just saw. And the blue arrow is going to represent the instruction

102
00:10:25,760 --> 00:10:30,800
pointer for the virtual machine. And the empty space on the right is going to house the contents

103
00:10:31,440 --> 00:10:38,080
of the stack. So, initially, it starts out empty. After we execute the first instruction,

104
00:10:38,080 --> 00:10:44,560
this first load fast instruction pushes the value of A onto the stack. Then we execute the second

105
00:10:44,560 --> 00:10:51,360
load fast instruction. And that pushes the value of B onto the stack. And then we execute binary add.

106
00:10:51,360 --> 00:10:56,640
And this pops two values off of the stack, adds them. And so, for whatever that means,

107
00:10:56,640 --> 00:11:02,480
because this is Python, if it's two integers, it could be doing integer addition. If it was

108
00:11:02,480 --> 00:11:06,560
two strings, it could be concatenating them. And then it pushes it back onto the stack.

109
00:11:07,680 --> 00:11:12,960
And then finally, return value pops the stack and returns the result.

110
00:11:14,640 --> 00:11:21,360
Okay. So, under the hood, Python implements this pretty much as you would expect it to.

111
00:11:21,920 --> 00:11:29,200
The interpreter loop is basically just a giant switch statement. So, for every opcode, there's

112
00:11:29,200 --> 00:11:33,120
a case statement. And then it's just constantly going over the byte code doing this fetch,

113
00:11:33,120 --> 00:11:38,560
decode, execute cycle. So, grab an instruction, decode it, figure out what to do with it,

114
00:11:38,560 --> 00:11:44,400
then execute the corresponding case statement. Okay. So, this kind of begs the question,

115
00:11:45,360 --> 00:11:51,760
if CPython already has C code for every opcode, can we compile a function by looping over the

116
00:11:51,760 --> 00:11:56,880
byte code and just emitting the equivalent native code? So, we basically would just take

117
00:11:57,440 --> 00:12:02,880
the assembly code for the case statement and we spit it into an executable piece of memory.

118
00:12:03,920 --> 00:12:09,760
So, as you probably anticipated, the answer to that question is yes. And this is the essence

119
00:12:09,840 --> 00:12:16,960
of a template JIT compiler. So, instead of a fetch, decode, execute cycle, we have a fetch,

120
00:12:16,960 --> 00:12:21,840
decode, emit cycle. We just walk through the byte code and we go boom, boom, boom, boom, boom,

121
00:12:21,840 --> 00:12:29,040
and we emit some assembly code. Okay. So, what does that look like in code? This is the basic

122
00:12:29,040 --> 00:12:34,480
skeleton of the compiler. It's pretty straightforward. It's not really a lot more code than we saw

123
00:12:34,560 --> 00:12:42,320
in our world's simplest JIT example. I've omitted imports for brevity. So, we have a function that

124
00:12:42,320 --> 00:12:48,880
takes as input now a Python function and it produces as output a JIT compiled version.

125
00:12:50,160 --> 00:12:57,280
So, we have to emit a prolog. We're executing code on the... We're executing native code now,

126
00:12:57,280 --> 00:13:00,800
which means we're going to be running on the machine stack. So, our prolog is going to need

127
00:13:01,520 --> 00:13:06,080
to handle things like saving the base pointer, copying arguments to the stack,

128
00:13:06,080 --> 00:13:13,040
and then some CPython-specific stuff like grabbing the gil. After we've omitted our prolog,

129
00:13:13,040 --> 00:13:18,400
we can just iterate over all of the instructions like we said before. We can emit some assembly

130
00:13:18,400 --> 00:13:24,320
code for them. And then finally, we get into our epilogue. And so, here we see another feature of

131
00:13:24,320 --> 00:13:30,880
peach pie, which are symbolic labels. So, we emit a label for the epilogue and that lets us

132
00:13:31,760 --> 00:13:36,160
reference it from jump instructions throughout the code. And then we emit our epilogue,

133
00:13:36,160 --> 00:13:41,520
which is just emitting some assembly code to pop the stack and clean it up and then return.

134
00:13:45,360 --> 00:13:48,400
Okay. So, now that we have the skeleton in place, we're going to

135
00:13:48,400 --> 00:13:54,880
fetch out some basic primitives. Once these are done, we're going to have a solid foundation

136
00:13:54,880 --> 00:14:00,400
in place that we can take and we can extend as desired. So, to start off with, we're going to

137
00:14:00,400 --> 00:14:04,640
walk through how we'll emit some assembly code for a couple of opcodes. This will just kind of

138
00:14:04,640 --> 00:14:10,400
help us get some intuition and see how simple things are. So, first off, let's take that meaning

139
00:14:10,400 --> 00:14:16,160
of life function again. That compiles down into two opcodes, load constant return value. So,

140
00:14:16,160 --> 00:14:19,360
we're going to implement the emitters for this. Once we're done with this, we'll actually be able

141
00:14:19,360 --> 00:14:26,240
to compile our first Python function. Not super useful, but a cool milestone. Okay. So, to develop

142
00:14:26,240 --> 00:14:30,560
some intuition here, we're just going to look at the case statement from the interpreter loop.

143
00:14:30,560 --> 00:14:35,280
And this has been simplified for brevity, but I think this captures the essence of what's going on.

144
00:14:35,920 --> 00:14:39,920
Load const just loads a constant value out of a code object for a function

145
00:14:40,800 --> 00:14:47,600
corresponding to the argument for the opcode. Then it increments its reference count and it pushes

146
00:14:47,600 --> 00:14:53,440
it onto the stack. So, what does our generated or what does our emitter look like for this?

147
00:14:53,440 --> 00:14:58,880
This is pretty straightforward and is not a ton of code. So, this is the first one we're seeing. So,

148
00:14:58,880 --> 00:15:03,520
we'll just go over the function signature. It takes as input the function being compiled

149
00:15:03,520 --> 00:15:10,560
and the instruction. We grab the constants for the function. Then we move the constant

150
00:15:10,560 --> 00:15:15,680
into a register. So, here you'll notice we're calling ID. This is just a C Python implementation

151
00:15:15,680 --> 00:15:21,440
trick. ID returns the address of its argument. We're putting it into a register. Register doesn't

152
00:15:21,440 --> 00:15:26,640
matter. The choice of register doesn't matter here. We increment its reference count and then

153
00:15:26,640 --> 00:15:31,600
we push it onto the stack. So, our assembly generation code looks pretty much like what we

154
00:15:31,600 --> 00:15:37,600
saw in the interpreter loop. Now let's look at return value. Return value again, super simple.

155
00:15:38,640 --> 00:15:45,120
We just pop the stack and then we exit the interpreter loop. The emitter code looks

156
00:15:45,120 --> 00:15:49,600
correspondingly simple. So, we're popping into a register. Here the register is actually important.

157
00:15:50,480 --> 00:15:55,040
So, this RACS register is where return values go. Then we jump into our epilogue,

158
00:15:55,040 --> 00:16:00,720
which again cleans up the stack and returns. Okay. Awesome. Moving right along. We can look at

159
00:16:00,720 --> 00:16:07,600
local variables. So, keeping in this theme of simple pieces of code, we have an identity function

160
00:16:07,600 --> 00:16:12,640
that returns its argument and we have its corresponding byte code. We only have one new

161
00:16:12,640 --> 00:16:20,560
opcode here, which is load fast. So, if we look at the interpreter code or the case statement

162
00:16:20,560 --> 00:16:26,320
inside of the interpreter loop here, we see that we're fetching an argument from the locals array

163
00:16:26,320 --> 00:16:31,840
in the frame and that's specified by the argument to the opcode. We increment its reference count

164
00:16:32,640 --> 00:16:37,120
and then we push it onto the stack. Awesome. So, it's pretty straightforward. Before we can dive

165
00:16:37,120 --> 00:16:42,480
into the emitter, though, we're going to have to talk a little bit about stack layout. So, we said

166
00:16:42,480 --> 00:16:47,520
that we're going to execute on the C stack now and we're going to use that for local variable storage.

167
00:16:48,080 --> 00:16:52,640
So, when we enter into a function, our prolog is actually reserving space on the stack for

168
00:16:52,640 --> 00:16:59,360
local variables and then it updates RBP, which is the base pointer, to point at the first local.

169
00:17:00,640 --> 00:17:10,000
So, to load a local variable I, we compute its offset relative to RBP and on x86-64, the stack

170
00:17:10,000 --> 00:17:16,640
grows from higher memory addresses to lower memory addresses. So, we need to subtract from the base

171
00:17:16,640 --> 00:17:20,880
pointer. And then finally, we're going to have to scale it by the size of the things that are on the

172
00:17:20,880 --> 00:17:27,200
stack. So, we're just pushing pointers here, which are eight bytes on x86-64. So, we just multiply

173
00:17:27,200 --> 00:17:34,400
that index by eight. So, now that we have that, the emitter code is really straightforward.

174
00:17:34,400 --> 00:17:39,840
So, we just load the argument into a register. We increment its reference count. And you'll note

175
00:17:39,840 --> 00:17:45,200
here that we're no longer using the peach pie DSL. We've abstracted that away into a helper function.

176
00:17:45,200 --> 00:17:49,600
This is a great feature of being able to write assembly code in a high-level language like Python.

177
00:17:49,600 --> 00:17:55,120
You can do refactoring and it actually makes sense. And then you can just, then we just push

178
00:17:55,120 --> 00:17:59,120
it onto the stack. So, again, this is all pretty straightforward, which is really, really cool.

179
00:18:01,280 --> 00:18:07,920
Okay. So, we're almost done. Now we need to talk about calling into the C API. So, we want to be

180
00:18:07,920 --> 00:18:13,360
able to call back into CPython from generated code. It turns out that a lot of core CPython

181
00:18:13,360 --> 00:18:18,960
functionality is actually written in C and it's exposed through the C API. The interpreter loop

182
00:18:18,960 --> 00:18:23,600
often delegates work to these functions and we'd like to be able to do the same thing from our

183
00:18:23,600 --> 00:18:31,120
generated code. So, here we're going to revisit our simple addition example. And we have the

184
00:18:31,120 --> 00:18:37,680
disassembly on the right. We've covered everything so far except for binary add. So, let's dive into

185
00:18:37,680 --> 00:18:43,280
that. So, this looks pretty familiar. We're popping our arguments from the stack. But then

186
00:18:43,280 --> 00:18:49,200
we have to call this helper function pi number add using the arguments we popped. And so,

187
00:18:50,640 --> 00:18:54,320
this is where the interpreter loop is actually calling back into CPython.

188
00:18:56,240 --> 00:19:00,000
So, we'd like to be able to do the same thing. So, we need to figure out how to do this. And

189
00:19:00,000 --> 00:19:04,160
this basically amounts to just being able to resolve the address of a function at runtime.

190
00:19:05,200 --> 00:19:11,280
So, to do that, we lean on C types again. Here we import it. Then we import Python API. And Python

191
00:19:11,280 --> 00:19:18,240
API just exposes the C API as a shared library. Then we load a helper library for resolving

192
00:19:18,240 --> 00:19:25,200
symbols. Then we expose DLSim, which is just the function we need to resolve a symbol in a shared

193
00:19:25,200 --> 00:19:33,600
library. And now we can finally look up a symbol inside of CPython. So, now we have something

194
00:19:34,400 --> 00:19:41,120
that we can use to build on top of. Once this is in place, the emitter looks pretty straightforward

195
00:19:41,120 --> 00:19:46,000
again. So, now we're popping our arguments. And the registers here now are important because these

196
00:19:46,000 --> 00:19:51,520
are the registers that arguments or the first two arguments, to be precise, are expected to be in.

197
00:19:51,520 --> 00:19:57,360
So, we pop those. And then we call our helper function to resolve the address of pi number add.

198
00:19:58,400 --> 00:20:06,000
And we move it into a register. And then we call it. Okay. Great. We're almost ready to wrap up.

199
00:20:06,720 --> 00:20:10,560
The final piece I want to go over is how we represent control flow. So, up to now,

200
00:20:10,560 --> 00:20:14,400
we've only dealt with straight line code. How would we want to handle something a little bit

201
00:20:14,400 --> 00:20:20,480
more complicated? So, we're going to look at another helper function again. In this case,

202
00:20:20,480 --> 00:20:25,440
we have this max function that just computes the maximum of its arguments. And the disassembly on

203
00:20:25,440 --> 00:20:31,680
the right is a little longer. But we've implemented everything except for these two op codes, compare op

204
00:20:31,680 --> 00:20:37,280
and pop jump if false. So, we'll walk through how those work. So, compare op just pops two values off

205
00:20:37,280 --> 00:20:42,400
of the stack, compares them using a predicate, and in this case, it's greater than. And then it

206
00:20:42,400 --> 00:20:50,240
pushes the result back onto the stack. Okay. So, pop jump if false is probably pretty straightforward

207
00:20:50,240 --> 00:20:58,320
if you can read. So, we pop a result off of the stack. And then we jump depending on that result.

208
00:20:59,520 --> 00:21:04,960
So, in this case, if it was false, we update the instruction pointer to point to 12.

209
00:21:05,280 --> 00:21:13,040
And if it was true, we fall through to the next statement. So, what happens here if we wanted to

210
00:21:13,040 --> 00:21:17,840
modify the bytecode of a function? Say we wanted to optimize it. We could end up invalidating the

211
00:21:17,840 --> 00:21:22,880
offset that's used in the jump. How would we represent this more abstractly and in a way that

212
00:21:22,880 --> 00:21:29,040
lets us not have to worry about that? To do that, we're going to reach for control flow graphs. So,

213
00:21:30,000 --> 00:21:35,680
a control flow graph is a directed graph where nodes are basic blocks which are just sequences

214
00:21:35,680 --> 00:21:40,560
of straight line code with no branching in the middle of a block. You can only branch at the end.

215
00:21:40,560 --> 00:21:45,360
And then edges represent control flow. And this allows us to edit the graph without worrying about

216
00:21:45,360 --> 00:21:50,560
invalidating jump targets. Once we have a graph, we can transform it back into linear code just

217
00:21:50,560 --> 00:21:55,040
by performing a graph traversal. So, how would we want to build one of these things from some

218
00:21:55,840 --> 00:22:02,000
bytecode? The first step is to identify basic block boundaries. So, an instruction starts a

219
00:22:02,000 --> 00:22:06,640
new basic block. If it's the first instruction, if it's the target of a branch, or it follows a

220
00:22:06,640 --> 00:22:12,400
branch. So, in our example, zero starts a basic block because it's the first instruction. Eight

221
00:22:12,400 --> 00:22:18,240
starts a basic block because that follows the branch instruction. 12 starts a basic block

222
00:22:18,240 --> 00:22:22,480
because it's the target of a branch. And then we're done. Okay. So, now that we've got basic

223
00:22:22,480 --> 00:22:27,920
blocks, we can build a mapping from offset to basic block. And then finally, we can add edges

224
00:22:28,720 --> 00:22:34,000
by going through and mapping offsets to the corresponding basic block. So, integrating this

225
00:22:34,000 --> 00:22:40,000
into the compiler is pretty straightforward. This is the updated compiler. Again, not super different.

226
00:22:42,160 --> 00:22:47,760
Only now we're going to iterate over basic blocks instead of just iterating over instructions.

227
00:22:47,760 --> 00:22:54,320
For each basic block, we emit a label. And we saw this earlier. And this, again, lets us emit jump

228
00:22:54,320 --> 00:23:00,080
instructions using symbolic labels rather than offsets. And then we can delegate fixing up those

229
00:23:00,080 --> 00:23:07,440
symbolic jumps into absolute jumps to peach pie. So, then for each basic block, we go through and

230
00:23:07,440 --> 00:23:13,360
we iterate through the instructions in it. And we emit the assembly code for it. So, that's pretty

231
00:23:13,360 --> 00:23:20,480
straightforward. And then that's it. We're done. Now we've pulled an entire control flow graph

232
00:23:21,760 --> 00:23:29,280
into our compiler. So, takeaways. First off, Python is awesome for exploration and prototyping.

233
00:23:30,240 --> 00:23:34,560
At this point, we have the basic structure of the compiler in place. And we can compile real

234
00:23:34,560 --> 00:23:39,040
functions. So, we can't compile a whole lot yet. But we actually have a fully fledged working

235
00:23:39,040 --> 00:23:45,200
compiler here. And we have the structure that we can build on to make this into something more real.

236
00:23:45,200 --> 00:23:50,240
And that's really awesome. We did that in like 25 minutes. It's an incredibly small amount of code.

237
00:23:50,240 --> 00:23:55,600
It's like 200 or 300 lines of code. And it's just, yeah, I think it's pretty amazing.

238
00:23:56,880 --> 00:24:02,000
Low level work doesn't have to be scary. We did all of this from Python. And we generated

239
00:24:02,000 --> 00:24:07,440
native assembly code or native machine code, sorry. And we executed it. And now we have a JIT that

240
00:24:07,440 --> 00:24:14,640
has a REPL. Which is, I think it's pretty cool. Okay. So, future work. What is there to do?

241
00:24:15,680 --> 00:24:21,360
The answer is there is a lot to do. You can add emitters for more op codes.

242
00:24:21,360 --> 00:24:24,960
CPython has about 120 op codes. And we really only covered three.

243
00:24:27,360 --> 00:24:31,440
Exceptions. So, we didn't talk at all about how we deal with exceptions or how we represent them in

244
00:24:31,440 --> 00:24:38,880
the control flow graph. This is tricky, but it's doable. We haven't talked about generators or

245
00:24:38,880 --> 00:24:45,280
async functions. Those need to be able to suspend and resume at arbitrary points of execution.

246
00:24:46,160 --> 00:24:50,880
Which means we need to figure out how to save and restore the stack and register state.

247
00:24:51,520 --> 00:24:56,160
And then finally, the big one, which is optimization passes. So, the generated code,

248
00:24:56,720 --> 00:25:02,080
or the code that we've generated now is going to be pretty slow. It's going to be, at most,

249
00:25:02,080 --> 00:25:07,200
a little bit faster than CPython and maybe even a little bit slower. We've really only compiled away

250
00:25:07,200 --> 00:25:12,480
the interpreter loop. But with a little bit of work, we can actually start producing something

251
00:25:12,480 --> 00:25:17,440
that is much, much faster than what CPython can do. And the name of the game here is type

252
00:25:17,440 --> 00:25:23,840
specialization. So, if we go back to the example of addition, if we can generate specialized code

253
00:25:23,840 --> 00:25:29,360
and we know that the types of the arguments are integers, we can instead just emit machine code to

254
00:25:29,360 --> 00:25:37,600
add integers as opposed to calling out into the CPython API. All right. Thank you. I hope you guys

255
00:25:37,600 --> 00:25:43,280
thought that this was interesting. If you want to go play around, the source code is up and available

256
00:25:43,280 --> 00:25:57,760
on GitHub. I'm here all week now. Thank you for your time, guys.

