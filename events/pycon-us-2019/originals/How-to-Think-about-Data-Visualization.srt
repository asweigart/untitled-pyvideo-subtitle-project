1
00:00:00,000 --> 00:00:06,000
Good afternoon, everybody, and welcome to the final presentation in this room for PyCon 2019.

2
00:00:07,000 --> 00:00:11,000
On the subject of data visualization, please welcome Jake Vander Plaas.

3
00:00:17,000 --> 00:00:20,000
Well, thanks very much. Thanks for sticking around to the very end.

4
00:00:21,000 --> 00:00:26,000
Today I want to talk about thinking about data visualization.

5
00:00:26,000 --> 00:00:34,000
And this is sort of in opposition to the question where people usually start, like, what visualization tool should I use?

6
00:00:35,000 --> 00:00:38,000
And I don't know if any of you were here in 2017 or in Portland in 2017.

7
00:00:39,000 --> 00:00:43,000
I made the mistake of deciding to do a talk about this, what visualization tool should I choose?

8
00:00:44,000 --> 00:00:49,000
And my plan was to compare and contrast four or five good visualization tools in Python.

9
00:00:50,000 --> 00:00:54,000
But as I started digging, it turns out there's a lot of ways to visualize data in Python.

10
00:00:54,000 --> 00:00:59,000
And, yeah, so I don't think this is a good place to start right now because there...

11
00:01:00,000 --> 00:01:03,000
I saw someone putting their camera up. I'll let you take another picture of this.

12
00:01:04,000 --> 00:01:09,000
Yeah, so I don't think this is a good place to start because there's too much confusion and there's too many different things.

13
00:01:10,000 --> 00:01:17,000
And really what it boils down to is the Python community has such a wide diversity of use cases and applications

14
00:01:18,000 --> 00:01:23,000
that you need a lot of different tools to accomplish the different types of visualization that people are doing.

15
00:01:24,000 --> 00:01:27,000
So I don't want to start with that question, what visualization tool I should use.

16
00:01:28,000 --> 00:01:31,000
I want to start with this, how should I think about data visualization?

17
00:01:32,000 --> 00:01:35,000
Like, what is data visualization? And then we can build up from there, build up the concepts,

18
00:01:36,000 --> 00:01:45,000
and start thinking about how we can make effective visualizations no matter which of those million plotting packages you decide to use.

19
00:01:46,000 --> 00:01:50,000
So what is visualization? You know, if you take a data set like this,

20
00:01:50,000 --> 00:01:56,000
what visualization does is it tries to take this representation of the data set, kind of a tabular form,

21
00:01:57,000 --> 00:02:00,000
and put it in a form that makes it more intuitive.

22
00:02:01,000 --> 00:02:05,000
So at first glance you can find the relationships between the data.

23
00:02:06,000 --> 00:02:10,000
Does anyone recognize this data set? There's a few hands.

24
00:02:10,000 --> 00:02:17,000
How about if, so this is a data set where there are, if you take common statistical summary statistics,

25
00:02:18,000 --> 00:02:23,000
like the mean, standard deviation, correlation, things like that, all these four data sets are the same.

26
00:02:24,000 --> 00:02:29,000
But if you visualize them, you start to see that there's some very different properties in the data set.

27
00:02:30,000 --> 00:02:34,000
So visualization is important because it helps us see what's in our data.

28
00:02:34,000 --> 00:02:38,000
It helps us get an intuitive feel for what's in these tables of numbers.

29
00:02:39,000 --> 00:02:49,000
And what's going on here when we visualize data is we're essentially encoding the values in the data set into certain visual representations.

30
00:02:50,000 --> 00:02:57,000
So what are these here? In this case, the way we've encoded this data set is we've put the X value in the X position on each plot,

31
00:02:58,000 --> 00:03:01,000
the Y value in the Y position, we've drawn the Y value in the Y position,

32
00:03:01,000 --> 00:03:10,000
we've drawn a little circle, and then we've split up the data by, we've faceted the data into four different facets or four different panels

33
00:03:11,000 --> 00:03:14,000
to get a feel for what each individual data set is.

34
00:03:15,000 --> 00:03:21,000
So this is an encoding. This is something, a way we've transformed the data from numbers into individual properties.

35
00:03:22,000 --> 00:03:26,000
So we could think, like, what other encodings might we use?

36
00:03:26,000 --> 00:03:30,000
We use the X value, Y value, and the facet here to look at this.

37
00:03:31,000 --> 00:03:36,000
We could do something different, right? We could use, instead of the facet, we could encode the different data sets by color.

38
00:03:37,000 --> 00:03:46,000
And then we get the advantage of having everything on the same panel where it's easier to maybe compare, but this is sort of a muddled plot, right?

39
00:03:47,000 --> 00:03:51,000
So maybe color is not the best encoding for the data set. What if we do shape?

40
00:03:51,000 --> 00:03:57,000
You see a lot of these kinds of encodings where people draw different classes of data with different shapes.

41
00:03:58,000 --> 00:04:03,000
It can be effective and sometimes not effective. I would argue in this case it's not very effective.

42
00:04:04,000 --> 00:04:09,000
It's hard to pull out what's going on there. You know, you could do something like size. That might help here.

43
00:04:10,000 --> 00:04:14,000
These are all different visual features you can use to encode the same information.

44
00:04:15,000 --> 00:04:19,000
And maybe we can do the shape, size, and color altogether.

45
00:04:19,000 --> 00:04:27,000
This is starting to get a little bit crazy, but it does help you maybe distinguish what's going on in these data sets.

46
00:04:28,000 --> 00:04:35,000
But for argument's sake, I would say that this is not a very effective visualization, right?

47
00:04:36,000 --> 00:04:43,000
There's something about this visualization that's just not quite as appealing and not quite as intuitive compared to the four panel plot.

48
00:04:44,000 --> 00:04:50,000
So we'll get into that later. We can start thinking about what drives that intuition in visualizations.

49
00:04:51,000 --> 00:04:58,000
So here's one that's, you know, we just throw everything at it. We encode the data set by facet, by shape, by size, by color.

50
00:04:59,000 --> 00:05:02,000
It's a little bit crazy, but it helps us see what's going on.

51
00:05:03,000 --> 00:05:08,000
So this is good to think about. We can encode all these different properties in different ways.

52
00:05:08,000 --> 00:05:16,000
But there's one property in this data set that we're missing so far. Does anyone see what that might be?

53
00:05:17,000 --> 00:05:22,000
What does this visualization of the data set completely ignore?

54
00:05:27,000 --> 00:05:29,000
I hear some mumblings.

55
00:05:29,000 --> 00:05:37,000
Here's the row number. There's no information here that tells you the order of the points as they appear in the table, right?

56
00:05:38,000 --> 00:05:46,000
So we could add some more information to that. We could say maybe the index or the row number of the data is encoded in the color in this four panel plot.

57
00:05:47,000 --> 00:05:57,000
That's sort of hard to see. Maybe if you looked with a magnifying glass and kind of categorized things, you could maybe see which point is which.

58
00:05:57,000 --> 00:06:00,000
So maybe for the order we can do something like adding a line.

59
00:06:01,000 --> 00:06:07,000
And we see that here in this data set the order doesn't really mean anything, right?

60
00:06:08,000 --> 00:06:13,000
But that information was there, and that information was completely lost in the previous panels that we did.

61
00:06:14,000 --> 00:06:19,000
So we could start thinking about better ways to represent the order in the data.

62
00:06:20,000 --> 00:06:25,000
Like here's all these four values in the data set encoded in a completely different way.

63
00:06:26,000 --> 00:06:32,000
We have index on the y-axis, so the order of the points. We have the data set on the x-axis.

64
00:06:33,000 --> 00:06:37,000
And then we use color and size to show the relationships between the values.

65
00:06:38,000 --> 00:06:46,000
Now, this doesn't really work. You can't really, you can't see, color and size doesn't give you that intuitive view of the relationship between the data points, right?

66
00:06:46,000 --> 00:06:53,000
But we can play this game, we can do all sorts of things. We can split the data up into the four data sets and use the size of the points.

67
00:06:54,000 --> 00:07:04,000
Or we could use the color of the points. We could decide that instead of using a circle mark, we might use a rectangle patch and create kind of a heat map of the data.

68
00:07:05,000 --> 00:07:13,000
We could use a bar chart in each of those panels that shows the scale of the data split up by these various factors.

69
00:07:13,000 --> 00:07:18,000
So this is essentially, this is a data set with four different dimensions of information.

70
00:07:19,000 --> 00:07:24,000
And we can choose all these different ways to encode the information in that data.

71
00:07:25,000 --> 00:07:30,000
Some of them are more intuitive than others. Some of them let us see the relationships better than others.

72
00:07:31,000 --> 00:07:34,000
And some of them are just really, really bad.

73
00:07:35,000 --> 00:07:38,000
This is one that I really like. I played around for a while.

74
00:07:38,000 --> 00:07:47,000
And this one, the row of the data, the order isn't encoded, but kind of the grouping of the index is encoded.

75
00:07:48,000 --> 00:07:54,000
And we use a slope between two different points to show the relationships between the points in each table.

76
00:07:55,000 --> 00:08:07,000
And this actually gives you, it's not kind of, maybe not as intuitive as a normal familiar scatter plot, but this gives you a good idea of what some of the weird points in the data set are.

77
00:08:08,000 --> 00:08:15,000
So anyway, the point is all these different representations that I've run through are encodings of the exact same data.

78
00:08:16,000 --> 00:08:18,000
They contain all the exact same information.

79
00:08:19,000 --> 00:08:28,000
But there's something in here that we're starting to get an intuition for that makes some of these effective and some of these ineffective.

80
00:08:29,000 --> 00:08:36,000
So as we're thinking about how to visualize data, it would be nice to kind of put our finger on what makes a visualization effective.

81
00:08:36,000 --> 00:08:43,000
You know, what is it about these faceted bar charts that end up not being very useful?

82
00:08:44,000 --> 00:08:59,000
So just as a quick summary of what I've been talking about over the course of looking at all these visualizations, we've set up something where data properties are encoded in some sort of visual representation.

83
00:08:59,000 --> 00:09:12,000
We have the data represented by some mark that might be a line, a point, a bar, a patch on the heat map, and we have scales that map these encodings onto the values that go underneath.

84
00:09:13,000 --> 00:09:19,000
The scales might be the numbers on the x-axis or the labels in the legend or the color bar.

85
00:09:19,000 --> 00:09:25,000
And so what this suggests is that we can start talking about visualization in terms of grammar.

86
00:09:26,000 --> 00:09:31,000
And this goes back, a lot of people have been thinking about this for decades and decades.

87
00:09:32,000 --> 00:09:40,000
And one of the more famous books that talks about visualization as a grammar is this book by Wilkinson, The Grammar of Graphics.

88
00:09:40,000 --> 00:09:49,000
And basically lays out that when you want to create a chart, the grammar you have is you start with the data, you have some sort of transformation of the data.

89
00:09:50,000 --> 00:09:57,000
You have these marks that we talked about, whether it's a point or a line or a bar, or it might be multiple points of different shapes.

90
00:09:58,000 --> 00:10:04,000
We have the encoding, which could be x-position, y-position, color, shape, size.

91
00:10:04,000 --> 00:10:10,000
And then we have the scale, which are the ways that we indicate to the reader what the encoding means.

92
00:10:11,000 --> 00:10:21,000
So we have here, for example, we have labels on the x-axis and y-axis, and we have a legend that maps the shape and the color to values of interest.

93
00:10:22,000 --> 00:10:31,000
So the question is, when we're looking at two different representations of the same thing, what are the different ways that we can use this to create a chart?

94
00:10:31,000 --> 00:10:38,000
And when we're looking at two different representations of a data set like this, what visual encoding is going to be most effective?

95
00:10:39,000 --> 00:10:43,000
And what mark is going to be most effective? And what scale is going to be most effective for my data?

96
00:10:44,000 --> 00:10:48,000
And these are the kind of things that people in the visualization community have been researching for years.

97
00:10:48,000 --> 00:11:04,000
And an example is this Jacques Bertin put out this semiology of graphics, which basically tried to lay out the theory of which encodings, which marks are going to be most effective for a given data set.

98
00:11:05,000 --> 00:11:09,000
And it's in French, so we'll shift to French now for the rest of the talk.

99
00:11:10,000 --> 00:11:14,000
I have these in the translations here.

100
00:11:15,000 --> 00:11:23,000
So he basically laid out what, from the top to the bottom, what the most effective encodings are in terms of human perception.

101
00:11:24,000 --> 00:11:27,000
And at the top we have things like 2D position.

102
00:11:28,000 --> 00:11:36,000
And we can see that 2D position is useful for knowing the order of data and also the quantity of the data.

103
00:11:36,000 --> 00:11:43,000
We can eyeball that and figure out what the values are roughly as well as the order of the values.

104
00:11:44,000 --> 00:11:50,000
Size is similar. Size gives us a sense of order and a sense of quantity.

105
00:11:51,000 --> 00:11:57,000
Color value can also give us a sense of order and a sense of quantity. We go from lightest to darkest.

106
00:11:58,000 --> 00:12:08,000
As we start going down the scale, though, we get to encodings that are less useful for quantity or for order, but can be useful for categories.

107
00:12:09,000 --> 00:12:14,000
So for example, the color hue, the red, blue, green, yellow, this doesn't give you any sense of order.

108
00:12:15,000 --> 00:12:20,000
So you wouldn't want to apply this to a quantitative axis.

109
00:12:21,000 --> 00:12:24,000
You couldn't look and say red is less than green and green is greater than blue.

110
00:12:24,000 --> 00:12:28,000
But it does give you a good sense of category and similar for shape.

111
00:12:29,000 --> 00:12:31,000
It gives you a sense of category but not order.

112
00:12:32,000 --> 00:12:36,000
So as you start thinking about this, you can lay out this chart.

113
00:12:37,000 --> 00:12:46,000
And essentially here, if you think of three types of data, nominal data, which is categorical stuff that's not ordered at all,

114
00:12:47,000 --> 00:12:53,000
ordinal data, which is discrete ordered categories, and then quantitative data, which are continuous quantities.

115
00:12:54,000 --> 00:12:59,000
They're encoded better or worse by each of these possible encodings.

116
00:13:00,000 --> 00:13:04,000
So position, for example, is very good for nominal, ordinal, and quantitative data.

117
00:13:05,000 --> 00:13:10,000
It gives us a sense of data identity. It gives us a sense of scale and a sense of order.

118
00:13:11,000 --> 00:13:18,000
Whereas if you go down to things like color value, you can get nominal and ordinal quantities, the order of the data,

119
00:13:18,000 --> 00:13:23,000
but not necessarily as much the actual quantity that the data is representing.

120
00:13:24,000 --> 00:13:30,000
So as you start to think about this, there's a few practical takeaways when you're developing a visualization.

121
00:13:31,000 --> 00:13:34,000
One is not all encodings are created equally.

122
00:13:35,000 --> 00:13:40,000
So if we look at this, we sort of identified that this was a bad visualization earlier.

123
00:13:40,000 --> 00:13:52,000
I think the reason that this is a bad visualization is because it's encoding things that it's not using the most optimal encoding for a property in the data that's very important.

124
00:13:53,000 --> 00:13:56,000
Like here, we're distinguishing between four different data sets.

125
00:13:57,000 --> 00:14:03,000
So we probably want to use the most intuitive and most easily perceptible encoding to do that.

126
00:14:03,000 --> 00:14:10,000
And color and shape and size are not the most easily immediately perceptible encodings for people looking at a chart.

127
00:14:11,000 --> 00:14:21,000
Whereas position is. If we split these out positionally, we can immediately look at it and see that these are four different categories without having to think deeply about the meanings of the symbols.

128
00:14:22,000 --> 00:14:27,000
So one takeaway is we should prefer position encodings whenever possible.

129
00:14:27,000 --> 00:14:30,000
Because they there's something that we can just at a glance understand.

130
00:14:31,000 --> 00:14:43,000
So an example of this in the real world, here's some data from the census where we have the distribution of age versus population over the course of every decade going back to 1850.

131
00:14:44,000 --> 00:14:46,000
And this is not a very intuitive chart.

132
00:14:47,000 --> 00:14:50,000
All the information is there, but it's it's really difficult to understand.

133
00:14:50,000 --> 00:14:55,000
It's not easy to see the difference between the year 2000 and the year 1990 in this plot.

134
00:14:56,000 --> 00:15:03,000
So what can we do? We can instead of encoding this important information in color, we can encode this important information in position.

135
00:15:04,000 --> 00:15:10,000
We facet the data. And this is something that's known in the visualization community as something that's been around for a long time.

136
00:15:10,000 --> 00:15:20,000
And making use of these kinds of small multiples can be a way to really quickly create effective visualizations because you're encoding these important properties in in the data.

137
00:15:21,000 --> 00:15:22,000
And so we can do that.

138
00:15:23,000 --> 00:15:24,000
So we can do that.

139
00:15:25,000 --> 00:15:26,000
So we can do that.

140
00:15:27,000 --> 00:15:28,000
So we can do that.

141
00:15:29,000 --> 00:15:30,000
So we can do that.

142
00:15:30,000 --> 00:15:43,000
Making use of these kinds of small multiples can be a way to really quickly create effective visualizations because you're encoding these important properties in in an encoding that's easy to perceive the position.

143
00:15:45,000 --> 00:15:52,000
And I love this this visualization, by the way, because it you can sort of see where are we 1950?

144
00:15:52,000 --> 00:15:56,000
Here's the baby boomer bump and you can kind of follow them as they get older.

145
00:15:57,000 --> 00:16:02,000
And there's the bump. And then here's the baby boomers kids in 2000 that are starting to starting to populate.

146
00:16:03,000 --> 00:16:08,000
And it's sort of like marches along to the right and then tapers off at later years.

147
00:16:09,000 --> 00:16:17,000
So the the second takeaway that I want to show tell you is that the the best color scale is going to depend on data type.

148
00:16:17,000 --> 00:16:28,000
So we thought about we thought about using colors as encodings and and it really the effectiveness of a color encoding is going to change depending on what you're looking at.

149
00:16:29,000 --> 00:16:34,000
So this is an example of a not very good encoding.

150
00:16:35,000 --> 00:16:41,000
This is using the color hue, something that's very well suited to nominal data to categorical data.

151
00:16:42,000 --> 00:16:47,000
But this is using a color hue to try to encode quantitative information.

152
00:16:48,000 --> 00:17:01,000
And it the reason it's not very good is because it it can often call your attention to the wrong aspects of the data set like this in this rainbow color map.

153
00:17:02,000 --> 00:17:05,000
The yellow really stands out to you just because of the way that your eyes work.

154
00:17:05,000 --> 00:17:11,000
And yellow is not it's not at the extremes. It's not high unemployment rate or low unemployment rate.

155
00:17:12,000 --> 00:17:20,000
It's somewhere in the middle. And so so by kind of emphasizing the yellow on first glance, we're not really conveying the right information in this plot.

156
00:17:21,000 --> 00:17:28,000
So a better thing color scale to use for this sort of quantitative data is a perceptually uniform one like color value.

157
00:17:28,000 --> 00:17:38,000
So are so effectively we're looking at the the transparency of the color as well as a as a uniform change from yellow to blue.

158
00:17:39,000 --> 00:17:44,000
And this shows you a little bit more what's going on with this this unemployment in each county.

159
00:17:45,000 --> 00:17:50,000
We see that there's there are patches of high unemployment and across the Midwest.

160
00:17:51,000 --> 00:17:56,000
It's relatively low unemployment and it gives you a better intuitive sense of what's happening in the data.

161
00:17:56,000 --> 00:18:06,000
Another type of color map that you should keep in mind is if you're if you're doing something that has that has a symmetric distribution around a midpoint.

162
00:18:07,000 --> 00:18:20,000
Like for example here this is the unemployment with the average subtracted where average unemployment across the US is in white and higher unemployment is in blue and lower unemployment is in red.

163
00:18:21,000 --> 00:18:30,000
This kind of diverging color map lets you let's you see to to extreme quantities at once in a very intuitive way.

164
00:18:31,000 --> 00:18:39,000
So using these kinds of using your using color as effectively as possible can can help you create really nice visualizations.

165
00:18:40,000 --> 00:18:51,000
And the last thing I want to say is as a general principle is it can be it can be really useful to use a visualization API that has these kinds of grammatical approaches built in.

166
00:18:52,000 --> 00:19:04,000
Because then you need to spend less time thinking about it and making sure you're making good choices and instead you can you spend a lot of time with your with your package that you're using making the right choices for you.

167
00:19:05,000 --> 00:19:18,000
So the way I like to think about it is we want to have a visualization tool where the way we think about visualization in terms of these encodings and grammar is mapped onto the way we code in visualization and that that's mapped onto the way the visualizations are presented on the screen.

168
00:19:18,000 --> 00:19:28,000
And one of the things I've found over the years is that when you have a good set of APIs that maps on to how you should think about things it helps you think about things better.

169
00:19:29,000 --> 00:19:38,000
It's sort of this this positive feedback loop of making you more more aware of exactly how you should be approaching your problems.

170
00:19:38,000 --> 00:19:47,000
So there are a number of interesting grammar based plotting packages that are out there probably the best known as ggplot2 in the R world and that's that's an API that's built directly on this grammar of graphics book that I mentioned earlier.

171
00:19:48,000 --> 00:19:57,000
So in the R world you'll find that people people love this and you know swear by it and would never use Python because ggplot2 is not available.

172
00:19:57,000 --> 00:20:04,000
It's it's an incredibly powerful tool in the Python world there's an interesting package called plot9 which essentially its goal is to take the ggplot2 API and bring it to python and it uses matplotlib as a backend.

173
00:20:06,000 --> 00:20:14,000
Matplotlib is this this visualization tool that's been used in Python and it's a really good tool to help you get the best out of it and it's really good for you to use.

174
00:20:15,000 --> 00:20:17,000
So that's the end of this talk.

175
00:20:17,000 --> 00:20:24,000
So there are other approaches Vega light is a grammar that's implemented in JavaScript and and JSON that lets you specify visualizations in a grammar based approach and Altair is a package that I've been working on in the Python world.

176
00:20:24,000 --> 00:20:32,000
So there are other approaches Vega light is a grammar that's implemented in JavaScript and and JSON that lets you specify visualizations in a grammar based approach and Altair is a package that I've been working on in the Python world.

177
00:20:32,000 --> 00:20:44,000
So there are other approaches Vega light is a grammar that's implemented in JavaScript and and JSON that lets you specify visualizations in a grammar based approach and Altair is a package that I've been working on in the Python world.

178
00:20:44,000 --> 00:20:52,000
That that gives you a Python API for this Vega light grammar and because I'm the one speaking I'm going to focus on Altair.

179
00:20:52,000 --> 00:20:58,000
So a real quick look at Altair I know I told you I was not going to recommend a tool but you should really use Altair.

180
00:20:58,000 --> 00:21:04,000
So the idea with Altair is that if you if you take data that's in a tidy format.

181
00:21:04,000 --> 00:21:10,000
So the idea with Altair is that if you if you take data that's in a tidy format.

182
00:21:10,000 --> 00:21:29,000
It's kind of the with rows being being observations and columns being categories of those observations then you can you can create an Altair plot by kind of directly specifying this sort of grammar that we were talking about before and.

183
00:21:29,000 --> 00:21:44,000
So just real quick here we're specifying that we want the mark to be a point we want the the X encoding to go to the pedal length the Y to the simple width and the color to the species and it pops out right there you the the grammar maps directly onto the plot.

184
00:21:44,000 --> 00:21:56,000
So the another nice thing about Altair is because the visualization is implemented in JavaScript it's you can get interactive visualization very trivially by by adding an interactive tag and you know it's a very simple thing.

185
00:21:56,000 --> 00:22:08,000
And yeah just to emphasize this this whole grammar of visualization is built right into the API we specify the data in the chart we specify the marks we specify what encodings we want there to be.

186
00:22:08,000 --> 00:22:19,000
And then we we end up with a with a very flexible way to create charts so if we want to add a column for example we just say that we want the column to be species and it passes it.

187
00:22:19,000 --> 00:22:27,000
If we want a tooltip encoding where you hover over the point and it tells you the values in that point you can specify that exactly.

188
00:22:27,000 --> 00:22:39,000
And the power of this sort of grammatical approach means that you're not trying to remember whether you're going to make a tick plot or a bar plot the choice of the mark and the choice of the encoding specify exactly what it is you're not creating catapulting.

189
00:22:39,000 --> 00:22:49,000
You're creating a grammatical specification of what you want to be on the screen so here's a tick plot where we show all the pedal widths for each species.

190
00:22:49,000 --> 00:22:59,000
If we want to change this to a bar plot showing the mean all we have to do is change the mark there from tick to bar and then we're going to have to change the bar to the mean.

191
00:22:59,000 --> 00:23:09,000
So it's it's not about remembering what the API for a bar plot is versus the API for a tick plot it's about using the same unified grammar to specify those things.

192
00:23:09,000 --> 00:23:17,000
The other thing that's built into Altair which is really nice is that it's a very simple way to create charts.

193
00:23:17,000 --> 00:23:27,000
So if you specify a quantitative value for your color it'll choose a quantitative color map and we can we can if we want to be explicit about what sort of categories of variables you're going to be using.

194
00:23:27,000 --> 00:23:33,000
So we're going to be using the same grammar for each of those things.

195
00:23:33,000 --> 00:23:37,000
So we're going to be using the same grammar for each of those things.

196
00:23:37,000 --> 00:23:47,000
So if we change for example to an ordered value we get the same color map but we get automatically get a legend that tells you what the categories are rather than a continuous error bar with a continuous color bar with the categories.

197
00:23:47,000 --> 00:23:57,000
So if we change for example to an ordered value we get the same color map but we get automatically get a legend that tells you what the categories are rather than a continuous error bar with a continuous color bar with the categories.

198
00:23:57,000 --> 00:24:07,000
And if we change to a categorical type with no order like the origin it automatically shifts the color map to something that's appropriate for categorical data.

199
00:24:07,000 --> 00:24:17,000
So it's nice because you don't have to think about color maps anymore just as long as your data type is specified correctly.

200
00:24:17,000 --> 00:24:27,000
And so the other cool thing about Altair is on top of this grammar of visualization there's this there's a grammar of interaction that was added about a year ago before last year's PyCon.

201
00:24:27,000 --> 00:24:37,000
And what the grammar of interaction does is it allows you to specify types of variables that you want to use.

202
00:24:37,000 --> 00:24:43,000
So here I've added an interval selection to the plot.

203
00:24:43,000 --> 00:24:47,000
It doesn't do anything yet. You can click and drag it around.

204
00:24:47,000 --> 00:24:53,000
But what we need to do is attach this interval selection to some of the encodings.

205
00:24:53,000 --> 00:24:59,000
So instead of saying the color is the origin we can say the color is conditional.

206
00:24:59,000 --> 00:25:05,000
And if it's inside it'll be the origin if it's outside it'll be gray.

207
00:25:05,000 --> 00:25:19,000
So all of a sudden with a few lines of code we can we can specify in a grammatical grammatical grammar of visualization sense a really kind of complex interaction.

208
00:25:19,000 --> 00:25:23,000
Has anyone ever tried to make something like this in D3?

209
00:25:23,000 --> 00:25:27,000
It's a lot more than 12 lines of code.

210
00:25:27,000 --> 00:25:29,000
So and we can start tying things together.

211
00:25:29,000 --> 00:25:38,000
So if I take this chart and I store it in a variable called chart and then I tie it together with another chart with this or bar that puts charts next to each other then we have a cross linked

212
00:25:38,000 --> 00:25:48,000
dynamic brush that highlights points in each panel and it knows how to propagate those selections from panel to panel.

213
00:25:48,000 --> 00:25:58,000
And we can even do things like create a histogram where the X value is the count the Y value is the number of points.

214
00:25:58,000 --> 00:26:08,000
So we get the number of cars for each country and then attach that to the data set add this transform it by filtering by the selection and then we get a dynamic histogram of what's inside our selection.

215
00:26:08,000 --> 00:26:14,000
So this kind of grammatical approach to specify the selection is really important.

216
00:26:14,000 --> 00:26:24,000
So we can do things like create a histogram where the X value is the count the Y value is the origin.

217
00:26:24,000 --> 00:26:34,000
So this kind of grammatical approach to specifying visualizations can lead to a hugely powerful way of building up intuitive visualizations of data.

218
00:26:34,000 --> 00:26:38,000
So I'd encourage you to check out Altair.

219
00:26:38,000 --> 00:26:42,000
The website is there and there are a bunch of examples on there.

220
00:26:42,000 --> 00:26:50,000
We just did a big release of the package about a week ago of the 3.0 version that adds a number of new features.

221
00:26:50,000 --> 00:26:58,000
So anyway, sorry for telling you about a package even though I promised you I wouldn't but it's the thing that I've been working on a lot lately and it's been a lot of fun.

222
00:26:58,000 --> 00:27:13,000
So the summary of the takeaway is if you want to if you're doing visualization what you're trying to do is encode your data into visual properties and not all of those properties are equal.

223
00:27:14,000 --> 00:27:21,000
So for example, the position is probably the best and most intuitive way to encode data.

224
00:27:21,000 --> 00:27:27,000
So use position as much as possible before you start going into shape and color hue and things like that.

225
00:27:27,000 --> 00:27:33,000
Think about the colors, the marks and the encodings and the scales you use.

226
00:27:33,000 --> 00:27:42,000
One nice thing about this grammatical approach is that it categorizes things for you so it's easy to think about the possibilities that are out there.

227
00:27:42,000 --> 00:27:50,000
Rather than being tied to the API of some imperative visualization library that keeps you from exploring.

228
00:27:50,000 --> 00:27:52,000
The other thing is explore small multiples.

229
00:27:52,000 --> 00:27:59,000
Do lots of little visualizations of your data set because it can give intuition into what's going on.

230
00:27:59,000 --> 00:28:04,000
And I'd encourage you to choose a grammar based approach to data exploration.

231
00:28:04,000 --> 00:28:06,000
If you're into R try gdplot2.

232
00:28:06,000 --> 00:28:08,000
It's an incredible tool.

233
00:28:08,000 --> 00:28:11,000
If you're in Python try plot9 or try Altair.

234
00:28:11,000 --> 00:28:17,000
If you like JavaScript, using Vega and Vega Lite directly is a good option.

235
00:28:17,000 --> 00:28:23,000
Particularly in these observable notebooks in the JavaScript world that allow you to do that very quickly.

236
00:28:23,000 --> 00:28:30,000
So explore those tools and see how effective you can be at the visualizations you're doing.

237
00:28:30,000 --> 00:28:32,000
So thanks very much.

238
00:28:37,000 --> 00:28:39,000
Thank you again, Jake.

239
00:28:39,000 --> 00:28:41,000
I think we may have time for one question.

240
00:28:41,000 --> 00:28:43,000
If anyone has a question.

241
00:28:43,000 --> 00:28:45,000
There are microphones in the aisle.

242
00:28:50,000 --> 00:28:51,000
One question over here.

243
00:28:51,000 --> 00:28:53,000
Hey, great presentation.

244
00:28:53,000 --> 00:29:00,000
I guess if there was one feature you really wish you had and that isn't in existence in those packages, what would it be?

245
00:29:00,000 --> 00:29:02,000
What's on your wish list?

246
00:29:02,000 --> 00:29:13,000
Yeah, so one feature that I really wish was there is at this point there's kind of in this whole world there's a trade off between interaction and size of data that can be handled.

247
00:29:13,000 --> 00:29:18,000
So the tools that create static plots are good at handling large data sets.

248
00:29:18,000 --> 00:29:23,000
The tools that create interactive plots are not good at handling large data sets.

249
00:29:23,000 --> 00:29:28,000
Particularly in this sort of declarative grammar based world.

250
00:29:28,000 --> 00:29:36,000
I want a tool that has both of those together that can do millions of points with a grammar based specification in an interactive manner.

251
00:29:36,000 --> 00:29:44,000
And if you look at my talk from two years ago, I talk about some tools in the Python world that are out there that tick some of those boxes.

252
00:29:44,000 --> 00:29:48,000
But I don't think the killer app is out there just yet.

253
00:29:51,000 --> 00:29:53,000
Thank you again, Jake.

