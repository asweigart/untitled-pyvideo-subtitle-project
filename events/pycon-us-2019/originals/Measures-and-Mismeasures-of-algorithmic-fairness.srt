1
00:00:00,000 --> 00:00:07,600
Hi everyone, welcome. Our next speaker will be Manojit Nandi talking to us

2
00:00:07,600 --> 00:00:14,080
about measures and mismeasures of algorithmic fairness.

3
00:00:14,080 --> 00:00:34,640
How do I do things? There we go. That's how things happen. Okay. Sorry for the bit delay.

4
00:00:34,640 --> 00:00:39,040
Hello everyone. Thank you all for coming to my talk this afternoon on measures and mismeasures

5
00:00:39,040 --> 00:00:45,840
of algorithmic fairness. I'm Manojit Nandi and I'm a data scientist. So, quick about me.

6
00:00:46,560 --> 00:00:51,600
According to the Google Cloud Computer Vision API, so I ran a photo of myself through the

7
00:00:51,600 --> 00:00:57,760
Google Vision API and it produced some labels for me. So let's go through some of them and see how

8
00:00:57,760 --> 00:01:04,640
right they are. Am I a dancer? Yes, I'm an aerial dancer and circus acrobat. I just work as a data

9
00:01:04,640 --> 00:01:10,640
scientist for 40 hours a week because I got to pay rent somehow. Am I entertaining? I hope so.

10
00:01:10,640 --> 00:01:17,360
You're stuck with me for the next 40 minutes. Am I fun? Oh, most of the time. And lastly,

11
00:01:18,720 --> 00:01:28,160
am I a girl? Huh. So that one is not true. I am a male presenting sys man. So why did Google Vision

12
00:01:28,160 --> 00:01:32,880
think I am a girl? And the thing is it's not just Google Vision. If I were to run this through

13
00:01:32,880 --> 00:01:36,880
Microsoft Vision API, it produces similar results. It says, oh, this is a picture of a lady.

14
00:01:37,520 --> 00:01:42,560
And so the thing is, what are these really looking for when they say this is a girl? Are they saying

15
00:01:42,560 --> 00:01:48,960
how similar am I to what they've understood to be a girl? Is it because of, oh, this is a dancer and

16
00:01:48,960 --> 00:01:53,280
all the dancers in my training set were girls, so therefore this is a girl as well? Or is there some

17
00:01:53,280 --> 00:01:58,000
physical feature about me that makes it think that, oh, this is probably a girl? And this seems like

18
00:01:58,000 --> 00:02:03,200
a fun, innocuous example, but these types of systems of automatic gender recognition are used

19
00:02:03,200 --> 00:02:10,320
everywhere. For example, to basically show you better ads when you're in a taxi cab for some

20
00:02:10,320 --> 00:02:16,160
reason, because after all, it's not the true purpose of data science to show you better ads.

21
00:02:16,160 --> 00:02:20,880
And think about this. This sounds really dumb. It's actually quite dangerous, because if you

22
00:02:22,320 --> 00:02:26,800
basically have some prototypical understanding of what this is what a man looks like, or this is what

23
00:02:26,800 --> 00:02:32,080
a woman should look like, you could actually misgender people whom that is actually a huge issue

24
00:02:32,080 --> 00:02:39,280
too. And so there's a researcher at University of Washington, Oz Keyes, who's done a survey of how

25
00:02:39,280 --> 00:02:44,000
queerness and gender are thought about in human computer systems. And their overall findings is

26
00:02:44,000 --> 00:02:49,200
that the way we think about gender in human computer systems is inherently trans exclusionary.

27
00:02:49,200 --> 00:02:54,080
So now as we try to be data-driven leaders and we use these types of systems, we are sort of

28
00:02:54,080 --> 00:02:58,960
using systems that are inherently trans exclusionary. We inherently make decisions

29
00:02:58,960 --> 00:03:04,320
that ignore the existence and lived experience of the trans community. And I hope this example

30
00:03:04,320 --> 00:03:09,280
sort of motivates why we should care about fairness and ethics when we do data science

31
00:03:09,280 --> 00:03:16,160
and machine learning. So what is algorithmic fairness? So algorithmic fairness is a field

32
00:03:16,160 --> 00:03:22,960
of research that's aimed at trying to mitigate the effects of unwarranted bias on discrimination

33
00:03:23,040 --> 00:03:27,520
on people on machine learning. And so this is a kind of weird thing because like bias is a very

34
00:03:27,520 --> 00:03:32,160
overloaded term in data science and machine learning. It's probably the second most overloaded term

35
00:03:32,160 --> 00:03:37,680
after kernel, which has a bunch of different definitions in machine learning. And right now,

36
00:03:37,680 --> 00:03:41,600
sort of like the focus on this type of research has been in like coming up with mathematical

37
00:03:41,600 --> 00:03:45,840
definitions of fairness. We have this mathematical definition of fairness, we're going to find some

38
00:03:45,840 --> 00:03:51,760
solution to it and hope that maps back well to the original problem. But over the last year or so,

39
00:03:51,760 --> 00:03:56,240
we've seen some pushback against this. We've seen that, okay, in this paper, for example,

40
00:03:56,240 --> 00:04:01,680
called fairness, abstraction and social technical systems, that there's sort of an over a formalism

41
00:04:01,680 --> 00:04:06,080
trap that we care too much about mathematical formalisms, that even if you solve the mathematical

42
00:04:06,080 --> 00:04:11,120
formalism, it doesn't really translate well to a real world solution. And the key thing to keep

43
00:04:11,120 --> 00:04:17,600
about this is like, first is inherently a social and ethical concept. It cannot be perfectly captured

44
00:04:17,600 --> 00:04:23,920
or represented through mathematical definitions or statistical metrics. And we'll talk about the

45
00:04:23,920 --> 00:04:29,920
research. And I guess I want to take a slight, I guess, take a side about like, this is the idea

46
00:04:29,920 --> 00:04:34,960
of like, okay, well, we're using algorithms and math, and math can't be racist. That's silly.

47
00:04:35,600 --> 00:04:40,720
And this actually came up in January when this like alt-right journalist basically criticized

48
00:04:40,720 --> 00:04:46,080
New York Congresswoman Alex Ocasio-Cortez when she said that, okay, algorithms have been shown

49
00:04:46,080 --> 00:04:51,120
to discriminate against blacks and his schematics in different cases. And so the thing is that like,

50
00:04:51,120 --> 00:04:56,320
when we say that an algorithm is racist or sexist, we're not saying that like, math is inherently

51
00:04:56,320 --> 00:05:01,040
racist or sexist. We're not saying that like, the prime numbers are inherently racist or logistic

52
00:05:01,040 --> 00:05:06,880
regression is inherently sexist. We're talking about the ways we use math, the ways we use

53
00:05:06,880 --> 00:05:12,880
algorithms can reinforce societal inequalities. You know, prime numbers are not racist, but the

54
00:05:12,880 --> 00:05:18,800
way we can use prime numbers to break cryptographic systems has real world implications. The way we

55
00:05:18,800 --> 00:05:24,160
use logistic regression to decide who gets a credit loan or a bank loan or not has real

56
00:05:24,160 --> 00:05:27,920
world implications. And that's what we need to think about when we design these algorithms.

57
00:05:27,920 --> 00:05:33,200
They're not sort of just existing in the void by themselves. They're embedded in human systems.

58
00:05:35,200 --> 00:05:41,680
And on the research side of this algorithmic fairness. So now this field called fairness,

59
00:05:41,680 --> 00:05:46,720
accountability, transparency, FATSTAR ML. And this is like this interdisciplinary research area

60
00:05:46,720 --> 00:05:51,600
about how can we get machine learning and technical systems to care about ideas like fairness and

61
00:05:51,600 --> 00:05:56,960
justice and equality. And it's really exploded over the last few years. You're seeing a dedicated

62
00:05:56,960 --> 00:06:01,840
conference to it, ACM FATSTAR, which was held in Atlanta. We have seen lots of open source

63
00:06:01,840 --> 00:06:06,960
libraries about this. And even at this conference, we actually have another talk about model fairness

64
00:06:06,960 --> 00:06:11,840
later today. And I do for anyone who's also interested in attending that I have looked at

65
00:06:11,840 --> 00:06:16,000
that other talk. I do believe there's enough differentiation between our two talks that you

66
00:06:16,000 --> 00:06:20,880
would get something about worthwhile by attending both talks for anyone who has a slight concern.

67
00:06:22,000 --> 00:06:25,440
And it's not just researchers that are talking about this. We talk about this in popular media,

68
00:06:25,440 --> 00:06:32,080
too. So Weapons of Mass Destruction by Cathy O'Neill, Automating Equality by Virginia Eubanks,

69
00:06:32,080 --> 00:06:37,760
and Algorithms of Repression by Safia Noble. And so, you know, the big three. And I think

70
00:06:38,560 --> 00:06:43,680
they take this interesting approach of like, okay, here's how the algorithms work on the people.

71
00:06:43,680 --> 00:06:49,520
Like we as a data scientist, we as a machine learning researchers are sort of separated from

72
00:06:49,520 --> 00:06:55,920
the downstream effects of how our work impacts the general humanity, a general society. And the thing

73
00:06:55,920 --> 00:06:59,680
is like, as data scientists and like computer scientists, we're sort of trained in like math

74
00:06:59,680 --> 00:07:02,880
and statistics. We're not trained to think about ethics. We're not trained to think about

75
00:07:02,880 --> 00:07:08,800
our public policy. But we do need to understand that like our work has downstream implications.

76
00:07:08,800 --> 00:07:13,760
I think deep down, like we may not be comfortable like taking the role of an ethicist or taking the

77
00:07:13,760 --> 00:07:18,000
role of a public policy. But I think deep down, none of us want to cause the apocalypse. None of

78
00:07:18,000 --> 00:07:22,480
us want to do harm to others. Like no one wants to be the people written about in these books.

79
00:07:23,040 --> 00:07:26,400
And I think the way that they talk this problem of like talking about the dangers of these algorithms

80
00:07:26,480 --> 00:07:31,840
like how these hurt people is an interesting perspective from just research, which is okay,

81
00:07:31,840 --> 00:07:36,400
here's the cool things we could do. Whereas these popular media talk about like, how could this hurt

82
00:07:36,400 --> 00:07:40,240
people? And talk about how could this hurt people? Well, you have to think about like, okay, maybe

83
00:07:40,240 --> 00:07:44,000
you don't really care about the moral side of like, I don't want to hurt people. But I can maybe

84
00:07:44,000 --> 00:07:47,680
encourage you to think about these types of problems by legal regulations of you maybe don't

85
00:07:47,680 --> 00:07:51,200
care about hurting people, but you do care about keeping your job. Because if you break some of

86
00:07:51,200 --> 00:07:55,520
these things, well, you're violating legal laws. And then you can lose your job because your company

87
00:07:55,520 --> 00:08:00,720
will be fined millions of dollars. And so in the US, for example, we have disparate impact laws,

88
00:08:00,720 --> 00:08:06,320
which prevents again, which basically regulate how you can give out loans, how you can hire for

89
00:08:06,320 --> 00:08:13,920
people such that you don't harm them. And also in the EU, we now have GDPR, which is an overarching

90
00:08:13,920 --> 00:08:19,120
holistic set of laws and rules of like, how we should think about the way we use algorithms to

91
00:08:20,640 --> 00:08:24,800
process people's data. And we have to think about, okay, can you explain why your algorithm made a

92
00:08:25,040 --> 00:08:28,560
decision? Can you explain why your neural network didn't give someone a loan? And can they opt out

93
00:08:28,560 --> 00:08:34,640
of that as well? And so let's talk about the different types of algorithmic biases. And so

94
00:08:34,640 --> 00:08:39,760
this is based off some work by researchers at Microsoft and research in Cornell. And this was

95
00:08:39,760 --> 00:08:46,480
presented at the conference that is now NeurIPS. Someone told me it's pronounced like Europe, and

96
00:08:46,480 --> 00:08:51,760
I'm not actually sure if that was a joke or not. But really, these researchers sort of identified

97
00:08:52,400 --> 00:08:57,040
three big categories of way we think about like algorithmic biases of like, where we are now,

98
00:08:57,040 --> 00:09:01,680
where we're heading, and where we need to be. So first, let's talk about where we are now.

99
00:09:01,680 --> 00:09:08,160
So a lot of problems in algorithmic biases focus on this problem of allocation. Who gets a loan or

100
00:09:08,160 --> 00:09:13,120
not? Who gets a job? And these are really just like binary classification problems of like,

101
00:09:13,120 --> 00:09:20,160
do men get software engineering jobs more than women? Do whites get credit loans more than non-whites

102
00:09:20,160 --> 00:09:24,080
kind of thing? This is a binary yes or no. And this is really what most of the research has been

103
00:09:24,080 --> 00:09:28,560
on now, because it's sort of easy to formalize as a mathematics problem and easy to solve. And so

104
00:09:28,560 --> 00:09:33,920
this was in the news, big story last year, was that Amazon had to scrape their AI recruiting tool

105
00:09:34,720 --> 00:09:38,880
because they found out that it was discriminating against women. It was basically looking at things

106
00:09:38,880 --> 00:09:44,080
of like, oh, hey, if you have a name like Jared, or you were on the lacrosse team back in college,

107
00:09:44,080 --> 00:09:48,320
you're probably more likely to be hired as a software engineer, which really makes no sense.

108
00:09:48,320 --> 00:09:54,000
And so they scraped this because they realized, oh, hey, we were harming or choosing our model

109
00:09:54,000 --> 00:10:00,640
was selecting against women, but we use this. Next, let's talk about where we're sort of heading.

110
00:10:00,640 --> 00:10:05,120
And so next is this idea of bias representation. And so this is focused on looking at like how

111
00:10:05,120 --> 00:10:09,920
harmful stereotypes or harmful labels are propagated through machine learning systems.

112
00:10:09,920 --> 00:10:13,280
And these are often related to like language problems or computer vision problems, which are

113
00:10:13,280 --> 00:10:17,920
like the neural network problems of it just picks out a solution and do some black box neural

114
00:10:17,920 --> 00:10:22,560
network magic. But we don't really care about why the inner mechanics of what it's doing.

115
00:10:23,120 --> 00:10:27,440
And these are sort of hard to quantify the errors compared to those previous bias and allocation

116
00:10:27,440 --> 00:10:30,960
problem, which is why there's less research on this now, but like we started to think about it

117
00:10:30,960 --> 00:10:36,400
more. And so this is a famous example that when Google photos was deployed like 2015,

118
00:10:36,400 --> 00:10:41,200
that it actually labeled a group of black kids at a graduation party as gorillas. And so for those

119
00:10:41,200 --> 00:10:45,760
of you who are not familiar with like history of US, I don't know if it applies to other Western

120
00:10:45,760 --> 00:10:50,400
countries, but gorillas or apes has been historically used as a derogatory slur against

121
00:10:50,400 --> 00:10:55,680
African Americans in this nation. So this label, like one is wrong, but two, it also invokes

122
00:10:55,680 --> 00:11:00,400
harmful stereotypes against African Americans. And the thing is, it's like, it's harder to quantify

123
00:11:00,400 --> 00:11:04,800
as an error because yeah, this is the wrong label. But at the same time, Google photos is never going

124
00:11:04,800 --> 00:11:11,200
to label a group of white kids as gorillas. This is specifically a label it gives to these kids here.

125
00:11:11,200 --> 00:11:14,800
And it promotes this harmful stereotype. And other examples that are sort of hard to quantify

126
00:11:15,360 --> 00:11:18,400
are Snapchat filters, for example. So when you apply Snapchat filter,

127
00:11:19,040 --> 00:11:24,560
Flower Crown, it sort of brightens the skin of the individual. Why? Because Snapchat filters have

128
00:11:24,560 --> 00:11:29,200
sort of learned, okay, brighter skin is associated with prettiness. Therefore, when I apply the

129
00:11:29,200 --> 00:11:33,680
filter, brighten their skin. And so if you think about that, it's like really hard to quantify

130
00:11:33,680 --> 00:11:38,800
what's wrong. It's doing the optimization process it's supposed to do. But there's just something

131
00:11:38,880 --> 00:11:45,200
about that solution just sits uneasy with you. And another example is Google Translate. So when

132
00:11:45,200 --> 00:11:49,520
you translate sentences from like Malay, which is a language that doesn't have gendered pronouns,

133
00:11:49,520 --> 00:11:54,800
so you can have like, they are a doctor, they are a soldier, they are a professor, they are prostitute,

134
00:11:54,800 --> 00:11:58,160
they are a nurse. But when you translate to English, it sort of automatically genders them.

135
00:11:58,160 --> 00:12:03,520
He is a doctor, he's a soldier, he's a professor, she is a prostitute, she is a maid, she's a nurse.

136
00:12:03,520 --> 00:12:07,520
And at the same time, Google Translate is doing this mathematical optimization. It's like

137
00:12:07,520 --> 00:12:11,360
hard to say why this is wrong. This is a valid translation of those sentences,

138
00:12:11,360 --> 00:12:16,320
but there's just something about it that just sits uneasy with you. But to give credit to

139
00:12:16,320 --> 00:12:21,200
where it's due, Google has actually made a sort of hot fix where when you translate from a

140
00:12:21,200 --> 00:12:28,480
non-gendered language to a gendered language, it will actually show both gendered ways. So translating

141
00:12:28,480 --> 00:12:33,920
from they are a doctor translates to she is a doctor, he is a doctor now. So this is like a quick

142
00:12:33,920 --> 00:12:38,640
fix for this. It's still not available on the iOS and Android version of Google Translate.

143
00:12:38,640 --> 00:12:42,480
And you still see some of these biases when you do Google Autocomplete. So if you type in a search

144
00:12:42,480 --> 00:12:47,360
of he is a blah, you'll probably get doctor or soldier more than if you do she is a blah

145
00:12:47,360 --> 00:12:54,080
in Google Autocomplete. Finally, where do we need to be? And so this last version is called the

146
00:12:54,080 --> 00:12:58,800
weaponization of machine learning. And so the key idea of this is that as data scientists,

147
00:12:58,800 --> 00:13:02,480
we're taught to train models, we produce some metrics, but we don't really think about how

148
00:13:02,480 --> 00:13:07,120
our work can harm people or how it could be misused. And so there's this big story back in

149
00:13:07,120 --> 00:13:11,920
2017, a group of Stanford researchers tried to create a classification algorithm that tries to

150
00:13:11,920 --> 00:13:17,360
predict people's sexuality or sexual preferences based on images of their faces. And so they

151
00:13:17,360 --> 00:13:23,520
effectively created a GADAR of trying to predict if someone is heterosexual or homosexual. And the

152
00:13:23,520 --> 00:13:29,040
thing about this is there are countries in this world where homosexuality is criminalized by the

153
00:13:29,040 --> 00:13:33,120
state. If you are out as a homosexual, you will be executed by the state. And there are plenty

154
00:13:33,120 --> 00:13:39,200
more where it's not criminalized, but it's really dangerous to be out as a homosexual. So if someone

155
00:13:39,200 --> 00:13:44,320
takes this model, they could use it as a weapon against homosexuals in those countries. And think

156
00:13:44,320 --> 00:13:50,320
about this, there's no mathematical measure, no unit test you can write that can tell you this is

157
00:13:50,320 --> 00:13:56,640
a bad idea. You shouldn't do this. This is really going to require you to have cultural anthropologists

158
00:13:56,640 --> 00:14:01,600
and historians in the room to tell you, like, hey, this could harm people. And that's why I'm

159
00:14:01,600 --> 00:14:05,920
really glad that there is sort of this starting discussion of having data scientists and machine

160
00:14:05,920 --> 00:14:10,400
learning technologists really think about ethics training. Like when you do degree programs at

161
00:14:10,400 --> 00:14:13,760
like UC Berkeley or Carnegie Mellon, they're starting to include an ethics component. Well,

162
00:14:13,760 --> 00:14:18,480
I don't think that will really fully solve this. I do think it's the start of developing a different

163
00:14:18,480 --> 00:14:23,920
tech culture that really cares about the downstream effects of our work on people who are not like us.

164
00:14:26,800 --> 00:14:29,760
Let's talk about the different types of fairness measures. So these are like the mathematical

165
00:14:29,760 --> 00:14:34,880
definitions, and I'll really talk about like what they are and some of their big weaknesses of them.

166
00:14:34,880 --> 00:14:40,560
And so this is based off of work by two Stanford researchers, Sam Corbett Davies and Sherrod Goll.

167
00:14:41,200 --> 00:14:45,680
And they're based on this paper, Measures and Mismeasures of Fairness, which this talk is,

168
00:14:45,680 --> 00:14:50,560
the title of this talk is based on. And so let's talk about different definitions of

169
00:14:50,560 --> 00:14:55,520
algorithmic fairness. So there was a talk a year ago by this Princeton professor,

170
00:14:56,240 --> 00:15:00,160
Irvind Narayan, where he really showed that, okay, there were 21 definitions of algorithmic

171
00:15:00,160 --> 00:15:03,840
fairness at the time of the talk when he gave that talk. Now there's more than 30 definitions

172
00:15:03,840 --> 00:15:08,080
of like algorithmic fairness now. And think about this, like as computer scientists and

173
00:15:08,080 --> 00:15:12,080
met like statisticians, we think, okay, there's 30 definitions. There's got to be like some that

174
00:15:12,080 --> 00:15:15,600
are better than others. It's got to be like this one true definition of fairness we can use. And

175
00:15:15,600 --> 00:15:20,560
that's not really true. Why? Because fairness is a social and cultural concept. Like what was

176
00:15:20,560 --> 00:15:24,480
considered fair 50 years ago is not going to be considered what we consider fair now. And what

177
00:15:24,480 --> 00:15:31,200
we consider fair 50 years from now is different from what we consider fair today. But these 30

178
00:15:32,080 --> 00:15:35,360
so definitions of algorithmic fairness can be broken down into three big groups,

179
00:15:35,360 --> 00:15:40,400
additive classification, class of question parity, and finally calibration. And I'll just give a

180
00:15:40,400 --> 00:15:43,840
quick warning. This is where the talk gets a bit more technical and mathematical.

181
00:15:43,840 --> 00:15:49,120
So first up, there's these anti classification measures. And so what these are is that, okay,

182
00:15:49,120 --> 00:15:53,120
you have these protective features, such as like race, gender, religion, place of origin,

183
00:15:53,120 --> 00:15:57,360
and you have unprotected features for your model. And the way that these anti classification

184
00:15:57,360 --> 00:16:01,600
measures work ideally is that, okay, when you make an algorithmic decision, it should effectively

185
00:16:01,600 --> 00:16:05,840
ignore those demographic features. It should ignore race. It should ignore gender. It should

186
00:16:05,840 --> 00:16:10,960
ignore religion when it makes a decision or behave in a way it does so. And so this idea of

187
00:16:11,920 --> 00:16:15,760
individual fairness, you judge a person by the content of their character rather than the color

188
00:16:15,760 --> 00:16:20,720
of skin kind of thing. That two people should be treated equally if they have the same unprotected

189
00:16:20,720 --> 00:16:25,520
features. But the thing about this is that you can't just throw out race and gender from your

190
00:16:25,520 --> 00:16:29,040
model or from your data set and say, okay, I'm done. Because we're now starting to realize that

191
00:16:29,040 --> 00:16:34,880
there are proxy features that we can worry about. Features that are not directly encode race or

192
00:16:34,880 --> 00:16:40,000
gender, but are highly correlated with them. And so for example, there's a story back in 2012 when

193
00:16:40,000 --> 00:16:45,040
the office supply company Staples, they did a promotion that if you live within 20 miles

194
00:16:45,040 --> 00:16:50,320
of a competing store, we will give you a special coupon discount so that you'll come to our store

195
00:16:50,320 --> 00:16:55,840
instead. The thing is, the people who live in those neighborhoods at 20 miles from the competitors,

196
00:16:55,840 --> 00:16:59,600
those are sort of like wealthier suburban neighborhoods. And the people who live in

197
00:16:59,600 --> 00:17:04,960
wealthy suburban neighborhoods have certain racial demographics, for example. So when your

198
00:17:04,960 --> 00:17:10,400
model discriminates or makes a decision based on location, it's inadvertently acting as if it was

199
00:17:10,400 --> 00:17:15,120
making decisions based on race. And so Staples was used fine with that for disparate impact.

200
00:17:15,680 --> 00:17:18,400
And as a result, these types of features are very useful for designing

201
00:17:19,440 --> 00:17:24,960
fairness-aware models. So what is a fairness-aware model? So it's sort of like a traditional machine

202
00:17:24,960 --> 00:17:29,360
learning supervised model, except now we have this added component. We need to think about those

203
00:17:29,360 --> 00:17:34,000
protected features. So when you have these standard machine learning, you have your features X and

204
00:17:34,000 --> 00:17:38,880
your labels Y. And you basically want to map the features X to the labels Y. Well, now with

205
00:17:38,880 --> 00:17:43,440
fairness-aware algorithms, you have X, you have Y, and you have your protected attributes. So race,

206
00:17:43,440 --> 00:17:49,520
gender, religion, place of origin, as a protected class Z. And what you want to do is you want to

207
00:17:49,520 --> 00:17:53,680
learn features that learn your label. So you want to learn who to give a loan to. But you don't want

208
00:17:53,680 --> 00:18:00,240
to accidentally learn race or gender. And so one really cool algorithm that does this is fairness-

209
00:18:00,320 --> 00:18:05,040
aware GANs. So I think this is a really clever idea. So how this works with GANs is that you

210
00:18:05,040 --> 00:18:10,160
sort of have these two sub-models. One is sort of your standard machine learning classifier. It's

211
00:18:10,160 --> 00:18:14,720
trying to take the features and learn the labels. The other one is trying to take the labels and

212
00:18:14,720 --> 00:18:19,280
learn a protected class. And so the way you can think about it is sort of like this dual, like I'm

213
00:18:19,280 --> 00:18:25,440
trying to learn a good classifier. You're trying to break me by saying, oh, that classifier accidentally

214
00:18:25,440 --> 00:18:30,560
learns race or gender. And so this is how that really works. You have to look at this loss

215
00:18:30,560 --> 00:18:34,720
function. So the one in the red box is sort of saying, am I a good classifier or not? Am I

216
00:18:34,720 --> 00:18:39,040
learning features that predict who gets the loan or not? The one in the blue box is saying, okay,

217
00:18:39,040 --> 00:18:43,840
you learn who to give a credit loan to. Are you accidentally learning race or gender?

218
00:18:43,840 --> 00:18:47,760
Is your basically- who gets the loan or not? Basically just figuring out is this person white

219
00:18:47,760 --> 00:18:52,560
or non-white. And sort of this trade-off, if you really squint your eyes at this, if you've studied

220
00:18:52,560 --> 00:18:56,720
machine learning and regularization, so like ridge regression or lasso regression, you squint

221
00:18:56,720 --> 00:19:01,040
your eyes at this and stare at it deeply. It kind of looks like a regularization of I want to be a

222
00:19:01,040 --> 00:19:05,120
good classifier, but I don't want to be overly complex. And in this case, complex is sort of

223
00:19:05,120 --> 00:19:10,480
like I don't want to accidentally be discriminatory. And this is sometimes referred to as this accuracy

224
00:19:10,480 --> 00:19:15,440
fairness trade-off. And I'll be really honest, I don't like that this is referred to as a trade-off.

225
00:19:15,440 --> 00:19:18,720
Because if you really think about like, why are we doing all this stuff in the first place? Because

226
00:19:18,720 --> 00:19:22,640
our training set is biased, because our testing set is biased, and that means our testing labels

227
00:19:22,640 --> 00:19:28,640
are biased. Like our answer key is wrong. We don't want models that perfectly learn the testing set

228
00:19:28,640 --> 00:19:33,600
labels, because it is also learning those biases as well. So we expect the accuracy to go down.

229
00:19:33,600 --> 00:19:38,160
And I also really don't like calling this a trade-off, because it sort of ties into this like very

230
00:19:38,160 --> 00:19:46,880
harmful idea that somehow promoting diversity requires you to partake in sub-optimality. And so

231
00:19:46,880 --> 00:19:52,160
you see this in like tech hiring, where okay, if a software company says like, oh, hey, 50% of our

232
00:19:52,160 --> 00:19:56,560
software engineers are women, that's a good thing. But then there's always like some loser on Twitter

233
00:19:56,560 --> 00:20:01,120
is like, hey, I got a brilliant idea. Instead of hiring people based on their gender, why don't we

234
00:20:01,120 --> 00:20:05,200
hire the most talented person for their job? And this is sort of like causing a harmful idea. Like

235
00:20:05,200 --> 00:20:10,080
in order to get diversity, you have to pick less optimal candidates or less skilled candidates.

236
00:20:10,080 --> 00:20:14,160
And that's not true at all. Like diversity is very good for tech. Having people with different

237
00:20:14,160 --> 00:20:18,240
lived experiences and different ways of thinking in the room is important, because they can call

238
00:20:18,240 --> 00:20:24,480
you out when you're being stupid. And that's very important in data science. And so what are the

239
00:20:24,480 --> 00:20:27,920
dangers of using these anti-classification measures? Like, okay, this sounds like a good idea. Like we

240
00:20:27,920 --> 00:20:32,960
should judge people based on their important characteristics and not accidentally learn those

241
00:20:32,960 --> 00:20:38,400
protected features. Well, by removing those protected features, we're sort of ignoring the

242
00:20:38,400 --> 00:20:42,240
underlying process that acts on different demographic groups. Like these metrics sort of

243
00:20:42,240 --> 00:20:47,920
look on making the outcome equal. But really, fairness is making the process equal. And so one

244
00:20:47,920 --> 00:20:51,520
way this is a danger is sort of prison recidivism. So who gets, when you're a prisoner and you're

245
00:20:51,520 --> 00:20:56,720
defending, like do you get released on bail or not? And so how prison recidivism works is like

246
00:20:56,720 --> 00:21:00,880
there's this algorithm, Compass, it gives you a score between one and 10, which is sort of like,

247
00:21:00,880 --> 00:21:04,960
how risky is it to let you out? Like what's the likelihood you will commit a crime if we release

248
00:21:04,960 --> 00:21:10,240
you? And the thing is that male defendants are more likely, for the same score, male defendants

249
00:21:10,240 --> 00:21:15,040
are more likely to re-offend than female defendants. So if you ignore gender in this case

250
00:21:15,040 --> 00:21:19,680
and sort of average the two lines, you sort of get that average line that's higher than the female

251
00:21:19,680 --> 00:21:26,080
defendant rate. And so if you cut your threshold up off like a little below 60%, whereas for,

252
00:21:26,080 --> 00:21:30,880
if you just looked at gender exclusively, you would say, okay, we don't release female defendants

253
00:21:30,880 --> 00:21:34,960
when they have a score of eight or above. Whereas if you take the average and you now have that

254
00:21:34,960 --> 00:21:40,000
threshold, we don't release anyone if it's like seven or above. And thus you're now detaining more

255
00:21:40,000 --> 00:21:44,160
women who then you originally done if you had looked at gender separately as a feature in this.

256
00:21:46,720 --> 00:21:52,000
So next is classification parity. So classification parity is sort of extension of like

257
00:21:53,360 --> 00:21:55,920
traditional machine learning. When machine learning, you evaluate your model according

258
00:21:55,920 --> 00:22:01,600
some metric like accuracy or precision or recall or AUC. Whereas now you take those metrics and

259
00:22:01,600 --> 00:22:06,640
sort of do it across different demographic groups. For example, this is a famous example,

260
00:22:06,720 --> 00:22:12,400
gender studies by an MIT student Joy Bululani. And what she shows is that like these commercial

261
00:22:12,400 --> 00:22:17,600
face detection algorithms, Microsoft, IBM, and then Face++, they're really good at detecting

262
00:22:17,600 --> 00:22:21,760
white male faces. They're really pretty good at detecting white female faces, detecting black

263
00:22:21,760 --> 00:22:28,880
male faces, but the intersection of race and gender, they're really bad at detecting black

264
00:22:28,880 --> 00:22:34,080
women faces. And so this is still okay across these different demographic groups, the accuracy

265
00:22:34,080 --> 00:22:38,880
is significantly lower for black women. And Joy also has a very cool video on YouTube that's like

266
00:22:38,880 --> 00:22:45,520
a spoken word poem about talking about, okay, how commercial computer vision systems constantly

267
00:22:45,520 --> 00:22:51,840
misgender famous black women such as Michelle Obama or Oprah. And these types of metrics or

268
00:22:51,840 --> 00:22:55,200
these types of classification parity metrics are what's commonly used to enforce those legal

269
00:22:55,200 --> 00:23:00,400
regulations. So it's like the Equal Employment Opportunity Act or things of that nature,

270
00:23:00,400 --> 00:23:03,680
are sort of using these types of classification metrics behind the scenes.

271
00:23:05,680 --> 00:23:09,680
So what's the most common one? So most common one of these types of metrics is demographic

272
00:23:09,680 --> 00:23:14,960
parity. So demographic parity is like, how often do we get positive outcomes by different

273
00:23:14,960 --> 00:23:20,000
demographic groups or different protected groups? And this is what's really used to audit models for

274
00:23:20,000 --> 00:23:25,040
disparate impacts. So like who gets a loan or not, you look at different demographic groups and say,

275
00:23:25,040 --> 00:23:32,000
okay, the positive rate, like, okay, the percentage of loans given to white men versus the percentage

276
00:23:32,000 --> 00:23:39,520
of loans given to non-whites, non-males should be at most 20% difference. That's what that 80% rule

277
00:23:39,520 --> 00:23:43,440
means. They have to be within 80% of each other. And the thing about this is like, that sounds

278
00:23:43,440 --> 00:23:46,720
nice. Okay, if we're a bank, you have to sort of balance out the number of loans you give to

279
00:23:46,720 --> 00:23:52,720
different populations. The thing is about you can satisfy that immediate thing, but you could not

280
00:23:52,720 --> 00:23:57,520
really think about the long-term consequences of your actions. And so this is a paper by researchers

281
00:23:57,520 --> 00:24:01,920
out of UC Berkeley, Delayed Impact of Fair Machine Learning, where they show that, okay, if you do

282
00:24:01,920 --> 00:24:06,320
this demographic balance for demographic parity, so you give the same percentage of loans to people

283
00:24:06,320 --> 00:24:11,280
in the orange population and people in the blue population, you're giving a lot more loans to

284
00:24:11,280 --> 00:24:14,880
people in the blue population who would then default on that loan. And as a result, their credit

285
00:24:14,880 --> 00:24:19,440
score goes down and the overall credit score distribution for the blue population goes down

286
00:24:19,440 --> 00:24:22,720
as well. So if you just think about satisfying that immediate constraint of like, we just have

287
00:24:22,720 --> 00:24:27,120
to make the outcomes equal, positive outcomes equal, then you could harm them in the long run.

288
00:24:27,120 --> 00:24:31,040
And so another way to also think about this is that when it comes to tech hiring, of like, okay,

289
00:24:31,040 --> 00:24:34,720
we just need to hire, make sure the percentage of women we hire is equal to the number of

290
00:24:34,720 --> 00:24:39,360
percentage of men we hire for technical positions. Well, if those women leave in like five, six months

291
00:24:39,360 --> 00:24:43,200
because of a toxic workplace environment, do you really deserve your gold star for meeting that

292
00:24:43,200 --> 00:24:51,040
hiring quota? I don't think so. Next is parity of false positive rates. So this is really like,

293
00:24:51,040 --> 00:24:54,160
as the name suggests, we're looking instead of like the positive outcomes, we're looking at the

294
00:24:54,160 --> 00:25:02,560
false positives. So really false positives, like the model says yes, but the actuality is no. And

295
00:25:02,560 --> 00:25:06,160
this is sometimes called equal opportunity. And think about, like, if you think about what false

296
00:25:06,160 --> 00:25:10,720
positive rate is, it's false positive over false positive plus true negative. So if you want to

297
00:25:10,720 --> 00:25:15,200
drop that false positive rate now, ideally you're focusing on dropping the number of false positives.

298
00:25:15,200 --> 00:25:19,600
But if you think about it, I could just drop the false positive rate down by increasing the number

299
00:25:19,600 --> 00:25:24,160
of true negatives in the system. And so let's say you're a police chief and someone comes to you and

300
00:25:24,160 --> 00:25:30,000
says, oh, hey, you're overly detaining too many black men, that the number of black men who you've

301
00:25:30,000 --> 00:25:36,560
arrested and not denying bail to or denying parole to, who would otherwise not go on to reoffend,

302
00:25:36,560 --> 00:25:40,400
is too high. So what can you do about this? How can you drop that false positive rate down?

303
00:25:40,640 --> 00:25:46,000
Ideally, you're sort of dropping the false positive. You're sort of granting more bail to black men who

304
00:25:46,000 --> 00:25:50,400
would otherwise not go on to reoffend. But you could also just increase the number of true negatives.

305
00:25:50,400 --> 00:25:55,440
So what's the true negative? So a true negative is a person who is arrested, they are released,

306
00:25:55,440 --> 00:25:58,720
and they don't go on to commit a crime. So who are those? Those are people who commit misdemeanors.

307
00:25:58,720 --> 00:26:01,920
So how can you increase the number of true negatives in the system? You arrest more people

308
00:26:01,920 --> 00:26:07,920
for misdemeanors and then let them go. Boom, that false positive rate goes down. You've achieved

309
00:26:07,920 --> 00:26:11,600
equality. Isn't that great? You just have to arrest more black men for misdemeanors.

310
00:26:12,560 --> 00:26:16,800
That's terrible. And if you don't think about it, you just optimize that metric and not think about

311
00:26:17,920 --> 00:26:23,280
the societal factors that generate these numbers, you end up harming that vulnerable

312
00:26:23,280 --> 00:26:32,160
population we want to help. And finally, last one is calibration. So calibration is a really hard

313
00:26:32,160 --> 00:26:37,120
thing to explain in layman's terms. It comes to statistical calibration. And really the way you

314
00:26:37,120 --> 00:26:44,720
think about this is for any event, like who wins a state in a political election, there's some true

315
00:26:44,720 --> 00:26:48,240
outcome. We know someone who wins, someone who doesn't lose, and we're trying to figure out how

316
00:26:48,240 --> 00:26:53,120
well are we actually predicting that outcome with our model. And so the way this is used in, let's

317
00:26:53,120 --> 00:26:57,200
say, recidivism, for example, where people are given scores, come to scores from one to 10,

318
00:26:57,200 --> 00:27:02,880
or child protective services where a child is given a score of one to 20, representing how much

319
00:27:02,880 --> 00:27:07,920
danger the child is in. So one being the child is not really in danger in this household, or 20

320
00:27:07,920 --> 00:27:13,680
being like, that child is in serious danger, get him out of the household now. And so how this works

321
00:27:13,680 --> 00:27:18,720
is, okay, CPS goes in, they evaluate the household, and they give the child a score between one to 20.

322
00:27:18,720 --> 00:27:24,160
And if that score is high enough, then above some threshold T, or just say 15, for example,

323
00:27:24,160 --> 00:27:27,120
then you take the child out of the household because the child is in danger if they stay there.

324
00:27:27,120 --> 00:27:33,760
And the thing about statistical calibration is that, really, if two people get the same score,

325
00:27:33,760 --> 00:27:37,200
that should be all that matters. If you have a white child who gets a score of 15 and a black

326
00:27:37,200 --> 00:27:42,000
child who gets a score of 15, they should be considered to be in the same danger level. They

327
00:27:42,000 --> 00:27:46,960
should both have a 70% chance of being harmed, or 75% chance of being harmed if they stay in that

328
00:27:46,960 --> 00:27:52,880
household. It shouldn't matter on racial demographics, for example. It should just

329
00:27:52,880 --> 00:27:58,000
matter on, okay, we evaluate the score, and that score is the final thing. We take that score and

330
00:27:58,000 --> 00:28:04,000
make decisions solely based on that score. And if it's models well calibrated, if 15 is the cutoff

331
00:28:04,000 --> 00:28:08,480
for both, and we get the same percentage of children being removed from white households

332
00:28:08,480 --> 00:28:13,040
versus black households. And I guess the key idea is that the scores should mean the same thing for

333
00:28:13,040 --> 00:28:17,680
the same people. And so I guess, before, from my previous slides, when I talk about these metrics,

334
00:28:17,680 --> 00:28:21,600
there's usually a little bullet point at the bottom that says, danger, here's why this is bad.

335
00:28:21,600 --> 00:28:26,160
But I don't have this here. Why? Because I have a whole dedicated slide dedicated to what's the

336
00:28:26,160 --> 00:28:30,560
problem with calibration. And so anytime you give a talk on algorithmic fairness, you have to bring

337
00:28:30,560 --> 00:28:35,040
up this compass example. So here's the obligatory compass example. And I think this is an important

338
00:28:35,040 --> 00:28:39,600
case study. Because, so what is the issue with compass? So this is that prison recidivism

339
00:28:39,600 --> 00:28:47,280
algorithm. So in 2016, the publication ProPublica says that, oh, hey, when you use compass

340
00:28:47,360 --> 00:28:51,520
in order to decide who gets bail or not, you're accidentally defending, you're detaining black

341
00:28:51,520 --> 00:28:55,040
defendants at a higher rate. You're detaining black defendants who, if they were released,

342
00:28:55,040 --> 00:29:00,080
they would go on to not reoffend again. And so ProPublica North Point, which is the group that

343
00:29:00,080 --> 00:29:05,040
makes this compass algorithm, their argument was that, no, our risk scores are well calibrated.

344
00:29:05,040 --> 00:29:10,000
If you get a score of eight or above, we detain you. So score models are well calibrated, but the

345
00:29:10,000 --> 00:29:15,920
issue is that the underlying distribution of the scores are not. So if we give scores of eight or

346
00:29:15,920 --> 00:29:21,920
above a lot to black men, and we really give it to white males, for example, then okay, the chance

347
00:29:21,920 --> 00:29:27,200
of detaining a white male because scores of eight or above are rare for them is low, but the chance

348
00:29:27,200 --> 00:29:31,760
of detaining a black male who has a score of eight or above is high because they're more likely to

349
00:29:31,760 --> 00:29:39,040
have those higher scores, then well, your model is not kind of racist. And the thing about this is,

350
00:29:39,040 --> 00:29:44,320
I think it also shows another aspect of using these metrics is that you can do really well

351
00:29:44,320 --> 00:29:48,560
according to one of these metrics, like statistical calibration, but do horribly compared to another

352
00:29:48,560 --> 00:29:55,280
metric such as positive rates. And so just trying to aimlessly optimize for these metrics will result

353
00:29:55,280 --> 00:30:00,640
in solutions that really don't actually generate well to the real world. So what can we do?

354
00:30:02,000 --> 00:30:06,880
And so I'm going to propose sort of a low quantitative, low tech solution, which is really,

355
00:30:07,520 --> 00:30:12,960
let's write better documentation. And that might seem silly, but I think part of the issues we have

356
00:30:12,960 --> 00:30:19,520
now with AI ethics is that Silicon Valley is sort of insular. Silicon Valley believes it's filled

357
00:30:19,520 --> 00:30:25,600
with tech geniuses who believe they have achieved the 13th level of cognition by drinking their salt

358
00:30:25,600 --> 00:30:30,960
juice and going in their cryobaths, and they don't need plebeian mortals to tell them how to do their

359
00:30:30,960 --> 00:30:34,880
job. They will take care of them by themselves. They don't need government regulations. They don't

360
00:30:34,880 --> 00:30:40,560
need outsiders to do their job. And that's not true at all. I think better documentation allows

361
00:30:40,560 --> 00:30:46,080
for better communication of how these data sets are built, how these models work. And so even if

362
00:30:46,080 --> 00:30:50,880
you're not trained as a cultural anthropologist or an ethicist, if you can better communicate the

363
00:30:51,520 --> 00:30:55,280
outcomes of your work, better communicate how this is trained, how this is tested,

364
00:30:55,280 --> 00:31:00,160
to those people, they can do the evaluation of, okay, will this harm people in the long run? Will

365
00:31:00,160 --> 00:31:02,800
this accidentally harm these groups of people that you're not thinking about?

366
00:31:05,200 --> 00:31:10,160
And so here's this really cool idea of data seats for data sets. This is done by Timnit Gebru.

367
00:31:10,800 --> 00:31:16,560
T-I-M-N-I-T for the stenographer. And she basically proposes, like, well, other industries,

368
00:31:16,560 --> 00:31:21,200
like automobile industry or clinical testing, they sort of have standardizations of how they

369
00:31:21,200 --> 00:31:26,000
evaluate their models. And we should adopt something similar for data science data sets.

370
00:31:26,560 --> 00:31:30,960
And so if you think of the two, like automobile testing, for example, like, okay, historically,

371
00:31:30,960 --> 00:31:35,920
those automobile testers were tested on, like, dummy, crash test dummies with pro-typically adult

372
00:31:35,920 --> 00:31:41,760
male features. And as a result, in the real world, when actual car automobile collisions happen,

373
00:31:41,760 --> 00:31:45,920
women and children were extremely harmed or more likely to be severely injured in those.

374
00:31:45,920 --> 00:31:50,400
And so now we have legal regulations saying, okay, we need to make sure you also test on

375
00:31:50,400 --> 00:31:56,560
pro-typically adult female dummies and child dummies. And this documentation, I think,

376
00:31:56,560 --> 00:32:01,280
will answer a lot of questions, like, how is the data collected? Like, is this data collected in

377
00:32:01,280 --> 00:32:05,440
a way that doesn't exclude individuals? And I think this is very important because I do think

378
00:32:05,520 --> 00:32:10,320
a lot of the ways we think about machine learning is sort of focused on the model,

379
00:32:10,320 --> 00:32:14,640
not so much on the training set. We don't think about, like, what makes a good training set a

380
00:32:14,640 --> 00:32:18,960
good training set? And you may think, okay, what makes a good training set? Like, no missing data,

381
00:32:19,760 --> 00:32:23,840
no outliers. And that really answers the question of, like, can I do statistical analysis on this

382
00:32:23,840 --> 00:32:28,240
or not? It doesn't answer, like, did I exclude groups of individuals that I should care about?

383
00:32:28,240 --> 00:32:34,320
Did I exclude trans communities? Did I exclude non-heterosexual individuals from my data set?

384
00:32:34,320 --> 00:32:38,480
But just by the way I collected the data, I do think this shifts the conversation from, like,

385
00:32:38,480 --> 00:32:43,120
the model to, like, how do we actually collect good data that we've really not thought about

386
00:32:43,120 --> 00:32:46,720
before? Because we just sort of assume, okay, the data set lives in the aether. I got it from

387
00:32:46,720 --> 00:32:50,640
Kaggle or I got it from some other website. But we don't really think about, like, is that data

388
00:32:50,640 --> 00:32:58,480
set actually suitable for my prom or is it just conveniently available? And next, something similar

389
00:32:58,480 --> 00:33:03,680
from models. And so this is done out of Google research by Meg Mitchell's group, the Google AI

390
00:33:03,680 --> 00:33:08,880
Fairness group. And so what they propose is sort of, like, something similar. So standardized

391
00:33:08,880 --> 00:33:14,000
documentation for machine learning models. You document, okay, how was this when you produce a

392
00:33:14,000 --> 00:33:18,240
model and train it before you deploy it. Let's talk about how is it meant to be used? Like,

393
00:33:18,240 --> 00:33:24,000
what are the intended use cases of this? How was it evaluated? And not just, like, okay,

394
00:33:24,000 --> 00:33:28,720
is it looking at accuracy, calibration, or, like, precision? But also, how is it evaluated against

395
00:33:28,720 --> 00:33:32,720
different demographic groups? How is it evaluated against the intersection of those demographic

396
00:33:32,720 --> 00:33:37,120
groups? Let's bring in intersectional analysis into how we do data science and machine learning.

397
00:33:37,840 --> 00:33:42,240
And I think also the key thing is, like, what are those ethical concerns? So if you do design a

398
00:33:42,240 --> 00:33:47,280
sexuality classifier, what are the ethical concerns with that? Like, how could this be used to harm

399
00:33:47,280 --> 00:33:50,720
people or be weaponized against people in countries where homosexuality is illegal?

400
00:33:51,680 --> 00:33:55,520
And the key thing about this is, like, more transparent model reporting will also better

401
00:33:55,520 --> 00:33:58,640
communicate, like, how people should use our models. Because right now we sort of just, like,

402
00:33:58,640 --> 00:34:03,360
okay, I want to do computer vision. Let's grab, like, YoloNet or AlexNet and just, like, apply to

403
00:34:03,360 --> 00:34:08,480
all our problems, even though, like, those models are not really designed for your specific problem.

404
00:34:12,720 --> 00:34:17,600
And finally, there's also this tool, D.ON, which is an ethical checklist for data science projects.

405
00:34:17,600 --> 00:34:23,360
So this is D.ON. It's produced by an organization called Driven Data. They do, like, consulting with,

406
00:34:23,360 --> 00:34:27,440
like, non-profits, government organizations, and they also run data science for good competitions.

407
00:34:27,440 --> 00:34:34,480
So think, like, Kaggle competitions, but with the definite emphasis on helping, like,

408
00:34:34,480 --> 00:34:38,480
some social good aspect of, like, helping teachers, helping education, helping

409
00:34:39,920 --> 00:34:45,840
drug treatment, for example. And what this tool does, it creates a markdown file in your repo

410
00:34:45,840 --> 00:34:49,200
with a checklist that checks for different things. So, like, how is the data collected?

411
00:34:49,200 --> 00:34:53,360
Was it collected in a fair way? Did it actually cover all the groups of people you need to cover?

412
00:34:53,360 --> 00:34:57,200
Did you exclude a group of people? And also, were people informed of how their data set was

413
00:34:57,200 --> 00:35:01,200
collected? Like, informed consent is important in the social sciences. Like, you can't get

414
00:35:01,200 --> 00:35:05,200
research approval unless you have informed consent for your participants. But on the other hand,

415
00:35:05,200 --> 00:35:09,520
in data science, we're just able to run A-B tests on our users without telling them what we're doing

416
00:35:09,520 --> 00:35:15,600
so. And you can run some pretty horrible A-B tests on people that, like, why the hell did you do this

417
00:35:15,600 --> 00:35:20,880
without having to inform them? And then also think about data storage. And I think this is important

418
00:35:20,880 --> 00:35:26,000
because we now sort of have this idea of, like, right to be forgotten so that if someone doesn't

419
00:35:26,000 --> 00:35:30,480
want their data to be used in an algorithm, they should be able to say, I don't want my data to be

420
00:35:30,480 --> 00:35:33,600
used here. Delete it from your records. And you should be able to do that easily. You should be

421
00:35:33,600 --> 00:35:38,480
able to comply with that. That's part of GDPR. And then also, when you deploy the model, how do you

422
00:35:38,480 --> 00:35:43,040
think about taking it down? If it does come out that, okay, there's a new story that says, oh,

423
00:35:43,040 --> 00:35:46,960
your model's harming these groups of people, how do you take your model out of production?

424
00:35:46,960 --> 00:35:50,960
I think it's important to think about this beforehand. And I do want to know that, like,

425
00:35:50,960 --> 00:35:55,200
these things, like the model cards, the data sheets, and this checklist, they're not foolproof.

426
00:35:56,160 --> 00:36:00,720
It's not a foolproof way to prevent you from harming groups of people that you don't want to

427
00:36:00,720 --> 00:36:06,080
harm. But I do think it's a sort of good first step. It sort of helps us be a little less stupid

428
00:36:06,080 --> 00:36:12,880
when we do data science. And finally, I want to give a special shout out to the AI Now Institute.

429
00:36:12,880 --> 00:36:19,760
So this is an NYU research institute that looks at the cultural and societal impact of AI. And so

430
00:36:19,760 --> 00:36:24,080
they hold this annual symposium on ethics, organizing, and accountability. And they also

431
00:36:24,080 --> 00:36:28,240
recently produced this paper talking about the diversity crisis in AI. So a lot of the things,

432
00:36:28,960 --> 00:36:33,280
all the issues with AI now, sort of like, we don't really have diversity. We don't have

433
00:36:33,280 --> 00:36:40,000
different voices or different opinions in the room. And this makes a lot of ad hoc solutions to, like,

434
00:36:41,040 --> 00:36:45,200
algorithmic fairness, like, kind of moot. Okay, if you talk about, okay, instead of just having an

435
00:36:45,200 --> 00:36:50,720
algorithm, an automated algorithm, you just have a human in a loop to check decision. Well, if most

436
00:36:50,720 --> 00:36:55,440
of your humans in a loop are, like, straight white men, they don't really understand the lived

437
00:36:55,440 --> 00:37:00,560
experience of non-straight white men. And they won't be able to check for decisions that your

438
00:37:00,560 --> 00:37:04,720
algorithm makes that they don't realize that this is an issue for other people who are different

439
00:37:04,720 --> 00:37:08,560
from them. And so I think it's important because it also talks about, like, some of the more

440
00:37:08,560 --> 00:37:13,440
technical, long-term technical consequences of not having diversity in AI and machine

441
00:37:13,440 --> 00:37:19,360
learning and data science. And so let's really end the talk. Here are all the papers I referenced

442
00:37:19,360 --> 00:37:24,240
throughout the talk if you want to check them out afterwards. And, yeah, I hope this talk was

443
00:37:24,240 --> 00:37:28,320
engaging. I hope you all learned something. And if you have the time, I hope you check out the

444
00:37:28,320 --> 00:37:31,200
other talk on measuring model fairness.

