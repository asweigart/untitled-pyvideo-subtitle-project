1
00:00:00,000 --> 00:00:19,240
Hey everyone. Thanks very much for coming to this tutorial this afternoon. So I'm Kevin.

2
00:00:19,240 --> 00:00:26,120
I'll be the instructor for this course. Just a little bit about me. I work at a company

3
00:00:26,120 --> 00:00:32,360
called Cambridge Park. We're doing courses in data science and all sorts of workshops

4
00:00:32,360 --> 00:00:40,400
a bit like this one. Just a little bit. So I've sent an email yesterday. I've seen at

5
00:00:40,400 --> 00:00:47,200
least some of you receive it. I already got a pull request on my repository. So you'll

6
00:00:48,200 --> 00:00:56,480
you've got that email, you'll see a link to GitHub where I've hosted all the content for

7
00:00:56,480 --> 00:01:06,040
today. So if you check the Read Me, I've got some instructions on how to install everything.

8
00:01:06,040 --> 00:01:11,320
So you've got two options. First one is you set it up on your machine. So it's just a

9
00:01:11,320 --> 00:01:16,520
matter of installing the requirements. So you've got a requirements.txt with all the

10
00:01:16,520 --> 00:01:21,800
libraries and the versions you need. And then start a Jupyter Notebook. So it's all

11
00:01:21,800 --> 00:01:28,520
based in Notebook. If you haven't done that, you can either do it now, but it might be

12
00:01:28,520 --> 00:01:34,360
a bit slow on the network. Otherwise, you've got a second option, which is to use Google

13
00:01:34,360 --> 00:01:38,960
Collab. So I don't know if you've all used it before. But basically, you just click on

14
00:01:39,960 --> 00:01:46,960
one of those links, and it should bring you to Google Collab. Yep. And so you've got the

15
00:01:46,960 --> 00:01:53,200
Notebooks loaded there, and you can just work in the browser directly. There is an extra

16
00:01:53,200 --> 00:01:57,280
step at the beginning where you're just setting up and installing all the libraries. So that

17
00:01:57,280 --> 00:02:04,280
might take a minute or so, and then you can get started. So that's it for the install

18
00:02:05,280 --> 00:02:12,280
instructions. Then just a quick word about the structure of the repo. So we'll have three

19
00:02:12,280 --> 00:02:19,280
main Notebooks that we're going to work through today. Each Notebook is divided, has two versions,

20
00:02:19,280 --> 00:02:23,160
version skeleton, which has some holes in it, and that's where you're going to have

21
00:02:23,160 --> 00:02:28,520
to work on. And then the version solution, if you just want to go straight to the solution

22
00:02:28,520 --> 00:02:35,520
and see how to do the different exercises. Yeah. And then this morning, someone opened

23
00:02:37,640 --> 00:02:42,640
a pull request to add an environment YAML. So thank you very much for that. So if you're

24
00:02:42,640 --> 00:02:48,480
using Anaconda, you can also directly install, create a new environment with it. Yes, that's

25
00:02:48,480 --> 00:02:55,480
pretty much it. And I haven't added the slides yet, but I will just after this talk, I'll

26
00:02:56,280 --> 00:03:03,280
push them there so you can also access them. Right. So this tutorial is about model interpretability.

27
00:03:06,720 --> 00:03:13,720
So that's how we will divide the time. So first an introduction, explaining a bit why

28
00:03:14,080 --> 00:03:19,520
do we even care about it? Why do we want to do that? And then we'll see three main libraries.

29
00:03:19,520 --> 00:03:25,200
So the first one, Eli5, and a technique called permutation importance. That is the most

30
00:03:25,200 --> 00:03:31,120
basic place to get started for model interpretability. And then two other libraries that are a bit

31
00:03:31,120 --> 00:03:37,620
more advanced. The first one, LIME, stands for local interpretable model agnostic explanations.

32
00:03:37,620 --> 00:03:44,620
And then SHAP, which is probably the most advanced one here. Just another thing, we'll

33
00:03:44,760 --> 00:03:50,480
have a break at 3 p.m. So it's in about an hour and a half, so right in the middle of

34
00:03:50,480 --> 00:03:57,480
the session. Cool. So first, why do we care about model interpretability? Why do we want

35
00:04:00,880 --> 00:04:07,880
to do that? Well, let's take this scenario. So you've worked quite hard on a project.

36
00:04:09,800 --> 00:04:14,240
You've cleaned and preprocessed some really messy data. You've engineered some really

37
00:04:14,240 --> 00:04:20,080
fancy features. You're really happy with that. You've done a lot of work to select the best

38
00:04:20,080 --> 00:04:25,280
model and tune it properly. And then you've got a final model that has really good performance

39
00:04:25,280 --> 00:04:32,280
on some test set. So you're pretty proud of that. You're pretty happy. But then there's

40
00:04:32,400 --> 00:04:39,080
a catch. Your boss comes to you and is asking, well, just one more thing. Can you explain

41
00:04:39,080 --> 00:04:44,280
how your model works? Can you explain how is your model taking decisions? What does

42
00:04:44,280 --> 00:04:51,280
your model care about in the data? And that, unless you take the steps to actually try

43
00:04:51,760 --> 00:04:57,400
to understand your model, you usually not know the answer. At least it's not straightforward.

44
00:04:57,400 --> 00:05:04,400
So that's what we're going to do today. So just a quick word. So you all know that algorithms

45
00:05:04,400 --> 00:05:11,400
are everywhere and more and more industries are starting using machine learning in their

46
00:05:11,400 --> 00:05:16,400
work. So you'll have things like insurance or banks or even the police trying to build

47
00:05:16,400 --> 00:05:21,400
models to predict all sorts of things. And that actually has an impact on people, the

48
00:05:21,400 --> 00:05:26,400
predictions that those models will do. So if you work for an insurance, maybe it will

49
00:05:26,400 --> 00:05:33,400
define whether or not, how much you're going to be charged for a specific model. And then

50
00:05:34,400 --> 00:05:38,000
if you work for a specific plan or if you work for a bank, your model might decide who

51
00:05:38,000 --> 00:05:44,480
is going to get a loan or not. So it's really important to know how your model works and

52
00:05:44,480 --> 00:05:49,640
how it takes its decisions. If you work for a bank, you don't want your model to have

53
00:05:49,640 --> 00:05:56,640
any sort of bias in who it's going to give a loan to or not. And another example that

54
00:05:56,640 --> 00:06:01,720
I quite like is social media. So it might look like a less important one than the three

55
00:06:02,040 --> 00:06:09,040
others here, but I think it can have an equally bad impact on people. So let's say you're

56
00:06:09,800 --> 00:06:15,720
working on a model that should predict who is most likely to click on an ad on whatever

57
00:06:15,720 --> 00:06:21,120
social media you're working for. And let's say you're working for an advertiser that,

58
00:06:21,120 --> 00:06:28,120
I don't know, is doing online gambling. So then your model, if you don't take care, will

59
00:06:29,120 --> 00:06:35,120
probably try to target someone who is addicted to gambling. And we'll try to find in the

60
00:06:35,120 --> 00:06:40,120
data set some features that are good proxy to know who is addicted to gambling, because

61
00:06:40,120 --> 00:06:45,520
that's those are the people that would be the most likely to click on an ad. So that

62
00:06:45,520 --> 00:06:52,520
can have a really bad impact on people, even though that seems to be a fairly simple thing.

63
00:06:53,480 --> 00:06:59,480
So all of that to say that black box models are not really an option, and you will need

64
00:06:59,480 --> 00:07:04,200
to interpret them if you work in all those fields where you might have quite a bad impact

65
00:07:04,200 --> 00:07:11,200
on, I mean, powerful impact on people. So I like to see interpretability having three

66
00:07:14,640 --> 00:07:21,400
main goals. So the first one is building trust in your model. So here, another example, you're

67
00:07:21,400 --> 00:07:26,880
trying to predict employees' performance for a large organization. And the data you've

68
00:07:26,880 --> 00:07:33,400
got available is performance reviews from the past 10 years, for example. But then what

69
00:07:33,400 --> 00:07:39,720
if the company just tends to promote men more than women? Then your model, the only thing

70
00:07:39,720 --> 00:07:46,720
your model is going to do is try to mimic this process that your company already has

71
00:07:47,280 --> 00:07:53,000
and will learn from this bias. So even if you're quite happy with your model at the

72
00:07:53,000 --> 00:07:58,840
end, your model has a really high accuracy, the only thing it will be doing is looking

73
00:07:58,840 --> 00:08:05,080
at the gender as a priority to predict the performance of people, which if you don't

74
00:08:05,080 --> 00:08:09,680
actually look inside the model how it works, you will not necessarily notice, and you'll

75
00:08:09,680 --> 00:08:15,120
end up putting that in production and potentially be really damaging. So there's a really good

76
00:08:15,120 --> 00:08:22,120
quote from Cathy O'Neill about that, which is models are opinions embedded in mathematics,

77
00:08:22,720 --> 00:08:27,080
so your model is only as good as your data, and it's really important to understand what

78
00:08:27,080 --> 00:08:34,080
your model is picking in the data to make its predictions. The second kind of aspect

79
00:08:35,680 --> 00:08:42,680
that I like to see is it helps decision makers. So let's say, for example, you work for a

80
00:08:42,680 --> 00:08:47,120
hospital, and you're going to have to predict the likelihood of a patient to develop, I

81
00:08:47,120 --> 00:08:53,160
don't know, some disease X, and you've got access to symptoms and information about all

82
00:08:53,160 --> 00:08:58,720
your patients, and you want to know whether or not they're going to have this disease.

83
00:08:58,720 --> 00:09:05,720
So here your model itself will, you'll never put your model in production to directly give

84
00:09:06,240 --> 00:09:11,120
a treatment to people or anything like that. You want this model to be used by doctors

85
00:09:11,440 --> 00:09:17,440
to help them with their diagnosis. So the doctor will have to not take the prediction

86
00:09:17,440 --> 00:09:23,640
itself, but understand how the prediction was made, then try to understand if that makes

87
00:09:23,640 --> 00:09:30,640
any sense or not, and just come up with their diagnosis based on that. So here the model

88
00:09:30,960 --> 00:09:37,960
is only to empower the decision maker. And finally, another example I quite like is using

89
00:09:38,640 --> 00:09:45,640
model interpretability to debug your model. So here an example that is used in the paper

90
00:09:45,640 --> 00:09:51,640
from Lime, which is one of the libraries we'll see today, is the following. So you've got

91
00:09:51,640 --> 00:09:58,640
images of wolves and huskies, and you're trying to build a model that will predict, classify

92
00:09:59,360 --> 00:10:06,360
those two classes. But then the problem, so they did, people actually built really good

93
00:10:07,960 --> 00:10:13,040
model, has a really good accuracy, and they were really happy with that. It predicts,

94
00:10:13,040 --> 00:10:20,040
I don't know, 99 point something accuracy, what is a wolf, what is a husky. But then

95
00:10:20,400 --> 00:10:27,000
when they actually used the library Lime, that we'll see later on this model to try

96
00:10:27,000 --> 00:10:32,520
to understand what was the model looking at in the data to make its predictions, they

97
00:10:32,520 --> 00:10:37,160
noticed that the model was just looking in the background, because the pictures of huskies

98
00:10:37,160 --> 00:10:44,160
or wolves, I forgot which one, tend to have more snow in the background. And that's what

99
00:10:44,160 --> 00:10:49,160
they noticed here, when they have the models provided with this picture here, it was only

100
00:10:49,160 --> 00:10:53,320
looking at the background and saying, if there is snow, it's probably a husky, if there is

101
00:10:53,320 --> 00:10:58,240
no snow, it's probably a wolf. So the problem here is that you're really happy, you have

102
00:10:58,240 --> 00:11:02,200
your fancy model, you think it's a really good classifier, but it's just basically a

103
00:11:02,440 --> 00:11:08,440
snow detector. It can only, it's only looking at the background, is there snow or not. So

104
00:11:08,440 --> 00:11:14,320
that's something that here, model interpreting your model would have helped actually seeing

105
00:11:14,320 --> 00:11:21,320
that, and not putting into production before debugging it.

106
00:11:21,760 --> 00:11:28,760
So then what do we do? Well, first some models will be easy to interpret by design, so something

107
00:11:28,760 --> 00:11:33,760
like a linear regression, for example, you'll have a weight on each feature, and it's really

108
00:11:33,760 --> 00:11:38,760
easy for you to interpret exactly, to see exactly what's the impact of each single feature.

109
00:11:38,760 --> 00:11:44,760
So here, if you've got something like that, increasing X1 by one single unit will increase

110
00:11:44,760 --> 00:11:49,760
the output by three units. So that's super easy for you to actually explain that model

111
00:11:49,760 --> 00:11:56,760
to anyone. Same thing for decision tree, right? You've got the exact decision path that you

112
00:11:56,760 --> 00:12:03,760
have to predict something. So here, that's I think some model from the Titanic, and we

113
00:12:06,240 --> 00:12:13,240
see that, well basically, if you're a male, you're probably in trouble, because it goes

114
00:12:14,120 --> 00:12:21,120
pretty bad from there, and you're the most likely to die. So you can explain exactly

115
00:12:21,680 --> 00:12:26,760
how your model made prediction, and that's pretty good.

116
00:12:26,760 --> 00:12:31,200
Some other models will be harder to interpret. So here, you've got the example of an ensemble

117
00:12:31,200 --> 00:12:37,440
model as an ensemble of trees, something like a random forest, and here, it's a bit harder

118
00:12:37,440 --> 00:12:43,240
to understand the role of each single feature, because you don't have a single tree anymore,

119
00:12:43,240 --> 00:12:47,160
you don't have a single decision path, you've got multiple trees that are grouped together

120
00:12:47,280 --> 00:12:52,920
in some way. So the good thing is that random forest, since they're based on trees, they

121
00:12:52,920 --> 00:12:58,400
can still provide you with a feature importance, but the feature importance is usually just

122
00:12:58,400 --> 00:13:04,800
given number to each feature, like kind of weighting how important it seems to be for

123
00:13:04,800 --> 00:13:09,980
your model, but you don't know how a single prediction was made, you don't know exactly

124
00:13:09,980 --> 00:13:14,880
how that feature impacts the outcome, is it positive, is it negative, you just know that

125
00:13:14,880 --> 00:13:21,080
it seems to be more important than others. So still not extremely bad, but you won't

126
00:13:21,080 --> 00:13:27,840
be able to explain your model properly. And then some other models are just really complicated.

127
00:13:27,840 --> 00:13:35,680
So something like a deep neural network, it's really hard to actually explain how changing

128
00:13:35,680 --> 00:13:40,360
the input layer will affect your output layer, and it's really hard to explain to someone

129
00:13:40,360 --> 00:13:47,360
whenever your deep neural network has made any prediction, how do you explain what it

130
00:13:47,360 --> 00:13:54,080
cared about, what it picked in the data. So those models will be seen more as black boxes.

131
00:13:54,080 --> 00:14:00,580
So great, a bit of a joy killer here. Does that mean that you've got to stick to only

132
00:14:00,580 --> 00:14:07,920
simple models if you care about interpretability? Well, sticking to simple models at least has

133
00:14:07,920 --> 00:14:14,360
the advantage that the interpretability will be granted. So you're sure that you'll be

134
00:14:14,360 --> 00:14:19,560
able to explain your model properly, but then there's a trade-off between the interpretability

135
00:14:19,560 --> 00:14:25,300
and the performance that you get from your data. And then the second option is to use

136
00:14:25,300 --> 00:14:29,840
some of those techniques that we'll see today, which are model agnostics, so they'll work

137
00:14:30,840 --> 00:14:38,280
any sort of model of arbitrary complexity, and they'll allow you to have both the performance

138
00:14:38,280 --> 00:14:45,280
and the interpretability. So that's the motivation for what we'll do today. Just a quick word

139
00:14:45,280 --> 00:14:54,280
on two different types of interpretation before we get started. So I like to divide it as

140
00:14:54,680 --> 00:15:01,680
you've got the local interpretations, so those will be basically explaining how a single

141
00:15:01,680 --> 00:15:08,680
prediction was made. For example, in the case of our doctor, for a single person, we predicted

142
00:15:08,680 --> 00:15:15,680
that they will have a disease Y or X. How do you explain to the doctor how that specific

143
00:15:15,960 --> 00:15:22,320
prediction was made, what attributes of the patient was important towards making this

144
00:15:22,320 --> 00:15:27,360
decision. And then the global interpretation is more explaining how overall your model

145
00:15:27,360 --> 00:15:32,480
will work, what on average your model seems to care about, what it doesn't seem to care

146
00:15:32,480 --> 00:15:36,520
about. So that's the example that we saw with the husky maybe. Like you would like to know

147
00:15:36,520 --> 00:15:41,000
that on average your model tends to look mostly at the background and not actually on the

148
00:15:41,000 --> 00:15:48,000
animal itself. Or for the case of the hospital, it's also important to explain that those

149
00:15:49,000 --> 00:15:53,600
symptoms seem to be important for your model, whereas those other symptoms are never looked

150
00:15:53,600 --> 00:15:59,120
at. So those would be the two different kinds of interpretation we can do and we'll see

151
00:15:59,120 --> 00:16:06,120
how all the libraries we'll see today provide one or the other or sometimes both. Right,

152
00:16:07,400 --> 00:16:14,400
so let's get started with the first library for today, Eli5. So Eli5 stands for Explain

153
00:16:15,400 --> 00:16:22,400
Like M5. So I think there's a subreddit for that where they explain all sorts of things.

154
00:16:28,560 --> 00:16:34,760
So Eli5 will have two things. It implements something called permutation importance that

155
00:16:34,760 --> 00:16:41,760
is an algorithm we'll see a bit later. So Eli5, it's a Python implementation that is

156
00:16:44,880 --> 00:16:50,840
an algorithm that works with all models that have the scikit-learn API. So any sort of

157
00:16:50,840 --> 00:16:54,760
model that will have a dot fit dot predict, so that means every model from scikit-learn,

158
00:16:54,760 --> 00:17:01,760
that means xgboost, lightgbm, catboost, any library that implements a scikit-learn API.

159
00:17:04,280 --> 00:17:08,880
If you've got white box models, so like models like linear regression, decision tree that

160
00:17:08,880 --> 00:17:14,320
are actually easy to open and look at what's going on inside, you'll be able to create

161
00:17:14,320 --> 00:17:19,560
a nice visualization that gives you both a local interpretation and a global interpretation.

162
00:17:19,560 --> 00:17:24,680
So it can be really useful to communicate the result of your simple models. Now if you've

163
00:17:24,680 --> 00:17:30,520
got more complex models like, I don't know, like some boosting algorithms, some neural

164
00:17:30,520 --> 00:17:36,120
networks, it implements another technique called permutation importance, but here it's

165
00:17:36,120 --> 00:17:41,140
only limited to global interpretation, so you won't be able to explain a specific prediction.

166
00:17:41,260 --> 00:17:48,260
So that's the plan for this first exercise. So just to show you a bit what it will look

167
00:17:49,540 --> 00:17:55,900
like, so if you're applying Eli-5 on a white box model, so here I think I've applied it

168
00:17:55,900 --> 00:18:02,160
on a linear logistic regression on the breast cancer data set, so here it gives you something

169
00:18:02,160 --> 00:18:06,900
like that. So if you want to explain your model globally, you're just calling the function

170
00:18:06,900 --> 00:18:12,900
show weights from Eli-5, passing your model, and you will be able to look inside, try to

171
00:18:12,900 --> 00:18:18,540
find the weights, and give you some output like that, which I think it's quite nice to

172
00:18:18,540 --> 00:18:23,420
explain to some decision maker how your model works. So here, that's the breast cancer data

173
00:18:23,420 --> 00:18:27,940
set. If I want to go to a doctor and explain what my model seems to care about in the data,

174
00:18:27,940 --> 00:18:34,940
I just go with this table and I show that, well, the most important, the three most important

175
00:18:35,940 --> 00:18:42,940
features seem to be those three here, asking does it seem to make sense with your intuition

176
00:18:42,940 --> 00:18:48,500
and your experience, and those features here seem to have a negative impact. So I think

177
00:18:48,500 --> 00:18:55,500
here we're predicting whether something is cancerous or not. So in the first, those features

178
00:18:56,380 --> 00:19:03,380
will increase the probability of it being cancerous. Those features here will decrease

179
00:19:04,300 --> 00:19:09,220
that probability. So I can start to explain what my model seems to care about when it

180
00:19:09,220 --> 00:19:16,100
comes to making decisions based on this data. Then as I said earlier, Eli-5 on a white box

181
00:19:16,100 --> 00:19:22,340
model can also explain how a single prediction was made. So here, if I want to explain a

182
00:19:22,380 --> 00:19:29,220
given prediction on a given observation, so for a specific patient, I just call the show

183
00:19:29,220 --> 00:19:34,540
prediction function, passing my model and the observation, and it gives me a table that

184
00:19:34,540 --> 00:19:40,860
is really similar where it tells me exactly the contribution of each feature. So here

185
00:19:40,860 --> 00:19:47,860
I know that the fact that the worst area contributed that amount to the final decision and the

186
00:19:48,300 --> 00:19:55,300
final probability is that the cells aren't cancerous with a probability of .79. So I

187
00:20:01,140 --> 00:20:08,140
can really explain how that specific prediction was made by my model and justify what's going

188
00:20:08,180 --> 00:20:14,260
on. Now if your model isn't white box, so if you can't actually open it and look at

189
00:20:14,300 --> 00:20:21,300
what's going on inside, Eli-5 also implements a method called permutation importance. So

190
00:20:21,300 --> 00:20:26,060
this one will be more diagnostic. You can use it on any type of model. It doesn't need

191
00:20:26,060 --> 00:20:33,060
to open it. It will provide unfortunately only a global interpretation. So do you have

192
00:20:33,060 --> 00:20:40,060
to use the same method? Not even. So those ones here work only for white box models,

193
00:20:45,780 --> 00:20:51,060
so when it can actually find the weights directly in the model. So here those weights are exactly

194
00:20:51,060 --> 00:20:58,060
the weights of your logistic regression. Here the weights are, the contribution is the weight

195
00:20:58,380 --> 00:21:05,380
times the actual value of your, yeah. So Eli-5 wouldn't tell that to you. If you actually

196
00:21:16,340 --> 00:21:22,060
created a term yourself for the interaction, then it will just show that the multiplication

197
00:21:22,060 --> 00:21:29,060
is the same. So that's for white box models. Then as I said, you can use it on black box

198
00:21:34,380 --> 00:21:40,380
models as well, but you'll only get a global interpretation. And here even, it's not even

199
00:21:40,380 --> 00:21:44,060
that good either because it only gives you an amplitude. So really something similar

200
00:21:44,060 --> 00:21:48,380
to the feature importance that you're probably used to on random forest or decision trees

201
00:21:48,380 --> 00:21:55,380
where you have a number associated to each feature and tells you how relatively to each

202
00:21:55,380 --> 00:21:59,460
other they're important, but you don't know in which direction they affect the outcome

203
00:21:59,460 --> 00:22:06,460
or whatever. So it's quite limited, but it's a nice place to start. So the way that works,

204
00:22:07,500 --> 00:22:14,500
briefly, you'll get your data set and your model that you want to explain and you'll

205
00:22:15,060 --> 00:22:22,060
focus on each feature one by one. What you'll do is first for a given feature you'll shuffle

206
00:22:22,060 --> 00:22:29,060
all the values in the provided data sets to kind of make this feature not powerful anymore.

207
00:22:31,340 --> 00:22:35,500
Like whatever your model used to pick and to care about in this feature, since it has

208
00:22:35,500 --> 00:22:41,260
been shuffled, it's not supposed to make any sense anymore. And then you'll generate predictions

209
00:22:41,500 --> 00:22:46,300
using this new data set, using your model on this new data set, and you'll compute the

210
00:22:46,300 --> 00:22:50,460
decrease in accuracy. So you're expecting the accuracy to go down once you've shuffled

211
00:22:50,460 --> 00:22:57,100
that feature, and this decrease in accuracy corresponds to the impact of the importance

212
00:22:57,100 --> 00:23:00,100
of that feature for your model. Yes?

213
00:23:00,100 --> 00:23:07,100
Can you describe the shuffling more? If we had a bunch of comments that are totally different,

214
00:23:07,100 --> 00:23:08,100
do you think shuffling would be worse, but is it between different?

215
00:23:08,500 --> 00:23:14,260
So for each feature, you pick a feature, you've got all the values and you shuffle completely

216
00:23:14,260 --> 00:23:21,260
all the values for that specific column. You don't mix the columns together. So shuffling

217
00:23:23,300 --> 00:23:30,020
is kind of equivalent to deactivating that feature or deactivating its predictive power.

218
00:23:30,020 --> 00:23:33,900
And then you just have to compare the impact on accuracy of each of the features and that

219
00:23:33,900 --> 00:23:38,900
gives you the importance of the feature. So it's a pretty simple method.

220
00:23:38,900 --> 00:23:45,140
And that's how it would work with Eli5. So you'd import the permutation importance model

221
00:23:45,140 --> 00:23:50,140
that has scikit-learn APIs. It's quite nice. You just import it, instantiate it with your

222
00:23:50,140 --> 00:23:56,220
model, then you call .fit on your data set, whichever data set you're trying to explain,

223
00:23:56,220 --> 00:24:01,220
and then you just call show weights, same way we did before, in order to display this.

224
00:24:01,780 --> 00:24:07,620
So in fact, it will not shuffle it only once, it will shuffle it a couple of times, return

225
00:24:07,620 --> 00:24:14,140
the average importance of that feature and the standard deviation as well. So you get

226
00:24:14,140 --> 00:24:18,860
a little bit more information, but you still don't know in which way this affects your

227
00:24:18,860 --> 00:24:25,860
model. If we go back here, we see that each feature, I know if it affects the outcome

228
00:24:26,860 --> 00:24:32,260
positively or negatively, because that's a white box model with permutation importance,

229
00:24:32,260 --> 00:24:39,260
I only have this, I only have this, which is just the amplitude of the impact.

230
00:24:40,700 --> 00:24:47,700
Right so any questions about that? Yes?

231
00:24:47,700 --> 00:24:54,700
So you don't retrain the model, you just generate predictions. Yeah, okay. Cool, so

232
00:25:03,140 --> 00:25:10,140
we've got the first practical then, which you can either open this notebook if you've

233
00:25:10,140 --> 00:25:17,140
got it locally, if not, you can just go there on Google collab, click on skeleton, and

234
00:25:25,060 --> 00:25:30,620
you'll have that. I think yours might look a bit different because you're not the owner

235
00:25:30,620 --> 00:25:35,940
of the notebook, so I think there'll be something open in playground or something like that,

236
00:25:36,020 --> 00:25:42,180
you can click on, that will just open it on your own Google drive, and then you can start

237
00:25:42,180 --> 00:25:49,180
working on it. So the way we'll do that is that you can all start going through this

238
00:25:53,060 --> 00:26:00,060
notebook, I'm here helping out, I'll walk and answer all the questions, and then in,

239
00:26:01,060 --> 00:26:08,060
let's say, okay the time is not right here, let's say in 20 minutes I'll go through the

240
00:26:09,060 --> 00:26:13,340
notebook and we'll go through it together and I'll take all the questions and everything

241
00:26:13,340 --> 00:26:20,340
and explain what's going on. Cool. 20 minutes, 25, let's see, depends how long,

242
00:26:30,060 --> 00:26:37,060
how long. So let's go through it together at 10 past.

243
00:27:00,060 --> 00:27:07,060
Okay.

244
00:27:30,060 --> 00:27:37,060
Okay.

245
00:28:00,060 --> 00:28:07,060
Okay.

246
00:28:30,060 --> 00:28:37,060
Okay.

247
00:29:00,060 --> 00:29:07,060
Okay.

248
00:29:30,060 --> 00:29:37,060
Okay.

249
00:30:00,060 --> 00:30:07,060
Okay.

250
00:30:30,060 --> 00:30:37,060
Okay.

251
00:31:00,060 --> 00:31:07,060
Okay.

252
00:31:30,060 --> 00:31:37,060
Okay.

253
00:32:00,060 --> 00:32:07,060
Okay.

254
00:32:30,060 --> 00:32:37,060
Okay.

255
00:33:00,060 --> 00:33:07,060
Okay.

256
00:33:30,060 --> 00:33:37,060
Okay.

257
00:34:00,060 --> 00:34:07,060
Okay.

258
00:34:30,060 --> 00:34:37,060
Okay.

259
00:35:00,060 --> 00:35:07,060
Okay.

260
00:35:30,060 --> 00:35:37,060
Okay.

261
00:36:00,060 --> 00:36:07,060
Okay.

262
00:36:30,060 --> 00:36:37,060
Okay.

263
00:37:00,060 --> 00:37:07,060
Okay.

264
00:37:30,060 --> 00:37:37,060
Okay.

265
00:38:00,060 --> 00:38:07,060
Okay.

266
00:38:30,060 --> 00:38:37,060
Okay.

267
00:39:00,060 --> 00:39:07,060
Okay.

268
00:39:30,060 --> 00:39:37,060
Okay.

269
00:40:00,060 --> 00:40:07,060
Okay.

270
00:40:30,060 --> 00:40:37,060
Okay.

271
00:41:00,060 --> 00:41:07,060
Okay.

272
00:41:30,060 --> 00:41:37,060
Okay.

273
00:42:00,060 --> 00:42:07,060
Okay.

274
00:42:30,060 --> 00:42:37,060
Okay.

275
00:43:00,060 --> 00:43:07,060
Okay.

276
00:43:30,060 --> 00:43:37,060
Okay.

277
00:43:37,060 --> 00:43:44,060
Okay.

278
00:43:44,060 --> 00:43:51,060
Okay.

279
00:44:14,060 --> 00:44:21,060
Okay.

280
00:44:44,060 --> 00:44:51,060
Okay.

281
00:45:14,060 --> 00:45:21,060
Okay.

282
00:45:44,060 --> 00:45:51,060
Okay.

283
00:45:51,060 --> 00:45:58,060
Okay.

284
00:45:58,060 --> 00:46:05,060
Okay.

285
00:46:05,060 --> 00:46:12,060
Okay.

286
00:46:12,060 --> 00:46:19,060
So you've got that format as data frame.

287
00:46:19,060 --> 00:46:26,060
Okay.

288
00:46:26,060 --> 00:46:33,060
Okay.

289
00:46:33,060 --> 00:46:40,060
Okay.

290
00:46:40,060 --> 00:46:47,060
Okay.

291
00:46:47,060 --> 00:46:54,060
Okay.

292
00:46:54,060 --> 00:47:01,060
Okay.

293
00:47:01,060 --> 00:47:08,060
Okay.

294
00:47:08,060 --> 00:47:15,060
Okay.

295
00:47:15,060 --> 00:47:17,060
All right.

296
00:47:17,060 --> 00:47:22,060
Do you want a little bit more time or should we go through it together?

297
00:47:22,060 --> 00:47:24,060
What do you prefer?

298
00:47:24,060 --> 00:47:27,060
Okay.

299
00:47:27,060 --> 00:47:31,060
So let's say that I go through it.

300
00:47:31,060 --> 00:47:32,060
Cool.

301
00:47:32,060 --> 00:47:36,060
So first thing we do is importing a bunch of things.

302
00:47:36,060 --> 00:47:39,060
So just a quick explanation of what we've got.

303
00:47:39,060 --> 00:47:41,060
So we've got pandas, of course.

304
00:47:41,060 --> 00:47:44,060
We've got some tools that we'll use from Psychiclone.

305
00:47:44,060 --> 00:47:46,060
So just to process the data.

306
00:47:46,060 --> 00:47:49,060
So the one hot encoder and the column transformer.

307
00:47:49,060 --> 00:47:56,060
And then the grid search CV and the train test plates just in order to tune our model properly.

308
00:47:56,060 --> 00:47:58,060
And then we're importing a couple of metrics.

309
00:47:58,060 --> 00:48:02,060
So the balance accuracy score, which is just the accuracy score,

310
00:48:02,060 --> 00:48:06,060
but taking into account of the imbalance in the data and the classification report.

311
00:48:06,060 --> 00:48:09,060
So we can see a little bit more about the performance of our models.

312
00:48:09,060 --> 00:48:13,060
And then the most important is we're importing four different models.

313
00:48:13,060 --> 00:48:20,060
So that's the models we'll actually try to interpret for all the different,

314
00:48:20,060 --> 00:48:23,060
with all the different libraries we'll see today.

315
00:48:23,060 --> 00:48:26,060
So logistic regression should be fairly easy to interpret that.

316
00:48:26,060 --> 00:48:29,060
Then decision tree, not too hard either.

317
00:48:29,060 --> 00:48:31,060
And then things are starting to get complicated.

318
00:48:31,060 --> 00:48:33,060
So random forest, it's a bit harder.

319
00:48:33,060 --> 00:48:36,060
And boosting, so here let's light GBM.

320
00:48:36,060 --> 00:48:38,060
So it's really similar to XGBoost.

321
00:48:38,060 --> 00:48:41,060
It's the Microsoft version of XGBoost.

322
00:48:41,060 --> 00:48:46,060
And that's probably the most complicated one to interpret.

323
00:48:46,060 --> 00:48:50,060
So let's import all of that and then loading some data.

324
00:48:50,060 --> 00:48:57,060
So the data that we provided here is from a bank in Portugal,

325
00:48:57,060 --> 00:49:01,060
and that corresponds to some marketing campaign that they run.

326
00:49:01,060 --> 00:49:10,060
And they wanted to have their prospects or clients to subscribe to some plan,

327
00:49:10,060 --> 00:49:16,060
I think to some like deposit something, a special plan that they put together.

328
00:49:17,060 --> 00:49:22,060
And so the data has got some information about the different clients,

329
00:49:22,060 --> 00:49:30,060
some demographics, some information about whether they've been defaulted before or not.

330
00:49:30,060 --> 00:49:39,060
And then the targets, what you're trying to predict is whether or not the person subscribes to the plan.

331
00:49:39,060 --> 00:49:42,060
So here our goal will be first we want to model that.

332
00:49:42,060 --> 00:49:47,060
So we want to build a model that can predict given characteristics about the people

333
00:49:47,060 --> 00:49:52,060
and the campaign if that person actually ended up subscribing.

334
00:49:52,060 --> 00:49:55,060
And then the second thing we want to do once we've got that model,

335
00:49:55,060 --> 00:50:00,060
we're pretty happy with the model, is go backwards and explain that model in Tap Reddit

336
00:50:00,060 --> 00:50:03,060
so we can maybe give some recommendations to our marketing team

337
00:50:03,060 --> 00:50:06,060
and tell them well, looks like you did something differently at this point.

338
00:50:06,060 --> 00:50:10,060
Looks like targeting those people is probably more efficient.

339
00:50:10,060 --> 00:50:13,060
That kind of recommendation.

340
00:50:13,060 --> 00:50:14,060
So let's do that.

341
00:50:14,060 --> 00:50:16,060
First we're going to load the data.

342
00:50:16,060 --> 00:50:18,060
So that's what I was talking about.

343
00:50:18,060 --> 00:50:21,060
We've got a lot of different features.

344
00:50:21,060 --> 00:50:24,060
They explained a bit in more details below.

345
00:50:24,060 --> 00:50:30,060
But basically we've got the age of the person, their job, marital status, education,

346
00:50:30,060 --> 00:50:32,060
whether or not they've been in default.

347
00:50:32,060 --> 00:50:36,060
And then you've got information about the campaign itself.

348
00:50:37,060 --> 00:50:41,060
So we know how those people were contacted.

349
00:50:41,060 --> 00:50:45,060
Some were contacted by telephone, some by email.

350
00:50:45,060 --> 00:50:49,060
So we want to know if also that affects the outcome.

351
00:50:49,060 --> 00:50:52,060
They've been contacted at different month of the year,

352
00:50:52,060 --> 00:50:57,060
so we can also see how that changes something, and at different days of the week.

353
00:50:57,060 --> 00:51:00,060
We know also how many times they've been contacted before,

354
00:51:00,060 --> 00:51:02,060
so that's the campaign feature,

355
00:51:02,060 --> 00:51:06,060
and how many days between two times we've contacted them.

356
00:51:06,060 --> 00:51:12,060
So here, if we contacted them only once, P days is going to be the maximum,

357
00:51:12,060 --> 00:51:16,060
because there were no delay between two times they were contacted,

358
00:51:16,060 --> 00:51:19,060
since it was only one time.

359
00:51:19,060 --> 00:51:24,060
But sometimes we've got some people that contacted multiple times in the same week,

360
00:51:24,060 --> 00:51:28,060
which might end up not being efficient, or maybe it's really efficient.

361
00:51:28,060 --> 00:51:30,060
That's what we want to know.

362
00:51:30,060 --> 00:51:36,060
Cool. So with that in mind, just a quick thing that we need to check is the imbalance in your data.

363
00:51:36,060 --> 00:51:44,060
So here we see that most people actually didn't subscribe to our plan, which is a bit disappointing.

364
00:51:44,060 --> 00:51:46,060
But that's something to keep in mind,

365
00:51:46,060 --> 00:51:52,060
because we need to ensure that we don't only look at accuracy straight,

366
00:51:52,060 --> 00:51:56,060
because it wouldn't be representative here.

367
00:51:56,060 --> 00:52:01,060
We also need to stratify when we are creating our train test set.

368
00:52:01,060 --> 00:52:08,060
So first thing is we're proposing a little bit, getting the X, the feature matrix,

369
00:52:08,060 --> 00:52:12,060
with all our features and the target vector Y.

370
00:52:12,060 --> 00:52:18,060
We're mapping a 0 to a 0, yes to a 1, and we're ready to get started.

371
00:52:18,060 --> 00:52:21,060
So here just quickly check the types.

372
00:52:21,060 --> 00:52:26,060
So we've got a few numerical values, so the age, the number of days,

373
00:52:26,060 --> 00:52:30,060
number of times they've been called to campaign,

374
00:52:30,060 --> 00:52:36,060
and we've got some categorical features, so their job, their education, etc.

375
00:52:36,060 --> 00:52:39,060
So you can read more information about all those features right here.

376
00:52:39,060 --> 00:52:42,060
I'm not going to describe everything.

377
00:52:42,060 --> 00:52:46,060
So we're ready to go.

378
00:52:46,060 --> 00:52:51,060
In order to preprocess the data, we'll first define two different lists,

379
00:52:51,060 --> 00:52:55,060
because we want to treat differently the numerical values and the categorical values.

380
00:52:55,060 --> 00:53:01,060
So here I've just provided a list of numerical features, a list of categorical features,

381
00:53:01,060 --> 00:53:05,060
run that, and then I can create a column transformer object.

382
00:53:05,060 --> 00:53:10,060
Who has used a column transformer before?

383
00:53:10,060 --> 00:53:13,060
Only one person? Really? Okay.

384
00:53:13,060 --> 00:53:18,060
So it's fairly new. It was released in Scikit-learn version 20,

385
00:53:18,060 --> 00:53:20,060
so that's probably the latest.

386
00:53:20,060 --> 00:53:27,060
But it's really powerful because it allows you to map a different transformation

387
00:53:27,060 --> 00:53:29,060
on different groups of features.

388
00:53:29,060 --> 00:53:33,060
So here I can easily say, well, for my numerical features,

389
00:53:33,060 --> 00:53:38,060
so that's the list here, numerical features, I'm just deciding to pass through,

390
00:53:38,060 --> 00:53:42,060
means I'm just going to keep them as they are and not changing anything.

391
00:53:42,060 --> 00:53:46,060
But for my categorical features, I decide to use the one hot encoder.

392
00:53:46,060 --> 00:53:53,060
So in just one single object, I can decide what transformations I map on different features.

393
00:53:53,060 --> 00:53:55,060
So it's really powerful.

394
00:53:55,060 --> 00:53:59,060
I just run that, and I've got this object, the preprocessor,

395
00:53:59,060 --> 00:54:05,060
that I can apply on my data and directly get everything out of it.

396
00:54:05,060 --> 00:54:11,060
So once we've got that, we can split the data into training set and test sets,

397
00:54:11,060 --> 00:54:17,060
so I'm just going to pass X and Y. I'm going to do test size equal, let's do 30%,

398
00:54:17,060 --> 00:54:21,060
and I need to make sure I stratify that because it's imbalanced,

399
00:54:21,060 --> 00:54:24,060
so I'll do stratify over Y.

400
00:54:24,060 --> 00:54:31,060
And then out of that, I get X train, X test, Y train, Y test.

401
00:54:31,060 --> 00:54:33,060
Cool, so I'm ready to go.

402
00:54:33,060 --> 00:54:38,060
Now I can actually fit this object to the preprocessor that I've created before.

403
00:54:38,060 --> 00:54:44,060
I train it on the train set and I call .transform in order to actually apply the transformation,

404
00:54:44,060 --> 00:54:47,060
and that returns something like that, so that's a NumPy array.

405
00:54:47,060 --> 00:54:52,060
Not necessarily the easiest to read, but I've got everything there.

406
00:54:52,060 --> 00:55:08,060
The next code provided here is just me trying to wrap the whole thing into a data frame

407
00:55:08,060 --> 00:55:10,060
so it's a bit nicer to visualize and interpret,

408
00:55:10,060 --> 00:55:18,060
so I'm just basically getting the list of all the categories that it has created when it created the dummies,

409
00:55:18,060 --> 00:55:25,060
and I'm here having a list comprehension where I'm basically saying that each column should be called

410
00:55:25,060 --> 00:55:30,060
the name of the feature it maps to, for example, if it's the education,

411
00:55:30,060 --> 00:55:34,060
first it's going to be education double underscore and whatever value it has,

412
00:55:34,060 --> 00:55:40,060
so education university or education high school or something.

413
00:55:40,060 --> 00:55:43,060
And then I'm just grouping all my features together,

414
00:55:43,060 --> 00:55:48,060
so my numerical features that I haven't changed and my new one hot encoded features.

415
00:55:48,060 --> 00:55:58,060
So if I check all features, that's basically a list of names for my features, right, job, student, blah, blah, blah.

416
00:55:58,060 --> 00:56:05,060
So with that, now what I can do is wrapping my data again into a data frame,

417
00:56:05,060 --> 00:56:10,060
and it's much nicer to look at.

418
00:56:10,060 --> 00:56:17,060
So I've got all my data here, quite happy with that, and I can start training models.

419
00:56:17,060 --> 00:56:24,060
So the first model here will be logistic regression, so I make sure that I have class weight equal balanced.

420
00:56:24,060 --> 00:56:29,060
I'm setting a random state just because I'm hoping we'll all get the same results.

421
00:56:29,060 --> 00:56:33,060
Hopefully that will work, everything will be good. Yes?

422
00:56:33,060 --> 00:56:42,060
Can you explain a little bit how the Stratify algorithm works?

423
00:56:42,060 --> 00:56:50,060
Yes, up here? Yes, Stratify is just, since my model is imbalanced,

424
00:56:50,060 --> 00:56:55,060
whenever I'm calling the train test split, which is in charge of separating,

425
00:56:55,060 --> 00:56:59,060
of creating a test set in the train set by sampling at random from my data,

426
00:56:59,060 --> 00:57:09,060
I want to make sure that the distribution of values in my Y target vector will be the same in the test data and in the train data.

427
00:57:09,060 --> 00:57:16,060
So like that ratio that I've got up here, that ratio of nos and yeses,

428
00:57:16,060 --> 00:57:23,060
I want to make sure that I've got the same ratio on the train set and on test set so the two are representative of each other.

429
00:57:23,060 --> 00:57:28,060
So Stratify is just doing that by saying Stratify equal Y is just going to ensure that whenever it's doing the splits,

430
00:57:28,060 --> 00:57:37,060
I've got the same ratio in the train set and the test set.

431
00:57:37,060 --> 00:57:41,060
So back here, I'm instantiating a logistic regression.

432
00:57:41,060 --> 00:57:50,060
I'll do the same thing for decision tree, random forest, and logistic, and light GBM.

433
00:57:51,060 --> 00:57:57,060
So if I just do DT model equal decision tree classifier class weights, same thing.

434
00:57:57,060 --> 00:58:02,060
I want that to be balanced. That's pretty much it.

435
00:58:02,060 --> 00:58:06,060
RF model is going to be random forest. Same story here.

436
00:58:06,060 --> 00:58:11,060
I want it to be balanced.

437
00:58:11,060 --> 00:58:24,060
And finally, GBM model is going to be light GBM classifier and it needs to also be balanced.

438
00:58:24,060 --> 00:58:31,060
So now I've got my four models, so logistic regression, decision tree, random forest, and gradient boosting,

439
00:58:31,060 --> 00:58:34,060
and I'm ready to start using Eli5.

440
00:58:34,060 --> 00:58:41,060
So first thing, I'll tune my model to make sure it actually has good parameters.

441
00:58:41,060 --> 00:58:45,060
I can check the parameters.

442
00:58:45,060 --> 00:58:49,060
Best parameters are here with C equal 1.

443
00:58:49,060 --> 00:58:56,060
And the score I got on the training set was 70% accuracy.

444
00:58:56,060 --> 00:59:03,060
And I'll just set my model to be the best estimator, get the parameters just to make sure everything worked well.

445
00:59:03,060 --> 00:59:08,060
And now I can do...

446
00:59:08,060 --> 00:59:12,060
If I go back here, yes.

447
00:59:12,060 --> 00:59:23,060
So now I can just call it predict on X test and generate some predictions, right? Yes.

448
00:59:23,060 --> 00:59:28,060
Yep.

449
00:59:28,060 --> 00:59:33,060
What did that mean?

450
00:59:33,060 --> 00:59:37,060
So that's a grid search, right?

451
00:59:37,060 --> 00:59:44,060
I'm telling it, the logistic regression will have this parameter C, so I want to test multiple versions of C,

452
00:59:44,060 --> 00:59:46,060
and you return to me the one that gives the best score.

453
00:59:46,060 --> 00:59:50,060
So here it tried all those values and returned.

454
00:59:50,060 --> 00:59:55,060
So banded the one that has the best score is 1.

455
00:59:55,060 --> 00:59:59,060
Cool. So I've got my predictions here.

456
00:59:59,060 --> 01:00:02,060
I'm quite happy with that. I can compute the accuracy.

457
01:00:02,060 --> 01:00:09,060
So I'll do the balance accuracy just to make sure it takes into account the imbalance and properly weights it.

458
01:00:09,060 --> 01:00:16,060
And I do white test first, white bread.

459
01:00:16,060 --> 01:00:20,060
That's the accuracy I've got on the test set, right? 69%.

460
01:00:20,060 --> 01:00:27,060
I can also check the classification report, white test, white bread.

461
01:00:27,060 --> 01:00:32,060
And I need to print that.

462
01:00:32,060 --> 01:00:39,060
So I've got information here. So it's not that good, but whatever.

463
01:00:39,060 --> 01:00:43,060
Here the idea is not really to have the best model in the world, right?

464
01:00:43,060 --> 01:00:46,060
We want to spend more time interpreting it.

465
01:00:46,060 --> 01:00:49,060
So that's great. I've got my first model. That's a linear model.

466
01:00:49,060 --> 01:00:55,060
That is white box so I can actually see the weights easily, understand it easily.

467
01:00:55,060 --> 01:00:57,060
So Eli5 will do that for me.

468
01:00:57,060 --> 01:01:08,060
So if I just do Eli5 show weights and I pass my linear model, that's what it provides.

469
01:01:08,060 --> 01:01:14,060
Yeah, that's what it provides. So, yeah, not the nicest, right?

470
01:01:14,060 --> 01:01:21,060
I've got the weights. That's great. But the features here are just numbers.

471
01:01:21,060 --> 01:01:24,060
So it's not super easy to read.

472
01:01:24,060 --> 01:01:31,060
So Eli5 has an argument feature. Oh my God, I can't spell. Names.

473
01:01:31,060 --> 01:01:38,060
And here I can just say X test columns.

474
01:01:38,060 --> 01:01:42,060
What did I do wrong?

475
01:01:42,060 --> 01:01:45,060
I think it doesn't make sense.

476
01:01:45,060 --> 01:01:50,060
Oh, okay. So I would have to do two lists, right?

477
01:01:50,060 --> 01:01:52,060
Yeah, thank you.

478
01:01:52,060 --> 01:01:56,060
Right, so yeah, it has to be the feature names has to be a list.

479
01:01:56,060 --> 01:02:02,060
We could have used all features, the list that we created earlier as well.

480
01:02:02,060 --> 01:02:05,060
But anyway, so this is much nicer to interpret.

481
01:02:05,060 --> 01:02:11,060
So here we see that the most important feature in my data set seems to be the fact that the month is March.

482
01:02:11,060 --> 01:02:14,060
So maybe we've done something different in March.

483
01:02:14,060 --> 01:02:19,060
I don't know. It's worth asking the marketing team what happened in March.

484
01:02:19,060 --> 01:02:21,060
Same with a few other months, actually.

485
01:02:21,060 --> 01:02:27,060
The fact that the person was contacted by phone seems to be also important.

486
01:02:27,060 --> 01:02:29,060
Maybe students are more responsive.

487
01:02:29,060 --> 01:02:40,060
So all those things in green here will increase the probability of the person to subscribe to our plan, according to our model.

488
01:02:40,060 --> 01:02:43,060
And the things in red will decrease that probability.

489
01:02:43,060 --> 01:02:55,060
So here, maybe in November and August, we did things a bit differently that maybe we had less chances of people subscribing at those months.

490
01:02:55,060 --> 01:02:57,060
But it's worth investing a bit more.

491
01:02:57,060 --> 01:03:00,060
And that's information that we can use.

492
01:03:00,060 --> 01:03:05,060
I mean, not that we can use directly, but we can start discussing with the domain experts.

493
01:03:05,060 --> 01:03:11,060
So here you're not actually expected to provide a full explanation of everything.

494
01:03:11,060 --> 01:03:14,060
It has to be discussed with the domain experts.

495
01:03:14,060 --> 01:03:17,060
What's going on? Does that make sense? Is it a bug?

496
01:03:17,060 --> 01:03:22,060
Maybe it's not normal that your model peaks the month of March.

497
01:03:22,060 --> 01:03:26,060
So maybe there's something wrong with your model or maybe there is actually something meaningful that happened in March.

498
01:03:26,060 --> 01:03:29,060
So that's something that needs to be discussed.

499
01:03:29,060 --> 01:03:35,060
Right. So with that in mind, now we can look.

500
01:03:35,060 --> 01:03:42,060
Look, so as we said earlier, if your model is wide boxed, the i5 also allows you to explain a specific observation.

501
01:03:42,060 --> 01:03:45,060
So here we'll select an observation.

502
01:03:45,060 --> 01:03:57,060
So I've selected the fourth, the fifth row in the data set, which for some reason is not the same one I had earlier.

503
01:03:57,060 --> 01:04:03,060
Great. So the random doesn't seem to work so well.

504
01:04:03,060 --> 01:04:07,060
Anyway, that's fine. Was it number three?

505
01:04:11,060 --> 01:04:17,060
Okay. I can't find. Do you guys have the one with someone who is 27 years old?

506
01:04:17,060 --> 01:04:22,060
Yeah. Okay. So it's just me then. That's going to be fun.

507
01:04:22,060 --> 01:04:30,060
Oh, yeah. Thank you. That's probably it.

508
01:04:30,060 --> 01:04:36,060
Yes, I don't have the random seed. So we put random stay equal 42, right?

509
01:04:44,060 --> 01:04:47,060
You're getting what?

510
01:04:47,060 --> 01:04:55,060
You're getting this. Okay. Maybe you're just lucky then.

511
01:04:55,060 --> 01:04:59,060
Let me go back.

512
01:04:59,060 --> 01:05:05,060
Yes, that's the one I wanted. Great.

513
01:05:05,060 --> 01:05:10,060
So I have selected this row here. So that's person 27 years old.

514
01:05:10,060 --> 01:05:21,060
So 27 years old, we've contacted that person four times already with on average three times, three days between two contacts.

515
01:05:21,060 --> 01:05:27,060
This person is something. We don't know their job.

516
01:05:27,060 --> 01:05:35,060
I mean, we know it, but I would have to actually do.

517
01:05:35,060 --> 01:05:48,060
I'm not going to check the row now, but we would have to go back to the original data set to actually see without the one hot and coded features because here it makes it a bit harder to to find which one is actually equal to one.

518
01:05:48,060 --> 01:05:58,060
Anyway, and that person ended up subscribing to that's why I wanted absolutely this one because that that specific person subscribe to plan.

519
01:05:58,060 --> 01:06:13,060
So we'll see. We'll try to explain why do we why did that person subscribe to a plan actually something we can verify is that our model.

520
01:06:13,060 --> 01:06:19,060
Our model predicts properly this.

521
01:06:19,060 --> 01:06:23,060
Well, actually, we'll see it when we plot it. So let's just do that.

522
01:06:23,060 --> 01:06:34,060
So in order to explain a specific specific prediction, we're going to use show prediction passing the model passing the specific observation that we want to explain.

523
01:06:34,060 --> 01:06:40,060
So here that's my X test. I look I and then the name of the name of the feature.

524
01:06:40,060 --> 01:06:45,060
So if I run that, yes, so here gives me all the information I need.

525
01:06:45,060 --> 01:06:53,060
So first thing tells me, well, the true the true value that was supposed to predict is y equal one.

526
01:06:53,060 --> 01:06:58,060
I predicted that with a probability of 96 percent.

527
01:06:58,060 --> 01:07:04,060
So my model is quite confident that this person would have subscribed to the to the to the plan.

528
01:07:04,060 --> 01:07:08,060
And here is the individual contribution of all the other features.

529
01:07:08,060 --> 01:07:23,060
All the features. It looks like the most important thing my model cares about to make this precision is that this prediction is that is the fact that the marketing team contacted the person on their phone.

530
01:07:23,060 --> 01:07:32,060
And then the second most important thing is that the person did not have default before here.

531
01:07:32,060 --> 01:07:41,060
Also, some interesting information looks like the fact that we contacted that person four times actually decrease that probability of the person subscribing.

532
01:07:41,060 --> 01:07:52,060
So here I have information also about about how maybe I should have even though the person still end up ended up subscribing.

533
01:07:52,060 --> 01:07:59,060
It looks like the fact that I contacted them too many times might have decreased my chances.

534
01:07:59,060 --> 01:08:06,060
So that's that's quite interesting information here. You can also share with the marketing team and reflect with them.

535
01:08:06,060 --> 01:08:19,060
See if if if they would agree or if there is something that they would have done differently and you can explain why your model says that.

536
01:08:19,060 --> 01:08:25,060
So the next step is to do that with the decision tree.

537
01:08:25,060 --> 01:08:30,060
So we'll use the grid search the same way.

538
01:08:30,060 --> 01:08:35,060
So here I don't want to type the whole thing. So I'm just going to move to the solution.

539
01:08:35,060 --> 01:08:43,060
So we're going a bit faster on this part.

540
01:08:43,060 --> 01:08:48,060
Decision tree just copy paste that here.

541
01:08:48,060 --> 01:08:53,060
I'm just training the grid search on the decision tree instead.

542
01:08:53,060 --> 01:08:59,060
And I'll check the best parameters. And what we care about the most is to actually interpret it.

543
01:08:59,060 --> 01:09:04,060
So could be the one below as well.

544
01:09:04,060 --> 01:09:08,060
So now I've trained my decision tree. I can.

545
01:09:08,060 --> 01:09:11,060
Get the best model in here.

546
01:09:11,060 --> 01:09:16,060
Compute its performance. So let's see how well our decision tree is doing.

547
01:09:16,060 --> 01:09:32,060
So that's already a bit better than now.

548
01:09:32,060 --> 01:09:44,060
The contribution is is yeah it's really model specific. So here for the logistic regression the contribution will be the like the weight multiplied by the by the actual value.

549
01:09:44,060 --> 01:09:56,060
So here well the value is one. So there is the weight basically.

550
01:09:56,060 --> 01:10:03,060
So here this is model specific. So you want you wouldn't you wouldn't compare two models with with those values.

551
01:10:03,060 --> 01:10:12,060
Well other methods that we'll see later actually allow you to since they're more the logistic that they give you things in the same unit every time.

552
01:10:12,060 --> 01:10:16,060
But here no I wouldn't I wouldn't compare two models.

553
01:10:16,060 --> 01:10:19,060
So back to our decision tree.

554
01:10:19,060 --> 01:10:24,060
Again we want to call Eli five show weights to explain it.

555
01:10:24,060 --> 01:10:34,060
I passing I'm passing the decision tree model and I need to pass feature names equal all features.

556
01:10:34,060 --> 01:10:45,060
And here we are. So that's what my model seems to care about. And that's where things are starting to be a bit disappointing.

557
01:10:45,060 --> 01:11:01,060
So if I compare that to the result I had before for my logistic regression what's less good because this is only a feature importance only giving me amplitude of how important relative relative to each other.

558
01:11:01,060 --> 01:11:07,060
Those features seem to be but it doesn't actually tell me in which direction. So here you see that I don't have anything in in red.

559
01:11:07,060 --> 01:11:15,060
It just tells me what seems to be the most important for my decision tree decision trees the number of days between two contacts.

560
01:11:15,060 --> 01:11:21,060
But that's that's all I know. I don't know if I increase the number of days. Is it better. Is it worse.

561
01:11:21,060 --> 01:11:24,060
I don't get this information.

562
01:11:24,060 --> 01:11:34,060
The best thing that I can do is the decision tree here is probably just floating it assuming that my trees and too complex which here it's starting to get a bit complex.

563
01:11:34,060 --> 01:11:41,060
But that's probably the best I can get in terms of in terms of interpretability. So that's a bit disappointing.

564
01:11:41,060 --> 01:11:53,060
And in fact it's it won't stop here like when decision tree is probably the last model that I can keep visualizing.

565
01:11:53,060 --> 01:12:01,060
But then when we have if you remember we have the random forest coming next we have the XGBoost the light GBM model coming next.

566
01:12:01,060 --> 01:12:06,060
So those ones will not be able to to interpret them at all.

567
01:12:06,060 --> 01:12:15,060
So that's why we're introducing another method that is more the like no stick which is the permutation importance and that we discussed before.

568
01:12:15,060 --> 01:12:23,060
So to use the permutation importance you import it first from Eli 5 and then you just instantiate it.

569
01:12:23,060 --> 01:12:29,060
So if I do perm equal permutation importance you take the features that it takes.

570
01:12:29,060 --> 01:12:35,060
So here first the first thing it needs is the actual model that I want to interpret.

571
01:12:35,060 --> 01:12:47,060
So here my decision tree model my decision tree model and something else that I want to specify by default scoring will be accuracy.

572
01:12:47,060 --> 01:12:52,060
Here I want to make sure it computes the balance accuracy right because my data is imbalanced.

573
01:12:52,060 --> 01:13:00,060
So I'll just do balance accuracy and that's that's going to be my my permutation importance object.

574
01:13:00,060 --> 01:13:07,060
Then I call I fit it on the test set.

575
01:13:07,060 --> 01:13:13,060
Oh by the way something I haven't mentioned yet is here all my explanations.

576
01:13:13,060 --> 01:13:20,060
I will do it with using that test set or that validation set or like whatever is not the train sets.

577
01:13:20,060 --> 01:13:29,060
The reason why I'm doing that is because I want to know how my model works when it's generalized on some new data.

578
01:13:29,060 --> 01:13:35,060
So I don't want to know how it's mapped the training set and it learned something from the training set.

579
01:13:35,060 --> 01:13:41,060
What I want to know is that if I put this model in the wild how would how would it behave with some data it hasn't seen before.

580
01:13:41,060 --> 01:13:46,060
So that's why here all my interpretation I'm doing it with the test set.

581
01:13:46,060 --> 01:13:55,060
So I'm calling permutation importance I'm fitting it and now I've got the object train.

582
01:13:55,060 --> 01:14:01,060
So I've done that and I'm ready to pass it.

583
01:14:01,060 --> 01:14:10,060
So if I just do show weight and I pass it and again I'll need to do feature names equal all features.

584
01:14:10,060 --> 01:14:13,060
And here that's the information it provides to me.

585
01:14:13,060 --> 01:14:22,060
So as we said before what it's doing in the background is that it's just shuffling each feature one by one and seeing how that impacts the final the final outcome.

586
01:14:22,060 --> 01:14:35,060
And then it's able to tell me how in terms of amplitude how how all those features like had an impact on the on the predictions on the quality of the predictions.

587
01:14:35,060 --> 01:14:44,060
So again it doesn't actually tell me the direction in which the feature contributes it doesn't tell me if it's helps.

588
01:14:44,060 --> 01:14:52,060
So like here the fact that the person was contacted on the phone it doesn't tell me if it's better to contact them on the phone or not.

589
01:14:52,060 --> 01:14:57,060
But it at least tells me that this contacted them on the phone is important.

590
01:14:57,060 --> 01:15:12,060
The month of June seem to be important as well the number of days we've seen before the age and then all those features here seem to be relatively not important.

591
01:15:12,060 --> 01:15:21,060
And again since this since this method here is completely model agnostic you can use it on any any model.

592
01:15:21,060 --> 01:15:29,060
So if you've got a new network if you've got the random forest or the XGBoost like we have later you can you can just call it on that.

593
01:15:29,060 --> 01:15:35,060
Here I think I'm going to skip that part just to make sure we've got enough time to do everything.

594
01:15:35,060 --> 01:15:45,060
But if you were to go through the following steps we just training let's check in the solution.

595
01:15:45,060 --> 01:15:55,060
So we just training the random forest the same way we're getting the output of it and we're training the light GBM model.

596
01:15:55,060 --> 01:16:01,060
Same thing in the same way and we're just comparing the feature importance for those different models.

597
01:16:01,060 --> 01:16:14,060
So here the takeaway is really use Eli5 if you've got a white box model and you want to just try to visualize a bit a bit more nicely either local or global explanations and share that with with some people.

598
01:16:14,060 --> 01:16:15,060
That's great.

599
01:16:15,060 --> 01:16:26,060
If your model happens not to be white box you'll have to use something like permutation importance that gives you only some feature importance with only amplitude for each feature.

600
01:16:26,060 --> 01:16:32,060
But then you can compare different models and see what they seem to care about at global level.

601
01:16:32,060 --> 01:16:40,060
Any question about this Eli5 permutation importance this not work.

602
01:16:40,060 --> 01:16:42,060
No great.

603
01:16:42,060 --> 01:16:54,060
Yeah.

604
01:16:54,060 --> 01:16:55,060
Yeah.

605
01:16:55,060 --> 01:17:03,060
It's so it just wants to know how each individual feature contributes just on its own.

606
01:17:03,060 --> 01:17:18,060
Like so it has no notion of how how it would interact with other features or anything just because they can map.

607
01:17:18,060 --> 01:17:20,060
Yeah.

608
01:17:20,060 --> 01:17:22,060
Yeah.

609
01:17:22,060 --> 01:17:23,060
Yeah.

610
01:17:23,060 --> 01:17:27,060
So that that is like probably the simplest method.

611
01:17:27,060 --> 01:17:43,060
We'll see another library called chef that actually has is more interesting for what you're saying it can look at all the kind of the marginal contribution of a feature based on all the other features it's linked to.

612
01:17:43,060 --> 01:17:45,060
So it's going to be better.

613
01:17:45,060 --> 01:17:47,060
But for now that's that's what we've got.

614
01:17:47,060 --> 01:18:02,060
And yeah that's not great but it does the job and then gives you some first so maybe that's where you'd start and like start debugging a bit your model and see what what you can get out of it.

615
01:18:02,060 --> 01:18:05,060
Great.

616
01:18:05,060 --> 01:18:08,060
So with that in mind.

617
01:18:09,060 --> 01:18:27,060
So that's that's good but what the main limitation that we've seen is that if you if you've got something if you've got a model that is a bit more complex you'll only be able to see the global explanation of your model and not local so the new the other library will

618
01:18:27,060 --> 01:18:43,060
see will see lime allows you to is both local and model agnostic so it will allow you to explain a single prediction in your from your model.

619
01:18:43,060 --> 01:19:00,060
So that's the local aspect but for any any model so here we actually don't care at all if it's random forest and your network or anything it treats the model really as a black box and can tell you how important each feature is so here that's a diagram that I stole actually from the from the paper

620
01:19:00,060 --> 01:19:20,060
from the paper why should I trust you which is the paper that was released with the library lime and written by the authors of lime and so that explains a little bit what's going on so here the idea is that we've got a model so that's ugly thing here with arbitrary complexity and at the same time we've got a row a single

621
01:19:20,060 --> 01:19:41,060
observation with some information here that's the case like for for hospital again with a passion that a patient that has multiple symptoms so the person sneezes we've got the age of the person we know that the person doesn't have any fatigue and our model predicts that this person has a flu.

622
01:19:41,060 --> 01:20:10,060
Great so that's the modeling part of it then where lime comes to play is that it trains something that will call next planner and we'll see later what what that is exactly and the explainer is able to give you something like that so that's a bit similar to what we've seen with the visualization in in Eli five so it tells you that the fact that that specific person sneezes or has a headache will contribute and contributes towards predicting the future.

623
01:20:11,060 --> 01:20:32,060
So that's predicting that the person has a flu whereas the fact that the person doesn't have any fatigue contributes negatively to that that prediction so then when I go is that to whoever is taking decisions so here a doctor will say well that's what the model seems to care about and that's why the model predicted that the person has a flu.

624
01:20:32,060 --> 01:20:50,060
So what do you want to do do you want to investigate a bit more do you want to give a treatment to that person do you want to maybe tell us that we should be able to model again because it's not good all those those outcomes are possible so how does that work so.

625
01:20:50,060 --> 01:21:18,060
So this is the most simplified version of how high works and again that's something that is using the in the paper so it's not exactly how it works but I think it gives the intuition so let's start let's start with that so here the idea is that we have a model that is really complex really not linear or simple and it predicts two different classes.

626
01:21:18,060 --> 01:21:21,060
That's in blue that red orange.

627
01:21:23,060 --> 01:21:32,060
You see that we've got two different features about X one and X two and your prediction is here let's say that you've got the flu.

628
01:21:32,060 --> 01:22:00,060
That's a really complex model and we also have a single observation that we want to explain so here this observation is the red cross that you can see here right so provided with that lime will be in charge of explaining how X one and X two interact and and and can be used to to expect the

629
01:22:00,060 --> 01:22:15,060
use to to explain this this prediction so here by the way the model has predicted that this single observation is in the red zone so I think I said it's no flu or is it flu forgot what I said.

630
01:22:15,060 --> 01:22:32,060
So, with that, with that, the first thing that lime does is sampling a new fake data set around the observation so it will, it will take this observation and create a lot of points all around it so that's all the points that you see here.

631
01:22:32,060 --> 01:22:48,060
And for all those points since we've got the model we can also generate a prediction. So we've got with sampling all those points here. We call the predict from our model and we know that those are in the zone in the blue zone.

632
01:22:48,060 --> 01:22:52,060
We can sample all those points here we know that they are in the red zone.

633
01:22:52,060 --> 01:23:03,060
And then once we have all those points, we weight them, according to their distance to the to the prediction that we are interested in.

634
01:23:03,060 --> 01:23:18,060
So here we only care about this prediction here so surely those points that we've created around there and that really event to explain what's going on here, whereas those points here and here really relevant so those will have a larger weight.

635
01:23:18,060 --> 01:23:38,060
Those ones will have a smaller weight. And then the next thing that lime does is just fitting a linear regression on those, on those points by taking into account the weights and that linear regression here will be your approximation of your model in that local in that local area.

636
01:23:38,060 --> 01:23:53,060
So we we basically assuming that the model could be approximated by something linear in that area. We actually fitting the linear model and we using that to explain the specific the specific prediction.

637
01:23:53,060 --> 01:24:10,060
So here if if I'm using this line then to explain what's going on around here, I can explain that around here. X one seems to have really like it is a certain impact and X two has a certain impact as well.

638
01:24:10,060 --> 01:24:13,060
So I'm using the weights of this to explain what's going on that.

639
01:24:13,060 --> 01:24:33,060
Yeah.

640
01:24:33,060 --> 01:24:47,060
So it's not necessarily over fitting. It's more a notion of so first thing how it actually samples those points so it knows the distribution of of all the points from the training data over X one.

641
01:24:47,060 --> 01:25:00,060
So it just samples from that and knows like it's trying to build a new fake data set that is representative of your training data over X one over X two creates all the points and then it generates prediction out of it.

642
01:25:00,060 --> 01:25:15,060
But I don't think over fitting would be the world the word here because what that's that's exactly what we're trying to we want to understand how model works. So we generating predictions from our model and then we fit a simpler model that explains it.

643
01:25:15,060 --> 01:25:29,060
Yeah, you could you could see it like that. So we we're trying to say that in that local area model could be seen as almost linear.

644
01:25:29,060 --> 01:25:41,060
And then we fit a linear model data is supposed to approximate it right. So if everyone gets the intuition that it's already already pretty good turns out it's not exactly what's going on.

645
01:25:41,060 --> 01:25:55,060
So I've got next slide to explain a bit into more details what's going on. The reason why it's not what's going on is because we're not working with binary binary outcome.

646
01:25:55,060 --> 01:26:06,060
So either blue or red here we're working with probably probabilities. So instead of your model is 100% sure that the prediction is red or the prediction is blue.

647
01:26:06,060 --> 01:26:14,060
Instead of that your model will always predict probabilities of being in one or the other. But then probabilities are a bit harder to show in two dimensions.

648
01:26:14,060 --> 01:26:29,060
So I've instead I've done another plot with one single dimension. So here let's say we've got one single feature and super simple simple model which is the age of a person.

649
01:26:29,060 --> 01:26:44,060
And let's say that based on the age only so imagine there would be much more features than that. But here to simplify only the age your model is able to predict the probability of of the person having a specific disease.

650
01:26:44,060 --> 01:26:54,060
So on the y axis here that's your that's your probability of having a disease. So between zero and one and there the age goes from zero from zero to 80.

651
01:26:54,060 --> 01:27:09,060
So let's say we've got a first prediction that we want to explain. So we're like this person is 20 years old and my model predicts that they have 20% point two probability of having that specific disease.

652
01:27:09,060 --> 01:27:23,060
So I'm asking what can you explain why do you think this person has the specific probability of having the disease. So then as I mentioned earlier lime will generate new points weighted by the distance from the observation.

653
01:27:23,060 --> 01:27:40,060
Around that axis here and it will use the model itself to generate predictions right so it's it's sampling different values of the age and then for each value of the age is able to use your model that you're trying to explain to predict the probability.

654
01:27:40,060 --> 01:27:55,060
So here age equal I don't know 25 or something like that I can use your model to predict that gives you this specific probability here I've sampled age 60 what it looks like the model has a higher probability for this specific age.

655
01:27:55,060 --> 01:28:03,060
So I end up with data sets it's similar to what we've seen earlier where I've got points associated to a probability.

656
01:28:03,060 --> 01:28:09,060
Yes it's generating new points.

657
01:28:09,060 --> 01:28:17,060
So yeah.

658
01:28:18,060 --> 01:28:34,060
Yes so that's what I was saying earlier so it learns from the training data the distribution of points over age so it has actually a long distribution of how how your points look like on the on the axis age.

659
01:28:34,060 --> 01:28:49,060
Then it samples from that new points and it's able to generate prediction using your model so it's kind of see maybe you can see it as it's simulating your model in that area to see like how your model seems to work in that area to create a data set that can then use to fit a linear regression.

660
01:28:49,060 --> 01:29:05,060
And once it's get that it's able to just fit a line at this at this locality and and that line will have specific weights and those weights will be used for the for the explanation.

661
01:29:05,060 --> 01:29:21,060
So here you can see that if I'm using lime on this specific points and the specific data set it will learn that the weights associated to age at around age 20 seems to be quite quite low.

662
01:29:21,060 --> 01:29:30,060
So it will tell me here that the age doesn't seem to be such an important feature for this specific for this specific observation.

663
01:29:30,060 --> 01:29:54,060
Whereas if I had something like that so maybe around age 70 when I sample all my points the probability seems to go all this way so when I fit a linear model in that area here to wait on for the feature age in that in that local space is much higher.

664
01:29:54,060 --> 01:30:03,060
So it will tell me for this observation age doesn't seem to matter so much it's almost flat for this observation age seems to matter a lot and that's exactly the kind of explanation I want.

665
01:30:03,060 --> 01:30:13,060
I want to know why for this person it seems that age is the main reason why we think they have this disease for this person we don't think age is that relevant.

666
01:30:13,060 --> 01:30:15,060
Right. Does that make sense.

667
01:30:15,060 --> 01:30:17,060
Yeah.

668
01:30:17,060 --> 01:30:22,060
Cool. So that is pretty much how lime works.

669
01:30:22,060 --> 01:30:28,060
So that's a little bit more to it which is better described in the algorithm here.

670
01:30:28,060 --> 01:30:31,060
So the first step is that for a specific observation.

671
01:30:31,060 --> 01:30:34,060
We actually not asking to explain the observation.

672
01:30:34,060 --> 01:30:42,060
Yeah.

673
01:30:42,060 --> 01:30:48,060
Yes. So it's got the older the distribution of all the older features together.

674
01:30:48,060 --> 01:30:50,060
It's. Yeah.

675
01:30:50,060 --> 01:31:04,060
So, the, the only thing that was missing in this in this explanation is that we don't actually, because you've got to imagine that your model potentially will have 100 features.

676
01:31:04,060 --> 01:31:10,060
And if I'm if I give you an explanation with 100 features that will be much harder to interpret.

677
01:31:10,060 --> 01:31:22,060
So the way lime works actually when whenever you're calling it you're providing it a model you're providing it an observation but also how many features you want to use in your in your explanation.

678
01:31:22,060 --> 01:31:27,060
So like how what are the top how many top features you want to do when I return.

679
01:31:27,060 --> 01:31:41,060
And then from there the way the way it does it so it creates a new data set as we've as we've discussed then create it calculates the distances between the points to be able to measure the seminar similarity so what points we're going to care about what points we're not going to care about.

680
01:31:41,060 --> 01:31:48,060
Then it's generally it predicts the probability of the points. And here, the number four is the thing we didn't mention earlier.

681
01:31:48,060 --> 01:31:53,060
It's using some feature selection technique in order to select only and feature.

682
01:31:54,060 --> 01:32:17,060
So if you're asking explain the model using only three features, it will select the top three features fast and run the linear model on those on those three features only right so it's got an extra step of feature selection to reduce your the dimensionality of your data first and then it fits the linear model and then it gives you the explanation.

683
01:32:17,060 --> 01:32:23,060
Yes.

684
01:32:23,060 --> 01:32:33,060
Yeah, so it's actually doing something like last so before like other feature selection technique to to reduce the dimensionality and then it trains.

685
01:32:33,060 --> 01:32:37,060
It turns normal linear model on it.

686
01:32:37,060 --> 01:32:44,060
So it doesn't do linear regression in the it doesn't do regularization the last step.

687
01:32:44,060 --> 01:32:47,060
And yeah, so.

688
01:32:47,060 --> 01:32:55,060
So what you've got at the end is is weights on the different features on the top and features that you're interested in.

689
01:32:55,060 --> 01:32:59,060
So how it looks like in in practice when you're using it.

690
01:32:59,060 --> 01:33:14,060
So you, it will give you something like that where the plot at the left corresponds to the actual probability that you you predicted so here for this specific sample and again I'm using the breast cancer data set so here predicted.

691
01:33:14,060 --> 01:33:22,060
Class zero with probability of 80.8 sorry and here on the right.

692
01:33:22,060 --> 01:33:36,060
You see the importance of each feature and that's the part we interested in here is the the explanation of how each feature contributed to that outcome.

693
01:33:36,060 --> 01:33:46,060
So here we see that everything in blue will predict will contribute to the probability increasing for the class zero.

694
01:33:46,060 --> 01:34:02,060
Whatever I've got in orange here contributes to this the to push it on the class one so you have to see it as like features push your your prediction one side or another for the binary classification here.

695
01:34:02,060 --> 01:34:14,060
And then something else to note is that for categorical features it's being a little bit funny for categorical features sorry for numerical features it's being a bit funny.

696
01:34:14,060 --> 01:34:35,060
It's just that since your outcome will be a linear model to to see the actual contribution of a numerical feature you'd have to take the value the actual value of that of that features multiplied by the weights which means that it becomes really hard to compare.

697
01:34:35,060 --> 01:34:57,060
To compare contributions of different of different features together because you wouldn't have the weight only but the value multiplied by the weight that means that if you've got a value that is negative multiplied by weight that is negative it will end up contributing to your model positively which makes it harder to interpret that on the plot.

698
01:34:57,060 --> 01:35:10,060
So what lime decided to do that to make sure that all the weights are easily comparable as they will discretize your your numerical values into different bins.

699
01:35:10,060 --> 01:35:36,060
So here for example I've got the worst texture which is supposed to be a numerical value it will say well if was texture is in the box higher than 25 and smaller than something else then it's got this specific weight and it's user dummy value to it so that means that the weight is actually just multiplied by one for each feature which makes it like directly comparable.

700
01:35:36,060 --> 01:35:59,060
So here you'll say the interpretation of this graph is that the fact that it's a shame we don't see the end here but the fact that the mean radius is between 13 and some other value has this exact impact on your on your final outcome the fact that the worst area is between this value and this value has this exact impact here.

701
01:36:00,060 --> 01:36:08,060
Right does that make sense so that's the kind of graph you will be able to see with lime and again that explains only a local local prediction.

702
01:36:08,060 --> 01:36:31,060
Lime can also be used on images so here we've seen tabular data only but if you've got say some some fancy neural network that can classify images you can apply a lime to it so what it will do is they will first divide your images into what it calls super pixels which are just larger areas that contain multiple pixels.

703
01:36:31,060 --> 01:36:56,060
Then it will it will gray out some of the areas and same thing like create multiple new images with some of the so each area will be a feature and so like changing some of the blocks and see how how that affects everything and then it gives you some output like this so for example this is a frog.

704
01:36:56,060 --> 01:37:12,060
This is a frog it tells you probability of point 54 of being a frog and that's what is associated to this prediction so that seems to be looking at the head seems to be the most important feature to predict that this is a frog.

705
01:37:12,060 --> 01:37:36,060
It also has a really small probability of being just a bull and that's those are the parts in the image that it looks at to actually predict that so here you see that the eye actually looks like it could be a bull so it predicted it based on that and an air balloon as well so it looks like the heart here might be seen as a as an air balloon.

706
01:37:36,060 --> 01:37:47,060
So we'll do that later and if you remember at the beginning when I was showing the pictures of the husky and the wolf that's basically the technique that was used to show that you just have a snow detector here.

707
01:37:47,060 --> 01:38:15,060
So some drawbacks of lime so first thing it depends on the random sampling at the beginning of the new points so it can be it can be unstable like you might want to run lime multiple times before you actually believe the the explanation that gives you because you could just be happen to have sampled some points that are not representative and then the explanation isn't great.

708
01:38:15,060 --> 01:38:17,060
Yes.

709
01:38:17,060 --> 01:38:22,060
How would you report that how do you mean.

710
01:38:22,060 --> 01:38:33,060
Yeah, so if you run it several times then if it keeps if it keeps giving you the same explanation because every time it gives you tells you what features seem to be important what feature is not important.

711
01:38:33,060 --> 01:38:43,060
So if it always tells you the same thing you start having more confidence. If it changes every time well maybe you don't trust it so much.

712
01:38:43,060 --> 01:39:12,060
Yeah, and then it also happens a lot that your linear model will be inaccurate because the the assumption here is that your model can be approximated locally by your linear model which is not always the case so the good thing is that you can actually check the R squared and see what's how good your model your local approximation is but it will often be quite inaccurate.

713
01:39:12,060 --> 01:39:26,060
It's also quite slow sometimes in particular with images because it needs to generate a bunch of data, then call your model to predict on all those points and then fit fit the model.

714
01:39:26,060 --> 01:39:31,060
Just before we, we dive in the notebook.

715
01:39:31,060 --> 01:39:39,060
A few available explainers so lime works really well with tabular data with.

716
01:39:39,060 --> 01:39:44,060
So like just all the, that's what we're going to do like everything that fits in the spreadsheet.

717
01:39:44,060 --> 01:39:48,060
It has also an explainer for time series.

718
01:39:48,060 --> 01:39:53,060
It has an explainer for images and for text for text it's quite nice actually that.

719
01:39:53,060 --> 01:40:02,060
If you check the repository lime it's got an example where it's, it has read some images and it can predict.

720
01:40:02,060 --> 01:40:18,060
Forgot what it predict predict some characteristic of the person who sent the email, based on some words that it picks in the data and able to highlight this the fact that the person use that word means that they're probably X or Y.

721
01:40:18,060 --> 01:40:25,060
And then in terms of the API so it looks quite similar to what we've done with the permutation importance.

722
01:40:25,060 --> 01:40:32,060
So first you'll need to instantiate your explainers whatever explaining you choose to use with your data.

723
01:40:32,060 --> 01:40:39,060
So that's when it actually learning the distributions of the different features.

724
01:40:39,060 --> 01:40:45,060
Then you pick an observation that you want to explain and you call explain instance on your explainer.

725
01:40:45,060 --> 01:40:52,060
Here you provide the observation instead of providing the whole model you only need to provide the predict function.

726
01:40:52,060 --> 01:41:08,060
So that's because lime is completely model agnostic so you don't even need to stick to some psychic learn model here whatever function you create that can generate prediction lime can use it to to approximate it.

727
01:41:08,060 --> 01:41:12,060
And then you also pick the number of features you want it to use.

728
01:41:12,060 --> 01:41:18,060
And finally, you'll have multiple functions so you've got one.

729
01:41:18,060 --> 01:41:26,060
The main one that we're going to use today is showing notebook that just shows a bit that shows the graph that we've seen earlier so this one.

730
01:41:26,060 --> 01:41:31,060
And then if you want to show the pictures the same as we've seen for the frog.

731
01:41:31,060 --> 01:41:36,060
You'll call the function get image and mask, which returns exactly that.

732
01:41:36,060 --> 01:41:38,060
And we'll see that in the notebook as well.

733
01:41:38,060 --> 01:41:43,060
So, it is time for the break.

734
01:41:43,060 --> 01:41:51,060
And then after the break we can go through the notebook.

735
01:41:51,060 --> 01:41:59,060
Apparently the snacks are going quickly so you might want to run.

736
01:41:59,060 --> 01:42:02,060
Do you know how long the break is 20 minutes.

737
01:42:02,060 --> 01:42:04,060
Okay cool.

738
01:42:29,060 --> 01:42:56,060
Okay.

739
01:42:56,060 --> 01:43:21,060
Okay.

740
01:43:21,060 --> 01:43:44,060
Yeah.

741
01:43:44,060 --> 01:44:06,060
Yeah.

742
01:44:06,060 --> 01:44:33,060
Yeah.

743
01:44:33,060 --> 01:45:00,060
Yeah.

744
01:45:00,060 --> 01:45:27,060
Yeah.

745
01:45:27,060 --> 01:45:55,060
Yeah.

746
01:45:55,060 --> 01:46:22,060
Yeah.

747
01:46:22,060 --> 01:46:49,060
Yeah.

748
01:46:49,060 --> 01:47:16,060
Yeah.

749
01:47:16,060 --> 01:47:44,060
Yeah.

750
01:47:44,060 --> 01:48:11,060
Yeah.

751
01:48:11,060 --> 01:48:39,060
Yeah.

752
01:48:39,060 --> 01:49:06,060
Yeah.

753
01:49:06,060 --> 01:49:33,060
Yeah.

754
01:49:33,060 --> 01:50:01,060
Yeah.

755
01:50:01,060 --> 01:50:28,060
Yeah.

756
01:50:28,060 --> 01:50:56,060
Yeah.

757
01:50:56,060 --> 01:51:24,060
Yeah.

758
01:51:24,060 --> 01:51:51,060
Yeah.

759
01:51:51,060 --> 01:52:19,060
Yeah.

760
01:52:19,060 --> 01:52:46,060
Yeah.

761
01:52:46,060 --> 01:53:13,060
Yeah.

762
01:53:13,060 --> 01:53:41,060
Yeah.

763
01:53:41,060 --> 01:54:03,060
Yeah.

764
01:54:03,060 --> 01:54:25,060
Yeah.

765
01:54:33,060 --> 01:55:01,060
Yeah.

766
01:55:01,060 --> 01:55:28,060
Yeah.

767
01:55:28,060 --> 01:55:50,060
Yeah.

768
01:55:50,060 --> 01:56:12,060
Yeah.

769
01:56:12,060 --> 01:56:39,060
Yeah.

770
01:56:39,060 --> 01:57:04,060
Yeah.

771
01:57:04,060 --> 01:57:31,060
Yeah.

772
01:57:31,060 --> 01:57:48,060
Yeah.

773
01:57:48,060 --> 01:57:49,060
All right.

774
01:57:49,060 --> 01:57:56,060
So let's take another 20 minutes to look at this notebook and then again we'll go through it together.

775
01:57:56,060 --> 01:58:00,060
So that's for Lime this time.

776
01:58:00,060 --> 01:58:03,060
So just maybe we can just get started together.

777
01:58:03,060 --> 01:58:11,060
So it's basically doing all the same steps we did at the beginning to train the model just quicker so we can you can just run that nothing new.

778
01:58:11,060 --> 01:58:13,060
We're doing the train test splits.

779
01:58:13,060 --> 01:58:19,060
We're creating the preprocessor then we're training all the we preprocessing the data.

780
01:58:19,060 --> 01:58:27,060
Then we are training all the models so we're creating the logistic regression decision tree random forest light GBM.

781
01:58:27,060 --> 01:58:28,060
And here we are.

782
01:58:28,060 --> 01:58:33,060
So at this you can just execute all the cells quickly up to this point.

783
01:58:33,060 --> 01:58:38,060
And that's where things are starting to be fun where we actually using Lime.

784
01:58:38,060 --> 01:58:44,060
So then we'll ask you to create a new explainer and start playing a bit with it.

785
01:58:44,060 --> 01:58:50,060
So let give you maybe 20 minutes.

786
01:58:50,060 --> 01:58:53,060
Take a look on your site.

787
01:59:20,060 --> 01:59:47,060
Yeah.

788
01:59:47,060 --> 01:59:52,060
Yeah. For those of you they're using collab to run it.

789
01:59:52,060 --> 01:59:53,060
There's just one thing.

790
01:59:53,060 --> 02:00:06,060
So I forgot to I forgot to add help us that the help us that by file that you see here to collab which is used only for the Lime notebook.

791
02:00:06,060 --> 02:00:20,060
Look it's like I just created a function that does some ugly stuff with the column names just to to properly like handle the data sets.

792
02:00:20,060 --> 02:00:25,060
So the easiest way if you're using if you're not using collab you're fine.

793
02:00:25,060 --> 02:00:26,060
You've got everything in the repo.

794
02:00:26,060 --> 02:00:39,060
If you're using collab you might want to copy paste that function here in the help us that pie and just add it to the top of your file.

795
02:00:39,060 --> 02:00:42,060
What is doing.

796
02:00:42,060 --> 02:00:44,060
Sure. So it's in.

797
02:00:44,060 --> 02:00:45,060
So it's in.

798
02:00:45,060 --> 02:00:56,060
I mean it's going to be explained later in the notebook but basically Lime is is expecting your get critical features to be with integer labels.

799
02:00:56,060 --> 02:01:11,060
So there is a lot of like just boring stuff that you've got to do to change them to integers and then change them back to strings, which I don't think it's the most important part of this tutorial so I provided the helper function for that.

800
02:01:11,060 --> 02:01:20,060
But yeah so if you're using collab you're going to have to copy paste that in your notebook just to make sure you can access it later down.

801
02:01:41,060 --> 02:02:02,060
Okay.

802
02:02:02,060 --> 02:02:29,060
Okay.

803
02:02:29,060 --> 02:02:56,060
Okay.

804
02:02:56,060 --> 02:03:23,060
Okay.

805
02:03:23,060 --> 02:03:50,060
Okay.

806
02:03:50,060 --> 02:04:12,060
Okay.

807
02:04:12,060 --> 02:04:41,060
Okay.

808
02:04:41,060 --> 02:05:10,060
Okay.

809
02:05:10,060 --> 02:05:37,060
Okay.

810
02:05:37,060 --> 02:06:05,060
Okay.

811
02:06:05,060 --> 02:06:20,060
Okay.

812
02:06:20,060 --> 02:06:48,060
Okay.

813
02:06:48,060 --> 02:07:03,060
Okay.

814
02:07:03,060 --> 02:07:31,060
Okay.

815
02:07:31,060 --> 02:07:42,060
Okay.

816
02:07:42,060 --> 02:07:57,060
Okay.

817
02:07:57,060 --> 02:08:12,060
Okay.

818
02:08:12,060 --> 02:08:27,060
Okay.

819
02:08:27,060 --> 02:08:42,060
Okay.

820
02:08:42,060 --> 02:08:57,060
Okay.

821
02:08:57,060 --> 02:09:12,060
Okay.

822
02:09:12,060 --> 02:09:27,060
Okay.

823
02:09:27,060 --> 02:09:42,060
Okay.

824
02:09:42,060 --> 02:09:57,060
Okay.

825
02:09:57,060 --> 02:10:12,060
Okay.

826
02:10:12,060 --> 02:10:27,060
Okay.

827
02:10:27,060 --> 02:10:42,060
Okay.

828
02:10:42,060 --> 02:10:57,060
Okay.

829
02:10:57,060 --> 02:11:12,060
Okay.

830
02:11:12,060 --> 02:11:37,060
Okay.

831
02:11:37,060 --> 02:11:52,060
Okay.

832
02:11:52,060 --> 02:12:07,060
Okay.

833
02:12:07,060 --> 02:12:22,060
Okay.

834
02:12:22,060 --> 02:12:37,060
Okay.

835
02:12:37,060 --> 02:12:52,060
Okay.

836
02:12:52,060 --> 02:13:07,060
Okay.

837
02:13:07,060 --> 02:13:22,060
Okay.

838
02:13:22,060 --> 02:13:37,060
Okay.

839
02:13:37,060 --> 02:13:52,060
Okay.

840
02:13:52,060 --> 02:14:07,060
Okay.

841
02:14:07,060 --> 02:14:22,060
Okay.

842
02:14:22,060 --> 02:14:37,060
Okay.

843
02:14:37,060 --> 02:14:52,060
Okay.

844
02:14:52,060 --> 02:15:13,060
Okay.

845
02:15:13,060 --> 02:15:34,060
Okay.

846
02:15:34,060 --> 02:15:55,060
Okay.

847
02:15:55,060 --> 02:16:17,060
Okay.

848
02:16:17,060 --> 02:16:39,060
Okay.

849
02:16:39,060 --> 02:17:01,060
Okay.

850
02:17:01,060 --> 02:17:23,060
Okay.

851
02:17:23,060 --> 02:17:45,060
Okay.

852
02:17:45,060 --> 02:18:07,060
Okay.

853
02:18:07,060 --> 02:18:29,060
Okay.

854
02:18:29,060 --> 02:18:51,060
Okay.

855
02:18:51,060 --> 02:19:13,060
Okay.

856
02:19:13,060 --> 02:19:35,060
Okay.

857
02:19:35,060 --> 02:19:57,060
Okay.

858
02:19:57,060 --> 02:20:19,060
Okay.

859
02:20:19,060 --> 02:20:41,060
Okay.

860
02:20:41,060 --> 02:21:03,060
Okay.

861
02:21:03,060 --> 02:21:25,060
Okay.

862
02:21:25,060 --> 02:21:26,060
Okay.

863
02:21:33,060 --> 02:21:55,060
Okay.

864
02:21:55,060 --> 02:22:17,060
Okay.

865
02:22:17,060 --> 02:22:39,060
Okay.

866
02:22:39,060 --> 02:23:01,060
Okay.

867
02:23:01,060 --> 02:23:23,060
Okay.

868
02:23:23,060 --> 02:23:45,060
Okay.

869
02:23:45,060 --> 02:24:07,060
Okay.

870
02:24:07,060 --> 02:24:29,060
Okay.

871
02:24:29,060 --> 02:24:30,060
Okay.

872
02:24:37,060 --> 02:24:38,060
Okay.

873
02:25:07,060 --> 02:25:08,060
Okay.

874
02:25:37,060 --> 02:25:38,060
Okay.

875
02:26:07,060 --> 02:26:08,060
Okay.

876
02:26:37,060 --> 02:26:38,060
Okay.

877
02:27:07,060 --> 02:27:08,060
Okay.

878
02:27:08,060 --> 02:27:09,060
Okay.

879
02:27:09,060 --> 02:27:10,060
Okay.

880
02:27:10,060 --> 02:27:11,060
Okay.

881
02:27:11,060 --> 02:27:12,060
Okay.

882
02:27:12,060 --> 02:27:13,060
Okay.

883
02:27:13,060 --> 02:27:14,060
Great.

884
02:27:14,060 --> 02:27:16,940
So once I've got that now things should be easier so I'm just going to call explain instance

885
02:27:16,940 --> 02:27:21,020
on my explainer and I need to pass the row.

886
02:27:21,020 --> 02:27:26,060
So here the row will be my observation.

887
02:27:26,060 --> 02:27:28,420
And then whatever function I want to explain.

888
02:27:28,420 --> 02:27:34,060
So here let's say I want to explain my...

889
02:27:34,060 --> 02:27:37,060
Let's do the most complicated one.

890
02:27:37,060 --> 02:27:38,260
So the light GBM.

891
02:27:39,300 --> 02:27:41,620
And I want to specify as well how many features

892
02:27:41,620 --> 02:27:43,060
I want my solution to have.

893
02:27:43,060 --> 02:27:45,620
So let's do five, I think.

894
02:27:48,300 --> 02:27:51,980
And I need to create to save that in an object.

895
02:27:51,980 --> 02:27:53,980
And now I've got an explanation object

896
02:27:53,980 --> 02:27:55,660
that I can visualize.

897
02:27:55,660 --> 02:27:57,340
So I think it's the next step.

898
02:27:58,620 --> 02:28:00,180
Yeah, that's here.

899
02:28:00,180 --> 02:28:03,460
Then whenever I call showing notebook

900
02:28:03,460 --> 02:28:06,280
on my specific explanation,

901
02:28:06,280 --> 02:28:09,280
it gives me the graph that we've seen earlier.

902
02:28:09,280 --> 02:28:14,280
So here I've got the prediction that my model did.

903
02:28:14,400 --> 02:28:17,640
And most importantly, I've got the explanation right here.

904
02:28:17,640 --> 02:28:18,920
So here it says that

905
02:28:21,160 --> 02:28:23,800
the fact that the month is equal June

906
02:28:23,800 --> 02:28:26,040
seems to be the thing that contributed the most

907
02:28:26,040 --> 02:28:30,240
to my model, to my prediction, sorry.

908
02:28:30,240 --> 02:28:33,240
The fact that the person was contacted by a phone

909
02:28:33,240 --> 02:28:35,080
seems to be important as well.

910
02:28:35,080 --> 02:28:38,840
And again, so we're seeing some consistency here.

911
02:28:38,840 --> 02:28:41,400
That's quite nice, I guess.

912
02:28:41,400 --> 02:28:43,440
Like Eli 5, if you remember,

913
02:28:43,440 --> 02:28:46,040
was telling me that the fact that we contacted

914
02:28:46,040 --> 02:28:48,360
that person too many times so that the campaign

915
02:28:48,360 --> 02:28:50,960
is the number of times I've contacted a person.

916
02:28:50,960 --> 02:28:52,400
So here it's telling me that the fact

917
02:28:52,400 --> 02:28:54,960
that we've contacted them more than three times

918
02:28:54,960 --> 02:28:58,240
actually played against me and decreased the probability

919
02:28:58,240 --> 02:28:59,280
of the person subscribing.

920
02:28:59,280 --> 02:29:00,120
Yep.

921
02:29:00,280 --> 02:29:03,600
So it's a balanced, unbalanced dataset.

922
02:29:03,600 --> 02:29:06,960
Could the campaign situation of contacting them

923
02:29:06,960 --> 02:29:09,120
more somehow if you're aware of that?

924
02:29:09,120 --> 02:29:14,000
Like if I do sign up, if I sign up early on,

925
02:29:14,000 --> 02:29:16,480
maybe they will continue to contact me.

926
02:29:16,480 --> 02:29:18,960
Would this outbreak be able to see that

927
02:29:18,960 --> 02:29:21,040
or are we confident there?

928
02:29:21,040 --> 02:29:22,000
So I think the data set,

929
02:29:22,000 --> 02:29:24,240
the way the data set was constructed is that

930
02:29:25,200 --> 02:29:28,680
campaign is how many times you've contacted the person

931
02:29:28,680 --> 02:29:31,000
before they actually subscribed to the plan

932
02:29:31,000 --> 02:29:33,280
and then we've got the outcome either they subscribed

933
02:29:33,280 --> 02:29:36,720
or they didn't subscribe and then that's it,

934
02:29:36,720 --> 02:29:38,160
end of the story.

935
02:29:38,160 --> 02:29:40,160
We don't keep collecting data after that.

936
02:29:42,200 --> 02:29:43,040
Right, so that's good.

937
02:29:43,040 --> 02:29:46,160
So that gives you a local explanation of that.

938
02:29:46,160 --> 02:29:48,880
Then you might want to try it again

939
02:29:48,880 --> 02:29:50,480
multiple times with different seeds

940
02:29:50,480 --> 02:29:52,240
and see if that has an impact

941
02:29:52,240 --> 02:29:54,960
or if it gives you the same prediction over and over again

942
02:29:54,960 --> 02:29:56,960
to know if you trust it.

943
02:29:57,000 --> 02:29:59,720
Also to get more confidence in that

944
02:30:02,720 --> 02:30:05,000
kind of approximation you can look,

945
02:30:05,000 --> 02:30:09,600
you've got access to the actual weights of your model.

946
02:30:09,600 --> 02:30:11,840
So here I can get the intercept,

947
02:30:11,840 --> 02:30:14,840
I can get the exact weights that are associated to each.

948
02:30:14,840 --> 02:30:15,920
So if you check,

949
02:30:18,760 --> 02:30:23,040
if you check here, for example it says feature seven

950
02:30:23,040 --> 02:30:27,640
has a weight of .153 something something

951
02:30:27,640 --> 02:30:32,640
which I don't know why it was rounded to .16 but it was.

952
02:30:36,120 --> 02:30:39,440
So that's this one here, oh no that's this one.

953
02:30:39,440 --> 02:30:40,880
Wait where is the 16 then?

954
02:30:45,440 --> 02:30:47,680
Oh it's here, it's the first one, sorry.

955
02:30:47,680 --> 02:30:49,000
I'm just blind.

956
02:30:49,000 --> 02:30:52,000
Right, so you can actually access the different weights

957
02:30:52,000 --> 02:30:55,040
that have been associated to each of your features

958
02:30:55,040 --> 02:30:57,560
and the ones that are displayed in this table.

959
02:30:57,560 --> 02:30:59,400
You can also access the intercept

960
02:30:59,400 --> 02:31:01,800
which here is an extremely useful

961
02:31:01,800 --> 02:31:04,520
but you can also access the R squared

962
02:31:04,520 --> 02:31:06,360
of your linear regression.

963
02:31:06,360 --> 02:31:11,360
So here it's quite low but it will always end up being low

964
02:31:11,640 --> 02:31:14,640
because it's a bad approximation in the first place.

965
02:31:14,640 --> 02:31:18,400
But at least it gives you some idea of how good it is

966
02:31:18,400 --> 02:31:21,040
compared to other approximations that you might do.

967
02:31:22,720 --> 02:31:27,160
Pretty cool as well, you can take your explanation

968
02:31:27,160 --> 02:31:30,360
and save it to an HTML file if you've got to share that

969
02:31:30,360 --> 02:31:32,240
with your colleagues that don't necessarily

970
02:31:32,240 --> 02:31:34,280
want to open a notebook.

971
02:31:34,280 --> 02:31:37,040
So here I'll call that explanation HTML.

972
02:31:38,120 --> 02:31:43,120
And then if you just go there, where did I save it?

973
02:31:48,560 --> 02:31:49,560
Can't see it, yeah.

974
02:31:50,080 --> 02:31:53,320
Is it gonna open, damn it, anyway.

975
02:31:55,240 --> 02:31:56,680
Okay, I'm giving up.

976
02:31:57,960 --> 02:31:59,560
Anyway, so you can save it as HTML,

977
02:31:59,560 --> 02:32:01,400
it saves exactly what you see there.

978
02:32:01,400 --> 02:32:03,240
It's just I'm struggling to open it.

979
02:32:03,240 --> 02:32:06,200
I don't know what I've done badly.

980
02:32:06,200 --> 02:32:07,760
It opens it with Visual Studio.

981
02:32:07,760 --> 02:32:11,240
Anyway, so the point here is that you can save it

982
02:32:11,240 --> 02:32:14,200
as an HTML, it saves exactly that graph here.

983
02:32:14,200 --> 02:32:16,240
So you can just put that on the website

984
02:32:16,240 --> 02:32:18,480
or share it with one of your colleagues

985
02:32:18,480 --> 02:32:19,960
that will be able to open it

986
02:32:19,960 --> 02:32:21,600
without having all the context around

987
02:32:21,600 --> 02:32:23,360
and see exactly this thing,

988
02:32:23,360 --> 02:32:27,400
see how all features impacted that specific prediction.

989
02:32:32,400 --> 02:32:34,080
So here that's exactly what we've done,

990
02:32:34,080 --> 02:32:37,600
so we're not gonna do it again, but you can explain.

991
02:32:37,600 --> 02:32:38,960
Actually, that would be interesting

992
02:32:38,960 --> 02:32:41,360
to explain different models.

993
02:32:41,360 --> 02:32:43,600
So let's just copy paste what we've got above.

994
02:32:43,600 --> 02:32:48,600
So here, this same explanation showing up,

995
02:32:49,400 --> 02:32:51,440
I'm just gonna copy paste that as well.

996
02:32:57,800 --> 02:33:02,800
Right, so I can check, so here that's the explanation

997
02:33:03,120 --> 02:33:06,120
for my logistic for the model.

998
02:33:06,120 --> 02:33:09,120
So I can just copy paste that as well.

999
02:33:09,120 --> 02:33:12,120
So I can just copy paste that as well.

1000
02:33:14,200 --> 02:33:16,600
For my gradient boosting function,

1001
02:33:16,600 --> 02:33:20,040
I can explain my logistic regression here.

1002
02:33:20,040 --> 02:33:24,240
So seem to be the same features actually that are important,

1003
02:33:24,240 --> 02:33:25,480
which is a good sign.

1004
02:33:25,480 --> 02:33:28,760
So it seems that my model are picking up

1005
02:33:28,760 --> 02:33:31,120
similar things in the data.

1006
02:33:31,120 --> 02:33:33,640
So the fact that the campaign was quite high,

1007
02:33:33,640 --> 02:33:37,160
the number of times I've contacted the person,

1008
02:33:37,160 --> 02:33:38,720
and the fact that the month was June

1009
02:33:38,720 --> 02:33:40,080
seems to be important as well.

1010
02:33:40,080 --> 02:33:41,600
If I check my decision tree,

1011
02:33:42,440 --> 02:33:44,320
oh, here campaign doesn't seem to be important

1012
02:33:44,320 --> 02:33:46,040
for my decision tree,

1013
02:33:46,040 --> 02:33:49,160
but the fact that the month is June is really important.

1014
02:33:49,160 --> 02:33:51,320
And if I check finally the random forest,

1015
02:33:51,320 --> 02:33:52,160
got the same thing.

1016
02:33:52,160 --> 02:33:55,080
So here, you've got that single framework

1017
02:33:55,080 --> 02:34:00,080
that allows you to get local explanation of your models,

1018
02:34:00,760 --> 02:34:01,840
but also compare them,

1019
02:34:01,840 --> 02:34:03,680
because they will all be in the same unit,

1020
02:34:03,680 --> 02:34:06,000
and you'll be able to see how different features

1021
02:34:06,000 --> 02:34:07,480
might be important for your different models.

1022
02:34:07,480 --> 02:34:11,380
And maybe I trust more one of my models than the other.

1023
02:34:11,920 --> 02:34:14,400
Based on that, but at least I'm able to explain them.

1024
02:34:19,680 --> 02:34:21,440
Yeah, any question about that part?

1025
02:34:21,440 --> 02:34:24,080
So I'm using Lime on tabular data.

1026
02:34:26,280 --> 02:34:27,320
Think that's, yeah.

1027
02:34:28,520 --> 02:34:30,960
How does the data get normalized

1028
02:34:30,960 --> 02:34:33,640
when you're looking to see this list

1029
02:34:33,640 --> 02:34:35,760
for the way you're normally normalized

1030
02:34:35,760 --> 02:34:37,760
for a logistic progression?

1031
02:34:38,120 --> 02:34:41,120
I'm just wondering, especially because of the kind of

1032
02:34:41,120 --> 02:34:43,760
you've weighted in code, categorical.

1033
02:34:43,760 --> 02:34:44,600
Yeah.

1034
02:34:51,480 --> 02:34:56,480
So I don't think it does any of that,

1035
02:34:57,360 --> 02:34:59,560
because here,

1036
02:35:01,120 --> 02:35:03,400
I don't think it does any of that.

1037
02:35:04,120 --> 02:35:07,120
Any of that, because here,

1038
02:35:09,360 --> 02:35:12,680
that would be, you would do that

1039
02:35:12,680 --> 02:35:14,940
when you're training your model, potentially,

1040
02:35:14,940 --> 02:35:18,920
but then you want whatever units to be the same as

1041
02:35:20,920 --> 02:35:21,960
your original data.

1042
02:35:21,960 --> 02:35:26,960
So if you're changing your features themselves,

1043
02:35:26,960 --> 02:35:31,960
it isn't gonna be in the same units

1044
02:35:33,680 --> 02:35:34,880
as what you've got originally,

1045
02:35:34,880 --> 02:35:39,880
so your explanation isn't compatible, basically.

1046
02:35:42,320 --> 02:35:43,600
It becomes inconsistent.

1047
02:35:44,560 --> 02:35:47,920
I mean, like, build the approximation here.

1048
02:35:47,920 --> 02:35:48,760
Yeah.

1049
02:35:51,400 --> 02:35:53,480
So, because whatever output it returns,

1050
02:35:53,480 --> 02:35:55,440
like the weights need to be compatible

1051
02:35:55,440 --> 02:35:57,760
with the data that your model is expecting.

1052
02:35:57,760 --> 02:36:01,800
So if your model is, if you've actually scaled your data

1053
02:36:01,800 --> 02:36:04,320
already for your model that you wanna explain,

1054
02:36:04,320 --> 02:36:06,640
then your weights that you return

1055
02:36:06,640 --> 02:36:08,280
will be compatible to this model.

1056
02:36:08,280 --> 02:36:09,320
Okay, that makes sense.

1057
02:36:09,320 --> 02:36:10,140
Yeah.

1058
02:36:11,920 --> 02:36:14,280
More questions about line for tabular data?

1059
02:36:14,280 --> 02:36:18,240
So here, I think it's supposed to be quite

1060
02:36:18,240 --> 02:36:19,960
straightforward and easy to use,

1061
02:36:19,960 --> 02:36:22,680
except for categorical data where you've got to

1062
02:36:23,440 --> 02:36:26,960
to do quite a bit of work before.

1063
02:36:28,200 --> 02:36:31,400
And then another cool thing with Lime

1064
02:36:31,400 --> 02:36:34,940
is that you can use them on non-tabular data,

1065
02:36:34,940 --> 02:36:36,200
for example, on images.

1066
02:36:36,200 --> 02:36:37,440
So that's what we're gonna do here.

1067
02:36:37,440 --> 02:36:42,440
So we're just loading some random neural network,

1068
02:36:42,560 --> 02:36:47,560
just instantiating it with preloaded weights.

1069
02:36:48,640 --> 02:36:51,840
So it's been trained on image nets,

1070
02:36:51,840 --> 02:36:56,840
so it already should be able to predict

1071
02:36:58,000 --> 02:36:59,960
that this is a token.

1072
02:36:59,960 --> 02:37:01,480
So I'm loading this image,

1073
02:37:01,480 --> 02:37:04,600
and I'm just gonna check actually that,

1074
02:37:04,600 --> 02:37:06,560
okay, first I preprocess it.

1075
02:37:06,560 --> 02:37:08,720
I'm gonna check that it predicts properly.

1076
02:37:11,000 --> 02:37:12,800
Right, so those are the predictions

1077
02:37:12,800 --> 02:37:14,360
of the model I just loaded, right?

1078
02:37:14,360 --> 02:37:18,680
So it's quite confident that this is a token,

1079
02:37:18,680 --> 02:37:23,520
99.999999, and then it thinks for some reason

1080
02:37:23,520 --> 02:37:28,440
that it might be a school bus or animal and fish.

1081
02:37:28,440 --> 02:37:29,640
Do you have a question?

1082
02:37:30,760 --> 02:37:33,760
When I execute predict in Co-Ed,

1083
02:37:33,760 --> 02:37:36,760
it says what it expects it to be,

1084
02:37:36,760 --> 02:37:40,360
2.9 by 2.9 by 3, and it found that it was in fact

1085
02:37:40,360 --> 02:37:43,640
in the dimensions of the pixel size in that time.

1086
02:37:43,640 --> 02:37:44,480
Yeah.

1087
02:37:45,480 --> 02:37:48,000
And it looks like it was that net.

1088
02:37:48,840 --> 02:37:51,000
Right there, if you get the 29.99.

1089
02:37:51,000 --> 02:37:51,960
Yeah, yeah, yeah.

1090
02:37:53,080 --> 02:37:54,440
I forgot where.

1091
02:37:56,680 --> 02:37:58,040
You did some setup, yeah.

1092
02:37:58,040 --> 02:38:01,040
We did that line, and then we typed it for made up.

1093
02:38:01,040 --> 02:38:02,400
Okay, that's great.

1094
02:38:02,400 --> 02:38:04,080
Interesting, okay.

1095
02:38:04,080 --> 02:38:07,760
Great, and let's just do that, and hope for the best.

1096
02:38:08,920 --> 02:38:13,160
Right, so where was I?

1097
02:38:13,160 --> 02:38:16,280
Yeah, so I've loaded a model, I've loaded an image,

1098
02:38:16,280 --> 02:38:19,200
and I've just generated prediction on that image,

1099
02:38:19,200 --> 02:38:21,080
and it looks like that model is pretty good.

1100
02:38:21,080 --> 02:38:23,200
Now what I want to do is use Lime to actually know

1101
02:38:23,200 --> 02:38:24,960
what it's trying to look at in the data

1102
02:38:24,960 --> 02:38:26,680
to make those predictions.

1103
02:38:28,560 --> 02:38:32,240
So here, I'm gonna load the explainer for images,

1104
02:38:32,240 --> 02:38:35,240
so Lime image explainer, instantiating it.

1105
02:38:36,360 --> 02:38:40,520
Then, as usual, once I've instantiating the explainer,

1106
02:38:40,520 --> 02:38:43,440
I can explain a specific instance

1107
02:38:43,440 --> 02:38:46,440
and generate an explanation object.

1108
02:38:46,440 --> 02:38:48,280
So here, same thing, I'm passing the image,

1109
02:38:48,280 --> 02:38:52,360
I'm passing the model, I'm saying how many features

1110
02:38:52,360 --> 02:38:53,400
I want to return.

1111
02:38:54,760 --> 02:38:57,320
That's running now.

1112
02:38:57,320 --> 02:39:01,480
It's a bit slow, so that's what I was saying earlier.

1113
02:39:01,480 --> 02:39:06,480
Now it needs to actually work with many images,

1114
02:39:06,920 --> 02:39:11,920
process a lot of images, and then fit a model on it,

1115
02:39:12,480 --> 02:39:16,720
so it can be fairly slow, hope not too slow,

1116
02:39:16,720 --> 02:39:18,480
so that I can show the outputs.

1117
02:39:25,680 --> 02:39:27,360
I don't know if it's gonna run on colab.

1118
02:39:27,360 --> 02:39:30,560
Have someone managed to run that on colab?

1119
02:39:30,560 --> 02:39:32,000
Did it kill it before?

1120
02:39:32,880 --> 02:39:34,040
No?

1121
02:39:34,040 --> 02:39:35,040
Took quite a while.

1122
02:39:36,400 --> 02:39:38,560
Five minutes, okay, great.

1123
02:39:39,240 --> 02:39:41,720
I don't know, I've tried it before,

1124
02:39:41,720 --> 02:39:42,960
it was faster than that.

1125
02:39:46,840 --> 02:39:50,000
Anyway, so maybe I can explain the rest in the meantime.

1126
02:39:50,000 --> 02:39:52,480
So once we're creating our explanation,

1127
02:39:52,480 --> 02:39:55,480
here you'll have a function from Lime,

1128
02:40:00,120 --> 02:40:02,240
that is get image and mask,

1129
02:40:02,240 --> 02:40:06,000
that you can pass, that you can use directly,

1130
02:40:06,240 --> 02:40:09,280
and then here what you need to specify is,

1131
02:40:09,280 --> 02:40:11,400
the first thing is the class that you want to explain,

1132
02:40:11,400 --> 02:40:15,080
so my model has predicted multiple classes,

1133
02:40:15,080 --> 02:40:16,760
so that's what we see here.

1134
02:40:16,760 --> 02:40:20,280
So it's predicted 96, 96 corresponds to the token.

1135
02:40:20,280 --> 02:40:21,560
That's the first one here.

1136
02:40:21,560 --> 02:40:25,960
Then it's predicted 779,

1137
02:40:25,960 --> 02:40:27,760
so that corresponds to the school bus.

1138
02:40:27,760 --> 02:40:31,960
So here whenever I'm calling the get image and mask,

1139
02:40:31,960 --> 02:40:35,800
I'm saying, well, try to explain

1140
02:40:35,800 --> 02:40:39,680
why it predicted this specific class

1141
02:40:39,680 --> 02:40:42,560
and explain this specific thing.

1142
02:40:42,560 --> 02:40:44,160
So that's what I have later.

1143
02:40:44,160 --> 02:40:48,320
So if I just run that, still not ready.

1144
02:40:50,120 --> 02:40:50,960
Oh my God.

1145
02:41:02,920 --> 02:41:05,840
Do you have any questions in the meantime?

1146
02:41:06,960 --> 02:41:08,080
I really want to show the output

1147
02:41:08,080 --> 02:41:09,920
because it's supposed to be quite nice,

1148
02:41:09,920 --> 02:41:12,000
so I don't really want to skip that, but yeah.

1149
02:41:12,000 --> 02:41:16,160
Is the non-sample argument for the explain and something,

1150
02:41:16,160 --> 02:41:19,240
is that 1,000, which is like the ballpark number

1151
02:41:19,240 --> 02:41:21,520
that normally works, or is it like,

1152
02:41:21,520 --> 02:41:24,480
what's the threshold for reducing that for speed

1153
02:41:24,480 --> 02:41:28,240
versus something high enough that you don't lose control?

1154
02:41:28,240 --> 02:41:33,240
So yeah, 1,000 seems to be the good trade-off.

1155
02:41:33,360 --> 02:41:37,320
It just happens to be fast enough,

1156
02:41:37,320 --> 02:41:38,480
which isn't great either,

1157
02:41:38,480 --> 02:41:41,280
but yeah, if you go much higher than that,

1158
02:41:41,280 --> 02:41:42,640
it would just take forever.

1159
02:41:43,840 --> 02:41:48,160
Lower than that, it starts to be less trustable, I guess.

1160
02:41:51,320 --> 02:41:54,520
So yeah, but maybe I should have taken something lower here.

1161
02:41:58,240 --> 02:42:03,240
So, in the meantime, we can take a look at the picture.

1162
02:42:06,440 --> 02:42:07,280
It's quite nice.

1163
02:42:10,840 --> 02:42:12,480
Oh, it's done, cool, perfect.

1164
02:42:12,480 --> 02:42:15,840
So let's see, so let's try to generate,

1165
02:42:15,840 --> 02:42:18,440
so here what I'm doing is I'm saying,

1166
02:42:18,440 --> 02:42:21,160
explain why do you think it's a toucan?

1167
02:42:21,160 --> 02:42:23,200
Give me only those features

1168
02:42:23,200 --> 02:42:25,240
that contribute positively to the outcome,

1169
02:42:25,240 --> 02:42:28,560
so what specific things in the image

1170
02:42:28,560 --> 02:42:30,360
do you think looks like a toucan?

1171
02:42:31,400 --> 02:42:34,480
Then I'm saying, give me five features and hide the rest.

1172
02:42:36,280 --> 02:42:39,440
So here, if I run that, that's pretty good.

1173
02:42:39,440 --> 02:42:42,240
So it tells me that those things

1174
02:42:42,240 --> 02:42:44,880
are what things looks like a toucan in the image.

1175
02:42:44,880 --> 02:42:49,000
Those are the five first, so it calls it super pixels,

1176
02:42:49,000 --> 02:42:53,720
so it's some group of pixels that it put together.

1177
02:42:53,720 --> 02:42:58,760
Right, so let's see now what happens

1178
02:42:58,760 --> 02:43:01,560
if I select only one feature, so I'm asking,

1179
02:43:02,400 --> 02:43:04,040
oh my God, that's disappointing.

1180
02:43:05,200 --> 02:43:07,560
So I'm asking, show me only the top feature,

1181
02:43:07,560 --> 02:43:09,200
the one that you think is the most important

1182
02:43:09,200 --> 02:43:11,040
to say it's a toucan here.

1183
02:43:11,040 --> 02:43:12,760
I would really have hoped

1184
02:43:12,760 --> 02:43:15,120
they would show the beak of the bird,

1185
02:43:15,120 --> 02:43:19,480
but it shows that, so I guess it tells you

1186
02:43:19,480 --> 02:43:22,360
that it's not really that reliable,

1187
02:43:22,720 --> 02:43:25,280
like, did you all get that result,

1188
02:43:25,280 --> 02:43:26,880
or did someone get the beak?

1189
02:43:26,880 --> 02:43:28,360
Because if I run it multiple times,

1190
02:43:28,360 --> 02:43:30,760
sometimes I have the beak and I'm quite happy.

1191
02:43:30,760 --> 02:43:35,760
Yeah, true, true.

1192
02:43:36,160 --> 02:43:39,240
Yeah, you need to see the bright side.

1193
02:43:39,240 --> 02:43:43,280
Great, so that's the explanation of why it thinks it's,

1194
02:43:43,280 --> 02:43:46,080
let's try two actually, give me two, yeah, okay.

1195
02:43:46,080 --> 02:43:48,760
So the beak is the second one, it's not so bad.

1196
02:43:49,360 --> 02:43:52,760
But then I can also ask to explain

1197
02:43:52,760 --> 02:43:54,640
the other predictions it made, right?

1198
02:43:54,640 --> 02:43:57,280
Like, the first, the most likely class was a toucan.

1199
02:43:57,280 --> 02:44:00,320
It also thinks for some reason it might be a school bus,

1200
02:44:00,320 --> 02:44:02,240
so, and that's probably more interesting here.

1201
02:44:02,240 --> 02:44:06,520
So I wanna know why would you think that's a school bus?

1202
02:44:06,520 --> 02:44:10,280
And so I can run that, and there it tells me,

1203
02:44:10,280 --> 02:44:11,240
and it's actually pretty good.

1204
02:44:11,240 --> 02:44:14,160
So here, another thing I've added

1205
02:44:14,160 --> 02:44:16,560
is that I do positive only false,

1206
02:44:16,560 --> 02:44:18,120
so that means it will plot in red

1207
02:44:18,120 --> 02:44:21,960
what it thinks, like, put the prediction

1208
02:44:21,960 --> 02:44:25,920
in the other direction, so less likely to be a school bus,

1209
02:44:25,920 --> 02:44:28,400
in green, what is most likely to be a school bus.

1210
02:44:28,400 --> 02:44:30,000
And here, what it shows is that

1211
02:44:30,000 --> 02:44:31,400
this one actually makes sense.

1212
02:44:31,400 --> 02:44:34,520
So if you look at it, so it, everything that

1213
02:44:34,520 --> 02:44:36,440
is basically a toucan tells me, well,

1214
02:44:36,440 --> 02:44:39,680
this decreases the likelihood of being a school bus,

1215
02:44:39,680 --> 02:44:42,440
but this little bit here is what I think

1216
02:44:42,440 --> 02:44:43,520
might look like a school bus,

1217
02:44:43,520 --> 02:44:47,120
and if you were zooming on the image a little bit,

1218
02:44:47,160 --> 02:44:50,520
that's this part here that is a bit angled,

1219
02:44:50,520 --> 02:44:55,120
a bit square, so that, you could argue, maybe,

1220
02:44:55,120 --> 02:44:59,160
something that I'm interpreting my interpretation, right?

1221
02:45:00,040 --> 02:45:03,640
Please, so this little bit here,

1222
02:45:03,640 --> 02:45:06,200
at least that's a way to see it.

1223
02:45:06,200 --> 02:45:07,480
No, you don't buy that.

1224
02:45:10,760 --> 02:45:12,120
Yeah, anyway, that's.

1225
02:45:17,160 --> 02:45:18,360
Yeah, that's a school bus.

1226
02:45:19,560 --> 02:45:22,800
Anyway, so I guess that,

1227
02:45:24,720 --> 02:45:29,720
so LIME is pretty useful to kind of debugging a bit

1228
02:45:30,040 --> 02:45:33,160
and try to approximate your model,

1229
02:45:33,160 --> 02:45:37,480
but yeah, as we've seen, just with those toy examples,

1230
02:45:37,480 --> 02:45:41,160
it's not extremely reliable either, yeah?

1231
02:45:41,160 --> 02:45:42,640
I think before you said that what it was doing

1232
02:45:42,640 --> 02:45:44,480
was like graying out different parts

1233
02:45:44,480 --> 02:45:45,400
and then looking for features.

1234
02:45:45,400 --> 02:45:46,240
Yeah.

1235
02:45:46,240 --> 02:45:47,560
So really stupid question.

1236
02:45:47,560 --> 02:45:51,320
Would this be awful with classifying gray walls?

1237
02:45:51,320 --> 02:45:56,320
I think there isn't actually changing a color of it

1238
02:45:57,440 --> 02:46:00,440
like smooth gradient like that.

1239
02:46:00,440 --> 02:46:03,840
I don't know, actually.

1240
02:46:03,840 --> 02:46:06,960
I don't know if that's like, is it hiding the pixels

1241
02:46:06,960 --> 02:46:08,200
or is it actually smoothing?

1242
02:46:08,200 --> 02:46:10,600
Well, so it can't completely hide the pixel.

1243
02:46:10,600 --> 02:46:13,600
It has to somehow deactivate them in a way

1244
02:46:13,600 --> 02:46:16,320
by replacing them by some other numbers.

1245
02:46:17,320 --> 02:46:20,240
So I think it's not actually gray but black.

1246
02:46:20,240 --> 02:46:22,640
So it's just setting everything to black.

1247
02:46:25,000 --> 02:46:27,720
Yeah, if you've got different, yeah.

1248
02:46:31,000 --> 02:46:32,400
That's a really good question.

1249
02:46:33,920 --> 02:46:34,760
Is that what you do?

1250
02:46:34,760 --> 02:46:36,040
You classify black walls?

1251
02:46:38,160 --> 02:46:39,240
Okay, that's great.

1252
02:46:43,840 --> 02:46:45,080
You classify them as black.

1253
02:46:45,080 --> 02:46:47,440
Yeah, no, that's a really good point.

1254
02:46:47,440 --> 02:46:48,920
But I guess, yeah.

1255
02:46:51,320 --> 02:46:53,320
I kind of want to try it actually

1256
02:46:53,320 --> 02:46:57,680
just to see if it decreases everything.

1257
02:46:57,680 --> 02:47:00,240
But I guess even if you've got your black wall,

1258
02:47:00,240 --> 02:47:02,720
like you're interested in the parts of lights

1259
02:47:02,720 --> 02:47:07,320
that you're not interested in the whole black part of it.

1260
02:47:07,320 --> 02:47:10,720
So probably it would do something.

1261
02:47:10,720 --> 02:47:11,560
I'm not sure.

1262
02:47:12,240 --> 02:47:13,080
Yeah, more questions?

1263
02:47:13,080 --> 02:47:13,920
Yeah.

1264
02:47:13,920 --> 02:47:17,440
You mentioned briefly that when you pre-run lines

1265
02:47:17,440 --> 02:47:19,320
sometimes they give you different images.

1266
02:47:19,320 --> 02:47:20,160
Yeah.

1267
02:47:20,160 --> 02:47:22,440
More focus on the beam, more focus on the lighting.

1268
02:47:22,440 --> 02:47:23,280
Yeah.

1269
02:47:23,280 --> 02:47:26,240
So if you run this once and you take action on it,

1270
02:47:26,240 --> 02:47:30,800
how certain can it be that you actually can turn it off?

1271
02:47:30,800 --> 02:47:33,480
If the image doesn't change, more clarity about it.

1272
02:47:33,480 --> 02:47:34,320
Yeah.

1273
02:47:35,600 --> 02:47:37,480
What do you mean by you take actions on it?

1274
02:47:37,480 --> 02:47:38,320
So.

1275
02:47:38,320 --> 02:47:40,560
So if I wanted to change my color

1276
02:47:40,560 --> 02:47:41,800
based on what I saw here,

1277
02:47:41,800 --> 02:47:42,880
how would I do it?

1278
02:47:45,280 --> 02:47:47,880
So first you'll need to try to build confidence

1279
02:47:47,880 --> 02:47:50,960
that what Lime is telling you is true.

1280
02:47:50,960 --> 02:47:55,480
So I see it more as kind of a way to point

1281
02:47:55,480 --> 02:47:57,600
potential errors that you've made,

1282
02:47:57,600 --> 02:47:59,600
but not necessarily like telling you,

1283
02:47:59,600 --> 02:48:01,440
hey, this is wrong, fix it right now.

1284
02:48:01,440 --> 02:48:03,960
So it's more like if you see that

1285
02:48:03,960 --> 02:48:06,400
if I'm running Lime multiple times on this image

1286
02:48:06,400 --> 02:48:08,720
and it keeps telling me that it's another part

1287
02:48:08,720 --> 02:48:12,360
of the image, like if it would tell me every time,

1288
02:48:12,360 --> 02:48:14,160
well, here I'm trying to classify a toucan

1289
02:48:14,160 --> 02:48:16,640
and it consistently shows the background

1290
02:48:16,640 --> 02:48:18,680
as the most important feature on my thing.

1291
02:48:18,680 --> 02:48:20,440
Well, maybe there's something wrong with my data set.

1292
02:48:20,440 --> 02:48:24,880
Maybe I've got only the toucan is pictured

1293
02:48:24,880 --> 02:48:28,240
in a jungle or something, and then it picks the background

1294
02:48:28,240 --> 02:48:30,080
as the most important feature.

1295
02:48:30,080 --> 02:48:32,280
And then you go back to your data set

1296
02:48:32,280 --> 02:48:34,720
and you see if you've got some bias in there.

1297
02:48:34,720 --> 02:48:37,680
So that's more like giving you an indication

1298
02:48:37,680 --> 02:48:40,040
of what seems to be important for your model.

1299
02:48:40,040 --> 02:48:43,000
And then you don't necessarily take action directly,

1300
02:48:43,000 --> 02:48:46,920
but you try to explore and validate or not this thing.

1301
02:48:46,920 --> 02:48:50,160
So if it tells you the most important here is the background,

1302
02:48:50,160 --> 02:48:55,160
go back to your data set, see if there is some bias

1303
02:48:55,160 --> 02:48:59,040
like with images, mostly with this background or something,

1304
02:48:59,040 --> 02:49:01,200
and you can confirm or not.

1305
02:49:01,200 --> 02:49:02,800
Does that answer your question?

1306
02:49:02,800 --> 02:49:03,640
Yeah, yeah.

1307
02:49:03,640 --> 02:49:06,720
It feels like you can offer this a little bit.

1308
02:49:06,800 --> 02:49:08,000
It's too familiar.

1309
02:49:08,000 --> 02:49:12,480
Yeah, but that's the funny thing with interpretability

1310
02:49:12,480 --> 02:49:16,160
is that you, so you've got your first step

1311
02:49:16,160 --> 02:49:17,160
is you're building a model

1312
02:49:17,160 --> 02:49:18,960
that you don't necessarily trust that much.

1313
02:49:18,960 --> 02:49:21,200
And then your interpretability is basically a model

1314
02:49:21,200 --> 02:49:22,440
that explains the model,

1315
02:49:22,440 --> 02:49:25,080
which itself can be not that reliable.

1316
02:49:25,080 --> 02:49:30,080
So yeah, that's like you always have to kind of iterate

1317
02:49:32,440 --> 02:49:34,840
and build confidence step by step.

1318
02:49:35,840 --> 02:49:37,840
Yeah, more questions about Lime?

1319
02:49:39,320 --> 02:49:40,800
Nope, great.

1320
02:49:40,800 --> 02:49:44,680
So we can see, oh my God, and we've got half an hour.

1321
02:49:44,680 --> 02:49:48,360
That's perfect for SHAP.

1322
02:49:48,360 --> 02:49:53,360
So the last library I wanna talk about today is called SHAP.

1323
02:49:53,480 --> 02:49:57,400
So that's something that came up after Lime

1324
02:49:57,400 --> 02:50:01,120
and try to generalize to build a framework

1325
02:50:01,120 --> 02:50:04,520
that kind of generalize all those approaches.

1326
02:50:04,520 --> 02:50:07,760
So here the idea is a bit of what I just said a second ago.

1327
02:50:07,760 --> 02:50:09,200
So we've got your data set,

1328
02:50:09,200 --> 02:50:10,800
you're building a complex model

1329
02:50:12,760 --> 02:50:15,120
and you're generating predictions from it.

1330
02:50:15,120 --> 02:50:18,840
And then whenever you wanna actually explain this model,

1331
02:50:18,840 --> 02:50:22,040
you'll build an explainer.

1332
02:50:22,040 --> 02:50:23,120
So in the case of Lime,

1333
02:50:23,120 --> 02:50:27,120
the explainer was that linear model locally,

1334
02:50:27,120 --> 02:50:31,160
and that explainer can be used to generate explanations.

1335
02:50:31,160 --> 02:50:34,360
So that's the generic framework, the idea.

1336
02:50:35,200 --> 02:50:40,200
And then what SHAP is saying is that basically

1337
02:50:40,320 --> 02:50:42,960
all those libraries like Lime

1338
02:50:44,320 --> 02:50:47,000
and what SHAP is doing, what we'll see later,

1339
02:50:47,880 --> 02:50:51,360
consists in having an explainer that can generate

1340
02:50:51,360 --> 02:50:55,160
an explanation as a linear combination of all your features.

1341
02:50:55,160 --> 02:50:58,760
So like every sort of explanation that you'd get

1342
02:50:58,760 --> 02:51:02,880
out of SHAP or out of Lime is basically a weight

1343
02:51:02,880 --> 02:51:04,560
associated to each feature.

1344
02:51:04,560 --> 02:51:06,520
And when you combine all those weights together,

1345
02:51:06,520 --> 02:51:10,320
that kind of explain the final prediction.

1346
02:51:10,320 --> 02:51:12,880
So that's kind of the really generic framework.

1347
02:51:12,880 --> 02:51:17,640
Now more in depth, what SHAP is trying to do here

1348
02:51:17,640 --> 02:51:19,360
is that for a given,

1349
02:51:19,360 --> 02:51:24,360
so it works mainly as a local interpretability model.

1350
02:51:27,280 --> 02:51:28,600
So for a given observation,

1351
02:51:28,600 --> 02:51:31,280
what we want to do is compute the SHAP values.

1352
02:51:31,280 --> 02:51:35,520
And the SHAP values that define need to follow

1353
02:51:35,520 --> 02:51:40,200
the following equation, I guess,

1354
02:51:41,160 --> 02:51:45,640
which is whenever, so you'd have a SHAP value

1355
02:51:45,640 --> 02:51:48,560
associated to each single feature in your data set.

1356
02:51:48,560 --> 02:51:51,480
So if you've got age, you'll have a SHAP value for age,

1357
02:51:51,480 --> 02:51:54,680
and that will correspond to the weights associated to age.

1358
02:51:54,680 --> 02:51:55,640
If you've got education,

1359
02:51:55,640 --> 02:51:57,680
you have a SHAP value for the education.

1360
02:51:57,680 --> 02:52:01,160
That will be the weight that will be shown later, right?

1361
02:52:02,040 --> 02:52:05,080
So those are the weights of your linear combination in here.

1362
02:52:05,080 --> 02:52:08,160
And the idea is that SHAP values is basically trying

1363
02:52:08,160 --> 02:52:13,160
to find the correct weight for each of your single features

1364
02:52:13,560 --> 02:52:16,960
so that when you're summing all those weight together,

1365
02:52:16,960 --> 02:52:20,080
they explain the difference between the prediction

1366
02:52:20,080 --> 02:52:22,080
for that specific observation

1367
02:52:22,080 --> 02:52:25,000
and the expected value of the model.

1368
02:52:25,000 --> 02:52:27,440
So just to explain a little bit the terminology here,

1369
02:52:27,440 --> 02:52:29,880
what we call the expected value of the model

1370
02:52:29,880 --> 02:52:33,640
is the average prediction made by your model.

1371
02:52:33,640 --> 02:52:36,640
So like if you've got a data set

1372
02:52:39,200 --> 02:52:42,040
where you've got 50-50 of two different classes,

1373
02:52:42,040 --> 02:52:46,560
well, the expected value of your model will be 0.5.

1374
02:52:46,560 --> 02:52:49,840
So it's expecting that on by default,

1375
02:52:49,840 --> 02:52:51,160
if it doesn't do any prediction,

1376
02:52:51,160 --> 02:52:54,000
it will predict 0.5 for each value, right?

1377
02:52:54,000 --> 02:52:58,140
So that's kind of the distribution of your training data.

1378
02:52:58,140 --> 02:53:01,180
That's what your model would predict by default.

1379
02:53:01,180 --> 02:53:03,260
And then what we were saying with SHAP

1380
02:53:03,260 --> 02:53:08,260
is that every set of features

1381
02:53:08,620 --> 02:53:10,940
for a specific observation contributes

1382
02:53:10,940 --> 02:53:14,380
to pushing this expected value

1383
02:53:14,380 --> 02:53:16,820
towards whatever your model ends up predicting

1384
02:53:16,820 --> 02:53:18,140
for this observation.

1385
02:53:18,140 --> 02:53:21,140
So here, let's say my expected value is at 0.5

1386
02:53:21,140 --> 02:53:24,660
because my data set has 50-50 of each class.

1387
02:53:24,660 --> 02:53:26,380
And for this specific observation,

1388
02:53:27,180 --> 02:53:31,100
I predict 0.75 probability of being in the class.

1389
02:53:31,100 --> 02:53:36,100
Then I've got a shift from the expected value of 0.25, right?

1390
02:53:37,140 --> 02:53:40,020
And that's what I'm gonna try to explain

1391
02:53:40,020 --> 02:53:40,860
with the SHAP value.

1392
02:53:40,860 --> 02:53:42,780
So I'm saying, well, I've got,

1393
02:53:42,780 --> 02:53:44,660
for this specific observation,

1394
02:53:44,660 --> 02:53:48,260
I've got a shift from the expected value,

1395
02:53:48,260 --> 02:53:50,540
the average value of my model of 0.25.

1396
02:53:50,540 --> 02:53:53,540
How do you explain that feature per feature?

1397
02:53:53,540 --> 02:53:55,960
And then you'll have age maybe contributes

1398
02:53:55,960 --> 02:53:59,640
to 0.1 in that shift.

1399
02:53:59,640 --> 02:54:02,600
Education contributes to 0.5, et cetera.

1400
02:54:02,600 --> 02:54:07,200
So it will be able to say how each single feature

1401
02:54:07,200 --> 02:54:09,280
contributes to this shift.

1402
02:54:09,280 --> 02:54:11,800
So that's the main idea, that's what they want to do.

1403
02:54:11,800 --> 02:54:14,600
And then the second question is how they actually do it.

1404
02:54:16,600 --> 02:54:20,320
And yet here, just to reiterate that the interpretation

1405
02:54:20,320 --> 02:54:22,220
will be really that the SHAP value,

1406
02:54:22,220 --> 02:54:26,220
so the value for each feature for a specific observation,

1407
02:54:26,220 --> 02:54:30,820
a specific prediction will tell you how each feature

1408
02:54:30,820 --> 02:54:35,220
contributes to push the prediction the way it is.

1409
02:54:36,140 --> 02:54:37,960
So how it does that?

1410
02:54:37,960 --> 02:54:41,700
So from a really model-agnostic point of view,

1411
02:54:41,700 --> 02:54:44,020
and we'll see that later,

1412
02:54:44,020 --> 02:54:47,540
SHAP actually does more complex things under the hood,

1413
02:54:47,540 --> 02:54:51,780
but if we take the completely model-agnostic aspects,

1414
02:54:52,300 --> 02:54:53,860
SHAP is leveraging something called Shapley values

1415
02:54:53,860 --> 02:54:58,860
from game theory, which is just a way to know

1416
02:54:59,220 --> 02:55:03,640
the contribution of the marginal contribution

1417
02:55:03,640 --> 02:55:08,640
of something in a set of features in this case.

1418
02:55:12,540 --> 02:55:14,220
So the domain example that you've got is,

1419
02:55:14,220 --> 02:55:19,220
say you've got three people that contribute

1420
02:55:22,420 --> 02:55:27,420
some money or some skills to a team,

1421
02:55:30,260 --> 02:55:33,820
and what you want to know is how on average

1422
02:55:33,820 --> 02:55:38,080
each person brings to the equation,

1423
02:55:38,080 --> 02:55:43,080
but this person can have skills that overlap

1424
02:55:43,140 --> 02:55:46,360
with other people, so maybe myself alone,

1425
02:55:46,360 --> 02:55:50,820
I've got a certain contribution, so I get a certain score,

1426
02:55:50,820 --> 02:55:54,340
I'm that good at that level, maybe me combined

1427
02:55:54,340 --> 02:55:57,940
with another person, say we're talking about students

1428
02:55:57,940 --> 02:56:00,780
and we're doing group projects, so maybe me alone

1429
02:56:00,780 --> 02:56:04,060
working on a project got a certain grade,

1430
02:56:04,060 --> 02:56:06,100
me working with another student, I've got a grade

1431
02:56:06,100 --> 02:56:09,460
that is slightly better, me working with two other students,

1432
02:56:09,460 --> 02:56:11,300
I've got a grade that is even better,

1433
02:56:11,300 --> 02:56:13,740
how do you know the contribution of each student to that?

1434
02:56:13,740 --> 02:56:16,100
That's what Shapley values are in game theory,

1435
02:56:16,100 --> 02:56:20,020
to basically try to see the marginal contribution

1436
02:56:20,020 --> 02:56:24,220
of a single student, so the way to apply that on features

1437
02:56:24,220 --> 02:56:27,620
is that you first get a subset of,

1438
02:56:27,620 --> 02:56:29,660
like all the possible subset of features

1439
02:56:29,660 --> 02:56:31,940
that do not contain a specific feature,

1440
02:56:31,940 --> 02:56:35,940
so let's say we want to see how important is age

1441
02:56:35,940 --> 02:56:39,980
for your model, so you first get all the subset

1442
02:56:39,980 --> 02:56:42,660
of features that do not have age in it,

1443
02:56:42,660 --> 02:56:45,260
and you try to see how well your model

1444
02:56:45,260 --> 02:56:46,900
can generate prediction without knowing

1445
02:56:46,900 --> 02:56:49,860
the age of the person, so that's the first thing,

1446
02:56:50,700 --> 02:56:55,700
and then you see how adding the age to each of the subsets

1447
02:56:57,260 --> 02:57:00,220
improves the accuracy of your model,

1448
02:57:00,220 --> 02:57:03,020
and then what you've got to do is just aggregate,

1449
02:57:03,020 --> 02:57:07,420
average those contributions over all the sets,

1450
02:57:07,420 --> 02:57:10,060
so here the idea is a bit different

1451
02:57:10,060 --> 02:57:12,100
than what we've seen with permutation importance,

1452
02:57:12,100 --> 02:57:14,860
so permutation importance, we're just taking

1453
02:57:14,860 --> 02:57:17,780
all the features every time and randomizing

1454
02:57:18,700 --> 02:57:22,060
one single column and see how that affects the output,

1455
02:57:22,060 --> 02:57:25,500
so we only see the impact of one single column,

1456
02:57:25,500 --> 02:57:30,500
here we're actually observing how adding this column

1457
02:57:31,300 --> 02:57:36,300
to every other possible group of columns of features

1458
02:57:36,860 --> 02:57:39,700
impacts the model, so that takes into account

1459
02:57:39,700 --> 02:57:43,940
what you were saying, if you've got some overlap

1460
02:57:43,940 --> 02:57:46,420
between two features, it will take it into account,

1461
02:57:46,420 --> 02:57:50,460
maybe if say you've got, with a concrete example,

1462
02:57:50,460 --> 02:57:55,460
let's say you've got education, age, and job,

1463
02:57:55,620 --> 02:58:00,620
so maybe adding age to only job has a high impact,

1464
02:58:01,960 --> 02:58:05,260
but maybe adding age to job and education

1465
02:58:05,260 --> 02:58:08,660
has really limited impact to your model,

1466
02:58:08,660 --> 02:58:09,840
so here we'll be able to see it,

1467
02:58:09,840 --> 02:58:12,020
because maybe age and education are,

1468
02:58:12,860 --> 02:58:14,300
okay, let's do job and education,

1469
02:58:14,300 --> 02:58:16,460
maybe job and education are correlated,

1470
02:58:16,460 --> 02:58:19,020
so adding education to a set of features

1471
02:58:19,020 --> 02:58:23,820
that already contains job will have a really limited impact,

1472
02:58:23,820 --> 02:58:25,740
really limited improvement on your model,

1473
02:58:25,740 --> 02:58:30,000
whereas adding education to something that has only age

1474
02:58:30,000 --> 02:58:31,840
will improve your model, so here we take

1475
02:58:31,840 --> 02:58:35,220
every possible subset and we see how adding

1476
02:58:35,220 --> 02:58:39,000
this specific feature improves it, am I making sense, yeah?

1477
02:58:39,720 --> 02:58:41,960
So you have a lot of features that would be-

1478
02:58:41,960 --> 02:58:46,960
Yeah, so that is super expensive computationally,

1479
02:58:49,440 --> 02:58:52,680
because you've gotta try every possible subset of features,

1480
02:58:52,680 --> 02:58:55,440
so if you've got a lot of features, that's a lot of subsets,

1481
02:58:55,440 --> 02:58:58,200
and then every time add the single feature

1482
02:58:58,200 --> 02:59:01,080
you wanna test for, and that's for every feature,

1483
02:59:01,080 --> 02:59:04,200
so for every feature that you wanna get the importance of,

1484
02:59:04,200 --> 02:59:07,800
you have to do that, right, so does it make sense,

1485
02:59:07,800 --> 02:59:10,160
or should I do it again?

1486
02:59:11,400 --> 02:59:13,420
It's fine, not fine, fine?

1487
02:59:15,160 --> 02:59:17,420
Yeah, so just to summarize, I think the example

1488
02:59:17,420 --> 02:59:20,240
is actually the easiest to understand,

1489
02:59:20,240 --> 02:59:24,080
so I've got age, education, job, I've got three features

1490
02:59:24,080 --> 02:59:26,000
I wanna see which one is more important,

1491
02:59:26,000 --> 02:59:28,840
if I were to use the permutation importance,

1492
02:59:28,840 --> 02:59:30,800
I would just randomize each one one by one,

1493
02:59:30,800 --> 02:59:35,800
and I would know how each one is important on its own,

1494
02:59:35,920 --> 02:59:38,080
which is good, but it doesn't take into account

1495
02:59:38,080 --> 02:59:40,040
if two features are correlated,

1496
02:59:40,040 --> 02:59:41,880
so here by using this technique,

1497
02:59:41,880 --> 02:59:46,880
I've got those three features, I see how if I take only age,

1498
02:59:47,520 --> 02:59:51,480
so I wanna see how important the education is for my model,

1499
02:59:51,480 --> 02:59:54,720
so if I only have age, I add education to that,

1500
02:59:54,720 --> 02:59:56,880
I see how that improves the model,

1501
02:59:56,880 --> 03:00:00,040
if I have only age and job together,

1502
03:00:00,040 --> 03:00:02,920
I add education to that, and I see how that improves my model,

1503
03:00:02,920 --> 03:00:06,160
turns out that when I add education to age only,

1504
03:00:06,160 --> 03:00:11,160
it has a huge impact, if I add education to age and job,

1505
03:00:11,880 --> 03:00:13,160
it has a really small impact,

1506
03:00:13,160 --> 03:00:17,120
because most of the information contained in education

1507
03:00:17,120 --> 03:00:22,120
is already contained in job, right, so it handles that, yes?

1508
03:00:22,640 --> 03:00:24,680
So if you're not reprinting your model,

1509
03:00:24,680 --> 03:00:28,880
the models will expect three variables coming in, let's say,

1510
03:00:28,880 --> 03:00:31,960
so you have to choose some kind of an arbitrary variable

1511
03:00:32,160 --> 03:00:35,480
to put in there, you need to choose what you need.

1512
03:00:35,480 --> 03:00:38,400
So that's the next slide, so yeah,

1513
03:00:38,400 --> 03:00:42,080
so if you get that as the first step,

1514
03:00:42,080 --> 03:00:44,900
then there is this great question, which is,

1515
03:00:44,900 --> 03:00:49,900
well, here I'm talking about deactivating features,

1516
03:00:49,980 --> 03:00:52,920
like trying to generate predictions

1517
03:00:52,920 --> 03:00:54,680
with some features missing,

1518
03:00:54,680 --> 03:00:58,080
and with your model already trained,

1519
03:00:58,080 --> 03:01:01,160
you won't be able to provide some data

1520
03:01:01,160 --> 03:01:05,040
with features missing, so instead,

1521
03:01:05,040 --> 03:01:10,040
you need to be able to fill whatever feature

1522
03:01:12,880 --> 03:01:15,360
you want to deactivate by your default value,

1523
03:01:15,360 --> 03:01:20,360
so the way SHAP does that, which is just a choice

1524
03:01:23,320 --> 03:01:25,320
they've made is that they're saying

1525
03:01:26,160 --> 03:01:31,040
there's the same idea of expected value for your features,

1526
03:01:31,040 --> 03:01:34,000
and we say that if you're given the distribution

1527
03:01:34,000 --> 03:01:38,120
of all the values you've got for a given feature,

1528
03:01:38,120 --> 03:01:43,120
if you set that to the average value of all the values

1529
03:01:44,960 --> 03:01:46,980
that this specific feature can take,

1530
03:01:46,980 --> 03:01:51,160
you're basically saying that you take central value,

1531
03:01:51,160 --> 03:01:54,680
and you can't, it's like, will be a default,

1532
03:01:54,680 --> 03:01:58,640
and then your feature can either move higher or lower,

1533
03:01:58,640 --> 03:02:03,080
so you're setting, to deactivate a value,

1534
03:02:03,080 --> 03:02:06,160
you set it to its expected value,

1535
03:02:06,160 --> 03:02:09,400
to its average on some training data that you've learned,

1536
03:02:09,400 --> 03:02:12,360
and that will be the default behavior,

1537
03:02:12,360 --> 03:02:15,240
you'll see that as your feature is neutral,

1538
03:02:15,240 --> 03:02:18,200
it's not higher or lower than usual,

1539
03:02:18,200 --> 03:02:20,360
it's just the average, and then when you increase,

1540
03:02:20,360 --> 03:02:22,440
you can either increase or decrease this value,

1541
03:02:22,440 --> 03:02:25,360
and that's when you're starting to measure the impact.

1542
03:02:25,360 --> 03:02:27,160
Missing feature in this case

1543
03:02:30,400 --> 03:02:32,160
would be the wrong way to see it,

1544
03:02:32,160 --> 03:02:35,280
what we're saying instead is that we get average,

1545
03:02:35,280 --> 03:02:39,260
so like a neutral contribution of that feature.

1546
03:02:40,520 --> 03:02:43,440
And then, so those two things,

1547
03:02:43,440 --> 03:02:44,600
whenever you wanna use SHAP,

1548
03:02:44,600 --> 03:02:46,840
you'll need to provide some training data

1549
03:02:48,560 --> 03:02:50,340
from which it can learn all this information,

1550
03:02:50,340 --> 03:02:53,140
so it will need to learn the expected value,

1551
03:02:53,140 --> 03:02:56,340
so it will need to learn what's on average

1552
03:02:56,340 --> 03:02:57,540
the prediction of your model,

1553
03:02:57,540 --> 03:03:01,220
so it's 50-50 if your classes are perfectly balanced,

1554
03:03:01,220 --> 03:03:02,860
but it can be something different,

1555
03:03:02,860 --> 03:03:05,420
and it also needs to learn the expected value

1556
03:03:05,420 --> 03:03:06,980
of each feature, so the average,

1557
03:03:06,980 --> 03:03:10,380
the neutral version of each feature.

1558
03:03:10,380 --> 03:03:11,460
And then once it can do that,

1559
03:03:11,460 --> 03:03:14,340
it can start simulating the exact,

1560
03:03:14,340 --> 03:03:15,740
the thing we just said,

1561
03:03:15,740 --> 03:03:18,780
where it's building all the subsets of features,

1562
03:03:18,780 --> 03:03:20,740
some of them being set to their neutral value,

1563
03:03:20,740 --> 03:03:23,840
some of them keeping the value we expect,

1564
03:03:23,840 --> 03:03:25,620
and then we see the contribution

1565
03:03:25,620 --> 03:03:28,060
of adding the specific feature we want.

1566
03:03:30,180 --> 03:03:33,180
But as you mentioned earlier,

1567
03:03:33,180 --> 03:03:34,660
this is something that is gonna be

1568
03:03:34,660 --> 03:03:37,940
really computationally expensive

1569
03:03:37,940 --> 03:03:40,780
because that's tons of things to simulate

1570
03:03:40,780 --> 03:03:42,860
and predictions to generate every time,

1571
03:03:42,860 --> 03:03:46,020
and let's say you've got something like a K nearest neighbor

1572
03:03:46,020 --> 03:03:48,240
which is really slow to generate predictions,

1573
03:03:49,340 --> 03:03:50,700
and you need to generate predictions

1574
03:03:50,700 --> 03:03:52,700
in every subset of the data,

1575
03:03:52,700 --> 03:03:55,620
so that's gonna be really, really, really slow.

1576
03:03:55,620 --> 03:03:59,000
So what SHAP does is that it's got

1577
03:03:59,000 --> 03:04:02,620
some optimized version of that for different models,

1578
03:04:02,620 --> 03:04:06,340
so for example, for trees, we've got here,

1579
03:04:06,340 --> 03:04:08,300
it will have something called a tree explainer

1580
03:04:08,300 --> 03:04:12,140
that works only for trees where it's able to learn

1581
03:04:12,140 --> 03:04:15,420
the expected value instead of just take,

1582
03:04:15,420 --> 03:04:17,340
learning it from some,

1583
03:04:17,340 --> 03:04:19,260
approximating it from some training data,

1584
03:04:19,260 --> 03:04:22,220
it will know exactly the expected value of your model

1585
03:04:24,060 --> 03:04:25,740
given by the structure of the tree,

1586
03:04:25,740 --> 03:04:29,460
and we also know the contribution of each individual feature.

1587
03:04:29,460 --> 03:04:32,100
It's got an algorithm to actually learn it

1588
03:04:32,100 --> 03:04:33,820
from every single node in the tree

1589
03:04:33,820 --> 03:04:35,580
directly without having to simulate it,

1590
03:04:35,580 --> 03:04:39,580
so it's just a shortcut instead of having to simulate

1591
03:04:39,580 --> 03:04:40,940
and generate all those,

1592
03:04:41,900 --> 03:04:43,480
like manipulate all those,

1593
03:04:44,700 --> 03:04:47,020
all the data and generating prediction,

1594
03:04:47,020 --> 03:04:50,740
it will learn it directly by looking at the different rules,

1595
03:04:50,740 --> 03:04:52,480
binary rules you've got in your tree,

1596
03:04:52,480 --> 03:04:55,100
taking the different values from there

1597
03:04:55,100 --> 03:04:58,700
and compute directly as a closed form solution

1598
03:04:58,700 --> 03:05:00,980
the contribution of each feature.

1599
03:05:00,980 --> 03:05:03,660
So conclusion of that is,

1600
03:05:04,820 --> 03:05:08,260
if, so SHAP is model agnostic,

1601
03:05:08,260 --> 03:05:11,180
so you can use it on any sort of model,

1602
03:05:11,180 --> 03:05:15,500
but it will be really slow on,

1603
03:05:16,380 --> 03:05:18,160
on around the model,

1604
03:05:19,540 --> 03:05:23,660
so it has optimization for different classes of models.

1605
03:05:23,660 --> 03:05:25,240
If you've got trees,

1606
03:05:25,240 --> 03:05:27,680
you can use something called a tree explainer,

1607
03:05:27,680 --> 03:05:28,940
which will be really fast

1608
03:05:28,940 --> 03:05:30,460
and will take a lot of shortcuts.

1609
03:05:30,460 --> 03:05:32,700
So that works with scikit-learn, xgboost, lightgb,

1610
03:05:32,700 --> 03:05:35,900
and catboost anything that has trees

1611
03:05:35,900 --> 03:05:38,660
in scikit-learn way inside.

1612
03:05:38,660 --> 03:05:39,500
So that's really good,

1613
03:05:39,500 --> 03:05:43,140
because that's all the models we've got to test today.

1614
03:05:43,140 --> 03:05:46,420
For deep learning, they just released a class as well

1615
03:05:46,420 --> 03:05:50,060
that is able to also same thing as a shortcut,

1616
03:05:50,060 --> 03:05:52,520
just directly compute the contribution of each feature

1617
03:05:52,520 --> 03:05:54,340
without having to simulate it.

1618
03:05:54,340 --> 03:05:57,580
But then if instead your model is completely a black box

1619
03:05:57,580 --> 03:06:01,540
and you can't, it's not able to find shortcuts

1620
03:06:01,540 --> 03:06:04,320
to contribute those, to compute those contributions,

1621
03:06:04,320 --> 03:06:05,640
it will have to simulate it,

1622
03:06:05,640 --> 03:06:07,680
and then that will be really slow.

1623
03:06:07,680 --> 03:06:08,720
But if you need to do that,

1624
03:06:08,720 --> 03:06:10,620
you just import the kernel explainer.

1625
03:06:12,760 --> 03:06:16,500
So what does SHAP provide?

1626
03:06:18,640 --> 03:06:21,840
So that's the output that you can get

1627
03:06:21,840 --> 03:06:25,200
for each prediction that you wanna explain.

1628
03:06:25,200 --> 03:06:27,800
So you'll have, it will display the base value.

1629
03:06:27,800 --> 03:06:31,020
So the base value is the expected value,

1630
03:06:31,020 --> 03:06:33,360
so what your model would predict

1631
03:06:33,360 --> 03:06:35,000
if it wouldn't have any information,

1632
03:06:35,000 --> 03:06:37,040
so like just the default value.

1633
03:06:37,040 --> 03:06:39,920
So here, .59 for the specific example,

1634
03:06:39,920 --> 03:06:42,400
and then the output value is what it actually predicted

1635
03:06:42,400 --> 03:06:44,080
for this specific observation.

1636
03:06:44,080 --> 03:06:48,440
So here, from here to here, is the actual shift

1637
03:06:48,440 --> 03:06:51,560
that I have for the specific observation.

1638
03:06:51,560 --> 03:06:55,040
And then SHAP displays nicely

1639
03:06:55,040 --> 03:06:58,000
how each of the feature contributes to this shift.

1640
03:06:58,000 --> 03:06:59,720
So it says that this specific feature

1641
03:06:59,760 --> 03:07:03,320
for the worst concave points contributed

1642
03:07:03,320 --> 03:07:07,800
quite a large amount to push the prediction

1643
03:07:07,800 --> 03:07:11,040
in this direction, so increasing the probability.

1644
03:07:11,040 --> 03:07:14,640
So this in red here increased the probability quite a bit.

1645
03:07:14,640 --> 03:07:17,840
The worst radius here decreased the probability,

1646
03:07:17,840 --> 03:07:19,440
so pushed it on the other side,

1647
03:07:19,440 --> 03:07:21,160
and then you've got all the features like this.

1648
03:07:21,160 --> 03:07:24,240
So it's pretty nice visualization

1649
03:07:24,240 --> 03:07:27,640
that shows how the features are kind of working together

1650
03:07:27,640 --> 03:07:30,800
to push the prediction on one side or another,

1651
03:07:31,960 --> 03:07:35,600
which again comes down to this.

1652
03:07:35,600 --> 03:07:38,320
Like when you're summing the contribution

1653
03:07:38,320 --> 03:07:43,320
of each single feature through the SHAP values,

1654
03:07:43,520 --> 03:07:46,680
you have the shift from the expected value.

1655
03:07:49,280 --> 03:07:51,240
Right, so in blue you have the negative SHAP values,

1656
03:07:51,240 --> 03:07:54,120
everything that pushes your prediction

1657
03:07:54,280 --> 03:07:57,000
to decrease the probability,

1658
03:07:57,000 --> 03:07:59,880
and in red increases the probability.

1659
03:07:59,880 --> 03:08:00,720
Any question about it?

1660
03:08:00,720 --> 03:08:03,440
So that's SHAP for local interpretation.

1661
03:08:05,160 --> 03:08:06,000
Yep.

1662
03:08:06,000 --> 03:08:08,920
How does it do the average selection of the categorical data

1663
03:08:08,920 --> 03:08:12,120
to expect the most probable class?

1664
03:08:12,120 --> 03:08:12,960
Yep.

1665
03:08:12,960 --> 03:08:13,880
Oh, the most probable?

1666
03:08:13,880 --> 03:08:14,720
Yep.

1667
03:08:14,720 --> 03:08:15,560
Okay.

1668
03:08:15,560 --> 03:08:18,280
So that's, because that's gonna be your like neutral level.

1669
03:08:19,160 --> 03:08:22,160
Yeah, so that's for local interpretations,

1670
03:08:22,160 --> 03:08:25,800
but then the cool thing with SHAP is that you've got,

1671
03:08:25,800 --> 03:08:28,720
since you've got contribution of each feature

1672
03:08:30,720 --> 03:08:33,120
for each specific observations,

1673
03:08:33,120 --> 03:08:37,120
if you plot all the observations together,

1674
03:08:37,120 --> 03:08:39,400
you can understand how your model

1675
03:08:39,400 --> 03:08:41,880
works at a global level as well.

1676
03:08:41,880 --> 03:08:45,760
So here what SHAP does is that it computes

1677
03:08:46,280 --> 03:08:49,280
this graph for each single observation,

1678
03:08:49,280 --> 03:08:50,600
if you need a new data set,

1679
03:08:50,600 --> 03:08:51,920
it plots everything together,

1680
03:08:51,920 --> 03:08:53,960
and that shows you the importance of all features.

1681
03:08:53,960 --> 03:08:57,280
So here what you're seeing is,

1682
03:08:57,280 --> 03:09:01,400
and this one is a bit hard to understand at first,

1683
03:09:01,400 --> 03:09:04,600
so if you see on the X axis is the SHAP values.

1684
03:09:05,680 --> 03:09:09,360
So on the left, it's negative,

1685
03:09:09,360 --> 03:09:11,240
on the right, it's positive.

1686
03:09:11,240 --> 03:09:14,360
So you can see that the X axis is positive,

1687
03:09:14,360 --> 03:09:15,320
and it's positive.

1688
03:09:15,320 --> 03:09:18,480
So all the points that will be on the left

1689
03:09:18,480 --> 03:09:23,480
will contribute to push the prediction negatively.

1690
03:09:24,560 --> 03:09:27,400
So here we're predicting whether something is cancerous

1691
03:09:27,400 --> 03:09:29,440
or not, whether a cell is cancerous or not.

1692
03:09:29,440 --> 03:09:31,120
So everything that is on the left

1693
03:09:35,840 --> 03:09:37,200
will make the likelihood of something

1694
03:09:37,200 --> 03:09:38,800
being cancerous smaller.

1695
03:09:38,800 --> 03:09:41,880
Everything on the right will make the likelihood higher.

1696
03:09:41,880 --> 03:09:42,720
So that's the first thing.

1697
03:09:42,720 --> 03:09:47,320
Then it shows every single feature as a rule like this,

1698
03:09:47,320 --> 03:09:50,320
so it's similar to the feature importance that we're used to.

1699
03:09:50,320 --> 03:09:52,560
And then this axis here shows the value.

1700
03:09:52,560 --> 03:09:54,480
So if it's in blue, it's a small value.

1701
03:09:54,480 --> 03:09:56,480
If it's in red, it's a high value.

1702
03:09:56,480 --> 03:09:59,120
So how do we interpret that then?

1703
03:09:59,120 --> 03:10:02,080
So here what we can see is that for this specific feature,

1704
03:10:02,080 --> 03:10:05,840
LSTAT, high values of LSTAT,

1705
03:10:05,840 --> 03:10:08,960
so all the things in red,

1706
03:10:09,120 --> 03:10:12,120
seems to decrease the likelihood

1707
03:10:15,080 --> 03:10:17,320
of a cell being cancerous.

1708
03:10:17,320 --> 03:10:20,000
So it's like in one single graph,

1709
03:10:20,000 --> 03:10:23,200
you can already start seeing not only how important

1710
03:10:23,200 --> 03:10:27,120
each feature is, but also in what direction it pushes it.

1711
03:10:27,120 --> 03:10:30,160
So here looks like for some of the points,

1712
03:10:31,120 --> 03:10:34,280
so some of the points that have a high LSTAT

1713
03:10:34,280 --> 03:10:37,280
seem to have a super high impact

1714
03:10:37,280 --> 03:10:39,600
on decreasing the probability.

1715
03:10:39,600 --> 03:10:43,560
So here that has a really, really small,

1716
03:10:43,560 --> 03:10:45,920
I mean highly negative sharp value.

1717
03:10:45,920 --> 03:10:48,760
So those points here, I might wanna look at them,

1718
03:10:48,760 --> 03:10:51,240
but it looks like the fact that LSTAT was high

1719
03:10:51,240 --> 03:10:55,960
had a really huge impact on the probability being lower.

1720
03:10:55,960 --> 03:10:57,680
Here same thing, I've got some points

1721
03:10:57,680 --> 03:11:00,960
with really low value and seems to have a really high

1722
03:11:00,960 --> 03:11:05,960
positive impact on the prediction.

1723
03:11:06,160 --> 03:11:07,560
Yes, you had a question?

1724
03:11:07,560 --> 03:11:10,960
Computationally, are these pretty inexpensive

1725
03:11:10,960 --> 03:11:13,360
and maybe because you already need to run it

1726
03:11:13,360 --> 03:11:14,480
to get one of those?

1727
03:11:14,480 --> 03:11:18,880
So there's multiple levels here.

1728
03:11:18,880 --> 03:11:22,960
The first one is we need to be able to compute

1729
03:11:22,960 --> 03:11:25,280
the sharp values for a single observation,

1730
03:11:25,280 --> 03:11:27,320
which for trees happens to be fast,

1731
03:11:27,320 --> 03:11:31,920
for non-trees, or yeah, for something that is not a tree

1732
03:11:31,920 --> 03:11:34,640
and you'll have to do that process,

1733
03:11:34,640 --> 03:11:35,480
that would be really slow.

1734
03:11:35,480 --> 03:11:36,400
So that's the first thing.

1735
03:11:36,400 --> 03:11:39,680
And then you've got to compute a bunch of points,

1736
03:11:39,680 --> 03:11:41,640
a bunch of observations to be able to generate

1737
03:11:41,640 --> 03:11:43,480
a graph like that.

1738
03:11:43,480 --> 03:11:48,000
So maybe dozens of them, which adds another level

1739
03:11:48,000 --> 03:11:49,280
of complexity.

1740
03:11:49,280 --> 03:11:51,080
So yeah, that can be quite slow.

1741
03:11:51,080 --> 03:11:53,440
If you're not using tree, that is gonna be really hard

1742
03:11:53,440 --> 03:11:54,400
to generate this graph.

1743
03:11:54,400 --> 03:11:59,040
It will take like hours, like really.

1744
03:11:59,040 --> 03:12:01,760
If you're using trees and you're limiting it

1745
03:12:01,760 --> 03:12:05,840
to maybe a dozen points or something,

1746
03:12:05,840 --> 03:12:09,320
that should be doable in a couple of minutes maybe.

1747
03:12:10,280 --> 03:12:12,480
Compared to the time it would take to categorize one

1748
03:12:12,480 --> 03:12:15,920
into linear, or do you get some savings?

1749
03:12:15,920 --> 03:12:19,480
So here you're literally computing one by one,

1750
03:12:19,480 --> 03:12:23,080
all the sharp values on every observation you give.

1751
03:12:23,080 --> 03:12:23,920
So yeah.

1752
03:12:25,000 --> 03:12:25,840
Yeah.

1753
03:12:26,040 --> 03:12:27,880
I'm sorry, I didn't hear you.

1754
03:12:27,880 --> 03:12:30,120
In this individual line, what is the weight

1755
03:12:30,120 --> 03:12:33,280
of the points that you're computing?

1756
03:12:33,280 --> 03:12:36,560
The weight here, it's just trying to show you

1757
03:12:36,560 --> 03:12:37,800
how many points are there.

1758
03:12:37,800 --> 03:12:38,720
The three of them?

1759
03:12:38,720 --> 03:12:39,560
Yeah.

1760
03:12:39,560 --> 03:12:43,440
So here you, so those seem to be potentially outliers

1761
03:12:43,440 --> 03:12:45,480
and most of your points seem to be here.

1762
03:12:47,000 --> 03:12:49,320
So this graph shows you a lot of information.

1763
03:12:49,320 --> 03:12:51,640
Here I see, actually that's a really good point,

1764
03:12:51,640 --> 03:12:55,600
I see for the DIS feature, so I'm not sure what the DIS is,

1765
03:12:56,240 --> 03:12:59,120
but what I see here is that there is a big chunk

1766
03:12:59,120 --> 03:13:02,680
of points here that have quite limited impact,

1767
03:13:02,680 --> 03:13:04,720
so it's pretty much zero sharp value.

1768
03:13:04,720 --> 03:13:07,880
So all those points here, for all those points,

1769
03:13:07,880 --> 03:13:10,320
this feature seems to have almost zero impact

1770
03:13:10,320 --> 03:13:13,560
on my predictions, but then for some outliers

1771
03:13:13,560 --> 03:13:16,760
that I see here, this feature, DIS,

1772
03:13:16,760 --> 03:13:19,480
seems to have really high impact

1773
03:13:19,480 --> 03:13:21,000
and those points have a low value.

1774
03:13:21,000 --> 03:13:23,080
So those points have a low value of DIS

1775
03:13:23,080 --> 03:13:26,400
and they seem to have, the DIS feature

1776
03:13:26,400 --> 03:13:29,640
seem to have a really high impact on my prediction

1777
03:13:29,640 --> 03:13:34,280
and here, positive impact, that means the cell is cancerous,

1778
03:13:34,280 --> 03:13:35,880
so probably I wanna look at those points

1779
03:13:35,880 --> 03:13:37,920
and see what's going on there.

1780
03:13:37,920 --> 03:13:39,600
So that provides you this information.

1781
03:13:39,600 --> 03:13:42,360
Same story for the RM here.

1782
03:13:42,360 --> 03:13:45,200
It looks like a few values that have a high,

1783
03:13:47,520 --> 03:13:50,600
a few observations that have a high value of RM

1784
03:13:50,600 --> 03:13:52,400
seem to have a really high impact

1785
03:13:52,400 --> 03:13:56,800
on the final prediction here, this RM feature,

1786
03:13:56,800 --> 03:14:00,000
whereas most of the points seem to have a low sharp value,

1787
03:14:00,000 --> 03:14:02,000
like a close to zero sharp value.

1788
03:14:02,880 --> 03:14:05,840
And then all those features here, so age

1789
03:14:05,840 --> 03:14:08,200
and all the other features seem to have

1790
03:14:08,200 --> 03:14:11,240
really small impact on my predictions

1791
03:14:11,240 --> 03:14:13,520
because they're all around zero sharp value.

1792
03:14:15,000 --> 03:14:16,400
All right, so that's how you'd interpret that.

1793
03:14:16,400 --> 03:14:17,240
Yes?

1794
03:14:17,240 --> 03:14:18,680
What happens if you use the average

1795
03:14:18,680 --> 03:14:20,600
to define what's neutral?

1796
03:14:20,600 --> 03:14:24,560
What happens if someone's using the average?

1797
03:14:26,560 --> 03:14:30,680
Yeah, that's a problem.

1798
03:14:33,920 --> 03:14:37,160
It's just, so that's the assumption here

1799
03:14:37,160 --> 03:14:40,720
that, because it needs a way to define missing value

1800
03:14:40,720 --> 03:14:43,200
and the way it decided to define missing value

1801
03:14:43,200 --> 03:14:46,560
is just giving it a new, whatever it calls neutral

1802
03:14:46,560 --> 03:14:48,880
and the neutral would be the average.

1803
03:14:48,880 --> 03:14:50,600
So yeah, that's a good point.

1804
03:14:50,600 --> 03:14:51,440
Yes?

1805
03:14:51,440 --> 03:14:53,240
That's when we can change that.

1806
03:14:53,240 --> 03:14:54,840
Like, could we just hard code,

1807
03:14:54,840 --> 03:14:56,960
change this as the reference point?

1808
03:14:58,400 --> 03:15:00,560
I don't think that the implementation

1809
03:15:00,560 --> 03:15:03,120
allows you to do that, but if the problem

1810
03:15:03,120 --> 03:15:06,120
is really skewed data, you could process your data

1811
03:15:06,120 --> 03:15:09,480
before trying to explain it.

1812
03:15:12,160 --> 03:15:13,000
Yeah.

1813
03:15:15,120 --> 03:15:15,960
Any questions?

1814
03:15:16,240 --> 03:15:19,480
So, I guess that, I don't know how obvious that was,

1815
03:15:19,480 --> 03:15:23,280
but we start with like, sharp is a way to explain a local,

1816
03:15:23,280 --> 03:15:24,960
to give a local interpretation,

1817
03:15:24,960 --> 03:15:27,840
to explain a specific observation,

1818
03:15:27,840 --> 03:15:31,480
but if you plot all the points together,

1819
03:15:31,480 --> 03:15:33,680
that gives you actually a really powerful

1820
03:15:33,680 --> 03:15:35,160
global explanation of your model,

1821
03:15:35,160 --> 03:15:38,440
which is much better than the feature importance

1822
03:15:38,440 --> 03:15:41,920
that you usually get on trees and those things.

1823
03:15:42,880 --> 03:15:45,160
Just, oh my God.

1824
03:15:46,240 --> 03:15:50,400
Just a quick word on the API.

1825
03:15:50,400 --> 03:15:53,240
So yeah, you'll build your explainer,

1826
03:15:53,240 --> 03:15:55,040
you'll compute sharply values,

1827
03:15:55,040 --> 03:15:58,440
and then you can either use something called force plot

1828
03:15:58,440 --> 03:16:01,840
to explain a specific explanation, observation,

1829
03:16:01,840 --> 03:16:05,000
or summary plot is the second one I see here.

1830
03:16:05,000 --> 03:16:07,040
It's time for the notebook.

1831
03:16:07,040 --> 03:16:08,920
So, how is that gonna work?

1832
03:16:08,920 --> 03:16:10,760
We've got two minutes left,

1833
03:16:10,760 --> 03:16:14,000
which is not a lot of time to work on a notebook.

1834
03:16:14,040 --> 03:16:15,840
So, maybe I'll just go through it

1835
03:16:15,840 --> 03:16:17,720
to show you what's going on there,

1836
03:16:17,720 --> 03:16:21,120
and you can take a closer look later.

1837
03:16:21,120 --> 03:16:22,480
How does that sound?

1838
03:16:22,480 --> 03:16:23,520
Yeah.

1839
03:16:23,520 --> 03:16:26,520
So, we'll load everything first.

1840
03:16:27,880 --> 03:16:29,920
Here, I'm training all my models,

1841
03:16:29,920 --> 03:16:31,560
so same thing we've done before.

1842
03:16:33,760 --> 03:16:36,720
And I'm importing sharp, and now I'm ready to go.

1843
03:16:36,720 --> 03:16:39,680
So, all the models, like I only want to explain

1844
03:16:39,680 --> 03:16:41,520
the light GBM model here.

1845
03:16:41,520 --> 03:16:44,320
It happens to be my best model, so I'll stick to that.

1846
03:16:44,320 --> 03:16:47,680
So, light GBM is doing boosting with trees.

1847
03:16:47,680 --> 03:16:49,280
Quite lucky here,

1848
03:16:49,280 --> 03:16:50,880
because that means I can use the tree explainer,

1849
03:16:50,880 --> 03:16:52,360
which is fast.

1850
03:16:54,000 --> 03:16:56,880
And you can try, actually, if you want,

1851
03:16:56,880 --> 03:16:59,520
you can train a K-nearest neighbor model,

1852
03:16:59,520 --> 03:17:01,680
and try sharp with K-nearest neighbor,

1853
03:17:01,680 --> 03:17:04,040
and you'll see it's really slow.

1854
03:17:05,000 --> 03:17:07,880
Right, so now I'm selecting the same observation,

1855
03:17:07,880 --> 03:17:11,360
so we're really starting to know a lot of things

1856
03:17:11,360 --> 03:17:13,480
about this specific person.

1857
03:17:13,480 --> 03:17:18,480
And here, my light GBM model predicted

1858
03:17:18,720 --> 03:17:23,720
.89 probability of subscribing to my plan for this person.

1859
03:17:26,000 --> 03:17:28,360
Cool, so now let's compute the sharp values.

1860
03:17:28,360 --> 03:17:30,080
Ooh, you see it was really quick.

1861
03:17:32,280 --> 03:17:34,200
Because here it doesn't have to simulate anything,

1862
03:17:34,200 --> 03:17:36,440
it just gets it directly.

1863
03:17:36,440 --> 03:17:40,960
And if I check the dimension of this matrix

1864
03:17:40,960 --> 03:17:42,480
that is written by sharp value,

1865
03:17:42,480 --> 03:17:46,320
is a vector with 57 values, right?

1866
03:17:46,320 --> 03:17:48,360
57 is the number of features.

1867
03:17:48,360 --> 03:17:51,560
So here, if you don't believe me,

1868
03:17:51,560 --> 03:17:53,800
that's what it gives you.

1869
03:17:53,800 --> 03:17:58,680
So, an array with a value

1870
03:17:58,680 --> 03:18:02,640
for each of the features that you've got.

1871
03:18:05,120 --> 03:18:09,480
Here, just a quick sanity check that we do.

1872
03:18:09,480 --> 03:18:14,480
So, the units of the sharp values that will be returned

1873
03:18:14,640 --> 03:18:16,800
will be in the same unit

1874
03:18:16,800 --> 03:18:19,880
as the default output of your model.

1875
03:18:21,440 --> 03:18:23,920
What that means is that if your model predicts

1876
03:18:23,920 --> 03:18:25,720
probability by default,

1877
03:18:25,720 --> 03:18:30,720
the unit of your sharp values will be probability.

1878
03:18:31,240 --> 03:18:34,760
If you're here, turns out that the light GBM

1879
03:18:34,760 --> 03:18:39,760
by default returns predictions as log odds.

1880
03:18:41,280 --> 03:18:45,080
So, all the sharp values that I've computed will be log odds.

1881
03:18:45,080 --> 03:18:46,680
So, I just need to keep that in mind

1882
03:18:46,680 --> 03:18:49,720
if I use the array directly,

1883
03:18:50,760 --> 03:18:52,960
what unit that is in.

1884
03:18:52,960 --> 03:18:57,720
So here, just doing some sanity check where I'm generating.

1885
03:18:59,120 --> 03:19:00,960
So, I wanna basically, I'm saying,

1886
03:19:00,960 --> 03:19:04,520
I don't really trust that sharp,

1887
03:19:04,680 --> 03:19:07,240
what the expected value of sharp is.

1888
03:19:07,240 --> 03:19:09,000
So, if you click on your explainer,

1889
03:19:09,000 --> 03:19:12,200
explainer.expectedvalue, that's what it is.

1890
03:19:12,200 --> 03:19:15,440
So, here, I just wanna prove that the expected value

1891
03:19:15,440 --> 03:19:17,720
corresponds to this.

1892
03:19:17,720 --> 03:19:22,720
So, to what's, to the average log odds

1893
03:19:23,920 --> 03:19:26,680
of the predictions of my model.

1894
03:19:26,680 --> 03:19:29,680
And here, if I do that, it's the same thing.

1895
03:19:29,680 --> 03:19:31,120
So, here, the only thing I've demonstrated

1896
03:19:31,120 --> 03:19:33,680
is that the expected value is what I said it was.

1897
03:19:35,360 --> 03:19:36,760
Which is great.

1898
03:19:36,760 --> 03:19:41,280
So, once we've got that here, just wanna prove again.

1899
03:19:42,960 --> 03:19:46,280
So, that's difference between the expected value

1900
03:19:46,280 --> 03:19:48,360
of your model and,

1901
03:19:50,840 --> 03:19:55,840
and so, here, lgbmodel.predictProbar

1902
03:19:56,360 --> 03:19:59,720
is the probability that my model predicted.

1903
03:19:59,720 --> 03:20:02,160
I convert that to log odds to be able to have

1904
03:20:02,160 --> 03:20:04,200
the same unit as the expected value.

1905
03:20:04,200 --> 03:20:07,920
And I find a difference of 262.

1906
03:20:07,920 --> 03:20:09,800
And if I check the sum of my sharp values,

1907
03:20:09,800 --> 03:20:11,280
it's 262 as well, right?

1908
03:20:11,280 --> 03:20:14,040
So, the sharp values have the property

1909
03:20:14,040 --> 03:20:16,440
that we mentioned before where the sum of them

1910
03:20:16,440 --> 03:20:18,920
correspond to the difference between the expected value

1911
03:20:18,920 --> 03:20:21,400
and whatever you predicted.

1912
03:20:21,400 --> 03:20:22,240
Great.

1913
03:20:22,240 --> 03:20:24,040
So, that doesn't actually tell us much.

1914
03:20:24,040 --> 03:20:26,480
It just verifies that it seemed to be working.

1915
03:20:27,440 --> 03:20:28,800
So, now, the first thing you can do

1916
03:20:28,800 --> 03:20:32,400
is generate an explanation for your single observation.

1917
03:20:32,400 --> 03:20:34,880
So, we'll call force plots on,

1918
03:20:34,880 --> 03:20:36,800
I need to provide the expected value of my model.

1919
03:20:36,800 --> 03:20:39,200
So, I have it explain the expected value.

1920
03:20:39,200 --> 03:20:41,400
I need to provide the sharp values that I've computed

1921
03:20:41,400 --> 03:20:43,000
for the specific observation.

1922
03:20:45,080 --> 03:20:47,040
I need to provide the observation itself.

1923
03:20:47,040 --> 03:20:50,800
And here, actually, sharp is being quite nice with you

1924
03:20:50,800 --> 03:20:53,320
and giving you a parameter where you can choose

1925
03:20:53,320 --> 03:20:55,160
what you need to display it in.

1926
03:20:55,160 --> 03:20:58,960
So, by default, it will display in whatever unit it is.

1927
03:20:58,960 --> 03:21:02,320
Here, I'm saying, can you show it as probability

1928
03:21:03,240 --> 03:21:06,160
because it's probably easier to visualize.

1929
03:21:06,160 --> 03:21:07,760
So, here, that's what I've got.

1930
03:21:09,320 --> 03:21:10,520
So, that's quite interesting.

1931
03:21:10,520 --> 03:21:15,000
So, the base value is .38, so as a probability,

1932
03:21:15,000 --> 03:21:17,600
and the output value, so what my model actually ended up

1933
03:21:17,600 --> 03:21:20,640
predicting for this specific observation is .9.

1934
03:21:20,640 --> 03:21:24,640
And then, it's saying the fact that the number of days

1935
03:21:24,640 --> 03:21:28,960
is equal to three seems to be the most important thing

1936
03:21:28,960 --> 03:21:31,000
that mattered for this observation.

1937
03:21:31,000 --> 03:21:34,240
And it had this big of an impact.

1938
03:21:34,240 --> 03:21:36,800
And the fact, again, that the campaign,

1939
03:21:36,800 --> 03:21:39,160
so that we called the person four times,

1940
03:21:39,160 --> 03:21:41,520
had a negative impact, and that's how much

1941
03:21:41,520 --> 03:21:45,320
of a negative impact it had in the final outcome.

1942
03:21:45,320 --> 03:21:47,640
So, that's quite an interesting plot.

1943
03:21:49,440 --> 03:21:53,640
And here is all my interpretation of it.

1944
03:21:53,640 --> 03:21:57,400
And I haven't got time to read that,

1945
03:21:57,400 --> 03:22:01,120
but the idea is, it kind of leads to some questions,

1946
03:22:01,120 --> 03:22:03,440
like, well, that's great.

1947
03:22:03,440 --> 03:22:05,280
Seems to be, like, the number of days

1948
03:22:05,280 --> 03:22:09,720
between two calls seem to be important.

1949
03:22:09,720 --> 03:22:12,320
The number of times we call the person seem to be important.

1950
03:22:12,320 --> 03:22:15,440
What happened in June, is it important as well?

1951
03:22:15,440 --> 03:22:17,040
So, that kind of leads to questions,

1952
03:22:17,040 --> 03:22:19,440
like, first, you want to ask maybe to the marketing team,

1953
03:22:19,440 --> 03:22:22,160
but also, you want to explore if those things

1954
03:22:22,160 --> 03:22:24,560
that are important for this specific prediction

1955
03:22:24,560 --> 03:22:27,960
are actually important for the model as a whole or not.

1956
03:22:27,960 --> 03:22:29,520
So, that's why we want to see

1957
03:22:29,520 --> 03:22:31,960
some global explanation of that.

1958
03:22:31,960 --> 03:22:35,800
So, here, turns out that if you want to do that,

1959
03:22:35,800 --> 03:22:39,160
you first have to select some observations.

1960
03:22:39,160 --> 03:22:40,680
So, I could do it on the whole data set,

1961
03:22:40,680 --> 03:22:43,480
but as I say, it's probably gonna be slow.

1962
03:22:43,480 --> 03:22:48,160
So, instead, I've only sampled

1963
03:22:48,160 --> 03:22:52,160
a dozen observations at random from my data set.

1964
03:22:52,160 --> 03:22:53,720
I call that observations.

1965
03:22:53,720 --> 03:22:56,080
Then I compute Shapley values on that.

1966
03:22:56,080 --> 03:23:01,080
And here, if I check Shap values shape,

1967
03:23:01,240 --> 03:23:05,440
I've got a dozen rows and each row has 57 values, right?

1968
03:23:05,440 --> 03:23:07,720
So, I've got a Shap value for each feature

1969
03:23:07,720 --> 03:23:08,760
for each observation.

1970
03:23:09,720 --> 03:23:11,600
Then, I can call force plot.

1971
03:23:11,600 --> 03:23:14,760
So, force plot works if you pass Shapley values

1972
03:23:14,760 --> 03:23:16,080
for a single observation,

1973
03:23:16,080 --> 03:23:18,200
but it also will generate something

1974
03:23:18,200 --> 03:23:21,720
if you call it on multiple observations at once.

1975
03:23:21,760 --> 03:23:23,400
And that's what it looks like.

1976
03:23:24,840 --> 03:23:26,840
So, that might be hard to read,

1977
03:23:26,840 --> 03:23:31,360
but basically, imagine it as each line like this

1978
03:23:31,360 --> 03:23:33,720
over the y-axis, so like this,

1979
03:23:33,720 --> 03:23:36,680
corresponds to the red and blue line

1980
03:23:36,680 --> 03:23:39,160
that we used to see horizontally.

1981
03:23:39,160 --> 03:23:43,760
So, what that is, is that here is a single observation

1982
03:23:43,760 --> 03:23:47,160
and it has grouped the observation that look alike.

1983
03:23:47,160 --> 03:23:51,080
So, here, I've got a bunch of observations

1984
03:23:51,080 --> 03:23:55,240
but 100 observations from index zero to 100,

1985
03:23:55,240 --> 03:23:58,040
100 observation where the number of days

1986
03:23:58,040 --> 03:24:02,320
seem to have a really high positive impact on my prediction.

1987
03:24:02,320 --> 03:24:06,600
So, here, all those values seem to have p days

1988
03:24:07,560 --> 03:24:12,200
with a high positive Shap value.

1989
03:24:12,200 --> 03:24:13,600
So, pushing my prediction

1990
03:24:13,600 --> 03:24:16,800
to increasing the likelihood of subscribing.

1991
03:24:16,800 --> 03:24:19,000
So, all those points here seem to have that.

1992
03:24:19,000 --> 03:24:23,280
Then those points here seem to be around 50-50 prediction.

1993
03:24:26,560 --> 03:24:28,160
So, I don't really care much about those.

1994
03:24:28,160 --> 03:24:32,760
Here, I've got points where, again,

1995
03:24:32,760 --> 03:24:36,760
they have a high probability of subscribing to the plan.

1996
03:24:36,760 --> 03:24:38,960
So, that's the probability that you see here.

1997
03:24:38,960 --> 03:24:41,000
High probability of subscribing to the plan

1998
03:24:41,000 --> 03:24:43,520
and I can't see, but it would tell me

1999
03:24:43,520 --> 03:24:46,240
what features seem to be most important for those points.

2000
03:24:46,240 --> 03:24:48,400
So, it's kind of trying to group them like this

2001
03:24:48,400 --> 03:24:49,840
and then you can do all sort of things

2002
03:24:49,840 --> 03:24:54,840
like sorting them by number of days, for example.

2003
03:24:56,320 --> 03:24:59,400
And here, that's interesting because I see that,

2004
03:25:03,640 --> 03:25:07,120
let's do instead, let's do age actually.

2005
03:25:07,120 --> 03:25:09,160
Age is really interesting.

2006
03:25:09,160 --> 03:25:12,840
So, age, so if I say, okay, sort all my Shap values

2007
03:25:12,840 --> 03:25:16,360
for all my points by age.

2008
03:25:16,360 --> 03:25:18,360
So, here, you see on the x-axis is the age.

2009
03:25:19,320 --> 03:25:22,800
It goes from about something like zero to 90.

2010
03:25:23,880 --> 03:25:24,720
It grouped them.

2011
03:25:24,720 --> 03:25:27,180
So, I see that here for age around 20,

2012
03:25:28,320 --> 03:25:33,320
age around 20, I've got a really high chance

2013
03:25:33,560 --> 03:25:36,400
of subscribing to the plan and I've got all those features.

2014
03:25:36,400 --> 03:25:40,000
So, here, the age itself is actually really important

2015
03:25:40,000 --> 03:25:44,360
to push the likelihood of subscribing to the plan up.

2016
03:25:44,360 --> 03:25:49,160
So, there's a group of people between maybe 15 and,

2017
03:25:52,320 --> 03:25:54,040
that's not the unit, okay, that's not the same,

2018
03:25:54,040 --> 03:25:57,760
between 15 and around 25 that seem to have

2019
03:25:57,760 --> 03:26:00,040
a high likelihood of subscribing to my plan

2020
03:26:00,040 --> 03:26:01,720
and the fact that they are that age

2021
03:26:02,720 --> 03:26:04,800
seem to be the most important feature here.

2022
03:26:04,800 --> 03:26:05,840
So, that's one group.

2023
03:26:05,840 --> 03:26:07,240
Then I've got all those people here

2024
03:26:07,240 --> 03:26:10,160
between the age of 25 and around 60,

2025
03:26:10,160 --> 03:26:14,920
for which, first thing, they have about like .4 chance

2026
03:26:14,920 --> 03:26:16,320
of actually subscribing to my plan.

2027
03:26:16,320 --> 03:26:17,960
So, those people are actually not interested

2028
03:26:17,960 --> 03:26:22,600
in my plan at all and they have multiple features

2029
03:26:22,600 --> 03:26:24,900
that seem to be pushing it up and down,

2030
03:26:24,900 --> 03:26:26,920
but not having a huge impact.

2031
03:26:26,920 --> 03:26:29,960
So, here, it looks like for all those people,

2032
03:26:29,960 --> 03:26:32,160
the different features that I've got in my data set

2033
03:26:32,160 --> 03:26:35,400
don't seem to affect the outcome very much.

2034
03:26:35,400 --> 03:26:38,320
So, those guys probably, I wanna tell the marketing team,

2035
03:26:38,360 --> 03:26:42,400
well, for those people, you can't really do much.

2036
03:26:42,400 --> 03:26:44,460
They seem not to be interested in it.

2037
03:26:44,460 --> 03:26:47,240
And then I've got another huge group of people

2038
03:26:47,240 --> 03:26:52,240
between the age of 60 and 90 that seem to be really impacted

2039
03:26:54,000 --> 03:26:55,800
by different features.

2040
03:26:55,800 --> 03:26:58,800
So, here, it's hard to see.

2041
03:27:01,880 --> 03:27:03,040
I wanna know which one is that.

2042
03:27:03,040 --> 03:27:04,580
So, that's the month is June.

2043
03:27:06,000 --> 03:27:08,160
So, for some reason, one of the important feature

2044
03:27:09,000 --> 03:27:09,960
you see in red here seems to be the fact

2045
03:27:09,960 --> 03:27:13,140
that the month is June and the age as well.

2046
03:27:13,140 --> 03:27:17,760
So, all those things are pushing the predictions up.

2047
03:27:17,760 --> 03:27:20,360
So, you're seeing like already two groups of customers,

2048
03:27:20,360 --> 03:27:22,680
three groups of customers.

2049
03:27:22,680 --> 03:27:25,460
Young people were likely to buy my plan.

2050
03:27:26,800 --> 03:27:28,800
People in between that don't seem to care much

2051
03:27:28,800 --> 03:27:33,000
and older people that seem to potentially care a lot.

2052
03:27:34,000 --> 03:27:39,000
Then you can also select, for example, number of days.

2053
03:27:40,200 --> 03:27:42,400
And here, I can choose only the effect

2054
03:27:42,400 --> 03:27:44,000
of the number of days.

2055
03:27:44,000 --> 03:27:45,760
And that's, again, really interesting.

2056
03:27:45,760 --> 03:27:50,760
So, here, that's number of days from zero to a thousand.

2057
03:27:52,540 --> 03:27:56,160
And I see how the number of days between two contacts

2058
03:27:56,160 --> 03:28:01,160
seem to impact the likelihood of subscribing.

2059
03:28:01,280 --> 03:28:04,720
So, here, I see that if I don't wait too long

2060
03:28:04,720 --> 03:28:09,720
between two contacts, I increase quite a bit

2061
03:28:09,760 --> 03:28:13,180
the likelihood of subscribing to the plan.

2062
03:28:13,180 --> 03:28:18,180
So, here, from .4 up to .8, pretty much just because

2063
03:28:18,620 --> 03:28:21,360
of the fact that the number of days is so small.

2064
03:28:21,360 --> 03:28:24,140
But here, if the number of days is really high,

2065
03:28:24,140 --> 03:28:28,640
around here, I see that that is having

2066
03:28:28,680 --> 03:28:31,780
pretty much zero impact on the prediction.

2067
03:28:31,780 --> 03:28:36,240
So, here, if I'm waiting more than, I don't know,

2068
03:28:36,240 --> 03:28:40,960
about 600 days, then the number of days end up,

2069
03:28:40,960 --> 03:28:42,840
it doesn't matter much anymore.

2070
03:28:42,840 --> 03:28:43,680
Yeah?

2071
03:28:43,680 --> 03:28:46,160
So, could you say then that your customers

2072
03:28:46,160 --> 03:28:48,840
really are between this age and that age,

2073
03:28:48,840 --> 03:28:52,320
and you should contact them no more than three times,

2074
03:28:52,320 --> 03:28:54,960
but no more than 600 days?

2075
03:28:55,040 --> 03:28:56,280
Right.

2076
03:28:56,280 --> 03:28:59,480
Yeah, I mean, if you would want to jump to conclusion

2077
03:28:59,480 --> 03:29:02,320
directly, that's probably what you would do.

2078
03:29:02,320 --> 03:29:07,280
But it's more likely that what you want is to take notes

2079
03:29:07,280 --> 03:29:10,880
of those and discuss, actually, like, does that make sense?

2080
03:29:10,880 --> 03:29:13,560
Does it correspond to your intuition,

2081
03:29:13,560 --> 03:29:15,880
or like what you've seen on the field?

2082
03:29:16,960 --> 03:29:19,200
And also, one other interesting feature

2083
03:29:19,200 --> 03:29:20,920
that I wanted to show is campaign.

2084
03:29:20,920 --> 03:29:24,380
So, how many times, how many times,

2085
03:29:26,400 --> 03:29:28,480
how many times we've contacted the people?

2086
03:29:28,480 --> 03:29:31,800
So, here, I can see that,

2087
03:29:35,480 --> 03:29:40,480
like, if I contact up to maybe, like, about nine times

2088
03:29:41,160 --> 03:29:45,120
a single person, doesn't seem to matter too much.

2089
03:29:45,120 --> 03:29:47,000
It doesn't seem to have too much of an impact

2090
03:29:47,000 --> 03:29:48,700
on the final prediction.

2091
03:29:48,700 --> 03:29:51,820
But here, looks like, for a lot of people,

2092
03:29:51,820 --> 03:29:55,940
if I contact them more than, say, 12 times,

2093
03:29:55,940 --> 03:29:58,460
the number of times I contact them seem to be

2094
03:29:58,460 --> 03:30:01,620
super important and have a super high,

2095
03:30:01,620 --> 03:30:04,740
super high negative impact on the likelihood

2096
03:30:04,740 --> 03:30:05,580
of subscribing.

2097
03:30:05,580 --> 03:30:08,700
So, here, I can go to the marketing team and say,

2098
03:30:08,700 --> 03:30:11,540
well, it looks like if you contact them more than 10 times,

2099
03:30:11,540 --> 03:30:14,820
it's really being counterproductive

2100
03:30:14,820 --> 03:30:17,260
and really decreasing the likelihood

2101
03:30:17,260 --> 03:30:19,660
of the people subscribing in the end,

2102
03:30:19,660 --> 03:30:22,940
which, again, is some interesting finding.

2103
03:30:22,940 --> 03:30:26,900
So, also, I think, and there is more that you can explore

2104
03:30:26,900 --> 03:30:30,500
here by just changing how you wanna sort your SHAP values

2105
03:30:30,500 --> 03:30:33,820
and how you wanna filter if you wanna show the whole thing

2106
03:30:33,820 --> 03:30:35,980
or just the impact of a single feature.

2107
03:30:37,420 --> 03:30:40,540
So, if you haven't followed the whole interpretation,

2108
03:30:40,540 --> 03:30:42,460
I've tried to write it down here,

2109
03:30:42,460 --> 03:30:44,900
so you might want to look at that later.

2110
03:30:44,900 --> 03:30:47,660
Just to finish with that, summary plots,

2111
03:30:47,660 --> 03:30:50,300
so the one we've seen in the slides,

2112
03:30:50,300 --> 03:30:52,100
but for this specific thing here,

2113
03:30:52,100 --> 03:30:54,660
so tells you that the contact,

2114
03:30:54,660 --> 03:30:57,140
the fact that you contact people by phone here

2115
03:30:57,140 --> 03:30:59,020
seem to be quite an important feature.

2116
03:30:59,860 --> 03:31:02,420
I'll decrease that a bit.

2117
03:31:03,340 --> 03:31:05,220
Seems to be quite an important feature,

2118
03:31:05,220 --> 03:31:06,660
and it's perfectly ordered.

2119
03:31:06,660 --> 03:31:10,580
So, it looks like if contact cellular will be zero or one,

2120
03:31:10,580 --> 03:31:14,060
so it looks like if it's one, it has a positive impact.

2121
03:31:14,060 --> 03:31:16,140
So, I can tell the marketing team,

2122
03:31:16,140 --> 03:31:19,060
well, it looks like all the points

2123
03:31:20,140 --> 03:31:23,460
for which you've contacted people by phone

2124
03:31:24,380 --> 03:31:27,380
seem to have, the fact that you contacted them by phone

2125
03:31:27,380 --> 03:31:30,700
seem to have a positive impact on the outcome,

2126
03:31:30,700 --> 03:31:34,340
whereas all the people that you did not contact by phone,

2127
03:31:34,340 --> 03:31:36,840
it seems to have a negative impact,

2128
03:31:36,840 --> 03:31:39,900
and sometimes quite a high negative impact.

2129
03:31:39,900 --> 03:31:42,460
So, at this point here, it's further than this one.

2130
03:31:42,460 --> 03:31:44,260
Here, the number of days, that's something

2131
03:31:44,260 --> 03:31:47,780
we've already noticed on the previous graph.

2132
03:31:47,780 --> 03:31:50,740
So, number of days, if it's,

2133
03:31:54,900 --> 03:31:58,060
yeah, if it's just normal value,

2134
03:31:58,060 --> 03:32:00,660
it doesn't seem to have much of an impact.

2135
03:32:00,660 --> 03:32:04,140
So, if number of days is between,

2136
03:32:04,140 --> 03:32:06,620
like, I don't know, higher than some values,

2137
03:32:06,620 --> 03:32:09,460
I forgot what did we say, but if it's too high,

2138
03:32:09,460 --> 03:32:11,740
it doesn't seem to have an impact at all.

2139
03:32:11,900 --> 03:32:14,260
But now, if your number of days is really low,

2140
03:32:14,260 --> 03:32:15,780
so that will be the color blue,

2141
03:32:15,780 --> 03:32:20,160
it seems to have a huge positive impact on the outcome.

2142
03:32:20,160 --> 03:32:22,500
So, here again, just by looking at that,

2143
03:32:22,500 --> 03:32:24,420
I can say number of days, if it's low,

2144
03:32:24,420 --> 03:32:26,500
it has a high positive impact.

2145
03:32:26,500 --> 03:32:27,700
Then you can see all sorts of things.

2146
03:32:27,700 --> 03:32:31,700
So, you can see campaign is the opposite story.

2147
03:32:31,700 --> 03:32:34,260
Like, if I contact people too many times,

2148
03:32:34,260 --> 03:32:36,980
so here, I contacted that person too many times,

2149
03:32:36,980 --> 03:32:40,840
it looks like it has such a huge negative impact

2150
03:32:41,280 --> 03:32:42,160
on the final outcome.

2151
03:32:42,160 --> 03:32:43,920
So, here, those points are probably outliers,

2152
03:32:43,920 --> 03:32:46,120
but I contacted them so many times

2153
03:32:46,120 --> 03:32:49,280
that that ends up being the main reason

2154
03:32:49,280 --> 03:32:51,360
why they didn't actually subscribe to the plan.

2155
03:32:51,360 --> 03:32:54,440
So, you can see a lot of things like that in this graph.

2156
03:32:54,440 --> 03:32:57,600
Here, those things don't seem to matter too much.

2157
03:32:57,600 --> 03:33:00,500
Those points here are around zero, sharp value.

2158
03:33:00,500 --> 03:33:02,800
Yes, someone had a question, yeah?

2159
03:33:02,800 --> 03:33:05,120
I don't think it would cause a big relationship.

2160
03:33:05,120 --> 03:33:08,280
I think if you nag someone at a time,

2161
03:33:08,280 --> 03:33:09,840
and they haven't responded yet,

2162
03:33:09,960 --> 03:33:13,120
and you nag them more, it won't help you.

2163
03:33:13,120 --> 03:33:15,920
So, saying that calling them four times

2164
03:33:15,920 --> 03:33:19,640
is a negative predictor is because if I call them four times

2165
03:33:19,640 --> 03:33:21,760
and they haven't signed up yet,

2166
03:33:21,760 --> 03:33:25,840
calling them four times won't cause that.

2167
03:33:25,840 --> 03:33:26,680
Yeah, so.

2168
03:33:26,680 --> 03:33:28,760
Calling them more is gonna make it less likely to subscribe.

2169
03:33:28,760 --> 03:33:31,960
It's just that if you have called them four times

2170
03:33:31,960 --> 03:33:33,320
and they haven't subscribed yet.

2171
03:33:33,320 --> 03:33:34,160
Yeah, so that's.

2172
03:33:34,160 --> 03:33:36,040
Calling them more times is probably not gonna help you at all.

2173
03:33:36,040 --> 03:33:37,760
But so, that's exactly the kind of things

2174
03:33:37,760 --> 03:33:40,560
that you need to end up discussing

2175
03:33:40,560 --> 03:33:42,280
with whoever is the domain expert.

2176
03:33:42,280 --> 03:33:45,360
Instead of just going through conclusion yourself,

2177
03:33:45,360 --> 03:33:49,480
you're like, well, okay, so that's what seems to happen,

2178
03:33:49,480 --> 03:33:50,800
and that's what I can see in the data,

2179
03:33:50,800 --> 03:33:52,120
the impact of different things.

2180
03:33:52,120 --> 03:33:55,200
And then you discuss, well, is that something

2181
03:33:55,200 --> 03:33:56,680
we can actually take action on,

2182
03:33:56,680 --> 03:33:58,640
or is that something that, as you said,

2183
03:33:58,640 --> 03:34:01,360
that is just how it is,

2184
03:34:01,360 --> 03:34:04,480
and that we're just seeing a normal impact.

2185
03:34:04,840 --> 03:34:08,040
Changing that will not make any difference.

2186
03:34:08,040 --> 03:34:09,240
It's not like you're gonna call them

2187
03:34:09,240 --> 03:34:11,040
and they're like, hey, forget it.

2188
03:34:11,040 --> 03:34:11,880
Yeah.

2189
03:34:11,880 --> 03:34:13,280
And they're not gonna subscribe.

2190
03:34:13,280 --> 03:34:14,120
Yeah, exactly.

2191
03:34:14,120 --> 03:34:15,760
They stop once they say yes, right?

2192
03:34:17,320 --> 03:34:20,040
So there's a big difference between what you see

2193
03:34:20,040 --> 03:34:22,040
in the data and what recommendations you make,

2194
03:34:22,040 --> 03:34:25,560
and whether those recommendations will be any useful or not.

2195
03:34:25,560 --> 03:34:27,360
And then just to finish with that,

2196
03:34:27,360 --> 03:34:29,160
something that I didn't mention,

2197
03:34:29,160 --> 03:34:32,560
an extra thing that you can see,

2198
03:34:32,560 --> 03:34:35,960
what you can do with SHAP is dependence plots.

2199
03:34:35,960 --> 03:34:38,360
So here, just really quickly,

2200
03:34:39,440 --> 03:34:43,400
if I wanna see the impact of age condition

2201
03:34:43,400 --> 03:34:47,240
with whether the person was contacted by telephone,

2202
03:34:47,240 --> 03:34:51,040
for example, here you've got the age on the x-axis.

2203
03:34:51,040 --> 03:34:53,800
So from 20 to 90 and the impact,

2204
03:34:53,800 --> 03:34:55,840
so the SHAP value here.

2205
03:34:55,840 --> 03:34:57,400
So I see that around age 20,

2206
03:34:57,400 --> 03:34:59,360
I have actually a high impact.

2207
03:34:59,360 --> 03:35:01,520
The age has a high impact.

2208
03:35:01,520 --> 03:35:04,000
Age, like between 60 and 90,

2209
03:35:04,000 --> 03:35:05,880
it has a high impact as well.

2210
03:35:05,880 --> 03:35:08,720
Age between 25 and 60,

2211
03:35:08,720 --> 03:35:10,120
it doesn't seem to have a high impact.

2212
03:35:10,120 --> 03:35:13,280
And there you can see how contacted

2213
03:35:13,280 --> 03:35:15,640
those different groups by telephone

2214
03:35:15,640 --> 03:35:17,800
had a different outcome.

2215
03:35:17,800 --> 03:35:20,600
And here it doesn't seem to show anything interesting.

2216
03:35:23,640 --> 03:35:26,180
Yeah, not much to see here.

2217
03:35:26,180 --> 03:35:29,920
But ideally, where such a plot would be useful

2218
03:35:29,920 --> 03:35:33,040
is that if you see that around age 60 and 90,

2219
03:35:33,040 --> 03:35:36,720
maybe the impact of calling someone on the phone

2220
03:35:36,720 --> 03:35:40,320
would be more important than another age or something.

2221
03:35:40,320 --> 03:35:41,680
So that could be an interesting,

2222
03:35:41,680 --> 03:35:43,760
here it turns out there is nothing interesting to see

2223
03:35:43,760 --> 03:35:45,080
with this specific plot,

2224
03:35:45,080 --> 03:35:48,320
but you've got the option to do that

2225
03:35:48,320 --> 03:35:50,860
and see how different features,

2226
03:35:52,480 --> 03:35:53,800
when they're linked together,

2227
03:35:53,800 --> 03:35:56,220
they might end up having a different impact

2228
03:35:56,220 --> 03:35:57,500
on your final outcome.

2229
03:35:58,420 --> 03:36:03,420
Right, so that's the end of this tutorial, a bit late.

2230
03:36:04,580 --> 03:36:09,740
Just to finish, so model interpretability,

2231
03:36:09,740 --> 03:36:11,420
it's important to give trust

2232
03:36:11,420 --> 03:36:13,220
that your model is doing the right thing.

2233
03:36:13,220 --> 03:36:16,460
It can allow you to debug your model

2234
03:36:16,460 --> 03:36:19,180
and really spot some biases in your data potentially,

2235
03:36:19,180 --> 03:36:22,220
and you might want to fix your model after that.

2236
03:36:22,220 --> 03:36:24,580
It allows you to explain to others as well

2237
03:36:24,580 --> 03:36:25,700
how a prediction was made.

2238
03:36:25,700 --> 03:36:28,100
If they're the decision maker, it's really important.

2239
03:36:28,100 --> 03:36:31,620
And also you have regulations that make it mandatory

2240
03:36:31,620 --> 03:36:33,380
to actually explain your model.

2241
03:36:33,380 --> 03:36:38,380
Now practically, what you end up seeing is that, well,

2242
03:36:42,300 --> 03:36:45,900
if your model is tree-based or something,

2243
03:36:45,900 --> 03:36:47,500
SHAP will be perfectly fine

2244
03:36:47,500 --> 03:36:50,340
and will probably allow you to do a more in-depth analysis

2245
03:36:50,340 --> 03:36:53,820
and see, get much more information locally

2246
03:36:53,820 --> 03:36:55,100
and globally as well.

2247
03:36:56,020 --> 03:36:58,260
So if your model is tree-based, you're in luck,

2248
03:36:58,260 --> 03:37:00,700
and you can use SHAP, and it's gonna be quick enough

2249
03:37:00,700 --> 03:37:03,940
so that you don't have to wait ages to get a result.

2250
03:37:03,940 --> 03:37:06,540
Now if your model is, if you want a method

2251
03:37:06,540 --> 03:37:08,620
that is completely model-agnostic,

2252
03:37:08,620 --> 03:37:12,080
you can still use SHAP, but it will take forever to compute.

2253
03:37:12,080 --> 03:37:16,700
So it's either how much, I guess, you value the output of it,

2254
03:37:16,700 --> 03:37:20,240
and if you wanna take hours to compute the SHAP values

2255
03:37:20,240 --> 03:37:23,380
on this specific model to be able to interpret it later,

2256
03:37:23,380 --> 03:37:24,960
then you might stick on SHAP.

2257
03:37:24,960 --> 03:37:28,760
If you prefer to go for a quick approximation,

2258
03:37:28,760 --> 03:37:31,320
you might want to use LIME instead.

2259
03:37:31,320 --> 03:37:32,560
I guess that would be a rule of thumb.

2260
03:37:32,560 --> 03:37:33,400
Yeah?

2261
03:37:33,400 --> 03:37:34,220
Yeah.

2262
03:37:34,220 --> 03:37:35,060
It would make sense to do

2263
03:37:35,060 --> 03:37:38,120
like this sort of standard feature importance first,

2264
03:37:38,120 --> 03:37:39,200
and then, oh yeah.

2265
03:37:39,200 --> 03:37:41,240
Off like random features.

2266
03:37:41,240 --> 03:37:45,380
Yeah, so even like something like permutation importance

2267
03:37:45,380 --> 03:37:50,280
would generalize to all models fairly quick to compute

2268
03:37:50,280 --> 03:37:51,960
and gives you something that you can compare.

2269
03:37:51,960 --> 03:37:54,180
So yeah, probably you'd start with permutation importance

2270
03:37:54,300 --> 03:37:56,700
to see maybe as a feature selection as well,

2271
03:37:56,700 --> 03:37:59,340
or just to first debugging intuitively

2272
03:37:59,340 --> 03:38:01,220
what seems to be important or not,

2273
03:38:01,220 --> 03:38:04,560
and then if you want to really go into more depth,

2274
03:38:04,560 --> 03:38:06,140
SHAP or LIME depending what's,

2275
03:38:07,700 --> 03:38:10,900
how much time you wanna spend on it, I guess.

2276
03:38:10,900 --> 03:38:12,300
Yeah?

2277
03:38:12,300 --> 03:38:13,140
Great.

2278
03:38:13,140 --> 03:38:15,540
So that's all for me.

2279
03:38:15,540 --> 03:38:16,380
Yeah.

2280
03:38:16,660 --> 03:38:17,480
Yep.

2281
03:38:17,480 --> 03:38:18,380
Thank you.

2282
03:38:18,380 --> 03:38:19,220
Thank you.

2283
03:38:19,220 --> 03:38:20,220
Thank you.

2284
03:38:20,220 --> 03:38:21,060
Thank you.

2285
03:38:21,060 --> 03:38:21,880
Bye bye.

