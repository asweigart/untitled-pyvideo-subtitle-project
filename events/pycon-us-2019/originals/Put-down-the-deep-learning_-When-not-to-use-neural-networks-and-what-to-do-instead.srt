1
00:00:00,000 --> 00:00:05,600
Welcome, everyone.

2
00:00:05,600 --> 00:00:09,520
Just as a reminder, please make sure to put all of your devices in silence, otherwise

3
00:00:09,520 --> 00:00:13,440
you will have a glare from me.

4
00:00:13,440 --> 00:00:16,000
We are welcoming Rachel Tatman.

5
00:00:16,000 --> 00:00:20,560
She will be presenting Put Down Your Deep Learning, When Not to Use Neural Networks,

6
00:00:20,560 --> 00:00:23,560
and What to Do Instead.

7
00:00:23,560 --> 00:00:24,560
Hello.

8
00:00:24,560 --> 00:00:33,440
Sorry, I just wanted to make sure my phones were on silent, especially since they're right

9
00:00:33,440 --> 00:00:34,440
next to that microphone.

10
00:00:34,440 --> 00:00:36,440
Good afternoon, everybody.

11
00:00:36,440 --> 00:00:38,160
Thank you for coming.

12
00:00:38,160 --> 00:00:39,760
I am Rachel Tatman.

13
00:00:39,760 --> 00:00:41,800
I'm a data scientist advocate.

14
00:00:41,800 --> 00:00:43,760
I'm trying to say that my...

15
00:00:43,760 --> 00:00:46,160
Have a job title that says what I do, so I'm trying this one out.

16
00:00:46,160 --> 00:00:49,040
Let me know if it's helpful for you guys.

17
00:00:49,040 --> 00:00:50,040
At Kaggle.

18
00:00:50,240 --> 00:00:53,600
I'm very happy to talk about Kaggle, but that's not what I'm here to talk about today.

19
00:00:53,600 --> 00:00:58,640
Today I'm here to talk about deep learning and, more importantly, alternatives to deep

20
00:00:58,640 --> 00:00:59,640
learning.

21
00:00:59,640 --> 00:01:04,160
Just to get a sense of the room before I start out, who's heard of deep learning?

22
00:01:04,160 --> 00:01:05,160
Yay.

23
00:01:05,160 --> 00:01:06,160
Pretty much everybody.

24
00:01:06,160 --> 00:01:08,840
Presumably that's why you're here.

25
00:01:08,840 --> 00:01:10,640
Who's done some deep learning?

26
00:01:10,640 --> 00:01:14,040
Anything from TensorFlow to poets to it's part of your job?

27
00:01:14,040 --> 00:01:15,040
Okay.

28
00:01:15,040 --> 00:01:17,440
I'd say about half the people.

29
00:01:17,440 --> 00:01:21,520
Some folks were like, kind of.

30
00:01:21,520 --> 00:01:26,800
I probably actually don't need to tell you guys this, but I think it's worth going over.

31
00:01:26,800 --> 00:01:34,560
Deep learning is a fantastic set of technologies, and we've seen some amazing wins lately.

32
00:01:34,560 --> 00:01:38,600
I've just picked a couple of examples of things that I think are particularly impressive.

33
00:01:38,600 --> 00:01:43,600
Open AI 5 is the model that just beat, they say, Dota 2 gods.

34
00:01:43,600 --> 00:01:47,840
There are several video game players at the video game that they play professionally at

35
00:01:47,840 --> 00:01:52,080
a live sporting, e-sporting event for the first time.

36
00:01:52,080 --> 00:01:54,880
It's very exciting, and that happened, I think, last week.

37
00:01:54,880 --> 00:02:00,880
Previously, AlphaGo Zero was a Google model that beat the best Go players, it's a board

38
00:02:00,880 --> 00:02:05,120
game in the world, at the board game that they play professionally.

39
00:02:05,120 --> 00:02:10,440
Another really exciting instance of computers being better than humans at a very, very narrow

40
00:02:10,440 --> 00:02:12,440
task.

41
00:02:12,440 --> 00:02:17,000
There's a robot that came out relatively recently, or the paper came out recently, and it looks

42
00:02:17,000 --> 00:02:21,000
in a bin of things, and it picks out things and it tosses them into other bins, so it

43
00:02:21,000 --> 00:02:25,760
basically sorts by throwing, which seems really dumb, and like a toddler could do it, but

44
00:02:25,760 --> 00:02:27,320
it's actually very hard computationally.

45
00:02:27,320 --> 00:02:30,680
That's an impressive finding.

46
00:02:30,680 --> 00:02:36,000
This particular newspaper is a little bit hypey, a little bit fiari.

47
00:02:36,000 --> 00:02:41,720
I think the actual paper title is, like, towards human parody, and it sort of got juiced up

48
00:02:41,760 --> 00:02:47,280
a little bit in the PR rounds, but speech recognition has gotten much, much better.

49
00:02:47,280 --> 00:02:53,160
It's maybe not quite at the point where we can replace human transcribers, thank you,

50
00:02:53,160 --> 00:02:56,320
and especially if you want high-quality transcriptions, I probably wouldn't use it for everything,

51
00:02:56,320 --> 00:03:01,320
but we're getting to the point where it's fairly usable for most things.

52
00:03:01,320 --> 00:03:06,320
Thank you.

53
00:03:06,320 --> 00:03:16,320
So, it seems like, especially if you're sort of outside the field and just learning about

54
00:03:16,320 --> 00:03:20,280
it through headlines, machine learning can do anything these days, right, and especially

55
00:03:20,280 --> 00:03:24,680
deep learning, which means that we can use deep learning to do everything, right?

56
00:03:24,680 --> 00:03:26,440
Not so much.

57
00:03:26,440 --> 00:03:30,860
So I think that being in the deep learning field right now is a little bit like being

58
00:03:30,860 --> 00:03:32,480
a paleontologist.

59
00:03:32,480 --> 00:03:36,200
So you tell people you're a paleontologist, I'm assuming I'm not a paleontologist, and

60
00:03:36,200 --> 00:03:40,640
people are like, oh, cool, I'm thinking about dinosaurs, they're so powerful, they're so

61
00:03:40,640 --> 00:03:42,560
big, they're so exciting.

62
00:03:42,560 --> 00:03:44,800
Maybe small children are impressed.

63
00:03:44,800 --> 00:03:47,400
Small children generally aren't impressed when I tell them I do machine learning, but

64
00:03:47,400 --> 00:03:49,880
maybe someday they will be.

65
00:03:49,880 --> 00:03:53,920
And then you have the sort of the glamorous experience of what people think deep learning

66
00:03:53,920 --> 00:03:57,960
is like, and then you have the less glamorous reality.

67
00:03:57,960 --> 00:04:04,440
So building deep learning models, even if you're not doing research, is slow, it's tedious,

68
00:04:04,440 --> 00:04:09,000
it's painstaking, it takes a lot of time, it's really expensive.

69
00:04:09,000 --> 00:04:11,880
We'll talk about how expensive in a little bit.

70
00:04:11,880 --> 00:04:15,440
And it's frustrating because there's this sense of uncertainty.

71
00:04:15,440 --> 00:04:18,800
When you set out working on a project and you're building a specific model, you don't

72
00:04:18,800 --> 00:04:20,600
know that it's going to work.

73
00:04:20,600 --> 00:04:23,600
And you don't know that the specific parameters that you're using and the specific hyperparameters

74
00:04:23,600 --> 00:04:25,700
that you're using are going to work for your problem.

75
00:04:25,700 --> 00:04:28,240
You don't know what architecture is going to work best.

76
00:04:28,240 --> 00:04:31,160
The state of the art right now is guess and test.

77
00:04:31,160 --> 00:04:32,740
And I am in no way exaggerating.

78
00:04:32,740 --> 00:04:36,460
There's a lot of deep learning models that we don't have a deep theoretical understanding

79
00:04:36,460 --> 00:04:40,260
of, we just have a lot of empirical results that show that they're good for a certain

80
00:04:40,260 --> 00:04:42,460
set of problems, which is really frustrating.

81
00:04:42,460 --> 00:04:45,720
And I have a really good example of this.

82
00:04:45,720 --> 00:04:49,140
So this is a passage from the BERT paper.

83
00:04:49,140 --> 00:04:56,660
BERT is a large neural network based, what am I saying, language model, that's the words

84
00:04:56,660 --> 00:04:59,500
I'm looking for, which is unsupervised.

85
00:04:59,500 --> 00:05:04,100
So it's basically trained on a bunch of text data, it notices patterns more or less.

86
00:05:04,100 --> 00:05:07,860
And then the idea is that you have your big language model and on top of it you have a

87
00:05:07,860 --> 00:05:12,920
little bit of fine tuning for your specific task where you do have labeled data.

88
00:05:12,920 --> 00:05:16,060
So it's a lot of unsupervised learning, a little bit of supervised learning.

89
00:05:16,060 --> 00:05:18,700
It seems to be pretty transferable between tasks.

90
00:05:18,700 --> 00:05:20,740
So that's exciting.

91
00:05:20,740 --> 00:05:26,500
Unfortunately, and this is a really good paper, it just won best paper at NACL, which is one

92
00:05:26,500 --> 00:05:29,580
of the premier NLP, natural language processing conferences.

93
00:05:29,580 --> 00:05:31,860
This is state of the art research.

94
00:05:31,860 --> 00:05:35,420
And something in the paper that they said that really stuck with me is that as they

95
00:05:35,420 --> 00:05:39,940
were doing this fine tuning, building this sort of little layer on top, sometimes it

96
00:05:39,940 --> 00:05:41,460
just didn't work.

97
00:05:41,460 --> 00:05:43,660
And when that happened, they just restarted it randomly.

98
00:05:43,660 --> 00:05:45,080
And sometimes it did work.

99
00:05:45,080 --> 00:05:49,260
So you just have to run a bunch of models and figure out which one works well and use

100
00:05:49,260 --> 00:05:50,880
that one.

101
00:05:50,880 --> 00:05:57,480
That doesn't feel super satisfying as a working methodology, at least to me.

102
00:05:57,480 --> 00:06:00,720
I'd like to know what works ahead of time.

103
00:06:00,720 --> 00:06:03,320
I also mentioned expense, and I know these are a little bit hard to read.

104
00:06:03,320 --> 00:06:06,240
I'll give you guys a link to the slides at the end.

105
00:06:06,240 --> 00:06:10,280
People don't generally tell you how much it costs to train their model, but folks are

106
00:06:10,280 --> 00:06:14,300
also very willing to go on Twitter and do sort of back of the envelope math.

107
00:06:14,300 --> 00:06:18,780
So here's one estimate for big GAN, which is an image generation model.

108
00:06:18,780 --> 00:06:20,380
I think it came out in January.

109
00:06:20,380 --> 00:06:23,180
You might have seen headlines like, this dog doesn't exist.

110
00:06:23,180 --> 00:06:26,700
A computer imagined it, or something like that.

111
00:06:26,700 --> 00:06:32,200
And the estimate for back of the envelope math is around $60,000 just of compute.

112
00:06:32,200 --> 00:06:36,860
That does not count what you might need to pay to license the images to train it.

113
00:06:36,860 --> 00:06:41,760
GPT-2 is another big neural language model like BERT.

114
00:06:41,760 --> 00:06:44,560
This one was bigger and trained for longer.

115
00:06:44,560 --> 00:06:50,680
And again, back of the envelope math, maybe around $43,000 worth of compute.

116
00:06:50,680 --> 00:06:53,000
And these are by no means the biggest models.

117
00:06:53,000 --> 00:06:59,960
So I mentioned OpenAI 5, which is the algorithm, or sorry, the model that won Dota 2.

118
00:06:59,960 --> 00:07:04,120
That trained continuously for 10 months.

119
00:07:04,120 --> 00:07:05,680
I cannot tell you how much it cost.

120
00:07:05,680 --> 00:07:08,720
I feel very comfortable saying many millions of dollars.

121
00:07:08,720 --> 00:07:09,720
Right?

122
00:07:09,960 --> 00:07:13,760
And I don't know about you guys and your relationships with your managers, but if I go to my manager

123
00:07:13,760 --> 00:07:19,400
and ask for $50,000 worth of compute for something that might work, I would be encouraged to

124
00:07:19,400 --> 00:07:22,000
explore other approaches to that problem.

125
00:07:22,000 --> 00:07:27,200
So when would I actually use deep learning?

126
00:07:27,200 --> 00:07:31,600
And I'm going to keep this talk very high level and just sort of talk about my experiences.

127
00:07:31,600 --> 00:07:32,920
I'm not going to go into the math.

128
00:07:32,920 --> 00:07:37,840
There are many resources out there, and feel free to hit me up afterwards if you want recommendations.

129
00:07:37,880 --> 00:07:42,920
I would use deep learning for a problem if, one, a human can do it in less than a second.

130
00:07:42,920 --> 00:07:47,640
The longer it takes you to do something, especially something cognitive, the more complex it probably

131
00:07:47,640 --> 00:07:51,440
is, the more data I'm going to need, and the bigger the model I'm going to need to train

132
00:07:51,440 --> 00:07:53,560
in order to approximate it.

133
00:07:53,560 --> 00:07:57,960
So one second is just sort of my personal litmus test right now.

134
00:07:57,960 --> 00:08:01,880
I have to have a high tolerance for weird errors.

135
00:08:01,880 --> 00:08:04,160
Neural networks have neural in the name.

136
00:08:04,160 --> 00:08:05,560
They are not cognitive models.

137
00:08:05,560 --> 00:08:08,600
They are not telling you what a human would do in this situation.

138
00:08:08,600 --> 00:08:14,400
So a really good example of this is humans use silhouettes when identifying objects.

139
00:08:14,400 --> 00:08:17,840
That's just sort of a cognitive bias that we have.

140
00:08:17,840 --> 00:08:20,040
Most computer vision models do not use silhouettes.

141
00:08:20,040 --> 00:08:21,720
They use collection of pixels.

142
00:08:21,720 --> 00:08:25,120
So they're much easier to fool with textures than they are with silhouettes.

143
00:08:25,120 --> 00:08:29,920
So humans have a very gestalt view if you have taken much psychology, and neural networks

144
00:08:29,920 --> 00:08:30,920
don't.

145
00:08:31,800 --> 00:08:35,480
I would also only use a neural network if I didn't need to explain myself.

146
00:08:35,480 --> 00:08:40,360
So if I were, say, working in finance and needed to show with a certain degree of confidence

147
00:08:40,360 --> 00:08:44,160
that I did not use race or any correlative race to decide whether or not to give you

148
00:08:44,160 --> 00:08:47,520
a mortgage, I can't do that with a neural network.

149
00:08:47,520 --> 00:08:52,040
And we've been doing as a field, not me personally, a lot of work on interpretability and explainability,

150
00:08:52,040 --> 00:08:55,940
and it is getting much better, but we're not to the same point that we are already with

151
00:08:55,940 --> 00:08:58,000
regression models.

152
00:08:58,080 --> 00:09:01,800
I would also only use neural networks if I had a large quantity of labeled data.

153
00:09:01,800 --> 00:09:04,920
I say more than 5,000 labeled items per class.

154
00:09:04,920 --> 00:09:06,520
That is very much a low ball.

155
00:09:06,520 --> 00:09:10,640
You can get away with less if you already have a trained model and you can do transfer

156
00:09:10,640 --> 00:09:15,440
learning, but still, you need a lot of data, and all of the research so far has shown the

157
00:09:15,440 --> 00:09:19,480
more data, the bigger the model, the more compute, the better the results.

158
00:09:19,480 --> 00:09:24,400
And finally, I'd only do it if I had a lot of time and money.

159
00:09:24,400 --> 00:09:27,240
So training models takes time, especially if I'm doing model comparison.

160
00:09:27,480 --> 00:09:31,360
If I'm doing any sort of hyperparameter search, that takes much more time because I have to

161
00:09:31,360 --> 00:09:34,320
train multiple models and see which one is better.

162
00:09:34,320 --> 00:09:35,320
And money.

163
00:09:35,320 --> 00:09:39,520
So for data annotation, if I need additional labeled data, if I need someone to go through

164
00:09:39,520 --> 00:09:43,280
and maybe do some sort of human-in-the-loop tuning, that costs money.

165
00:09:43,280 --> 00:09:46,640
And compute, of course, also costs money, and we talked about sort of the orders of

166
00:09:46,640 --> 00:09:51,340
magnitude that we're talking about there for a really large model.

167
00:09:51,340 --> 00:09:53,180
So deep learning.

168
00:09:53,180 --> 00:09:54,180
You need a lot of time.

169
00:09:54,180 --> 00:09:55,440
You need a lot of money.

170
00:09:55,440 --> 00:09:57,440
You need a lot of data.

171
00:09:57,440 --> 00:09:58,920
Some of you may be in that position.

172
00:09:58,920 --> 00:09:59,920
Fantastic.

173
00:09:59,920 --> 00:10:01,240
Go forth and deep learn.

174
00:10:01,240 --> 00:10:03,360
If you are not, you have lots of options.

175
00:10:03,360 --> 00:10:07,920
I am not going to cover the entire fields of statistics and machine learning today,

176
00:10:07,920 --> 00:10:10,340
but I am going to talk about three different types of models.

177
00:10:10,340 --> 00:10:13,600
So in particular, regression, tree-based models.

178
00:10:13,600 --> 00:10:15,580
Tree-based models didn't fit well in my matrix.

179
00:10:15,580 --> 00:10:17,080
And then distance-based models.

180
00:10:17,080 --> 00:10:21,460
Let's start with regression.

181
00:10:21,460 --> 00:10:26,300
So if you have done much machine learning, I'm hoping you are already familiar with

182
00:10:26,300 --> 00:10:27,300
regression.

183
00:10:27,300 --> 00:10:28,300
Quick show of hands.

184
00:10:28,300 --> 00:10:31,300
Who here has never learned about regression?

185
00:10:31,300 --> 00:10:33,420
Either in a blog post or everyone has.

186
00:10:33,420 --> 00:10:34,420
Fantastic.

187
00:10:34,420 --> 00:10:36,060
I'm really, really, really happy to hear that.

188
00:10:36,060 --> 00:10:40,380
I talked to a lot of aspiring data scientists and people who want to get into the field

189
00:10:40,380 --> 00:10:44,340
who started with deep learning and aren't familiar with other methods like regression.

190
00:10:44,340 --> 00:10:46,140
That's part of the reason that I put this talk together.

191
00:10:46,140 --> 00:10:47,140
I love regression.

192
00:10:47,140 --> 00:10:49,580
I'm a big regression fangirl.

193
00:10:49,580 --> 00:10:55,060
In regression, you do need to do a little bit of hands-on pointing of the model in the

194
00:10:55,060 --> 00:10:56,260
direction that you want it to go.

195
00:10:56,260 --> 00:11:00,180
So you need to pick the family of the function you'll use to model your data.

196
00:11:00,180 --> 00:11:06,340
In this case, we have the simplest case of linear regression where the thing on the x-axis,

197
00:11:06,340 --> 00:11:10,220
whatever it is, has a linear relationship with the thing on the y-axis, whatever that

198
00:11:10,220 --> 00:11:11,220
is.

199
00:11:11,220 --> 00:11:14,660
This is just a picture of a regression that's in the public domain.

200
00:11:14,660 --> 00:11:18,340
And there's really a lot of kinds of regression models out there.

201
00:11:18,340 --> 00:11:22,260
And unlike with most types of neural networks, we have a really principled understanding

202
00:11:22,260 --> 00:11:25,060
of what types of problems these are going to be good for.

203
00:11:25,060 --> 00:11:28,620
So if you're using regression, you can basically sit down and work through a flow chart to

204
00:11:28,620 --> 00:11:33,460
fix the specific type of regression that's going to work best for your problem, which

205
00:11:33,460 --> 00:11:35,740
is really nice.

206
00:11:35,740 --> 00:11:40,220
Some really big advantages of regression, fast to fit, much faster to the fit than neural

207
00:11:40,220 --> 00:11:43,380
networks, especially if you're working with a well-optimized library.

208
00:11:43,380 --> 00:11:47,020
The Python regression libraries tend to vary wildly, so you might want to do a little bit

209
00:11:47,020 --> 00:11:49,060
of shopping around.

210
00:11:49,060 --> 00:11:50,420
It works well with small data.

211
00:11:50,420 --> 00:11:55,060
I've done regression on a dozen data points and learned something useful and interesting.

212
00:11:55,060 --> 00:11:59,820
And it's also really easy to interpret, and I'll walk through an example of interpreting

213
00:11:59,820 --> 00:12:02,580
it a bit, but it looks like most of you are already familiar with regression, so that

214
00:12:02,580 --> 00:12:05,300
might be a little bit redundant.

215
00:12:05,300 --> 00:12:06,580
There are some downsides.

216
00:12:06,580 --> 00:12:11,580
You need to do a little bit more data preparation than for some other methods.

217
00:12:11,580 --> 00:12:16,620
If there are two variables that are strongly correlated, you might want to spend some time

218
00:12:16,620 --> 00:12:20,620
either creating a feature based on those models or correcting because you're going to have

219
00:12:20,620 --> 00:12:26,020
a higher standard error on account of multicollinearity, classic machine learning interview question.

220
00:12:26,020 --> 00:12:28,660
And they also require validation.

221
00:12:28,660 --> 00:12:34,020
So regression models are based on strong assumptions about the distribution of the data points

222
00:12:34,020 --> 00:12:37,540
or the distribution of the errors, and if you are using a regression model, you need

223
00:12:37,540 --> 00:12:39,500
to check that those assumptions are true.

224
00:12:39,500 --> 00:12:40,900
Otherwise you'll get numbers out.

225
00:12:40,900 --> 00:12:45,420
They just won't mean anything.

226
00:12:45,420 --> 00:12:50,400
My personal favorite type of regression model is mixed effects regression.

227
00:12:50,400 --> 00:12:53,620
If I had to pick a single machine learning model to use for the rest of my life, honestly

228
00:12:53,620 --> 00:12:55,500
it would probably be mixed effects regression.

229
00:12:55,500 --> 00:12:59,340
And this is a little example of Simpson's paradox.

230
00:12:59,340 --> 00:13:01,780
So who's familiar with Simpson's paradox?

231
00:13:01,780 --> 00:13:03,620
Oh, not everybody.

232
00:13:03,620 --> 00:13:04,620
Excellent.

233
00:13:04,620 --> 00:13:09,340
So the idea with Simpson's paradox is if you look at all the points together as a group,

234
00:13:09,340 --> 00:13:14,220
you see one trend, but if you divide the groups into subgroups that are meaningful for your

235
00:13:14,220 --> 00:13:17,180
specific problem, you see the opposite trend.

236
00:13:17,180 --> 00:13:23,060
So here, for example, let's say that the x-axis is amount of time spent playing an educational

237
00:13:23,060 --> 00:13:26,220
video game and the y-axis is test scores.

238
00:13:26,220 --> 00:13:30,180
And we want to see if playing this video game improves people's test scores.

239
00:13:30,180 --> 00:13:33,860
If we look at all the students, so let's see each point as a student, we will see a negative

240
00:13:33,860 --> 00:13:34,860
trend.

241
00:13:34,860 --> 00:13:37,980
So the more time you spend playing the video game, the worse your test scores are.

242
00:13:37,980 --> 00:13:42,380
I'm asking you to imagine axes and axes levels here.

243
00:13:42,620 --> 00:13:47,580
If we break things down by classroom, where each color of student, each color of dot is

244
00:13:47,580 --> 00:13:50,500
a different classroom, we see the opposite trend.

245
00:13:50,500 --> 00:13:54,860
And so it might be that different classes pattern together differently for other reasons.

246
00:13:54,860 --> 00:14:00,460
So maybe some teachers were given an easier version of the test or maybe the classes were

247
00:14:00,460 --> 00:14:05,460
sort of divided by ability in the particular subject area that we're looking at.

248
00:14:05,460 --> 00:14:09,460
So when we model each class individually, we see that spending more time playing the

249
00:14:09,500 --> 00:14:11,900
video game actually improves our test scores.

250
00:14:11,900 --> 00:14:17,100
And with mixed effects regression, we can really easily pull out these effects.

251
00:14:17,100 --> 00:14:20,900
And that would be the random effect here would be which group you're assigned to or which

252
00:14:20,900 --> 00:14:21,900
classroom you are in.

253
00:14:21,900 --> 00:14:25,820
I'm not going to show you a whole bunch of code here.

254
00:14:25,820 --> 00:14:29,660
The main thing I want you to get from my code samples is that all of these models are very

255
00:14:29,660 --> 00:14:30,660
easy to implement.

256
00:14:30,660 --> 00:14:32,100
They don't take a lot of code.

257
00:14:32,100 --> 00:14:36,980
Even a very minimal TensorFlow or Keras deep learning model is going to be more code than

258
00:14:36,980 --> 00:14:37,980
this.

259
00:14:37,980 --> 00:14:42,100
So I'm using the stats model library module, sorry.

260
00:14:42,100 --> 00:14:43,980
And I've used the mix LM method.

261
00:14:43,980 --> 00:14:49,180
And here I am modeling the chance of being admitted to graduate school based on your

262
00:14:49,180 --> 00:14:53,140
GRE, which is a standardized test for people who are interested in going to graduate school

263
00:14:53,140 --> 00:14:56,640
and TOEFL score, which is a test of English as a foreign language.

264
00:14:56,640 --> 00:14:57,940
And then I am grouping things.

265
00:14:57,940 --> 00:15:01,580
So I'm using as a random variable here, university rating.

266
00:15:01,580 --> 00:15:06,180
So I think that maybe really prestigious universities are going to pattern together and maybe like

267
00:15:06,580 --> 00:15:10,500
smaller regional universities are likely to pattern together.

268
00:15:10,500 --> 00:15:11,700
And then I fit my model.

269
00:15:11,700 --> 00:15:15,380
And that's all there is to it.

270
00:15:15,380 --> 00:15:19,700
I mentioned that regression models were particularly easy to interpret.

271
00:15:19,700 --> 00:15:24,700
And here is the model output that we can use to answer specific questions.

272
00:15:24,700 --> 00:15:29,060
So if a student came to me and was like, hey, I have a limited amount of time to study,

273
00:15:29,060 --> 00:15:32,380
should I focus on my GRE or should I focus on my TOEFL exam?

274
00:15:32,380 --> 00:15:37,380
So by looking at the coefficients here, I can see that every one point increase in GRE

275
00:15:37,380 --> 00:15:41,100
score increases your chance to admit to graduate school by half a percent.

276
00:15:41,100 --> 00:15:46,740
And every one point increase in TOEFL score increases your chances of admitting to graduate

277
00:15:46,740 --> 00:15:49,380
school by seven tenths of a percent.

278
00:15:49,380 --> 00:15:53,940
So assuming that one unit of studying is going to raise your score in either test by the

279
00:15:53,940 --> 00:15:57,260
same amount, I suggest focusing on the TOEFL score.

280
00:15:57,260 --> 00:16:02,060
So that's a clear, actionable recommendation I can make based on the model output that

281
00:16:02,060 --> 00:16:03,060
I have.

282
00:16:03,060 --> 00:16:07,180
And if this was a business setting and people were like, hey, should we focus on, you know,

283
00:16:07,180 --> 00:16:11,420
I can't come up with a good example right now, I could tell them based on the features

284
00:16:11,420 --> 00:16:12,780
that I had.

285
00:16:12,780 --> 00:16:14,420
All right.

286
00:16:14,420 --> 00:16:16,520
So regression modeling takes some time.

287
00:16:16,520 --> 00:16:18,380
You need to do a little bit more hands-on stuff.

288
00:16:18,380 --> 00:16:19,520
You need to do your validation.

289
00:16:19,520 --> 00:16:21,500
You probably need to do some additional data cleaning.

290
00:16:21,500 --> 00:16:24,900
It does not take that much money, doesn't need a lot of compute, and can work with pretty

291
00:16:24,900 --> 00:16:27,140
small data sets.

292
00:16:27,140 --> 00:16:30,260
Trees.

293
00:16:30,260 --> 00:16:34,260
So hopefully everyone has seen something like a decision tree at some point.

294
00:16:34,260 --> 00:16:37,060
Has anyone not run across tree structures at some point?

295
00:16:37,060 --> 00:16:38,740
Okay, I see a sample hands.

296
00:16:38,740 --> 00:16:44,060
So the idea with a tree structure and specifically a decision tree is at each node you look at

297
00:16:44,060 --> 00:16:48,800
one feature for each of your items, and depending on the value of that feature, you decide which

298
00:16:48,800 --> 00:16:50,160
path to go down.

299
00:16:50,160 --> 00:16:54,220
And when you go down that path, the next node has another feature, and you base on the value

300
00:16:54,220 --> 00:16:55,220
of that feature.

301
00:16:55,220 --> 00:16:59,020
So basically, you are recursively cutting your decision region into smaller and smaller

302
00:16:59,020 --> 00:17:00,020
chunks.

303
00:17:00,020 --> 00:17:01,020
That's the idea.

304
00:17:01,020 --> 00:17:03,980
Nobody uses trees.

305
00:17:03,980 --> 00:17:05,900
Everybody uses forests.

306
00:17:05,900 --> 00:17:08,620
So random forests are ensemble models.

307
00:17:08,620 --> 00:17:13,340
So they combine a lot of different decision trees together into a single model.

308
00:17:13,340 --> 00:17:15,900
They are extremely popular on Kaggle.

309
00:17:15,900 --> 00:17:19,260
If you're in the machine learning community, you might actually associate random forests

310
00:17:19,260 --> 00:17:20,260
with Kaggle.

311
00:17:20,260 --> 00:17:26,220
And about from 2010 to 2016, about two-thirds of all Kaggle competition winners used random

312
00:17:26,220 --> 00:17:27,220
forests.

313
00:17:27,220 --> 00:17:29,700
Less than half used some form of deep learning.

314
00:17:29,700 --> 00:17:32,420
And random forests continue to do very well today.

315
00:17:32,420 --> 00:17:37,300
They also tend to have better performance than logistic regression, especially for classification.

316
00:17:37,300 --> 00:17:40,180
There's a paper there you can read if you're interested.

317
00:17:40,180 --> 00:17:44,100
And they work really well.

318
00:17:44,100 --> 00:17:45,740
Some benefits and drawbacks.

319
00:17:45,740 --> 00:17:48,780
You don't need to do a lot of data cleaning or model validation.

320
00:17:48,780 --> 00:17:51,980
You can pretty much shove anything into a random forest.

321
00:17:51,980 --> 00:17:54,500
You don't need to convert your categorical variables.

322
00:17:54,500 --> 00:17:58,640
It'll just sort of consume them and give you a model.

323
00:17:58,640 --> 00:18:02,740
And there are a lot of really easy to use packages with a really fantastic developer

324
00:18:02,740 --> 00:18:05,420
experience that are also very well optimized.

325
00:18:05,420 --> 00:18:08,520
So XGBoost, which I like, LightGBM, CatBoost.

326
00:18:08,520 --> 00:18:11,480
There's going to be a new one in the next Scikit-learn release candidate.

327
00:18:11,480 --> 00:18:16,020
You can check that out if you're interested in yet another very good option, I'm sure.

328
00:18:16,020 --> 00:18:17,020
Some drawbacks.

329
00:18:17,020 --> 00:18:19,440
They are very easy to overfit.

330
00:18:19,440 --> 00:18:23,060
You can imagine if you're building a tree, the natural end case would be you had one

331
00:18:23,060 --> 00:18:24,680
data point per leaf.

332
00:18:24,680 --> 00:18:27,880
And then you have perfectly described the distribution of your training data, which

333
00:18:27,920 --> 00:18:30,440
is the definition of overfitting.

334
00:18:30,440 --> 00:18:34,240
So to get around that, people will usually trim and remove the bottom couple layers from

335
00:18:34,240 --> 00:18:35,240
their tree.

336
00:18:35,240 --> 00:18:38,640
They're also really sensitive to differences between datasets.

337
00:18:38,640 --> 00:18:43,720
And this is a little bit blown out, but this is actually the decision region of a single

338
00:18:43,720 --> 00:18:44,720
decision tree.

339
00:18:44,720 --> 00:18:46,720
And it sort of looks like a plaid.

340
00:18:46,720 --> 00:18:49,760
And you can imagine that if there's a point in one of those plaid stripes of the opposite

341
00:18:49,760 --> 00:18:53,880
color, then a tree trained on that dataset would actually have that stripe be a different

342
00:18:53,880 --> 00:18:55,080
color, if that makes sense.

343
00:18:55,280 --> 00:18:56,280
It's a little bit hard to see.

344
00:18:58,280 --> 00:19:01,360
They are also less interpretable than regression.

345
00:19:01,360 --> 00:19:02,480
You can get information out.

346
00:19:02,480 --> 00:19:06,080
You can get feature importance out, ranked feature importance pretty easily.

347
00:19:06,080 --> 00:19:09,960
But it's harder to say, hey, if I change this, what would change?

348
00:19:09,960 --> 00:19:12,880
And tweak the knobs in quite the same way.

349
00:19:12,880 --> 00:19:15,800
And they can also require a lot more compute and training time.

350
00:19:15,800 --> 00:19:18,440
So I mentioned a lot of well-optimized packages.

351
00:19:18,440 --> 00:19:21,280
But ensembles require building a lot of models.

352
00:19:21,480 --> 00:19:25,480
And for every additional model, there's additional training time involved as a cost.

353
00:19:27,480 --> 00:19:29,480
And again, a little code example.

354
00:19:29,480 --> 00:19:31,760
I happen to like XGBoost.

355
00:19:31,760 --> 00:19:36,600
One thing that's really nice about XGBoost is that the default parameter, hyperparameter

356
00:19:36,600 --> 00:19:40,160
settings are very reasonable for the vast majority of problems.

357
00:19:40,160 --> 00:19:42,720
So you can pretty much just use XGBoost out of the box.

358
00:19:42,720 --> 00:19:45,520
And most of the time, it will work well enough.

359
00:19:45,520 --> 00:19:48,320
If you're hoping to win a Kaggle competition, you are going to need to do some tuning.

360
00:19:48,320 --> 00:19:50,280
But it's a really good starting point.

361
00:19:52,280 --> 00:19:57,640
So tree-based methods take some time, especially if you have a big ensemble, a big data set,

362
00:19:57,640 --> 00:20:01,360
and you want to do maybe some cross-validation, some jackknifing, some bootstrapping to help

363
00:20:01,360 --> 00:20:03,880
avoid that overfitting to data sets.

364
00:20:03,880 --> 00:20:05,280
Again, not a lot of money.

365
00:20:05,280 --> 00:20:07,400
They don't take that much compute.

366
00:20:07,400 --> 00:20:11,120
And they do need some data and a little bit more than for regression, again, because you're

367
00:20:11,120 --> 00:20:15,160
going to want to do more rigorous cross-validation.

368
00:20:15,160 --> 00:20:18,080
And finally, distance-based methods.

369
00:20:18,080 --> 00:20:22,840
So this is a very vague term that I've used to group together a large class of methods,

370
00:20:22,840 --> 00:20:28,200
where the general idea is the closer some things are in a projection of feature space,

371
00:20:28,200 --> 00:20:33,440
whatever feature space you're using, the more likely they are to be in the same group or

372
00:20:33,440 --> 00:20:34,440
to be related to each other.

373
00:20:34,440 --> 00:20:38,400
So I have a little example of k nearest neighbors up there.

374
00:20:38,400 --> 00:20:43,040
You decide the value of a point based on the whatever number nearest neighbors, so majority

375
00:20:43,040 --> 00:20:44,040
vote.

376
00:20:44,040 --> 00:20:47,240
Gaussian mixture models, which is more of a clustering thing, which suggests that any

377
00:20:47,400 --> 00:20:52,360
distribution of points is going to be a mixture of different Gaussians.

378
00:20:52,360 --> 00:20:55,720
And support vector machines, which I like to think of as a cat that doesn't want to

379
00:20:55,720 --> 00:21:01,200
be petted, so the line is trying to be as far away from all the data points as possible.

380
00:21:01,200 --> 00:21:04,960
Or support vector regression, which is sort of the opposite, where the cat wants to be

381
00:21:04,960 --> 00:21:09,880
as close to as many points as possible, where the points are hands, I guess, in this metaphor.

382
00:21:09,880 --> 00:21:12,960
So benefits and drawbacks.

383
00:21:12,960 --> 00:21:16,320
Support vector machines in particular work very well with small data sets.

384
00:21:16,400 --> 00:21:18,600
You need a minimum of four points.

385
00:21:18,600 --> 00:21:23,120
They also tend to be very fast to train, in my experience, at least in order of magnitude

386
00:21:23,120 --> 00:21:27,800
faster than a regression model on the same data.

387
00:21:27,800 --> 00:21:30,600
Overall accuracy.

388
00:21:30,600 --> 00:21:34,240
There are other methods that tend to do better, but for especially quick and dirty modeling,

389
00:21:34,240 --> 00:21:36,360
they're perfectly fine.

390
00:21:36,360 --> 00:21:37,520
They're good at classification.

391
00:21:37,520 --> 00:21:41,760
If you want to do estimation specifically with support vector machines, it's a little

392
00:21:41,760 --> 00:21:45,800
bit slower, so you might want to use regression for that.

393
00:21:46,280 --> 00:21:49,600
Usually these days people tend to use them in ensembles, but it's a nice fast first pass

394
00:21:49,600 --> 00:21:50,600
for a problem.

395
00:21:50,600 --> 00:21:54,080
If you're sitting in a meeting and someone's like, hey, is this predictable?

396
00:21:54,080 --> 00:21:56,640
You can just open R or PyCon or whatever you're using quickly.

397
00:21:56,640 --> 00:21:59,360
Run a few lines of code and tell them very quickly.

398
00:21:59,360 --> 00:22:04,000
And again, here is just the code that you need to train this.

399
00:22:04,000 --> 00:22:07,160
In the code sample I'll give you, I've done all the preprocessing beforehand, and the

400
00:22:07,160 --> 00:22:09,480
same preprocessing was done for all three models.

401
00:22:09,480 --> 00:22:12,800
And here I'm using support vector regression, because I'm doing regression in all of my

402
00:22:12,800 --> 00:22:15,320
examples, just so they're parallel.

403
00:22:15,840 --> 00:22:19,440
All right, so distance-based measures.

404
00:22:19,440 --> 00:22:21,840
Very little time, very little money, very little data.

405
00:22:21,840 --> 00:22:24,960
They're extremely lightweight.

406
00:22:24,960 --> 00:22:26,960
So what message should you use?

407
00:22:26,960 --> 00:22:31,360
And for some reason my shruggy emoji isn't showing up, even though other emojis have.

408
00:22:31,360 --> 00:22:32,360
Mysterious.

409
00:22:32,360 --> 00:22:35,740
So it depends on what you have a lot of.

410
00:22:35,740 --> 00:22:39,000
If you have a lot of time, data, and money, go ahead and use deep learning.

411
00:22:39,000 --> 00:22:41,560
If you don't have any of those things, maybe distance-based learning.

412
00:22:42,320 --> 00:22:44,400
But I have been a little bit disingenuous.

413
00:22:44,400 --> 00:22:48,600
There is a hidden fourth column that I haven't been mentioning, and that's performance.

414
00:22:48,600 --> 00:22:51,060
So what is the ideal maximum?

415
00:22:51,060 --> 00:22:54,840
How well can this model do in the best case?

416
00:22:54,840 --> 00:22:59,360
And the fact of the matter is, based on empirical evidence, right now it looks like deep learning

417
00:22:59,360 --> 00:23:06,080
will perform the best on a given data set, given sufficient time, money, and compute.

418
00:23:06,080 --> 00:23:12,400
Next, at least, again, empirically, trees and specifically random forests.

419
00:23:12,400 --> 00:23:17,360
After that, regression, and then finally, distance-based measures like SVM.

420
00:23:17,360 --> 00:23:21,520
But again, there are things to consider other than whether or not you're going to get state

421
00:23:21,520 --> 00:23:26,780
of the art results at NeurIPS, unless you are like a Google research lab, in which case

422
00:23:26,780 --> 00:23:29,200
you probably wouldn't be here anyway.

423
00:23:29,200 --> 00:23:34,960
And if I were going to give each of these model types a little high school superlative,

424
00:23:35,080 --> 00:23:39,640
I would vote deep learning the most powerful, the most flexible.

425
00:23:39,640 --> 00:23:41,600
Regression, the most interpretable.

426
00:23:41,600 --> 00:23:45,400
If you need to know why, if you need to answer what-if questions, I'd go with regression.

427
00:23:45,400 --> 00:23:49,880
Trees, the user-friendliest, especially if you're just sitting down to do classification.

428
00:23:49,880 --> 00:23:53,200
And then distance-based measures, the most lightweight, because they're fast and you

429
00:23:53,200 --> 00:23:56,200
don't need a lot of data.

430
00:23:56,200 --> 00:24:00,080
And just a final caveat, data science does not equal deep learning.

431
00:24:00,080 --> 00:24:03,520
I know that outside of the field, that's what we've been communicating out, because it's

432
00:24:03,520 --> 00:24:04,520
the most exciting.

433
00:24:04,720 --> 00:24:06,920
But you can do data science without deep learning.

434
00:24:06,920 --> 00:24:08,760
It's really okay, guys.

435
00:24:08,760 --> 00:24:11,000
It's extremely powerful and it's not for everything.

436
00:24:11,000 --> 00:24:15,560
So I would like to encourage you all, in all aspects of your life, to avoid being a person

437
00:24:15,560 --> 00:24:16,560
with a hammer.

438
00:24:16,560 --> 00:24:20,000
So there's a saying, to a person with a hammer, everything looks like a nail.

439
00:24:20,000 --> 00:24:22,800
There are many tools out there, and they're going to be better for different jobs.

440
00:24:22,800 --> 00:24:27,520
So explore your options and don't be wedded to your specific favorite tool.

441
00:24:27,520 --> 00:24:31,240
And also saying that to myself, there's places where you can't use mixed effects regression,

442
00:24:31,240 --> 00:24:32,240
and it's fine.

443
00:24:33,240 --> 00:24:39,000
And just a final note, deep learning isn't a core skill in professional data science,

444
00:24:39,000 --> 00:24:44,200
and it's going to depend on your particular job ad that you're looking at.

445
00:24:44,200 --> 00:24:48,160
But Dan Becker, who is in charge of Learn at Kaggle, posted the other day, and I thought

446
00:24:48,160 --> 00:24:49,400
this was really interesting.

447
00:24:49,400 --> 00:24:54,800
He was looking at, I think it was the hacker rank who's hiring thread, and he set out a

448
00:24:54,800 --> 00:24:56,480
400 job posts.

449
00:24:56,480 --> 00:25:01,920
Only five of them called for PyTorch, TensorFlow, or Keras, which are all deep learning frameworks,

450
00:25:01,920 --> 00:25:02,920
or deep learning.

451
00:25:02,920 --> 00:25:07,400
So people aren't necessarily only looking to hire deep learning people.

452
00:25:07,400 --> 00:25:08,400
All right.

453
00:25:08,400 --> 00:25:10,920
And I'm going to open it up for questions.

454
00:25:10,920 --> 00:25:11,920
Thank you.

455
00:25:19,920 --> 00:25:20,920
All right.

456
00:25:20,920 --> 00:25:25,920
We have time for about five minutes of questions, so use the mics on both sides.

457
00:25:27,920 --> 00:25:30,920
I also post a link to my slides.

458
00:25:31,920 --> 00:25:32,920
Hi.

459
00:25:32,920 --> 00:25:38,760
So I am a social scientist working in education, and so I guess I'm going to go opposite.

460
00:25:38,760 --> 00:25:41,520
Are there opportunities for deep learning in education?

461
00:25:41,520 --> 00:25:45,480
I think we have a tendency to want to understand why we're finding out why kids aren't being

462
00:25:45,480 --> 00:25:49,280
successful, so just being told at the end of a deep learning model, that doesn't help

463
00:25:49,280 --> 00:25:50,280
us as much.

464
00:25:50,280 --> 00:25:54,920
Can you think of some applications of deep learning within maybe the social science sphere?

465
00:25:54,920 --> 00:25:55,920
Yeah.

466
00:25:55,920 --> 00:25:59,000
I can think of some bad ones.

467
00:25:59,080 --> 00:26:03,960
So my big worry, particularly in social science and things with learning and things where

468
00:26:03,960 --> 00:26:07,920
you might, like, say, be allocating education funding is you're going to end up modeling

469
00:26:07,920 --> 00:26:12,320
a latent variable that you don't necessarily want to model, like how high property taxes

470
00:26:12,320 --> 00:26:16,420
are and how well that correlates with, you know, whether kids get a good lunch and then

471
00:26:16,420 --> 00:26:20,000
how well they do in school for that.

472
00:26:20,000 --> 00:26:21,680
Yeah.

473
00:26:21,680 --> 00:26:26,880
I think probably the most immediately useful applications of deep learning in social sciences

474
00:26:26,880 --> 00:26:30,640
would be for things like annotating large image data sets.

475
00:26:30,640 --> 00:26:36,640
So if you're looking at what's written on the board in the classroom, I guess, and you

476
00:26:36,640 --> 00:26:39,640
want to get that information into a way that's machine readable, I think that could be a

477
00:26:39,640 --> 00:26:40,640
good application.

478
00:26:40,640 --> 00:26:41,640
Okay.

479
00:26:41,640 --> 00:26:42,640
Yeah.

480
00:26:42,640 --> 00:26:43,640
Thank you.

481
00:26:43,640 --> 00:26:47,600
I think you could probably come up with better ones than I could.

482
00:26:47,600 --> 00:26:53,120
So given enough time and data, are there any reasons why you wouldn't want to try all of

483
00:26:53,800 --> 00:26:54,800
these, for example?

484
00:26:54,800 --> 00:27:00,240
Or are there certain situations or contexts where you would really say never do this as

485
00:27:00,240 --> 00:27:04,800
opposed to just doing a train test set and seeing what works best?

486
00:27:04,800 --> 00:27:07,640
There's no reason not to do all of them if you have the resources.

487
00:27:07,640 --> 00:27:13,040
And in fact, it's very common to do all of them and then squish them into a single ensemble.

488
00:27:13,040 --> 00:27:17,600
So yeah, no reason not to.

489
00:27:17,600 --> 00:27:18,600
Great talk.

490
00:27:18,600 --> 00:27:19,600
I really enjoyed it.

491
00:27:19,680 --> 00:27:24,680
I guess one question that comes to mind is it seems like there's a paradigm where there's

492
00:27:24,680 --> 00:27:28,200
those who have the resources and those who don't.

493
00:27:28,200 --> 00:27:30,880
So I guess, could you comment on that?

494
00:27:30,880 --> 00:27:34,840
And what should plebs like me do when I don't have the resources?

495
00:27:34,840 --> 00:27:38,020
Well, I'm glad you asked that.

496
00:27:38,020 --> 00:27:43,080
So there are a couple of places where you can get access to, say, large public data

497
00:27:43,080 --> 00:27:48,200
sets and a compute environment that's GPU enabled, and that's Kaggle.

498
00:27:48,200 --> 00:27:50,800
And there are also other places as well.

499
00:27:50,800 --> 00:27:56,680
So Colab is a hosted notebook that offers GPU access.

500
00:27:56,680 --> 00:28:00,920
Data there are some areas where it's just going to be really hard to find data without

501
00:28:00,920 --> 00:28:06,680
paying for it, particularly financial data or image data sets where the images aren't

502
00:28:06,680 --> 00:28:12,480
all under a friendly copyright or a friendly license.

503
00:28:12,480 --> 00:28:17,400
Yeah, no, that is something that I stay up at night worrying about as well.

504
00:28:17,800 --> 00:28:21,720
Because I mean, I can't spend $45 million training a game playing robot.

505
00:28:21,720 --> 00:28:25,600
That's not on my capability list.

506
00:28:25,600 --> 00:28:28,400
I have no good answers other than we're trying to help.

507
00:28:31,400 --> 00:28:32,600
Hi, Cass Storrs.

508
00:28:32,600 --> 00:28:33,800
Simon and Schuster.

509
00:28:33,800 --> 00:28:40,160
I'm curious, do you have any good resources on interpreting and validating models, particularly

510
00:28:40,160 --> 00:28:41,160
regression models?

511
00:28:41,160 --> 00:28:42,160
I think it's really important.

512
00:28:42,160 --> 00:28:46,240
You can easily run a regression model, but you might be using the wrong model in that

513
00:28:46,240 --> 00:28:47,240
case, right?

514
00:28:48,080 --> 00:28:49,080
Yeah, absolutely.

515
00:28:49,080 --> 00:28:53,840
I have actually written a series of notebooks that are hosted on Kaggle that go through

516
00:28:53,840 --> 00:28:58,720
â€“ it's in R. If you're going to be doing a lot of regression, consider maybe looking

517
00:28:58,720 --> 00:29:03,240
at R. It goes through model validation and selection for regression in particular that

518
00:29:03,240 --> 00:29:04,240
I'm happy about.

519
00:29:04,240 --> 00:29:05,960
So I'll tweet out a link to those.

520
00:29:05,960 --> 00:29:06,960
And I am R.C.

521
00:29:06,960 --> 00:29:07,960
Tatman on Twitter.

522
00:29:07,960 --> 00:29:08,960
That'd be great.

523
00:29:08,960 --> 00:29:09,960
Thank you.

524
00:29:09,960 --> 00:29:10,960
Of course.

525
00:29:10,960 --> 00:29:16,200
I have one problem where we use a deep neural net to solve this problem.

526
00:29:16,200 --> 00:29:20,360
But it turned out that actually all we really needed was regression.

527
00:29:20,360 --> 00:29:22,320
And where would you start?

528
00:29:22,320 --> 00:29:27,640
Should you start with a regression or would you start with your list and start at the

529
00:29:27,640 --> 00:29:29,600
bottom and work up?

530
00:29:29,600 --> 00:29:30,840
Yeah.

531
00:29:30,840 --> 00:29:36,280
I generally only use distance-based methods if I'm in a real time crunch.

532
00:29:36,280 --> 00:29:42,360
My usual step through if I'm doing a regression problem would probably be regression, random

533
00:29:42,920 --> 00:29:46,520
forests, deep learning if I have to.

534
00:29:46,520 --> 00:29:50,960
If I was doing specifically classification and I wasn't as concerned with interpretability,

535
00:29:50,960 --> 00:29:54,440
I might do random forests, regression, and then deep learning.

536
00:29:54,440 --> 00:29:56,400
But every problem's different, right?

537
00:29:56,400 --> 00:29:57,400
Sure.

538
00:29:57,400 --> 00:30:01,320
But it occurred to me in our problem we started in the wrong place.

539
00:30:01,320 --> 00:30:02,320
Yeah.

540
00:30:02,320 --> 00:30:03,320
Don't start with deep learning.

541
00:30:03,320 --> 00:30:07,880
Also in your personal learning journey, please put deep learning like after regression.

542
00:30:07,880 --> 00:30:10,560
It'll make me happy.

543
00:30:11,560 --> 00:30:14,560
I got one question.

544
00:30:14,560 --> 00:30:22,240
One of the issues that I see a lot is the issue dealing with extrapolation versus interpolation.

545
00:30:22,240 --> 00:30:25,880
One of the issues with neural networks is that outside of the domain, you get completely

546
00:30:25,880 --> 00:30:28,000
undefined output.

547
00:30:28,000 --> 00:30:32,440
Are there any models or any approaches that you would suggest to take when you want at

548
00:30:32,440 --> 00:30:36,940
least you have a reasonable expectation of what the value should be but not necessarily

549
00:30:36,940 --> 00:30:40,360
a data point there for the model to train on?

550
00:30:41,280 --> 00:30:48,760
If you have strong a priori reason to believe that given X, you'd get Y out, you can use

551
00:30:48,760 --> 00:30:53,240
simulation for data augmentation.

552
00:30:53,240 --> 00:30:55,920
That is kind of a dangerous path.

553
00:30:55,920 --> 00:31:01,160
If I were doing that, I'd want to have a really strong reason to believe that there is data

554
00:31:01,160 --> 00:31:04,680
that exists here that I just don't have access to and I believe that these are its qualities.

555
00:31:04,680 --> 00:31:11,840
We're going to have to wrap up questions here, but I'm sure you're willing to answer

556
00:31:11,840 --> 00:31:14,960
some questions out in the hall if you have any other further questions.

557
00:31:14,960 --> 00:31:15,960
Let's say thank you again.

558
00:31:15,960 --> 00:31:16,460
Thank you.

