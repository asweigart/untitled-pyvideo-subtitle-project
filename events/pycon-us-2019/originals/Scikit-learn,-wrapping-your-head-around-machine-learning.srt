1
00:00:00,000 --> 00:00:06,000
So I'm going to give a signal to the videographer and we're going to get started I think.

2
00:00:06,000 --> 00:00:14,000
Make sure this is on. On, on, on, on, on, on.

3
00:00:14,000 --> 00:00:21,000
Okay. So thumbs up here, thumbs up there. Everything's good.

4
00:00:21,000 --> 00:00:25,000
Alright folks. I'm excited to have you here with us.

5
00:00:25,000 --> 00:00:30,000
Just to make sure you're in the right room, right?

6
00:00:30,000 --> 00:00:34,000
We're going to talk about Scikit-learn and do an introduction to machine learning.

7
00:00:34,000 --> 00:00:39,000
I'm going to walk us through kind of a class overview, what you should expect, how this is going to roll out,

8
00:00:39,000 --> 00:00:44,000
and then we're going to dive right into it.

9
00:00:44,000 --> 00:00:48,000
First little bit, tell you a little bit about myself.

10
00:00:48,000 --> 00:00:52,000
We'll cover what we're going to do in the class, what you can expect,

11
00:00:52,000 --> 00:00:55,000
and more importantly probably what not to expect.

12
00:00:55,000 --> 00:01:04,000
And then we'll look at how you can access the class materials, both here and after the class is over.

13
00:01:04,000 --> 00:01:09,000
My name is Chalmer. I always like to check the screen to make sure I'm the right guy.

14
00:01:09,000 --> 00:01:13,000
I like to stay busy. I work for a company called Booz Allen Hamilton.

15
00:01:13,000 --> 00:01:16,000
They're a technology and consulting firm.

16
00:01:16,000 --> 00:01:21,000
And our primary focus is to help people to change the world.

17
00:01:21,000 --> 00:01:28,000
And I feel like I'm successful some days at doing that. So that's a win for me.

18
00:01:28,000 --> 00:01:33,000
On the side, I founded a small programming school in Hawaii called the Dark Art of Coding.

19
00:01:33,000 --> 00:01:37,000
I teach people Python. I teach them data analysis and data science.

20
00:01:37,000 --> 00:01:41,000
With one of my colleagues in the back, Jeff, we founded Pi Hawaii.

21
00:01:41,000 --> 00:01:44,000
It is the largest and most active programming meetup in Hawaii.

22
00:01:44,000 --> 00:01:50,000
Here at PyCon, I help serve on the Python Education Summit Committee.

23
00:01:50,000 --> 00:01:54,000
So tomorrow there's going to be an education summit where educators can come together and talk about things.

24
00:01:54,000 --> 00:01:55,000
So I hope that.

25
00:01:55,000 --> 00:02:00,000
And for those in the audience who are interested in maybe contributing to an open source project,

26
00:02:00,000 --> 00:02:03,000
on Sunday night I'll be with a number of my colleagues.

27
00:02:03,000 --> 00:02:07,000
We'll be running a introduction to sprinting tutorial.

28
00:02:07,000 --> 00:02:12,000
We'll walk you through how to use Git, GitHub, and how to contribute to an open source project.

29
00:02:12,000 --> 00:02:17,000
So come Monday, you can sit down with developers and you can help them to add features,

30
00:02:17,000 --> 00:02:21,000
fix bugs, and improve documentation on the projects you care about.

31
00:02:21,000 --> 00:02:24,000
This is my third PyCon tutorial.

32
00:02:24,000 --> 00:02:29,000
Prior to now, I've dove into lion's den on an introduction to Bokeh,

33
00:02:29,000 --> 00:02:34,000
and I've done an introduction to statistics and probability.

34
00:02:34,000 --> 00:02:43,000
In really minor ways, emphasis on minor, I've contributed to Bokeh, to Pandas, to Jupiter, and a variety of other projects.

35
00:02:43,000 --> 00:02:45,000
This is how you can get a hold of me.

36
00:02:45,000 --> 00:02:46,000
All right.

37
00:02:46,000 --> 00:02:50,000
Today we're going to move at a fairly rapid pace.

38
00:02:50,000 --> 00:02:54,000
I will take questions, but depending on the depth of the question,

39
00:02:54,000 --> 00:02:57,000
maybe the relevance of the question to what we're trying to accomplish,

40
00:02:57,000 --> 00:02:59,000
or whether I can even answer the question,

41
00:02:59,000 --> 00:03:02,000
I may defer your question to a parking lot.

42
00:03:02,000 --> 00:03:05,000
We might cover it at the end or after the tutorial is over.

43
00:03:05,000 --> 00:03:12,000
During certain parts of the tutorial, you know, I might be able to support some kind of one-on-one conversations during exercises, et cetera,

44
00:03:12,000 --> 00:03:16,000
but that'll depend a little bit on our progress.

45
00:03:16,000 --> 00:03:20,000
There's an amazing amount of math under the hood when you do machine learning.

46
00:03:20,000 --> 00:03:23,000
There is almost no math in this tutorial.

47
00:03:23,000 --> 00:03:29,000
I think I've said math more now than we will in the rest of this tutorial.

48
00:03:29,000 --> 00:03:33,000
If you run into a significant snag that can't be rectified in a timely fashion,

49
00:03:33,000 --> 00:03:37,000
things just aren't working for you, something is not going right.

50
00:03:37,000 --> 00:03:41,000
Do you have three mentors who kind of walk around and will try and help?

51
00:03:41,000 --> 00:03:46,000
If for whatever reason we just can't get it fixed, your machine is not working, whatever,

52
00:03:46,000 --> 00:03:50,000
I'll invite folks, just take a deep breath, relax, enjoy the show,

53
00:03:50,000 --> 00:03:56,000
maybe lean over somebody's shoulder sitting next to you, and we'll see what we can do.

54
00:03:56,000 --> 00:04:01,000
I'll be happy to visit with people afterwards during PyCon or at the sprints to try and do a deep dive,

55
00:04:01,000 --> 00:04:05,000
troubleshooting on your system to figure out what might go wrong.

56
00:04:05,000 --> 00:04:10,000
My main ambition for today is to teach you to learn.

57
00:04:10,000 --> 00:04:15,000
Less so teach you facts, more teach you how to find facts on your own.

58
00:04:15,000 --> 00:04:20,000
So what I'm going to do is try and enable you to know where to look for more information,

59
00:04:20,000 --> 00:04:24,000
help you to explore some of the most commonly used grammar and vocabulary

60
00:04:24,000 --> 00:04:29,000
that is seen when you talk about machine learning.

61
00:04:29,000 --> 00:04:35,000
Nobody's walking out of here as a ninja, a guru, a rock star after only three hours.

62
00:04:35,000 --> 00:04:38,000
For you to get to that point where you feel really strongly confident

63
00:04:38,000 --> 00:04:42,000
in your machine learning abilities, that'll take time, it's going to take effort, practice,

64
00:04:42,000 --> 00:04:47,000
a lot of repetition and plenty of extra study, but it's definitely possible.

65
00:04:47,000 --> 00:04:52,000
Each of our lessons is going to basically be a high level overview of a concept.

66
00:04:52,000 --> 00:04:54,000
We're going to provide you some code examples.

67
00:04:54,000 --> 00:04:57,000
You'll be able to run those live in Jupyter Notebooks.

68
00:04:57,000 --> 00:05:02,000
After the code, I typically have some sections where we maybe have a deep dive

69
00:05:02,000 --> 00:05:05,000
and we describe a bit more detail about that topic,

70
00:05:05,000 --> 00:05:09,000
or maybe I will outline some gotchas that I encountered in my learning journey

71
00:05:09,000 --> 00:05:12,000
as I was getting to this point.

72
00:05:12,000 --> 00:05:17,000
In order to kind of keep us on a schedule, each section is basically time boxed.

73
00:05:17,000 --> 00:05:21,000
After about X number of minutes, we're just going to stop on that lesson

74
00:05:21,000 --> 00:05:25,000
and maybe I got part way through the deep dive or maybe I got part way through the gotchas

75
00:05:25,000 --> 00:05:28,000
and we're going to pause that lesson and we'll move on to the next one.

76
00:05:28,000 --> 00:05:32,000
All that content in my talk track is all written in the Jupyter Notebooks.

77
00:05:32,000 --> 00:05:35,000
So if we don't get to a particular thing that's cool, you can always go back

78
00:05:35,000 --> 00:05:39,000
and reread my talk track and see the kinds of things that we want to talk about.

79
00:05:39,000 --> 00:05:43,000
At the bottom of every reference, or at the bottom of every reference,

80
00:05:43,000 --> 00:05:46,000
the bottom of every lesson, there's a set of references.

81
00:05:46,000 --> 00:05:50,000
There are links that will take you to other places where you can continue to learn things.

82
00:05:50,000 --> 00:05:56,000
In some of the sessions, there's going to be opportunities to do some hands-on exercises.

83
00:05:56,000 --> 00:06:00,000
When we get to an exercise, I'll give you a few minutes.

84
00:06:00,000 --> 00:06:02,000
You guys work on the exercise.

85
00:06:02,000 --> 00:06:07,000
A lot of them will be you researching topics, very quickly and at a high level, but you researching.

86
00:06:07,000 --> 00:06:11,000
When you finish your exercise, put your green sticky up on the monitor.

87
00:06:11,000 --> 00:06:16,000
When I see a nice sea of green, I'll kind of know that everybody's done and we can move on.

88
00:06:16,000 --> 00:06:20,000
If you run into a snag, you put your red post-it up on your monitor.

89
00:06:20,000 --> 00:06:24,000
One of my mentor folks will come and they'll try and help you out.

90
00:06:25,000 --> 00:06:30,000
Most of the lessons, most of the exercises and things in here will be fairly standalone.

91
00:06:30,000 --> 00:06:33,000
They shouldn't rely upon anything in a previous notebook, etc.

92
00:06:33,000 --> 00:06:39,000
So if a particular lesson doesn't work, something goes wrong, we move on to the next lesson.

93
00:06:39,000 --> 00:06:44,000
That should not impede your ability to execute any of the code in the next notebook.

94
00:06:44,000 --> 00:06:48,000
If you do run into a snag that we just can't resolve for you,

95
00:06:48,000 --> 00:06:52,000
I'd love for you to maybe make a note of that on one of your post-its,

96
00:06:52,000 --> 00:06:54,000
because we'll collect the post-its at the end of the day,

97
00:06:54,000 --> 00:06:59,000
and that way we can kind of go back and revisit the material and see what we can do to clean that up.

98
00:07:01,000 --> 00:07:04,000
We do have an opportunity for you to gain some experience points.

99
00:07:04,000 --> 00:07:06,000
This is your first exercise of the day.

100
00:07:06,000 --> 00:07:10,000
I'd like everybody to remind yourselves, I've got this.

101
00:07:10,000 --> 00:07:13,000
I'd like you to repeat that to yourself.

102
00:07:13,000 --> 00:07:17,000
And when you're done doing that, put your green post-its on your monitor so I know you're done.

103
00:07:22,000 --> 00:07:25,000
All right.

104
00:07:25,000 --> 00:07:27,000
So there's a question in the front.

105
00:07:27,000 --> 00:07:30,000
Yes, sir.

106
00:07:30,000 --> 00:07:32,000
All right.

107
00:07:32,000 --> 00:07:36,000
Let's put an X on one of those for you, or put a big G.

108
00:07:36,000 --> 00:07:40,000
The one in your left hand needs a G on it.

109
00:07:40,000 --> 00:07:42,000
Excellent.

110
00:07:42,000 --> 00:07:45,000
If there's other colorblind folks in the room, I'll have somebody come and help you.

111
00:07:45,000 --> 00:07:46,000
That's totally cool.

112
00:07:46,000 --> 00:07:49,000
I do appreciate you asking. That was excellent.

113
00:07:49,000 --> 00:07:53,000
All right.

114
00:07:53,000 --> 00:07:58,000
There are three main ways folks can get to the class materials.

115
00:07:58,000 --> 00:08:00,000
We handed out the little slips of paper.

116
00:08:00,000 --> 00:08:04,000
For some folks, you might be accessing it via a Jupyter Hub that's fully interactive.

117
00:08:04,000 --> 00:08:06,000
Some of you might have done the local installs.

118
00:08:06,000 --> 00:08:09,000
That should also be fully interactive.

119
00:08:09,000 --> 00:08:12,000
And you can also get to the material via GitHub.

120
00:08:12,000 --> 00:08:18,000
And that material will be on GitHub even after you go home.

121
00:08:18,000 --> 00:08:24,000
I will be turning off my Jupyter Hub after the class is over.

122
00:08:24,000 --> 00:08:27,000
So don't save anything important on there, et cetera.

123
00:08:27,000 --> 00:08:30,000
But you should have access to everything via GitHub.

124
00:08:30,000 --> 00:08:32,000
All right.

125
00:08:32,000 --> 00:08:34,000
So that's pretty much the intro.

126
00:08:34,000 --> 00:08:36,000
I'm about to move into the next section.

127
00:08:36,000 --> 00:08:41,000
Before I do, are there any pressing questions that maybe I didn't quite answer or cover?

128
00:08:41,000 --> 00:08:44,000
Any thoughts?

129
00:08:44,000 --> 00:08:46,000
All right.

130
00:08:46,000 --> 00:08:54,000
For those who are not familiar with JupyterLab, on the far left side, you have a little folder icon.

131
00:08:54,000 --> 00:08:59,000
If you click on the folder, it will open up an explorer style window.

132
00:08:59,000 --> 00:09:02,000
We were in a folder called zero basics.

133
00:09:02,000 --> 00:09:06,000
I'm now going to move to a folder called zero one intro.

134
00:09:06,000 --> 00:09:11,000
And in zero one intro, I have a notebook called zero one intro.

135
00:09:11,000 --> 00:09:12,000
All right.

136
00:09:12,000 --> 00:09:15,000
And this will be an intro to Scikit learn.

137
00:09:15,000 --> 00:09:18,000
So we're probably going to spend about 20 minutes or so.

138
00:09:28,000 --> 00:09:29,000
All right.

139
00:09:29,000 --> 00:09:36,000
So my buddy Jeff mentioned that some folks were clicking on the student login and were running into some snags.

140
00:09:36,000 --> 00:09:40,000
Anybody tried to log in with that little student login slip and have a fail?

141
00:09:40,000 --> 00:09:42,000
Let me see a show of hands.

142
00:09:42,000 --> 00:09:45,000
I've got one, two, three folks.

143
00:09:45,000 --> 00:09:46,000
All right.

144
00:09:46,000 --> 00:09:51,000
For those folks, I'm going to highly recommend that you try and do a local install.

145
00:09:51,000 --> 00:10:00,000
I'm going to put a URL up here on the screen for you and see what happens.

146
00:10:00,000 --> 00:10:02,000
OK.

147
00:10:02,000 --> 00:10:10,000
Instructions for folks to be able to do a local install are at machine install.

148
00:10:10,000 --> 00:10:11,000
All right.

149
00:10:11,000 --> 00:10:13,000
I'm going to leave that up there for a second.

150
00:10:13,000 --> 00:10:16,000
So there's four or five folks who had some struggles.

151
00:10:16,000 --> 00:10:23,000
If you want to make a note of that URL, there's instructions on how you can install on your own local machines.

152
00:10:23,000 --> 00:10:26,000
All right.

153
00:10:26,000 --> 00:10:27,000
Cool.

154
00:10:27,000 --> 00:10:34,000
And if I move on and you haven't made a note of it yet, put your red sticky up and the mentors will come help you out.

155
00:10:34,000 --> 00:10:36,000
All right.

156
00:10:36,000 --> 00:10:38,000
There we go.

157
00:10:38,000 --> 00:10:40,000
All right.

158
00:10:40,000 --> 00:10:46,000
So let's talk a little bit about Scikit-learn and what Scikit-learn can do for you.

159
00:10:46,000 --> 00:10:53,000
In this section, we'll probably cover in about 20 minutes or so, we're going to explore some machine learning techniques, some tools and some categories.

160
00:10:53,000 --> 00:11:05,000
We'll look at a high level of what supervised learning and unsupervised learning are, and we'll look at classification, regression, clustering, and we'll talk about dimensionality reduction.

161
00:11:05,000 --> 00:11:13,000
We'll then talk a little bit about Scikit-learn, especially a thing that is referred to as the application programming interface, the API.

162
00:11:13,000 --> 00:11:17,000
All right.

163
00:11:17,000 --> 00:11:22,000
So machine learning is going to fall into two main categories, supervised learning and unsupervised.

164
00:11:22,000 --> 00:11:34,000
The process of unsupervised learning is associated with modeling a relationship between data and features of a data set and a series of targets or labels that you know about that data set.

165
00:11:34,000 --> 00:11:35,000
Right.

166
00:11:35,000 --> 00:11:49,000
And when you have a model in hand that has a ability to identify a relationship between your data set and labels for your data set, you can then use that model to assign labels to new data that you've never seen before.

167
00:11:49,000 --> 00:11:53,000
Or to calculate output values as you provide inputs.

168
00:11:53,000 --> 00:11:56,000
What does that mean?

169
00:11:56,000 --> 00:11:59,000
There are two kinds of unsupervised learning we'll talk about here.

170
00:11:59,000 --> 00:12:00,000
First is classification.

171
00:12:00,000 --> 00:12:02,000
The second is regression.

172
00:12:02,000 --> 00:12:15,000
If I'm looking at a classification problem, for example, I might have some inputs, some texts, some emails or comments, and my algorithm may be able to detect is this spam or not spam.

173
00:12:15,000 --> 00:12:20,000
And so I can assign a label, spam or not spam, to an email or to a text or to a comment.

174
00:12:20,000 --> 00:12:36,000
If my inputs were data about flowers or insects or animals, say the length of some characteristic, the body length, the number of paws, whatever, I might be able to use that input data to assign a label to each animal.

175
00:12:36,000 --> 00:12:40,000
This is a dog or it's a Labrador or it's a poodle or a husky or whatever.

176
00:12:40,000 --> 00:12:41,000
Right.

177
00:12:41,000 --> 00:12:43,000
So I might be able to classify things.

178
00:12:43,000 --> 00:13:00,000
If I am a store owner, online store owner potentially, or I'm a content delivery service, and I want to know details about my readers, my buyers, my viewers, based on some of the information I retrieve about them, I may be able to classify them in particular ways.

179
00:13:00,000 --> 00:13:02,000
This is a potential customer.

180
00:13:02,000 --> 00:13:04,000
This is a high end, high paying customer.

181
00:13:04,000 --> 00:13:08,000
This is a customer that is probably not likely to come back more than once.

182
00:13:08,000 --> 00:13:11,000
Those kinds of things.

183
00:13:11,000 --> 00:13:16,000
And so with supervised learning, we have a set of data.

184
00:13:16,000 --> 00:13:18,000
We have labels for each of those things in the data.

185
00:13:18,000 --> 00:13:20,000
I can build a relationship.

186
00:13:20,000 --> 00:13:27,000
And now when I get new data that's not yet labeled, I can take that relationship and try and produce a label for something I've never seen before.

187
00:13:27,000 --> 00:13:30,000
With regression, the process is fairly similar.

188
00:13:30,000 --> 00:13:34,000
You have some inputs and you can identify a relationship to outputs.

189
00:13:34,000 --> 00:13:37,000
But those outputs are what we call continuous quantities.

190
00:13:37,000 --> 00:13:39,000
They're typically some value.

191
00:13:39,000 --> 00:13:42,000
So say I have auto characteristics.

192
00:13:42,000 --> 00:13:46,000
I've got a bunch of cars and I know things about their color, their model, their age, et cetera.

193
00:13:46,000 --> 00:13:50,000
I might be able to identify a price on this kind of continuous spectrum.

194
00:13:50,000 --> 00:13:56,000
Say this car with these characteristics should be priced at this value because it'll likely sell.

195
00:13:56,000 --> 00:14:08,000
If I am looking at my advertising dollars that I've spent, I should hopefully be able to predict what kind of sales revenue I have based on characteristics of where the dollars were spent, who they were spent on, et cetera.

196
00:14:08,000 --> 00:14:19,000
Next, if you have candidate characteristics, say you're doing interviews and you're looking at your candidates, maybe you can predict the salary you should offer them that they were likely to accept or receive.

197
00:14:19,000 --> 00:14:23,000
So that is supervised learning.

198
00:14:23,000 --> 00:14:25,000
We're going to go into that in a little more depth.

199
00:14:25,000 --> 00:14:27,000
We're going to see some hands-on examples of each of those.

200
00:14:27,000 --> 00:14:37,000
For unsupervised learning, this is a process of finding out a relationship amongst features in your data set in a way that classifies raw data without you ever having to tell it.

201
00:14:37,000 --> 00:14:38,000
Any input labels.

202
00:14:38,000 --> 00:14:41,000
There's a lot of algorithms that do this.

203
00:14:41,000 --> 00:14:45,000
A lot of these models try and replicate kind of how a human might think about things.

204
00:14:45,000 --> 00:14:52,000
If I looked at a scatter plot and I saw a bunch of things lumped together, I might presume that those things are related in some way.

205
00:14:52,000 --> 00:14:53,000
They're clustering together.

206
00:14:53,000 --> 00:14:55,000
They're probably similar.

207
00:14:55,000 --> 00:15:00,000
And so these algorithms tend to try and replicate what a human might do.

208
00:15:00,000 --> 00:15:03,000
We're going to talk about clustering in more detail.

209
00:15:03,000 --> 00:15:06,000
I'm just going to mention dimensionality reduction today.

210
00:15:06,000 --> 00:15:20,000
But with clustering and clustering analysis, you try to group a collection of objects so all the objects in one cluster are similar to each other and they are more similar to each other than they are objects in a different cluster.

211
00:15:20,000 --> 00:15:23,000
So say I have a bunch of images.

212
00:15:23,000 --> 00:15:25,000
I might want to group those images together.

213
00:15:25,000 --> 00:15:26,000
Here's a dog.

214
00:15:26,000 --> 00:15:27,000
Here's a muffin.

215
00:15:27,000 --> 00:15:29,000
Here's a human.

216
00:15:29,000 --> 00:15:31,000
Here's a not human.

217
00:15:31,000 --> 00:15:34,000
Here's a celebrity.

218
00:15:34,000 --> 00:15:35,000
Here's a not celebrity.

219
00:15:35,000 --> 00:15:36,000
Those kind of things.

220
00:15:36,000 --> 00:15:41,000
If you have marketing data, you might want to identify traits about your customers.

221
00:15:41,000 --> 00:15:43,000
These are all customers that are similar to each other.

222
00:15:43,000 --> 00:15:46,000
These are customers that are different from each other.

223
00:15:46,000 --> 00:15:51,000
On social networks, you might want to do some analysis to try and identify communities of folks.

224
00:15:51,000 --> 00:15:53,000
These folks have similar interests.

225
00:15:53,000 --> 00:15:56,000
They have different interests.

226
00:15:56,000 --> 00:16:01,000
And though we won't get into dimensionality reduction in depth here, I'll mention it briefly.

227
00:16:01,000 --> 00:16:14,000
This is a process of taking lots of features, lots of variables that you might know about something, and reducing the number of random variables by trying to identify some principle characteristics.

228
00:16:14,000 --> 00:16:21,000
This dimensionality reduction can allow you to reduce the number of features that you have to do math on.

229
00:16:21,000 --> 00:16:25,000
It can help you to do feature selection or extraction.

230
00:16:25,000 --> 00:16:27,000
So I'll give you kind of an example.

231
00:16:27,000 --> 00:16:31,000
Let's just say I have a data set and it's got a bunch of different features related to coffee.

232
00:16:31,000 --> 00:16:47,000
Cup sizes, whether it's dark roast, not dark roast, whether it's flavoring added, nutmeg, vanilla, what country the beans came from, whether they were grown organically, not organically, whether they were sustainably harvest or not sustainably harvested.

233
00:16:47,000 --> 00:16:50,000
The preparation, it's latte, espresso.

234
00:16:50,000 --> 00:16:56,000
By the way, I don't drink coffee, so I don't know anything about what I'm speaking about right there.

235
00:16:56,000 --> 00:17:08,000
So if I started to do some analysis, I might want to put all of these in here thinking, well, there's probably very complicated relationships amongst all these things that might help me figure out what I should sell in my coffee shop.

236
00:17:08,000 --> 00:17:23,000
If you do some dimensionality reduction on this, you might identify some trends that might say, you know what, it's good that you collected all this data, but there's only maybe four things that really impact whether the coffee is going to sell, yes or no.

237
00:17:23,000 --> 00:17:28,000
Maybe you might discover that cup size, the roast, the flavoring, the preparation are the things that actually drive sales.

238
00:17:28,000 --> 00:17:33,000
The other things are kind of maybe nice to haves, if you will.

239
00:17:33,000 --> 00:17:42,000
And so if you can do this reduction of dimensions, you can narrow down how much math you have to do and how much data you have to store and those kind of things.

240
00:17:42,000 --> 00:17:46,000
So that's the primary focus of dimensionality reduction.

241
00:17:46,000 --> 00:17:56,000
When you do dimensionality reduction, you can save on computation time, you can save on storage space, both memory and or on disk.

242
00:17:56,000 --> 00:18:05,000
Sometimes if you have lots of dimensions, lots of variables, and you can shrink them down two or three, you might be able to graph certain things in a two dimensional, three dimensional graph.

243
00:18:05,000 --> 00:18:08,000
It might be easier to visualize.

244
00:18:08,000 --> 00:18:12,000
And sometimes it can help you interpret your outputs more easily.

245
00:18:12,000 --> 00:18:16,000
And one last thing, and there is a link to this, so if you're interested, you can go read it.

246
00:18:16,000 --> 00:18:19,000
There's a thing called the curse of dimensionality.

247
00:18:19,000 --> 00:18:22,000
A lot of people think, well, I should collect as much data as I can.

248
00:18:22,000 --> 00:18:24,000
The more data I have, the better, right?

249
00:18:24,000 --> 00:18:34,000
What we often find is that as you start to have more and more columns of data, it is often a little tricky to get all of the rows filled in in a particular column.

250
00:18:34,000 --> 00:18:39,000
Maybe a customer didn't provide that data or maybe that customer doesn't have that particular characteristic.

251
00:18:39,000 --> 00:18:43,000
Or maybe this particular item doesn't have this nuance.

252
00:18:43,000 --> 00:18:51,000
And so you might end up with lots of columns, lots of rows and lots of empty spaces in there because you couldn't quite collect it.

253
00:18:51,000 --> 00:18:53,000
And so your data ends up being sparse.

254
00:18:53,000 --> 00:18:58,000
And if it's too sparse, actually trending things can be a little tough.

255
00:18:58,000 --> 00:19:07,000
So dimensionality reduction allows you to kind of eliminate some of that sparsity of data that's characterized as the curse of dimensionality.

256
00:19:07,000 --> 00:19:12,000
All right. So that was a lot. Let's talk a little bit about scikit-learn.

257
00:19:12,000 --> 00:19:16,000
We're going to see three of those different types of machine learning again.

258
00:19:16,000 --> 00:19:22,000
So scikit-learn scikit-learn is a package.

259
00:19:22,000 --> 00:19:28,000
It's pretty well known that allows you access to a lot of really common machine learning algorithms.

260
00:19:28,000 --> 00:19:33,000
And they provide you that access in a way that is tremendously consistent and well organized.

261
00:19:33,000 --> 00:19:36,000
It's their application programming interface.

262
00:19:36,000 --> 00:19:42,000
And what they provide to you is supported through very thorough and very comprehensive documentation.

263
00:19:42,000 --> 00:19:46,000
And you guys are going to have a lot of chances to dive into the documentation today.

264
00:19:46,000 --> 00:19:50,000
The API has very uniform syntax.

265
00:19:50,000 --> 00:19:54,000
It's very consistent in how it is designed, which is a benefit for you.

266
00:19:54,000 --> 00:20:00,000
As soon as you pick up your first model, you are well on your way to picking up your second and your third and your fourth and your fifth.

267
00:20:00,000 --> 00:20:07,000
Right. Primary goal here for us today is for you to understand that API,

268
00:20:07,000 --> 00:20:13,000
for you to have an improved knowledge of the vocabulary of machine learning and for you to figure out how to learn more.

269
00:20:13,000 --> 00:20:22,000
OK. So if we get past those goals and we succeed there, you're going to be very well poised to keep learning more as you go home and follow up.

270
00:20:22,000 --> 00:20:26,000
I have a link here to the API design paper.

271
00:20:27,000 --> 00:20:31,000
I've quoted a number of things here so you don't have to go click on it unless you're really interested.

272
00:20:31,000 --> 00:20:38,000
But the API design is covered in what they call an API contract.

273
00:20:38,000 --> 00:20:40,000
And they focused on a number of things.

274
00:20:40,000 --> 00:20:45,000
I'm going to shrink this down in size a little bit.

275
00:20:45,000 --> 00:20:47,000
And five main things kind of stood out to me.

276
00:20:47,000 --> 00:20:55,000
They wanted to be very consistent so that when you need to put data into your model, you do it the same way every time.

277
00:20:55,000 --> 00:21:00,000
When you expect data out, you'll get essentially the similar types of data out.

278
00:21:00,000 --> 00:21:04,000
They provided a way for you to inspect your models.

279
00:21:04,000 --> 00:21:09,000
When you construct a new model, there is a way to figure out exactly what you added to the model.

280
00:21:09,000 --> 00:21:18,000
If a parameter value is generated by the model, there's a way for you to go find that parameter, like a coefficient for the slope of a line.

281
00:21:18,000 --> 00:21:20,000
You can go and get that back out.

282
00:21:20,000 --> 00:21:24,000
They have opted to avoid proliferation of classes.

283
00:21:24,000 --> 00:21:31,000
There are a number of tools out there in the world where they need to store data in some odd way.

284
00:21:31,000 --> 00:21:34,000
So they created a data type that is unique to their tool.

285
00:21:34,000 --> 00:21:36,000
They opted to avoid that.

286
00:21:36,000 --> 00:21:45,000
They said, you know what, data sets represented by NumPy arrays or SciPy sparse matrices or maybe by data frames, that'll work for us.

287
00:21:45,000 --> 00:21:52,000
And so they've avoided creating random data types that are very specific to Scikit-learn.

288
00:21:52,000 --> 00:21:58,000
That allows you to take data directly from a model and feed it into some other component of your workflow.

289
00:21:58,000 --> 00:22:09,000
They have a focus on composition, and we'll see a really cool example of this, where you can take the results of one model and feed them directly into another model using a thing like a pipeline.

290
00:22:09,000 --> 00:22:14,000
You can combine models together to do some very sophisticated analysis.

291
00:22:15,000 --> 00:22:25,000
And lastly, and this is probably the best part for me, if there is some operation you have to do and it requires you to decide on provide a parameter,

292
00:22:25,000 --> 00:22:31,000
they have provided a default value for you and they've tried to make those as sane as possible.

293
00:22:31,000 --> 00:22:39,000
So basically, a number of the models, you can just provide data and run it and it'll work well.

294
00:22:39,000 --> 00:22:49,000
You may want to tune it and we'll talk a little bit about that, but by and large the defaults are very sensible and you don't need to really fuss with a number of those defaults for most things.

295
00:22:49,000 --> 00:23:00,000
All right. If you want to get some concrete details on how the API is put together, for folks who want to contribute to Scikit-learn, they have this contributors API overview.

296
00:23:00,000 --> 00:23:04,000
But it was actually really good for folks even if you don't intend to contribute.

297
00:23:04,000 --> 00:23:08,000
It helps to highlight the pros and cons and the benefits and things that are going on under the hood.

298
00:23:08,000 --> 00:23:10,000
So that's good homework reading.

299
00:23:12,000 --> 00:23:17,000
When you go to use your Scikit-learn API, there's a number of steps.

300
00:23:17,000 --> 00:23:20,000
I have trademarked it. It's called the process.

301
00:23:20,000 --> 00:23:23,000
We're going to use the process repeatedly here today.

302
00:23:25,000 --> 00:23:29,000
The process in this case has got six steps.

303
00:23:30,000 --> 00:23:37,000
Prep the data, choose your model, choose appropriate parameters, fit the model, apply the model, and then examine your results.

304
00:23:38,000 --> 00:23:43,000
And by and large, most of the sessions will actually walk us through that.

305
00:23:43,000 --> 00:23:45,000
There'll be a heading says, hey, now we're doing the data prep.

306
00:23:45,000 --> 00:23:47,000
Now we're choosing our model.

307
00:23:47,000 --> 00:23:50,000
Now we're going to use the appropriate hyperparameters, et cetera.

308
00:23:50,000 --> 00:23:52,000
But what does all that mean?

309
00:23:53,000 --> 00:23:57,000
In prepping the data, your data has got to be well prepared to be usable in the model.

310
00:23:57,000 --> 00:24:02,000
Preparation could include cleaning out corrupt data, things that aren't supposed to be there.

311
00:24:02,000 --> 00:24:06,000
It could be wrangling the data into a format that's more efficient or more effective.

312
00:24:06,000 --> 00:24:11,000
It could be normalizing your numbers to account for outliers, those kinds of things.

313
00:24:12,000 --> 00:24:16,000
And then your data is often going to need to be separated into what we're going to call a features matrix.

314
00:24:16,000 --> 00:24:18,000
And I'm going to talk about that more in a minute.

315
00:24:18,000 --> 00:24:20,000
Any target vector or a target array.

316
00:24:23,000 --> 00:24:24,000
We're going to get into that.

317
00:24:24,000 --> 00:24:26,000
The whole next section talks about data handling.

318
00:24:26,000 --> 00:24:28,000
So we're going to talk about that in more detail.

319
00:24:28,000 --> 00:24:30,000
The next part, you're going to choose your model.

320
00:24:30,000 --> 00:24:34,000
And so you'll import a class from Scikit-learn.

321
00:24:34,000 --> 00:24:39,000
Say, hey, I want to do a linear regression or I want to do a Gaussian naive Bayes.

322
00:24:39,000 --> 00:24:43,000
From there, we're going to select a set of hyperparameters.

323
00:24:43,000 --> 00:24:46,000
We'll put those into the model and get the model ready to run.

324
00:24:47,000 --> 00:24:52,000
We won't do a lot of that because, again, the defaults are already sane and they're already fine.

325
00:24:52,000 --> 00:24:55,000
Once we've chosen the parameters, we fit the model.

326
00:24:55,000 --> 00:25:01,000
We'll say, here's the data, run the model on the data and start to build those relationships behind the scenes.

327
00:25:02,000 --> 00:25:07,000
Once the model is done running, we're going to apply the model to our new data.

328
00:25:07,000 --> 00:25:09,000
We put the new data in, we run it.

329
00:25:09,000 --> 00:25:15,000
And when we run it, if we're doing something supervised learning, we'll typically use a dot predict method.

330
00:25:15,000 --> 00:25:20,000
If we're doing unsupervised learning, we might end up calling a dot predict or dot transform method.

331
00:25:21,000 --> 00:25:25,000
And then as soon as that's over, we start to look at the results.

332
00:25:25,000 --> 00:25:28,000
And I'll show you ways to get the results out.

333
00:25:28,000 --> 00:25:32,000
I mentioned Scikit-learn's designs. You can do inspection. I'll show you how to inspect.

334
00:25:32,000 --> 00:25:36,000
But we'll also show you how to plot some of these things so you can see what they look like.

335
00:25:37,000 --> 00:25:39,000
All right. I'm going to do a quick demo here.

336
00:25:39,000 --> 00:25:43,000
But before we go any further, any questions, any thoughts?

337
00:25:46,000 --> 00:25:48,000
Okay. All right. So let's whet our appetite.

338
00:25:48,000 --> 00:25:50,000
Let's take a look at what we got coming down the height.

339
00:25:52,000 --> 00:25:56,000
Again, I don't drink coffee, so I'm not sure why coffee plays such a prominent role in this particular thing.

340
00:25:56,000 --> 00:26:02,000
But I got online and I looked up all the local coffee shops on the north shore of Oahu, Hawaii.

341
00:26:02,000 --> 00:26:06,000
That's where I live. And I'm like, how much do people sell coffee for?

342
00:26:06,000 --> 00:26:10,000
And then I was horrified because like $4.50 for...

343
00:26:12,000 --> 00:26:15,000
These are prices for different size cups.

344
00:26:19,000 --> 00:26:25,000
In terms of prepping the data, I needed to get those numbers into a format that Scikit-learn would use.

345
00:26:25,000 --> 00:26:30,000
I also wanted to be prepared so I could maybe plot a scatter plot or something.

346
00:26:30,000 --> 00:26:35,000
So I started off with a set of standard imports. That's generally where you start.

347
00:26:37,000 --> 00:26:42,000
I created two NumPy arrays. We'll talk more about the data handling part here.

348
00:26:42,000 --> 00:26:44,000
So I'm just going to hand wave some of this away.

349
00:26:44,000 --> 00:26:51,000
I put in six numbers for the cup sizes and six prices for each of those cups.

350
00:26:51,000 --> 00:26:55,000
I put them in order so that there is a one-to-one relationship.

351
00:26:55,000 --> 00:27:00,000
12 ounce coffee is $2.95. A 16 ounce coffee was $3.65.

352
00:27:01,000 --> 00:27:06,000
I called these X train and Y train. We'll talk a little bit more about that.

353
00:27:06,000 --> 00:27:08,000
But this is the data I'm going to use to train my model.

354
00:27:10,000 --> 00:27:13,000
I want to really quickly get a look at this, see what it looks like on a graph.

355
00:27:13,000 --> 00:27:16,000
So I'm going to dump this into a map plot lib scatter plot.

356
00:27:16,000 --> 00:27:21,000
I'll give you a few more details on that later, but let's cruise through this.

357
00:27:21,000 --> 00:27:25,000
Okay. Let's start at the top. I'm going to execute the cells.

358
00:27:26,000 --> 00:27:28,000
It helps if you execute cells.

359
00:27:28,000 --> 00:27:32,000
For those who have not used JupyterLab before or Jupyter Notebooks,

360
00:27:32,000 --> 00:27:39,000
I'm going to click in this cell and I'm going to hit shift enter and it will run that cell.

361
00:27:39,000 --> 00:27:43,000
I'll know that it's finished running because there will be a small number here to say, hey, I'm done.

362
00:27:43,000 --> 00:27:45,000
If it's still thinking, there will be an asterisk.

363
00:27:45,000 --> 00:27:48,000
So I click in there, I hit shift enter.

364
00:27:48,000 --> 00:27:51,000
I'm going to go down, I click in the next one, I hit shift enter.

365
00:27:51,000 --> 00:27:55,000
And I go to the next one and click shift enter and we get a scatter plot.

366
00:27:55,000 --> 00:28:01,000
And so these are prices for 12 to 20 ounce cups of coffee on the north shore of Oahu.

367
00:28:01,000 --> 00:28:05,000
All right. We've not done any machine learning yet. All we've done is graphed stuff.

368
00:28:06,000 --> 00:28:13,000
Okay. I mentioned that we need to get our data in a format that our machine learning library will use.

369
00:28:15,000 --> 00:28:18,000
When I made Xtrain, it was a NumPy array.

370
00:28:18,000 --> 00:28:29,000
This is a technique you can use in NumPy to slice through all of the items in the array and create a new dimension.

371
00:28:31,000 --> 00:28:33,000
And we're going to talk about that in the data handling thing.

372
00:28:33,000 --> 00:28:39,000
Just bear with me. I've created an array that has a series of rows in it.

373
00:28:39,000 --> 00:28:42,000
Each of these things in the brackets is a separate row.

374
00:28:43,000 --> 00:28:48,000
My Y training data can actually be an array. It does not have to be in a series of rows.

375
00:28:48,000 --> 00:28:50,000
So that part is fine.

376
00:28:52,000 --> 00:28:54,000
I have some friends. They're a little esoteric.

377
00:28:54,000 --> 00:28:58,000
They don't like standard size cups of coffee or sometimes they do.

378
00:28:58,000 --> 00:29:00,000
But I have like 15 ounce cups and 17 ounce cups.

379
00:29:00,000 --> 00:29:03,000
They're like, well, if I came to Hawaii, how much would I have to pay?

380
00:29:05,000 --> 00:29:09,000
So I'm going to create a very small array that we're going to use for testing purposes.

381
00:29:09,000 --> 00:29:11,000
And I'm going to call this Xtest.

382
00:29:11,000 --> 00:29:16,000
I'm going to convert that to a NumPy array with rows.

383
00:29:16,000 --> 00:29:19,000
A minute ago, I used np new axis.

384
00:29:19,000 --> 00:29:21,000
You could also use none.

385
00:29:21,000 --> 00:29:25,000
np new axis, for those who don't know, is just an alias for none.

386
00:29:26,000 --> 00:29:28,000
So now I've got a brand new array.

387
00:29:29,000 --> 00:29:31,000
So what have we done so far?

388
00:29:31,000 --> 00:29:34,000
We've made a training set with cup sizes and prices.

389
00:29:34,000 --> 00:29:37,000
And we've made a test set with just cup sizes.

390
00:29:37,000 --> 00:29:39,000
We still haven't done any machine learning.

391
00:29:41,000 --> 00:29:43,000
Let's get started on the machine learning part.

392
00:29:44,000 --> 00:29:52,000
In Scikit-learn, they have a whole collection of modules that have classifiers in them.

393
00:29:52,000 --> 00:30:01,000
In the linear model module, they have a variety of regression types and linear types of classifiers.

394
00:30:01,000 --> 00:30:03,000
We're going to use the linear regression classifier.

395
00:30:03,000 --> 00:30:05,000
So shift enter to execute that.

396
00:30:05,000 --> 00:30:07,000
I've now imported it.

397
00:30:07,000 --> 00:30:09,000
That's the choose the model part.

398
00:30:10,000 --> 00:30:13,000
We're going to assign the appropriate hyperparameters.

399
00:30:13,000 --> 00:30:16,000
We'll look at hyperparameters in a little more depth in a few minutes.

400
00:30:16,000 --> 00:30:19,000
But right now, I'm just going to go with just the defaults.

401
00:30:19,000 --> 00:30:23,000
And I'm going to create an instance of this.

402
00:30:24,000 --> 00:30:30,000
When I ask Jupyter to evaluate this, it will spit out a copy of a string associated with it.

403
00:30:30,000 --> 00:30:35,000
And that string's going to tell me, here's all of the hyperparameters that got fed in.

404
00:30:35,000 --> 00:30:37,000
And each of these is one of those same defaults.

405
00:30:39,000 --> 00:30:42,000
We're going to talk more about each of these in a bit, so bear with me.

406
00:30:43,000 --> 00:30:45,000
So we've done the choose the hyperparameters part.

407
00:30:45,000 --> 00:30:46,000
Now we're going to fit the model.

408
00:30:46,000 --> 00:30:51,000
When I fit, I provide it my training data, and it'll start to develop those relationships.

409
00:30:51,000 --> 00:30:53,000
So I'm going to give it my X training and my Y training.

410
00:30:57,000 --> 00:30:58,000
And it's done.

411
00:30:59,000 --> 00:31:05,000
Behind the scenes, it has calculated a relationship between the input coffee sizes and the input prices.

412
00:31:06,000 --> 00:31:08,000
That relationship, in this case, is a straight line.

413
00:31:09,000 --> 00:31:14,000
And for those who remember your math from high school,

414
00:31:15,000 --> 00:31:21,000
every straight line has a slope and has a place essentially where it intersects the Y axis unless it's vertical.

415
00:31:22,000 --> 00:31:28,000
So I can ask, what is the Y intercept for this line that got created and what is the slope of the line?

416
00:31:29,000 --> 00:31:39,000
And it'll say, hey, I calculated that the slope of this line is 0.14, and this line should intersect the Y axis at 1.2.

417
00:31:40,000 --> 00:31:42,000
Okay, that's good.

418
00:31:43,000 --> 00:31:49,000
Now let's try and predict how much my friends would have to spend for their randomly sized cups of coffee.

419
00:31:51,000 --> 00:31:52,000
And there we go.

420
00:31:52,000 --> 00:31:59,000
Based on the fitting of the model and those two parameters, it's now able to calculate for 16 ounces, my friend should expect to pay about this much.

421
00:32:00,000 --> 00:32:03,000
For 15 ounces, they'll pay less. For 12, they'll pay significantly less.

422
00:32:04,000 --> 00:32:05,000
And for 20 ounces, they'll pay the most.

423
00:32:06,000 --> 00:32:09,000
And that's it. You've machine learned. We can all go home.

424
00:32:10,000 --> 00:32:15,000
But let's take a look at the results. So I've got a couple scatter plots we're going to put up.

425
00:32:16,000 --> 00:32:23,000
The first scatter plot in this cell is going to show our X training data and our Y training data, and it's going to be in purple.

426
00:32:24,000 --> 00:32:31,000
I'm going to then do the X test data, my five friends cup sizes, and the predicted prices.

427
00:32:32,000 --> 00:32:36,000
I'm going to do it in a slightly transparent color. That's what alpha means.

428
00:32:37,000 --> 00:32:42,000
So there we go. Purple was the original prices that we knew.

429
00:32:43,000 --> 00:32:46,000
The pinkish color are prices that we are predicting.

430
00:32:47,000 --> 00:32:52,000
Somebody getting a 15 ounce cup of coffee should expect to pay about this amount.

431
00:32:57,000 --> 00:33:04,000
If you don't want to have a scatter plot of your results, instead of calling plot scatter, you can always call plot plot.

432
00:33:05,000 --> 00:33:09,000
And you can get lines instead for these. And it's the same data.

433
00:33:18,000 --> 00:33:19,000
Sure.

434
00:33:27,000 --> 00:33:33,000
Because Scikit-learn will crush your soul under its hobnail boots if you don't.

435
00:33:35,000 --> 00:33:39,000
I'm going to talk more about this. We're going to get into the data handling part and we'll talk a little bit about it.

436
00:33:40,000 --> 00:33:42,000
Scikit-learn will crush your soul.

437
00:33:44,000 --> 00:33:50,000
I've got a couple more minutes for this little section, and then we're going to get into data handling and we'll talk more about your thing.

438
00:33:53,000 --> 00:34:03,000
So I highly recommend that folks start to figure out what the API design documents and what the API documentation covers.

439
00:34:04,000 --> 00:34:07,000
And we're going to go through them in some depth as we go through.

440
00:34:12,000 --> 00:34:19,000
When I first started playing with Scikit-learn, probably my biggest struggle was, here is an example in a tutorial.

441
00:34:20,000 --> 00:34:24,000
It looks great. Let me do something very similar with my own data.

442
00:34:25,000 --> 00:34:27,000
And epic fails repeatedly.

443
00:34:28,000 --> 00:34:33,000
And it wasn't until I started to figure out, wait, it wants my data to be in a particular format. Did things start to run and work well?

444
00:34:34,000 --> 00:34:36,000
Up until that point, it was just horrible.

445
00:34:39,000 --> 00:34:44,000
I'm going to turn the time over to you folks for just a moment.

446
00:34:45,000 --> 00:34:47,000
There is a link here to the API reference.

447
00:34:48,000 --> 00:34:55,000
I'm going to ask you to open up the reference and you'll find a section on a model called model selection train test split.

448
00:34:56,000 --> 00:34:57,000
Model selection train test split.

449
00:34:58,000 --> 00:35:02,000
Take a few minutes, about two minutes or so, high level, skim through it.

450
00:35:02,000 --> 00:35:05,000
If you see words that you don't know, make a note of those.

451
00:35:06,000 --> 00:35:09,000
Maybe we'll bring them up. Maybe we won't, right?

452
00:35:10,000 --> 00:35:11,000
It'll be something for you to start to think about.

453
00:35:12,000 --> 00:35:19,000
And then there is a section in there that describes the type of data that the train test split function returns.

454
00:35:20,000 --> 00:35:21,000
What are you going to get back?

455
00:35:22,000 --> 00:35:23,000
So go do that.

456
00:35:24,000 --> 00:35:25,000
Give it about two minutes or so.

457
00:35:26,000 --> 00:35:29,000
When you feel like you have an answer for both of these questions, put your little green stickies up.

458
00:35:32,000 --> 00:35:47,000
I'm going to turn the mic off before Jeff says bad things about me.

459
00:36:02,000 --> 00:36:05,000
I am fading fast.

460
00:36:06,000 --> 00:36:11,000
I'll check in with you later, some other time tomorrow.

461
00:36:12,000 --> 00:36:13,000
Sure.

462
00:36:14,000 --> 00:36:22,000
Some of the faults that said they had issues was just the jigger app showing the regular notebook interface empty.

463
00:36:23,000 --> 00:36:27,000
But I mean we can help them clone the repo there.

464
00:36:27,000 --> 00:36:31,000
Because I made one of the guns create a new notebook and do imports.

465
00:36:32,000 --> 00:36:33,000
I could learn it was working.

466
00:36:34,000 --> 00:36:37,000
We do not know what code it is.

467
00:36:38,000 --> 00:36:42,000
It happened last Saturday to a couple of folks.

468
00:36:43,000 --> 00:36:50,000
We thought it might be because they had already run the notebooks during my dry run.

469
00:36:51,000 --> 00:36:54,000
And we thought that there might be something cast in the browsers.

470
00:36:54,000 --> 00:37:00,000
For someone who is new and that was not in my workshop, for them to see the same problem.

471
00:37:01,000 --> 00:37:05,000
It's totally freaking out.

472
00:37:06,000 --> 00:37:10,000
So do you want us to tell them to just install everything new or try using that?

473
00:37:11,000 --> 00:37:14,000
Can we help them clone the things locally?

474
00:37:14,000 --> 00:37:15,000
Sure.

475
00:37:17,000 --> 00:37:23,000
So git should be installed on each of those machines.

476
00:37:24,000 --> 00:37:27,000
Git should be installed on the jibberhub because that's how I get my content down.

477
00:37:28,000 --> 00:37:29,000
We can client there.

478
00:37:30,000 --> 00:37:33,000
And we can also do a wget with the zip that it's unloaded in master.

479
00:37:34,000 --> 00:37:39,000
And if you want to work with them and see if you can get a copy of the material there, just open it up.

480
00:37:40,000 --> 00:37:41,000
I'm going to have to pull my laptop in.

481
00:37:41,000 --> 00:37:43,000
Let's take a look at something real quick here.

482
00:37:48,000 --> 00:37:52,000
They don't have their username, user student 100 or whatever.

483
00:37:53,000 --> 00:37:56,000
We'll replace it with lab.

484
00:37:59,000 --> 00:38:01,000
The repository is under your GitHub profile.

485
00:38:02,000 --> 00:38:04,000
Chomolo is all one word.

486
00:38:05,000 --> 00:38:07,000
Machine learning.

487
00:38:07,000 --> 00:38:11,000
All right.

488
00:38:12,000 --> 00:38:15,000
If you're done with the exercise, post-it notes are up.

489
00:38:16,000 --> 00:38:17,000
I see a lot of post-it notes.

490
00:38:18,000 --> 00:38:20,000
All right.

491
00:38:21,000 --> 00:38:24,000
Turn the thing back on.

492
00:38:25,000 --> 00:38:28,000
Any words pop up that you're not familiar with?

493
00:38:33,000 --> 00:38:34,000
Shuffle split.

494
00:38:35,000 --> 00:38:36,000
Okay.

495
00:38:37,000 --> 00:38:43,000
Maybe matrix in this context.

496
00:38:44,000 --> 00:38:45,000
Matrix. We'll talk more about matrices in a minute.

497
00:38:46,000 --> 00:38:48,000
So anything else?

498
00:38:49,000 --> 00:38:52,000
It's good that you guys know all the things.

499
00:38:53,000 --> 00:38:54,000
Stratify.

500
00:38:55,000 --> 00:38:56,000
Interesting. Okay.

501
00:38:57,000 --> 00:38:58,000
All right. Some of these things we'll talk about.

502
00:38:59,000 --> 00:39:01,000
Some of these things I may not know all the details of.

503
00:39:02,000 --> 00:39:03,000
I'm not going to tell you which is which.

504
00:39:04,000 --> 00:39:05,000
All right. We'll leave that as a surprise.

505
00:39:05,000 --> 00:39:13,000
What type of data will train test split return?

506
00:39:14,000 --> 00:39:15,000
List.

507
00:39:16,000 --> 00:39:18,000
It'll return what?

508
00:39:19,000 --> 00:39:21,000
A list containing what?

509
00:39:22,000 --> 00:39:28,000
What type of data is going to be stored in the list?

510
00:39:29,000 --> 00:39:32,000
Whatever type of inputs you got.

511
00:39:32,000 --> 00:39:33,000
Okay.

512
00:39:34,000 --> 00:39:37,000
So let's pull this up.

513
00:39:38,000 --> 00:39:40,000
API reference.

514
00:39:41,000 --> 00:39:43,000
Tick tock. Control F.

515
00:39:51,000 --> 00:39:52,000
Model selection.

516
00:39:53,000 --> 00:39:54,000
All right.

517
00:39:55,000 --> 00:39:57,000
Ch-ch-ch-ch-ch.

518
00:40:00,000 --> 00:40:01,000
There we go.

519
00:40:02,000 --> 00:40:07,000
So as you start looking at these things,

520
00:40:08,000 --> 00:40:13,000
the API documentation is generated out in a fairly efficient way.

521
00:40:14,000 --> 00:40:17,000
It may tell you a few words about what the thing does.

522
00:40:18,000 --> 00:40:22,000
I will say that that tends to be a little sparse in the details.

523
00:40:23,000 --> 00:40:26,000
You might notice that they have a link to take you to the user guide.

524
00:40:27,000 --> 00:40:30,000
The user guide typically has a lot more detail about what does this thing do,

525
00:40:30,000 --> 00:40:32,000
how to use it, when would you use it, et cetera.

526
00:40:33,000 --> 00:40:35,000
They then give you a nice summary of all the parameters.

527
00:40:36,000 --> 00:40:40,000
And again, these parameters all have defaults and this defaults are sane.

528
00:40:41,000 --> 00:40:45,000
And so it says, hey, when you feed data in, you're going to provide a series of arrays.

529
00:40:46,000 --> 00:40:48,000
Those could be lists. They could be some other things.

530
00:40:49,000 --> 00:40:50,000
Could be pandas data frames.

531
00:40:51,000 --> 00:40:54,000
They will give you some parameters on when you split your data into components,

532
00:40:55,000 --> 00:40:56,000
how should you do it.

533
00:40:56,000 --> 00:41:01,000
In the returns section, they will tell you what you get back from that thing.

534
00:41:02,000 --> 00:41:03,000
So you know what to expect.

535
00:41:04,000 --> 00:41:05,000
And it will tell you that it will be a list.

536
00:41:06,000 --> 00:41:11,000
It will be two times the length of the arrays.

537
00:41:12,000 --> 00:41:14,000
And it will basically break out your content.

538
00:41:15,000 --> 00:41:17,000
And we're going to see an example of this.

539
00:41:18,000 --> 00:41:19,000
All right.

540
00:41:20,000 --> 00:41:23,000
We will use train test split multiple times today.

541
00:41:24,000 --> 00:41:25,000
So you'll see lots of examples of it.

542
00:41:26,000 --> 00:41:27,000
All right.

543
00:41:28,000 --> 00:41:29,000
There we go.

544
00:41:30,000 --> 00:41:35,000
Look at the example of the actions like the returns more than the returns of the whole thing.

545
00:41:36,000 --> 00:41:37,000
Yeah. And we'll see that.

546
00:41:38,000 --> 00:41:39,000
We'll see that.

547
00:41:40,000 --> 00:41:41,000
Okay.

548
00:41:42,000 --> 00:41:43,000
All right.

549
00:41:44,000 --> 00:41:46,000
So now we're going to move to the next conversation.

550
00:41:47,000 --> 00:41:49,000
This will be section 02, data handling.

551
00:41:50,000 --> 00:41:53,000
All right.

552
00:41:54,000 --> 00:41:55,000
Okay.

553
00:41:56,000 --> 00:41:57,000
All right.

554
00:41:58,000 --> 00:42:03,000
So in this conversation, we're going to explore the data types that Scikit-learn uses.

555
00:42:04,000 --> 00:42:10,000
We will explore several techniques to prep our data for use in Scikit-learn.

556
00:42:11,000 --> 00:42:14,000
We're going to see some techniques to manipulate the data or review the data.

557
00:42:15,000 --> 00:42:18,000
And we're going to look at a couple of techniques to view that data using Matplotlib.

558
00:42:19,000 --> 00:42:24,000
I will warn you right off the bat, this is not expected to be an exhaustive overview of data handling or visualization.

559
00:42:24,000 --> 00:42:29,000
I'm going to give you just enough so that all the other examples make sense.

560
00:42:30,000 --> 00:42:31,000
All right.

561
00:42:32,000 --> 00:42:37,000
By and large, Scikit-learn is going to use NumPy arrays, Scipy sparse arrays, and Pandas data frames.

562
00:42:38,000 --> 00:42:45,000
To train a Scikit-learn classifier, you want to provide it a 2D array, a two-dimensional array.

563
00:42:46,000 --> 00:42:49,000
Often in literature, you'll see that referred to as a features matrix.

564
00:42:49,000 --> 00:42:54,000
Generally, it will be labeled with an X, but maybe not.

565
00:42:55,000 --> 00:43:02,000
For the labels, you typically provide that as a single one-dimensional array.

566
00:43:03,000 --> 00:43:06,000
Often we'll call it a target array. It might be labeled with a Y.

567
00:43:07,000 --> 00:43:13,000
Your features are the things that you know.

568
00:43:14,000 --> 00:43:17,000
I know, for example, the cup size.

569
00:43:17,000 --> 00:43:24,000
If I was looking at insects, I might know the lengths of specific beetle body parts.

570
00:43:25,000 --> 00:43:29,000
The wings are this long, the legs are this long, the thorax is such and such a dimension.

571
00:43:30,000 --> 00:43:38,000
For example, with these two features, I have soda sizes here and I have beetle dimensions there.

572
00:43:39,000 --> 00:43:46,000
This features matrix has only a single variable, just soda size.

573
00:43:47,000 --> 00:43:52,000
My second feature matrix happens to have three separate variables, three separate features.

574
00:43:53,000 --> 00:43:57,000
There's the dimensions for the head of the beetle, the thorax of the beetle, and the abdomen of the beetle.

575
00:43:58,000 --> 00:44:01,000
In both cases, I have four samples or four rows.

576
00:44:02,000 --> 00:44:06,000
Features are the columns, samples are the rows.

577
00:44:07,000 --> 00:44:12,000
By the way, the beetle dimensions are completely fake. I'm also not a beetle person.

578
00:44:12,000 --> 00:44:17,000
I don't do coffee, I don't do beetles. I don't know why this is in my presentation.

579
00:44:20,000 --> 00:44:25,000
Your targets, your Ys, are going to have a one-dimensional array.

580
00:44:26,000 --> 00:44:32,000
Those are going to be those labels or categories, or maybe the prices that were spent on a particular thing.

581
00:44:33,000 --> 00:44:38,000
For my soda prices, it's just a straight up one-dimensional target array.

582
00:44:38,000 --> 00:44:42,000
For my beetles, say I had two different types of beetles.

583
00:44:43,000 --> 00:44:45,000
I had checkered beetles and diving beetles.

584
00:44:46,000 --> 00:44:52,000
Say if I looked at the dimensions of their abdomens and their thoraxes and their heads,

585
00:44:53,000 --> 00:44:57,000
if I noticed this beetle and this beetle are a little smaller than the other one.

586
00:44:58,000 --> 00:45:01,000
This is probably one type of beetle and this is probably a different type of beetle.

587
00:45:02,000 --> 00:45:06,000
My target array may categorize those two beetles and say,

588
00:45:06,000 --> 00:45:10,000
all right, the first row was a checkered beetle, the third row was a checkered beetle,

589
00:45:11,000 --> 00:45:13,000
the second and fourth rows were diving beetles.

590
00:45:16,000 --> 00:45:23,000
The nuance here is for every row in my features matrix, I should theoretically have a value in my target array.

591
00:45:25,000 --> 00:45:29,000
And that makes sense. If my features matrix for beetles has four rows,

592
00:45:30,000 --> 00:45:33,000
I need to have four classifications for each beetle to train my model properly.

593
00:45:34,000 --> 00:45:36,000
Now, if we're doing something like clustering, you're not going to have targets.

594
00:45:37,000 --> 00:45:38,000
That doesn't make sense.

595
00:45:39,000 --> 00:45:42,000
All right. So let's take a moment. We're going to look at how do we make these arrays.

596
00:45:43,000 --> 00:45:46,000
We'll talk about four or five ways you can make a two-dimensional array

597
00:45:47,000 --> 00:45:49,000
and a couple of ways to make one-dimensional arrays.

598
00:45:50,000 --> 00:45:54,000
Let's start off with NumPy. Again, we'll recreate the coffee cup stuff.

599
00:45:55,000 --> 00:45:56,000
Make those.

600
00:45:56,000 --> 00:45:59,000
Notice I called this a lowercase x.

601
00:46:00,000 --> 00:46:04,000
When I change it into a features matrix, we're going to convert that to an uppercase x.

602
00:46:05,000 --> 00:46:06,000
You'll see that in a second.

603
00:46:07,000 --> 00:46:09,000
But let's quickly look at what this is.

604
00:46:10,000 --> 00:46:12,000
Okay. It's an array.

605
00:46:13,000 --> 00:46:14,000
Let's look at the shape of this array.

606
00:46:16,000 --> 00:46:17,000
This is a tuple.

607
00:46:19,000 --> 00:46:23,000
It has a five, a comma, and there's nothing after the comma.

608
00:46:23,000 --> 00:46:26,000
This tells me that this is a one-dimensional array,

609
00:46:27,000 --> 00:46:29,000
but it needs to be two-dimensional since it's a feature matrix.

610
00:46:30,000 --> 00:46:31,000
And we'll show you how to change that.

611
00:46:32,000 --> 00:46:35,000
Why is the target array and it can be one-dimensional? That's fine.

612
00:46:36,000 --> 00:46:38,000
For those of you who are looking at this and going, why is there a comma?

613
00:46:39,000 --> 00:46:40,000
I do have some commentary there in writing.

614
00:46:41,000 --> 00:46:43,000
Tupes require a comma if you have a single value in them.

615
00:46:44,000 --> 00:46:45,000
All right.

616
00:46:46,000 --> 00:46:48,000
So how do I make that one-dimensional array into a two-dimensional array?

617
00:46:48,000 --> 00:46:49,000
Here's a technique you can use with NumPy.

618
00:46:50,000 --> 00:46:53,000
Basically says, take all the values in the array.

619
00:46:54,000 --> 00:46:55,000
Does this slicing.

620
00:46:56,000 --> 00:46:57,000
Take all the values from the beginning to the end.

621
00:46:58,000 --> 00:46:59,000
That's what the colon tells us.

622
00:47:00,000 --> 00:47:01,000
And then add a new dimension to it.

623
00:47:02,000 --> 00:47:06,000
And when I do that, notice I changed this to a capital X

624
00:47:07,000 --> 00:47:09,000
because we often use capital Xs as our features matrix.

625
00:47:10,000 --> 00:47:14,000
And notice it went from being just a single value to a two-dimensional array.

626
00:47:14,000 --> 00:47:15,000
And then add a new element to it.

627
00:47:16,000 --> 00:47:20,000
And notice it went from being just a single value to a two-dimensional array.

628
00:47:21,000 --> 00:47:25,000
And if I check the shape, we'll see that it now has two dimensions.

629
00:47:26,000 --> 00:47:29,000
Still five rows, one column, if you will.

630
00:47:30,000 --> 00:47:31,000
All right.

631
00:47:32,000 --> 00:47:36,000
NumPy is an alias for none.

632
00:47:37,000 --> 00:47:39,000
So that does the exact same thing.

633
00:47:40,000 --> 00:47:41,000
You can use either of these techniques.

634
00:47:41,000 --> 00:47:45,000
You see these a lot in tutorials as people are trying to shape their data for use.

635
00:47:46,000 --> 00:47:48,000
There's a third technique.

636
00:47:49,000 --> 00:47:51,000
You see this one probably not as frequently, but it does show up.

637
00:47:52,000 --> 00:47:55,000
If your NumPy data is already two-dimensional,

638
00:47:56,000 --> 00:47:58,000
here I've got an array with one, two, three rows.

639
00:47:59,000 --> 00:48:00,000
You don't have to do anything.

640
00:48:01,000 --> 00:48:02,000
You can leave it just as is.

641
00:48:03,000 --> 00:48:07,000
It's just that weird thing where your data starts out as being one-dimensional

642
00:48:08,000 --> 00:48:09,000
that things get a little wonky.

643
00:48:09,000 --> 00:48:10,000
All right.

644
00:48:10,000 --> 00:48:11,000
All right.

645
00:48:40,000 --> 00:48:48,000
Yeah, because it's expecting every column to be a unique feature, unique variable.

646
00:48:49,000 --> 00:48:52,000
This is my, like if I'm categorizing customers,

647
00:48:53,000 --> 00:48:54,000
this column is maybe how many children they have.

648
00:48:55,000 --> 00:48:56,000
This column is what their salaries are.

649
00:48:57,000 --> 00:48:59,000
This column is what kind of car do they drive, those kind of things.

650
00:49:00,000 --> 00:49:03,000
So every column is going to be a unique feature or unique variable to assess.

651
00:49:04,000 --> 00:49:05,000
All right.

652
00:49:06,000 --> 00:49:08,000
We can do the exact same thing with Panda's data frames.

653
00:49:08,000 --> 00:49:11,000
I'm going to make, or sorry, Panda's series.

654
00:49:12,000 --> 00:49:13,000
I'm going to make two series.

655
00:49:14,000 --> 00:49:19,000
You might look at this and think, well, this is clearly two-dimensional.

656
00:49:20,000 --> 00:49:21,000
It looks like a table.

657
00:49:22,000 --> 00:49:24,000
If you check the shape on this thing,

658
00:49:25,000 --> 00:49:29,000
you will find that a Panda series is actually one-dimensional,

659
00:49:30,000 --> 00:49:35,000
which is part of what crushed my soul when I was first kind of playing with these things.

660
00:49:35,000 --> 00:49:38,000
I'd like, I have this very clearly two-dimensional looking thing,

661
00:49:39,000 --> 00:49:42,000
and I drop it into scikit-learn and all heck breaks loose and nothing ever worked.

662
00:49:43,000 --> 00:49:47,000
So there's a lot of crying in my first few forays into scikit-learn.

663
00:49:48,000 --> 00:49:52,000
To change a Panda series to a two-dimensional object,

664
00:49:53,000 --> 00:49:55,000
you can call the toFrame,

665
00:49:56,000 --> 00:49:59,000
and you can give it the name of what you want to call this series.

666
00:50:00,000 --> 00:50:01,000
There we go.

667
00:50:01,000 --> 00:50:02,000
There we go.

668
00:50:03,000 --> 00:50:06,000
Sure enough, it converts it from a series into an actual data frame.

669
00:50:07,000 --> 00:50:08,000
Data frames are always two-dimensional,

670
00:50:09,000 --> 00:50:10,000
and we check the shape, and sure enough,

671
00:50:11,000 --> 00:50:15,000
it went from being a one-dimensional series to a two-dimensional Panda's data frame.

672
00:50:20,000 --> 00:50:23,000
If you have a data frame that has several columns in it,

673
00:50:24,000 --> 00:50:25,000
you may think, well, oh, I'm good.

674
00:50:26,000 --> 00:50:30,000
Clearly, a data frame column is obviously two-dimensional,

675
00:50:31,000 --> 00:50:34,000
but you have to convert the individual series

676
00:50:35,000 --> 00:50:37,000
into its own essential standalone data frame

677
00:50:38,000 --> 00:50:42,000
to make sure that you get two dimensions for the series.

678
00:50:43,000 --> 00:50:44,000
Okay.

679
00:50:45,000 --> 00:50:46,000
Another technique to do this,

680
00:50:47,000 --> 00:50:51,000
if you have a set of data and you want to convert it into a data frame,

681
00:50:52,000 --> 00:50:53,000
you can do this.

682
00:50:54,000 --> 00:50:55,000
There you go.

683
00:50:56,000 --> 00:50:57,000
I don't want to spend too much time on that.

684
00:50:58,000 --> 00:50:59,000
You can go back and look at this at your leisure.

685
00:50:59,000 --> 00:51:02,000
You'll see me use one or two of these commands in future lessons,

686
00:51:03,000 --> 00:51:05,000
and you'll be like, okay, now I know why he's doing this.

687
00:51:06,000 --> 00:51:07,000
He's making it into a two-dimensional array.

688
00:51:08,000 --> 00:51:09,000
Okay.

689
00:51:10,000 --> 00:51:15,000
Does that make Scikit see the row ID?

690
00:51:16,000 --> 00:51:18,000
I don't know that Scikit Learn cares about the row ID,

691
00:51:19,000 --> 00:51:21,000
because when you use NumPy, NumPy doesn't have row IDs.

692
00:51:22,000 --> 00:51:25,000
It just, in its head, it needs it to be in a two-dimensional format,

693
00:51:26,000 --> 00:51:28,000
and this is two-dimensional from Panda's land.

694
00:51:29,000 --> 00:51:33,000
So, yeah, it's a thing.

695
00:51:34,000 --> 00:51:35,000
All right.

696
00:51:36,000 --> 00:51:37,000
So let's take a deep breath.

697
00:51:38,000 --> 00:51:43,000
How do we look at the data that we have or that we get back?

698
00:51:44,000 --> 00:51:45,000
So let's take a few moments to consider that.

699
00:51:46,000 --> 00:51:50,000
If I had a NumPy array and it's 100,000 items long,

700
00:51:51,000 --> 00:51:52,000
I can't look at all of it,

701
00:51:53,000 --> 00:51:54,000
so I often want to see just a snippet, right?

702
00:51:55,000 --> 00:51:56,000
Maybe the top couple rows.

703
00:51:56,000 --> 00:51:59,000
So you can use a slicing syntax

704
00:52:00,000 --> 00:52:02,000
to take a quick look at just a couple of the rows.

705
00:52:03,000 --> 00:52:05,000
Here I'm going to look at rows zero up to three.

706
00:52:06,000 --> 00:52:09,000
In Panda's, you can use the .head command

707
00:52:10,000 --> 00:52:11,000
to look at a certain number of rows.

708
00:52:12,000 --> 00:52:13,000
Here I'm choosing three again.

709
00:52:14,000 --> 00:52:16,000
In Panda's, you can also use this slicing syntax.

710
00:52:17,000 --> 00:52:20,000
So you'll see me call up data out of our machine learning outputs

711
00:52:21,000 --> 00:52:23,000
to take a look at these to see what we got back.

712
00:52:23,000 --> 00:52:24,000
All right.

713
00:52:25,000 --> 00:52:28,000
So we're going to stop away from just Panda's and NumPy for a second.

714
00:52:29,000 --> 00:52:30,000
We're talking about naming conventions.

715
00:52:31,000 --> 00:52:34,000
I've kind of alluded to the fact that by and large,

716
00:52:35,000 --> 00:52:37,000
in a lot of tutorials or examples,

717
00:52:38,000 --> 00:52:41,000
you'll see things labeled as capital X to be your features,

718
00:52:42,000 --> 00:52:44,000
Y might be your targets.

719
00:52:45,000 --> 00:52:46,000
You're going to see that a lot.

720
00:52:47,000 --> 00:52:51,000
When you see capital X or you see your features matrix,

721
00:52:51,000 --> 00:52:53,000
generally this is the stuff that you already know

722
00:52:54,000 --> 00:52:57,000
and it is potentially related to the target.

723
00:52:58,000 --> 00:53:00,000
I say potentially, we'll cover that in a sec.

724
00:53:01,000 --> 00:53:03,000
You will also see though that people will mention,

725
00:53:04,000 --> 00:53:06,000
hey, my features matrix is my independent variables

726
00:53:07,000 --> 00:53:10,000
or they are my observed variables or my explanatory variables

727
00:53:11,000 --> 00:53:12,000
or my input variables.

728
00:53:13,000 --> 00:53:15,000
So you'll see all these verbs and words and nouns and stuff.

729
00:53:16,000 --> 00:53:17,000
I don't do grammar either.

730
00:53:18,000 --> 00:53:20,000
Coffee, beetles and grammar are not my thing.

731
00:53:21,000 --> 00:53:23,000
You'll see all these words.

732
00:53:24,000 --> 00:53:30,000
What independent means here is that these variables,

733
00:53:31,000 --> 00:53:32,000
these components do not depend on something.

734
00:53:33,000 --> 00:53:36,000
When you say observed, it means you've observed this,

735
00:53:37,000 --> 00:53:42,000
either the sales of things or you've gathered data on your beetles, whatever.

736
00:53:43,000 --> 00:53:46,000
Explanatory means it will explain the results that you get back later

737
00:53:47,000 --> 00:53:49,000
or input should be self-explanatory.

738
00:53:49,000 --> 00:53:51,000
Some folks will label them as lowercase x,

739
00:53:52,000 --> 00:53:54,000
some people will use capital X, it's going to vary.

740
00:53:55,000 --> 00:53:58,000
For those targets, those labels that you assign to things

741
00:53:59,000 --> 00:54:02,000
or that you get back, you'll see those referred to as dependent variables.

742
00:54:03,000 --> 00:54:05,000
They depend on something else. They depend on the inputs.

743
00:54:06,000 --> 00:54:09,000
You might see them referred to as unobserved variables

744
00:54:10,000 --> 00:54:13,000
or the explained variable, the output outcome.

745
00:54:13,000 --> 00:54:16,000
You'll see why's either lowercase or uppercase.

746
00:54:18,000 --> 00:54:20,000
Why do I go through all this?

747
00:54:21,000 --> 00:54:23,000
As you're going through tutorials, everybody's going to refer to them

748
00:54:24,000 --> 00:54:25,000
as something completely different.

749
00:54:26,000 --> 00:54:28,000
Sometimes comparing one tutorial to another,

750
00:54:29,000 --> 00:54:31,000
you're like, what is this person talking about?

751
00:54:32,000 --> 00:54:33,000
When actually they are talking about the same thing,

752
00:54:34,000 --> 00:54:35,000
they're just using different terms.

753
00:54:36,000 --> 00:54:39,000
When I mentioned that sometimes the features are potentially

754
00:54:40,000 --> 00:54:41,000
but not always the same,

755
00:54:41,000 --> 00:54:44,000
the features are potentially but not always related to the target.

756
00:54:45,000 --> 00:54:47,000
What does that mean?

757
00:54:48,000 --> 00:54:51,000
Well, when we do machine learning, we're trying to figure out,

758
00:54:52,000 --> 00:54:54,000
here's my features and here's my labels or my categorizations.

759
00:54:55,000 --> 00:54:56,000
Is there a relationship?

760
00:54:57,000 --> 00:54:59,000
And if there is one, can I predict something in the future?

761
00:55:00,000 --> 00:55:03,000
But sometimes you might have a column of data or a variable

762
00:55:04,000 --> 00:55:07,000
that is completely unrelated, it's completely non-sequitur.

763
00:55:07,000 --> 00:55:11,000
And so sometimes your features may not actually provide any input

764
00:55:12,000 --> 00:55:14,000
into what labels applied to something.

765
00:55:15,000 --> 00:55:17,000
All right.

766
00:55:18,000 --> 00:55:21,000
There's another word that you're going to hear a lot called training and testing.

767
00:55:22,000 --> 00:55:23,000
Some data is going to be referred to as training data,

768
00:55:24,000 --> 00:55:25,000
some of it's going to be referred to as test data.

769
00:55:26,000 --> 00:55:29,000
By and large, throughout the rest of this session,

770
00:55:30,000 --> 00:55:32,000
I'm going to kind of settle on two or three different terms

771
00:55:33,000 --> 00:55:34,000
and we're just going to stick with those.

772
00:55:34,000 --> 00:55:37,000
Just be totally aware, your mileage is going to vary

773
00:55:38,000 --> 00:55:40,000
because everybody's got their own naming convention for how they label stuff.

774
00:55:41,000 --> 00:55:43,000
All right.

775
00:55:44,000 --> 00:55:47,000
So let's take a quick look at that train test split function

776
00:55:48,000 --> 00:55:50,000
that we read a little bit about.

777
00:55:51,000 --> 00:55:55,000
When you do machine learning, it is crazy important

778
00:55:56,000 --> 00:55:58,000
that as you kind of finish your machine learning episode,

779
00:55:59,000 --> 00:56:03,000
that you take a moment and you test and see what you have accomplished, right?

780
00:56:04,000 --> 00:56:07,000
See whether things are working the way you expect them to, et cetera.

781
00:56:08,000 --> 00:56:10,000
And so Scikit-learn has got some components built in

782
00:56:11,000 --> 00:56:12,000
to allow you to do testing and training.

783
00:56:13,000 --> 00:56:15,000
And this is a very sophisticated process.

784
00:56:16,000 --> 00:56:17,000
There are a lot of different ways to do it.

785
00:56:18,000 --> 00:56:19,000
We're just going to touch on one of them.

786
00:56:20,000 --> 00:56:21,000
It's probably the simplest and easiest one to accomplish,

787
00:56:22,000 --> 00:56:24,000
but it may not provide you all the details that you need

788
00:56:25,000 --> 00:56:29,000
and it may not provide you all of the fine granularity that you need to have.

789
00:56:29,000 --> 00:56:33,000
You will see references to maybe three different main types of data.

790
00:56:34,000 --> 00:56:35,000
You'll see the word training.

791
00:56:36,000 --> 00:56:37,000
You might see the word validation.

792
00:56:38,000 --> 00:56:39,000
You might see the word test.

793
00:56:40,000 --> 00:56:43,000
Sometimes people will use test and validation synonymously.

794
00:56:44,000 --> 00:56:47,000
Sometimes people will use them to mean two separate things.

795
00:56:48,000 --> 00:56:53,000
Often, if people are using these three things to mean different things,

796
00:56:54,000 --> 00:56:55,000
training will be here.

797
00:56:56,000 --> 00:56:57,000
I'm going to teach my model about relationships,

798
00:56:57,000 --> 00:56:58,000
the validation might be,

799
00:56:59,000 --> 00:57:01,000
let me check to see if everything is working out the way we expect.

800
00:57:02,000 --> 00:57:05,000
And then once I've trained and validated my model

801
00:57:06,000 --> 00:57:08,000
and I feel confident that my model is doing what it's supposed to be doing,

802
00:57:09,000 --> 00:57:12,000
let me then test it with something where I know the actual results

803
00:57:13,000 --> 00:57:14,000
and make sure that it comes out the way it should.

804
00:57:15,000 --> 00:57:16,000
You might say, well, isn't that what validation is doing?

805
00:57:17,000 --> 00:57:20,000
You would think yes, maybe a little no, maybe a little yes.

806
00:57:21,000 --> 00:57:22,000
I'm going to leave it up to you to kind of read that,

807
00:57:23,000 --> 00:57:24,000
but I want to put that thought in your head

808
00:57:24,000 --> 00:57:28,000
that training, validation, and test are important subjects

809
00:57:29,000 --> 00:57:31,000
and you guys should start to consider those

810
00:57:32,000 --> 00:57:33,000
as you go down your machine learning journey.

811
00:57:34,000 --> 00:57:36,000
We're just going to focus on training and test.

812
00:57:37,000 --> 00:57:38,000
We're going to skip out validation.

813
00:57:39,000 --> 00:57:40,000
Okay.

814
00:57:41,000 --> 00:57:44,000
So how do you get training data and test data?

815
00:57:45,000 --> 00:57:46,000
Normally you break your data into two piles.

816
00:57:47,000 --> 00:57:51,000
And that's the idea behind train, test, split.

817
00:57:51,000 --> 00:57:54,000
I'm going to split my data into training set and a test set.

818
00:57:55,000 --> 00:57:58,000
So I can import from Scikit-learn my model selection.

819
00:57:59,000 --> 00:58:01,000
From this module I'll input train, test, split.

820
00:58:02,000 --> 00:58:04,000
So let's do execute that.

821
00:58:05,000 --> 00:58:08,000
I have a data set related to coffee.

822
00:58:09,000 --> 00:58:11,000
Let's go take a look at it.

823
00:58:12,000 --> 00:58:14,000
I have a folder called universal data sets

824
00:58:15,000 --> 00:58:18,000
and we have a couple of data sets that we're going to look at today.

825
00:58:18,000 --> 00:58:23,000
So you should get used to the idea of looking at your data,

826
00:58:24,000 --> 00:58:25,000
figuring out what's there.

827
00:58:26,000 --> 00:58:29,000
This case I have 15 rows of data.

828
00:58:30,000 --> 00:58:34,000
These are prices for coffee of various sizes.

829
00:58:35,000 --> 00:58:39,000
Everything from about here down is completely fake.

830
00:58:40,000 --> 00:58:42,000
I actually pulled these off of menus on the North Shore

831
00:58:43,000 --> 00:58:47,000
and the rest of them like, well, 12 ounces, that's about 290.

832
00:58:48,000 --> 00:58:49,000
What the heck?

833
00:58:50,000 --> 00:58:51,000
So yeah, fake data, it's all good.

834
00:58:52,000 --> 00:58:53,000
It's like fake news.

835
00:58:54,000 --> 00:58:55,000
Okay.

836
00:58:56,000 --> 00:58:59,000
So we will read this in as a data frame.

837
00:59:03,000 --> 00:59:05,000
Let's take a look at the data frame.

838
00:59:06,000 --> 00:59:07,000
Let's see.

839
00:59:08,000 --> 00:59:09,000
Let's say DF.

840
00:59:10,000 --> 00:59:12,000
Sure enough, it read in successfully.

841
00:59:13,000 --> 00:59:16,000
I have two columns, one called size, one called price.

842
00:59:16,000 --> 00:59:21,000
I mentioned you could use something like data head to look at smaller snippets.

843
00:59:22,000 --> 00:59:23,000
So let's do that. There we go.

844
00:59:24,000 --> 00:59:27,000
But for my model, I just need the size column.

845
00:59:28,000 --> 00:59:31,000
So I'm going to convert that to a two dimensional array.

846
00:59:32,000 --> 00:59:34,000
I'm going to call it with a capital X.

847
00:59:35,000 --> 00:59:38,000
My prices, I'm going to use those as my target array.

848
00:59:39,000 --> 00:59:40,000
They don't have to be two dimensional.

849
00:59:41,000 --> 00:59:42,000
So there we go.

850
00:59:43,000 --> 00:59:44,000
Size is two dimensional.

851
00:59:44,000 --> 00:59:45,000
Sure enough.

852
00:59:46,000 --> 00:59:48,000
Head is one dimensional series.

853
00:59:49,000 --> 00:59:50,000
Okay.

854
00:59:51,000 --> 00:59:52,000
So I've now taken my data.

855
00:59:53,000 --> 00:59:54,000
I've read it in.

856
00:59:55,000 --> 00:59:56,000
It is in a features matrix and it's in an array.

857
00:59:57,000 --> 00:59:58,000
Okay.

858
00:59:59,000 --> 01:00:03,000
With train test split, I'll feed it my features matrix.

859
01:00:04,000 --> 01:00:08,000
I will feed it my target array and I can tell it how to split it up.

860
01:00:09,000 --> 01:00:13,000
In this case, I'm going to say the size of my test data should be about

861
01:00:14,000 --> 01:00:15,000
33% of the total data.

862
01:00:16,000 --> 01:00:18,000
So if I have 15 records, what's 33% of that?

863
01:00:19,000 --> 01:00:20,000
Five.

864
01:00:21,000 --> 01:00:23,000
I can assign a random state.

865
01:00:24,000 --> 01:00:27,000
So you'll see mentions of randomness a lot.

866
01:00:28,000 --> 01:00:29,000
Yeah, what's up?

867
01:00:30,000 --> 01:00:31,000
Are there any canonical like thumbnails?

868
01:00:32,000 --> 01:00:35,000
You know, ideas about how much you should reserve for tests?

869
01:00:36,000 --> 01:00:37,000
Cool.

870
01:00:38,000 --> 01:00:39,000
There are.

871
01:00:40,000 --> 01:00:41,000
I'll show you one in a sec.

872
01:00:42,000 --> 01:00:43,000
Don't let me forget.

873
01:00:44,000 --> 01:00:45,000
So this is a random state.

874
01:00:46,000 --> 01:00:47,000
You'll see this a lot in Scikit-learn.

875
01:00:48,000 --> 01:00:51,000
A lot of the models have a mechanism for you to allow to put a random number in.

876
01:00:52,000 --> 01:00:53,000
And it's a seed.

877
01:00:54,000 --> 01:00:55,000
You say, well, what does that mean?

878
01:00:56,000 --> 01:00:59,000
If I want each of you to get the exact same results as I get,

879
01:01:00,000 --> 01:01:02,000
we can all provide the same seed number.

880
01:01:03,000 --> 01:01:07,000
And it will then do a pseudo random algorithm to break things up in a particular way.

881
01:01:08,000 --> 01:01:10,000
And you'll all get the same random outputs that I get.

882
01:01:11,000 --> 01:01:12,000
Right. So that's what the seed is for.

883
01:01:12,000 --> 01:01:13,000
It's a random state seed.

884
01:01:14,000 --> 01:01:16,000
Let's run this real quick.

885
01:01:17,000 --> 01:01:22,000
When it breaks the X and Y values out,

886
01:01:23,000 --> 01:01:25,000
it will break them each into two components.

887
01:01:26,000 --> 01:01:28,000
So I end up getting a total of four things out.

888
01:01:29,000 --> 01:01:30,000
I provided two and I get four back.

889
01:01:31,000 --> 01:01:33,000
And it's going to dump out a list.

890
01:01:34,000 --> 01:01:37,000
But a lot of people will take that list and they will do what's called list unpacking.

891
01:01:38,000 --> 01:01:40,000
And so even though I get back a single list,

892
01:01:40,000 --> 01:01:43,000
I'm going to unpack the first value out of list to this variable,

893
01:01:44,000 --> 01:01:45,000
the second value to that one, third and fourth.

894
01:01:46,000 --> 01:01:50,000
And so I end up getting all four variables out of my list unpacked.

895
01:01:51,000 --> 01:01:55,000
And now they have individual names and I can refer to them one by one.

896
01:01:56,000 --> 01:02:00,000
And I can see that my new split off training data only has 10 rows,

897
01:02:01,000 --> 01:02:02,000
which is what we expect.

898
01:02:03,000 --> 01:02:05,000
My test data has five rows, which we expect.

899
01:02:05,000 --> 01:02:12,000
And my training categorizations have 10 items and my test has five.

900
01:02:13,000 --> 01:02:14,000
All right.

901
01:02:15,000 --> 01:02:16,000
We fed in a matrix, we got out a matrix.

902
01:02:17,000 --> 01:02:18,000
We fed in an array, we got out an array.

903
01:02:19,000 --> 01:02:21,000
And we had twice as many things out as we had in.

904
01:02:22,000 --> 01:02:23,000
All right. So Eric asked a question.

905
01:02:24,000 --> 01:02:28,000
He's like, hey, is there some canonical value about how you should split this up?

906
01:02:29,000 --> 01:02:33,000
Okay. Let's take a look here.

907
01:02:33,000 --> 01:02:36,000
Let's start by looking at what is the default for a test size.

908
01:02:37,000 --> 01:02:41,000
And the default for a test side is 0.0 or 0.25.

909
01:02:42,000 --> 01:02:46,000
If you want to give it a float, you can say,

910
01:02:47,000 --> 01:02:51,000
give me some value for my testing cutoff as between 0 and 1.

911
01:02:52,000 --> 01:02:53,000
And it gives you a proportion.

912
01:02:54,000 --> 01:02:58,000
So I gave it 0.33. It held out 33% of my values.

913
01:02:58,000 --> 01:03:03,000
You can give it an integer instead of a float.

914
01:03:04,000 --> 01:03:07,000
And you can say, hey, I just want you to hold out three records out of the 15.

915
01:03:08,000 --> 01:03:09,000
And it'll do that as well.

916
01:03:10,000 --> 01:03:11,000
All right.

917
01:03:12,000 --> 01:03:16,000
You can also set the training size versus the testing size.

918
01:03:17,000 --> 01:03:18,000
There's a variety of things.

919
01:03:19,000 --> 01:03:21,000
So you could approach it from either saying, this is how much I want to hold off for the training size.

920
01:03:22,000 --> 01:03:23,000
This is how much I want to hold off test size.

921
01:03:24,000 --> 01:03:25,000
Either of those work.

922
01:03:26,000 --> 01:03:27,000
How did I pop that out?

923
01:03:28,000 --> 01:03:30,000
Okay. So here's a cool thing about JupyterLab.

924
01:03:31,000 --> 01:03:33,000
It is very much designed for interactive computing.

925
01:03:34,000 --> 01:03:39,000
So if I put my cursor in this cell and I click, for example, on any given function,

926
01:03:40,000 --> 01:03:42,000
and so train test split is a function, I click there.

927
01:03:43,000 --> 01:03:46,000
If I press to shift and then press the tab key,

928
01:03:47,000 --> 01:03:50,000
it'll bring up the documentation for this particular thing.

929
01:03:51,000 --> 01:03:52,000
And you can scroll down in it.

930
01:03:55,000 --> 01:03:57,000
This is natural to me.

931
01:03:58,000 --> 01:03:59,000
I'm not thinking nowadays.

932
01:04:00,000 --> 01:04:01,000
So I pop it up.

933
01:04:02,000 --> 01:04:03,000
I'm glad you asked. That was excellent.

934
01:04:04,000 --> 01:04:06,000
I use this feature in JupyterLab all the time.

935
01:04:07,000 --> 01:04:08,000
All right.

936
01:04:09,000 --> 01:04:10,000
Yeah.

937
01:04:11,000 --> 01:04:18,000
So up here in the edit menu, if you want to clear all of the outputs,

938
01:04:19,000 --> 01:04:23,000
you can go to edit and you can go to clear all outputs and they're all gone.

939
01:04:24,000 --> 01:04:26,000
If you want to clear the output for a single cell, whatever cell you're in,

940
01:04:26,000 --> 01:04:29,000
you hit clear outputs and it'll clear the outputs for that one cell.

941
01:04:30,000 --> 01:04:31,000
Also good questions.

942
01:04:32,000 --> 01:04:34,000
All right. Did that help a little bit?

943
01:04:35,000 --> 01:04:40,000
Now, when you do validation, there are things called cross validation.

944
01:04:42,000 --> 01:04:43,000
Let me see something here.

945
01:04:44,000 --> 01:04:49,000
So cross validation scikit-learn.

946
01:04:52,000 --> 01:04:53,000
Do they have any pretty pictures?

947
01:04:53,000 --> 01:04:54,000
They have pictures.

948
01:04:55,000 --> 01:04:56,000
Am I going to say they're pretty? Maybe not.

949
01:05:01,000 --> 01:05:02,000
Okay.

950
01:05:05,000 --> 01:05:09,000
There are techniques that you can use for validating your models.

951
01:05:11,000 --> 01:05:14,000
One of them is k-fold cross validation where you might say,

952
01:05:15,000 --> 01:05:18,000
okay, I have this large data set.

953
01:05:19,000 --> 01:05:21,000
I'm going to break off a portion of it and I'm going to use it for testing.

954
01:05:21,000 --> 01:05:26,000
I'm going to redo my model though a second time and I'm going to grab a different portion of the data

955
01:05:27,000 --> 01:05:28,000
and use that one for testing now.

956
01:05:29,000 --> 01:05:31,000
I'm going to do it a third time and use a different portion of data.

957
01:05:32,000 --> 01:05:34,000
And so I end up running through my model four different times

958
01:05:35,000 --> 01:05:39,000
with four different sets of testing data out of there.

959
01:05:41,000 --> 01:05:44,000
And then I might take an average of these four results and say,

960
01:05:45,000 --> 01:05:48,000
you know what, now that I've done this four times and I take an average,

961
01:05:48,000 --> 01:05:54,000
I know that it wasn't just a fluke that I got some result because maybe these records were a little wonky.

962
01:05:55,000 --> 01:05:57,000
I can say I've looked at these and these and these and these

963
01:05:58,000 --> 01:06:03,000
and with an average I feel pretty confident that my model is classifying correctly.

964
01:06:04,000 --> 01:06:07,000
And you can get to the point where you can say one record at a time.

965
01:06:08,000 --> 01:06:11,000
You can say use 99 records for training and use one record for validation

966
01:06:12,000 --> 01:06:14,000
and then check the next record and the third record and fourth record.

967
01:06:14,000 --> 01:06:18,000
So as you start to figure out your training regimen and what you want to do,

968
01:06:19,000 --> 01:06:21,000
you'll start to research things like cross validation

969
01:06:22,000 --> 01:06:27,000
and figure out for this case what's necessary, for this model what's necessary or what's advised.

970
01:06:28,000 --> 01:06:34,000
But that gets crazy complicated and you can see there's a lot of discussion here on this page

971
01:06:35,000 --> 01:06:37,000
about various techniques that you can go through.

972
01:06:38,000 --> 01:06:39,000
So.

973
01:06:39,000 --> 01:06:43,000
Cross validation, evaluating the performance of your estimators.

974
01:06:44,000 --> 01:06:48,000
So in Scikit-Learn the classifiers or the models are often called estimators.

975
01:06:51,000 --> 01:06:52,000
All right.

976
01:06:54,000 --> 01:06:57,000
I think we've exceeded our recommended daily allotment of time for this.

977
01:06:58,000 --> 01:07:01,000
Let me make sure that there's nothing crazy here that we can do.

978
01:07:02,000 --> 01:07:04,000
So I'm going to go ahead and do the last one.

979
01:07:04,000 --> 01:07:06,000
So we've exceeded our recommended daily allotment of time for this.

980
01:07:07,000 --> 01:07:10,000
Let me make sure that there's nothing crazy here that we need to look at.

981
01:07:11,000 --> 01:07:12,000
Okay.

982
01:07:14,000 --> 01:07:16,000
I've got a couple of snippets of code in here.

983
01:07:17,000 --> 01:07:19,000
I'm going to skip. I'll let you guys play with those on your own.

984
01:07:20,000 --> 01:07:23,000
Really quickly I'm going to show you a couple examples of visualization things that we'll be using later.

985
01:07:24,000 --> 01:07:28,000
All right. So we're back in the data handling sheet and we've scrolled down a bit.

986
01:07:29,000 --> 01:07:31,000
A lot of folks use Matplotlib.

987
01:07:31,000 --> 01:07:35,000
You can use your own favorite graphing library, but this one is fast and easy.

988
01:07:36,000 --> 01:07:41,000
In this case I have a set of data called LinearRegTrainCSV.

989
01:07:42,000 --> 01:07:43,000
Let's take a look at it.

990
01:07:44,000 --> 01:07:47,000
This is completely random numbers. I pulled them out of thin air.

991
01:07:48,000 --> 01:07:52,000
I'll show you how I got them out of thin air later, but I pulled them out of thin air.

992
01:07:53,000 --> 01:07:59,000
I'm going to convert one column into a frame and one column into just a straight array.

993
01:08:01,000 --> 01:08:06,000
With scatter plots, if you drop your x values in and your y values in,

994
01:08:07,000 --> 01:08:09,000
Matplotlib will plot these things for you.

995
01:08:10,000 --> 01:08:14,000
We're going to see lots of examples of us plotting things by putting x values and y values in.

996
01:08:26,000 --> 01:08:29,000
Yeah. Okay. Mine does do it by default.

997
01:08:29,000 --> 01:08:31,000
That's an interesting question.

998
01:08:33,000 --> 01:08:37,000
And you used, what, percent Matplotlib, what?

999
01:08:46,000 --> 01:08:52,000
In some installs of JupyterLab, if you run this command, percent Matplotlib inline,

1000
01:08:53,000 --> 01:08:58,000
it will set up some feature in the background so that Matplotlib will display properly in JupyterLab.

1001
01:08:59,000 --> 01:09:00,000
Or your Jupyter notebooks.

1002
01:09:01,000 --> 01:09:05,000
I do not know why this works in some cases, but not others.

1003
01:09:06,000 --> 01:09:11,000
It has been many moons since I've ever had to use that particular command to make it work.

1004
01:09:12,000 --> 01:09:16,000
So I appreciate you bringing that up because that's probably perplexed a few folks.

1005
01:09:17,000 --> 01:09:19,000
I'll leave that in there for now. Okay.

1006
01:09:20,000 --> 01:09:21,000
Let's see.

1007
01:09:22,000 --> 01:09:25,000
But by and large, with scatter, you can put in some values and it does it.

1008
01:09:26,000 --> 01:09:27,000
It's pretty straightforward.

1009
01:09:27,000 --> 01:09:32,000
A neat thing though is that you can provide additional details to your scatter plot

1010
01:09:33,000 --> 01:09:35,000
that allow you to change colors based on the categorization of things.

1011
01:09:36,000 --> 01:09:42,000
So I'm going to create a set of labels for my linear regression data set.

1012
01:09:43,000 --> 01:09:45,000
My data set has about 25 pieces of data in it.

1013
01:09:46,000 --> 01:09:47,000
I'm going to create a set of labels.

1014
01:09:48,000 --> 01:09:54,000
And what this will do is if we were to look at all of the raw data

1015
01:09:54,000 --> 01:09:57,000
and say maybe the raw data for this one and the raw data for that one

1016
01:09:58,000 --> 01:10:01,000
and the raw data for that one, 25 rows of data.

1017
01:10:02,000 --> 01:10:05,000
I might say that whatever's in row one is category one.

1018
01:10:06,000 --> 01:10:10,000
Whatever's in row two is category zero, category zero, category zero, category one.

1019
01:10:11,000 --> 01:10:12,000
And so I might have categorized these things.

1020
01:10:13,000 --> 01:10:16,000
And I can show those categorizations through color differences on the screen.

1021
01:10:17,000 --> 01:10:19,000
Let's look at that. Okay.

1022
01:10:19,000 --> 01:10:29,000
How we do that is through the use of the color feature and the color map feature.

1023
01:10:30,000 --> 01:10:36,000
I can provide the X and Y values and then I provide the labels to this colors parameter.

1024
01:10:37,000 --> 01:10:41,000
And for every one in my labels, that will be assigned a color.

1025
01:10:42,000 --> 01:10:43,000
For every zero, it gets assigned a different color.

1026
01:10:43,000 --> 01:10:49,000
The C map is a way to provide a mapping to say here's two colors to use.

1027
01:10:50,000 --> 01:10:51,000
Use red and blue in this case.

1028
01:10:52,000 --> 01:10:54,000
And every time you have a zero, maybe apply a red.

1029
01:10:55,000 --> 01:10:56,000
Every time you have a one, apply a blue.

1030
01:10:57,000 --> 01:10:58,000
And it will map this out for you.

1031
01:10:59,000 --> 01:11:03,000
And so now we get that same data, but everything here has been categorized in red.

1032
01:11:04,000 --> 01:11:05,000
Everything over there is categorized in blue.

1033
01:11:06,000 --> 01:11:10,000
And so you're going to see examples of us using that categorization multiple times.

1034
01:11:10,000 --> 01:11:15,000
Map plot lib has several defined color palettes.

1035
01:11:16,000 --> 01:11:18,000
Red, blue is one of them.

1036
01:11:19,000 --> 01:11:21,000
Another one is seismic.

1037
01:11:22,000 --> 01:11:26,000
I'm going to use seismic pretty much the whole day.

1038
01:11:28,000 --> 01:11:33,000
I would be interested in having a conversation afterwards with folks who might be color blind.

1039
01:11:34,000 --> 01:11:38,000
If there's anyone in the room with that categorization about these colors.

1040
01:11:38,000 --> 01:11:41,000
I'm going to cut you folks loose for a moment.

1041
01:11:42,000 --> 01:11:46,000
I'm going to have you start to read ahead a little bit about the things we're going to cover next.

1042
01:11:47,000 --> 01:11:54,000
We're going to dive into a more in-depth discussion of linear models, in particular linear regression.

1043
01:11:55,000 --> 01:12:00,000
So take three, four minutes and churn through this content.

1044
01:12:01,000 --> 01:12:02,000
I've got two questions for you.

1045
01:12:03,000 --> 01:12:06,000
I think this is also probably a good time if people need to take a quick break.

1046
01:12:06,000 --> 01:12:07,000
We take a break about once an hour.

1047
01:12:08,000 --> 01:12:10,000
So I'm going to turn the mic off.

1048
01:12:11,000 --> 01:12:13,000
And you guys have about three or four minutes.

1049
01:12:36,000 --> 01:12:37,000
I'm going to turn the mic off.

1050
01:13:06,000 --> 01:13:12,000
Sure.

1051
01:13:16,000 --> 01:13:23,000
So you can do shapes, but you have to figure out what the parameter is to make matplotlib do shapes.

1052
01:13:24,000 --> 01:13:25,000
So bear with me for a second.

1053
01:13:30,000 --> 01:13:31,000
Choosing color maps.

1054
01:13:31,000 --> 01:13:32,000
Choosing color maps.

1055
01:13:33,000 --> 01:13:39,000
So matplotlib actually have an entire page devoted to strictly nothing but color maps.

1056
01:13:42,000 --> 01:13:47,000
And so they have the Viridis or the Magma color maps.

1057
01:13:48,000 --> 01:13:52,000
They have grays, purples, blues.

1058
01:13:53,000 --> 01:13:57,000
They've got things that are kind of rainbow spectrum or things that are kind of all blue and green, whatever.

1059
01:13:58,000 --> 01:14:00,000
They have things that are gray.

1060
01:14:01,000 --> 01:14:03,000
Gray scale.

1061
01:14:07,000 --> 01:14:09,000
They've got a whole collection of them.

1062
01:14:10,000 --> 01:14:13,000
And they have comparisons of a bunch of these.

1063
01:14:14,000 --> 01:14:19,000
If this was rendered in gray scale because you printed it on a black and white printer or whatever, how might this show up?

1064
01:14:20,000 --> 01:14:21,000
That kind of thing.

1065
01:14:31,000 --> 01:14:58,000
This link actually takes us right to this section of the page.

1066
01:14:58,000 --> 01:15:01,000
How very useful.

1067
01:15:02,000 --> 01:15:05,000
There is a tool apparently called VisCheck.

1068
01:15:06,000 --> 01:15:15,000
It says avoiding color maps with both red and green will avoid many problems in general.

1069
01:15:16,000 --> 01:15:18,000
And I did read that.

1070
01:15:19,000 --> 01:15:20,000
I'm like, oh, so both red and green are probably bad.

1071
01:15:21,000 --> 01:15:23,000
I won't use both. I'll use red and something else.

1072
01:15:24,000 --> 01:15:26,000
And apparently that's a fail.

1073
01:15:29,000 --> 01:15:35,000
There's actually some color blinded stimuli out there too if you want to add them to Google Maps.

1074
01:15:36,000 --> 01:15:38,000
All right.

1075
01:15:39,000 --> 01:15:47,000
By the way, when you finish these questions, make sure you put your little green stickies up.

1076
01:15:47,000 --> 01:15:59,000
Now that we got past the data handling, the rest of this is basically machine learning.

1077
01:16:00,000 --> 01:16:02,000
It will be way more fun.

1078
01:16:03,000 --> 01:16:07,000
So I did this for my local meetup.

1079
01:16:08,000 --> 01:16:10,000
There were about 37 people come out.

1080
01:16:11,000 --> 01:16:12,000
We did this.

1081
01:16:13,000 --> 01:16:15,000
We rewired the entire thing after that.

1082
01:16:15,000 --> 01:16:18,000
And we're getting the new and improved model.

1083
01:16:18,000 --> 01:16:20,000
It's great.

1084
01:16:48,000 --> 01:17:00,000
All right.

1085
01:17:01,000 --> 01:17:03,000
Quick answer.

1086
01:17:03,000 --> 01:17:09,000
So in Junior Lab, when you open a new tab, is there a new kernel for the shared variables?

1087
01:17:10,000 --> 01:17:12,000
Awesome.

1088
01:17:13,000 --> 01:17:15,000
I'm glad.

1089
01:17:16,000 --> 01:17:18,000
This is on the running man.

1090
01:17:19,000 --> 01:17:21,000
So open all.

1091
01:17:22,000 --> 01:17:24,000
Yep.

1092
01:17:25,000 --> 01:17:27,000
The console gets itself covered as well.

1093
01:17:28,000 --> 01:17:30,000
That's cool.

1094
01:17:30,000 --> 01:17:36,000
So you can actually open up another tab and use the same kernel for another.

1095
01:17:37,000 --> 01:17:42,000
You can use the same kernel more than a tab.

1096
01:17:43,000 --> 01:17:50,000
You can go into an individual cell and console related to a notebook you already have running to access all of that.

1097
01:17:51,000 --> 01:17:55,000
When you're in a console, how do you take the things that you've heard about?

1098
01:17:55,000 --> 01:17:57,000
Wait.

1099
01:17:58,000 --> 01:18:00,000
Say that one more time.

1100
01:18:00,000 --> 01:18:04,000
When you're in a console or a man-front, you open up a new kernel and you're in a directory.

1101
01:18:05,000 --> 01:18:07,000
How do you get that?

1102
01:18:37,000 --> 01:18:39,000
I'm going to go into the console.

1103
01:18:40,000 --> 01:18:42,000
I'm going to go into the console.

1104
01:18:43,000 --> 01:18:45,000
I'm going to go into the console.

1105
01:18:46,000 --> 01:18:48,000
I'm going to go into the console.

1106
01:18:49,000 --> 01:18:51,000
I'm going to go into the console.

1107
01:18:52,000 --> 01:18:54,000
I'm going to go into the console.

1108
01:18:55,000 --> 01:18:57,000
I'm going to go into the console.

1109
01:18:58,000 --> 01:19:00,000
I'm going to go into the console.

1110
01:19:01,000 --> 01:19:03,000
I'm going to go into the console.

1111
01:19:03,000 --> 01:19:05,000
I'm going to go into the console.

1112
01:19:08,000 --> 01:19:10,000
All right. Let's get rolling.

1113
01:19:11,000 --> 01:19:17,000
I'm going to turn on the mic so everything you say is being documented.

1114
01:19:18,000 --> 01:19:29,000
And my esteemed colleague in the back over there is doing all the things with the videography.

1115
01:19:30,000 --> 01:19:32,000
All right.

1116
01:19:33,000 --> 01:19:36,000
So, answer to our questions.

1117
01:19:37,000 --> 01:19:44,000
Who can identify a key benefit of the, and I cannot pronounce this probably, so we'll just call it Huber Regressor Model.

1118
01:19:46,000 --> 01:19:48,000
What do you got?

1119
01:19:49,000 --> 01:19:52,000
And that's going to be a pretty big thing in the butt.

1120
01:19:53,000 --> 01:19:57,000
When you do a regression, and we'll see a good example of this, right?

1121
01:19:58,000 --> 01:20:02,000
When you do a regression, you're looking at all this data and you're trying to figure out what are the trends through the data.

1122
01:20:03,000 --> 01:20:05,000
And all these major trends can be skewed if you have outliers.

1123
01:20:06,000 --> 01:20:14,000
And so there's actually a model, actually many models, that will look at outliers and help to kind of suppress the influence of outliers on the model.

1124
01:20:15,000 --> 01:20:16,000
That's cool.

1125
01:20:17,000 --> 01:20:21,000
For the linear regression model, what are the ranges of scores possible?

1126
01:20:22,000 --> 01:20:24,000
In particular, what did 1.0 mean?

1127
01:20:25,000 --> 01:20:27,000
Over trained.

1128
01:20:28,000 --> 01:20:30,000
1.0 means over trained.

1129
01:20:30,000 --> 01:20:32,000
1.0 means perfect match, right?

1130
01:20:33,000 --> 01:20:35,000
What are the range of scores?

1131
01:20:36,000 --> 01:20:38,000
The highest is at 1.0, and it goes down to where?

1132
01:20:39,000 --> 01:20:44,000
It can actually go into negative numbers, which means things are just gone completely off the rails.

1133
01:20:45,000 --> 01:20:47,000
All right. Cool.

1134
01:20:48,000 --> 01:20:50,000
So, I'm going to go into folder three.

1135
01:20:51,000 --> 01:20:53,000
We'll get into the linear regression conversation here.

1136
01:20:54,000 --> 01:20:55,000
All right.

1137
01:20:55,000 --> 01:20:59,000
And I'm going to go ahead and shut down a bunch of my kernels because I don't think we need them anymore.

1138
01:21:00,000 --> 01:21:02,000
There we go. All right. Close these off.

1139
01:21:03,000 --> 01:21:10,000
So, this one we're actually going to spend most of our time looking at kind of the model itself.

1140
01:21:11,000 --> 01:21:14,000
I'll give you a quick overview of what linear regression is.

1141
01:21:15,000 --> 01:21:18,000
We will look at a quick code sample that walks us through the trademarked process.

1142
01:21:19,000 --> 01:21:22,000
We will prep our data, choose our model, choose the hyperparameters.

1143
01:21:22,000 --> 01:21:23,000
We're going to fit the model.

1144
01:21:24,000 --> 01:21:26,000
We will apply the model, and then we'll examine our results.

1145
01:21:27,000 --> 01:21:31,000
And depending on how things play out, we may do a deep dive into certain things.

1146
01:21:32,000 --> 01:21:33,000
We might talk about some gotchas.

1147
01:21:34,000 --> 01:21:35,000
We'll see what kind of time we have.

1148
01:21:36,000 --> 01:21:37,000
All right.

1149
01:21:38,000 --> 01:21:40,000
Linear regression models.

1150
01:21:41,000 --> 01:21:45,000
These are really popular partly because in many cases they're really fast.

1151
01:21:46,000 --> 01:21:50,000
They generally come with very few tunable hyperparameters.

1152
01:21:50,000 --> 01:21:53,000
We'll look at the hyperparameters for linear regression.

1153
01:21:54,000 --> 01:21:55,000
I think there's only four if I remember right.

1154
01:21:56,000 --> 01:21:57,000
Generally, they're very easy to interpret.

1155
01:21:58,000 --> 01:22:03,000
And often they will give you a really good baseline classification to start with.

1156
01:22:04,000 --> 01:22:08,000
Before you even go down the road of do I need some really sophisticated processing system,

1157
01:22:09,000 --> 01:22:11,000
or do I need to really dive into a very complicated model,

1158
01:22:12,000 --> 01:22:14,000
this can often point you in the right direction before you go down that road.

1159
01:22:15,000 --> 01:22:18,000
A couple cases where you might use something like a linear regression.

1160
01:22:19,000 --> 01:22:23,000
Say you have ad dollars spent in various circumstances.

1161
01:22:24,000 --> 01:22:27,000
You might want to be able to predict how many sales dollars will that earn.

1162
01:22:28,000 --> 01:22:33,000
Again, if you have a car and it's a particular age, it's got particular characteristics,

1163
01:22:34,000 --> 01:22:36,000
it runs, it doesn't run, etc.

1164
01:22:37,000 --> 01:22:39,000
You might be able to help determine what your sale price will be.

1165
01:22:40,000 --> 01:22:42,000
There's an interesting data set out here.

1166
01:22:42,000 --> 01:22:49,000
Here it references latitude and skin cancer mortality, people who die from skin cancer.

1167
01:22:50,000 --> 01:22:54,000
We'll look at that for just a second and then we're going to breeze past it.

1168
01:22:55,000 --> 01:23:01,000
Linear regression model that we're going to examine relies upon a thing called ordinary least squares, OLS,

1169
01:23:02,000 --> 01:23:07,000
and is a way to figure out a linear function that fits your input data.

1170
01:23:08,000 --> 01:23:11,000
I've got a quote from Wikipedia and a link to the Wikipedia article.

1171
01:23:12,000 --> 01:23:23,000
What we do is we take the sum of square distances between each data point and a corresponding line.

1172
01:23:24,000 --> 01:23:30,000
And if we can reduce the sum of all those differences, we feel confident that our model better fits the data.

1173
01:23:31,000 --> 01:23:36,000
So I would look here at this data point and I'm saying that this blue line is probably a good fit.

1174
01:23:36,000 --> 01:23:43,000
I would take this distance and square it, take this distance and square it, that one and that one, and I take a sum.

1175
01:23:44,000 --> 01:23:47,000
And I might look at other lines as well and I might say,

1176
01:23:48,000 --> 01:23:51,000
okay, well what if my line was slanted a little bit differently in either direction?

1177
01:23:52,000 --> 01:23:56,000
If I do that same math and square the distances between the dots and the lines,

1178
01:23:57,000 --> 01:24:01,000
I will reach some nice kind of middle ground where that total sum is the smallest I can get.

1179
01:24:02,000 --> 01:24:08,000
And if I can get to the smallest sum, I can feel confident that my line is kind of basically nicely in the center, if you will.

1180
01:24:09,000 --> 01:24:10,000
And I use the word line.

1181
01:24:11,000 --> 01:24:14,000
This is two-dimensional data and so a line makes sense here.

1182
01:24:15,000 --> 01:24:22,000
If I had dots, if you will, in a three-dimensional space, I might separate those out using some sort of a curve.

1183
01:24:23,000 --> 01:24:25,000
Or if I have four-dimensional, I might have a hypercurve.

1184
01:24:26,000 --> 01:24:30,000
So you'll see people refer to a curve or a surface.

1185
01:24:31,000 --> 01:24:32,000
They basically mean the same thing.

1186
01:24:33,000 --> 01:24:38,000
Long story short, you calculate all the distances to your surface and you try and minimize those square distances.

1187
01:24:39,000 --> 01:24:40,000
All right.

1188
01:24:41,000 --> 01:24:45,000
And what you end up getting is some sort of a line or some sort of a surface.

1189
01:24:46,000 --> 01:24:48,000
I have a very simple formula here.

1190
01:24:49,000 --> 01:24:55,000
Some of you might remember back to your high school days and be looking at this in horror.

1191
01:24:55,000 --> 01:24:56,000
All right.

1192
01:24:57,000 --> 01:25:02,000
If I have a value for X and I know the slope of my line, I know that that angle the line is at,

1193
01:25:03,000 --> 01:25:08,000
and if I have a place where that line intersects the Y axis, I can figure out what the Y data point is.

1194
01:25:10,000 --> 01:25:13,000
So given some value of X with a slope and an intercept, we can calculate Y.

1195
01:25:14,000 --> 01:25:16,000
And that's as mathy as we've gotten.

1196
01:25:17,000 --> 01:25:18,000
We're done. OK.

1197
01:25:19,000 --> 01:25:20,000
Smiley face, no more math.

1198
01:25:22,000 --> 01:25:24,000
I'm going to leave it up to you to dive into the math.

1199
01:25:25,000 --> 01:25:31,000
There's a lot of linear models and they sometimes use calculations besides OLS.

1200
01:25:32,000 --> 01:25:33,000
Ridge is one of them.

1201
01:25:34,000 --> 01:25:35,000
Lasso is one of them.

1202
01:25:36,000 --> 01:25:38,000
We spoke about Huber for a minute ago and there's many more.

1203
01:25:39,000 --> 01:25:44,000
Everyone has a slightly different approach to trying to calculate that line or that surface that fits the data.

1204
01:25:44,000 --> 01:25:53,000
Ridge tries to address some issues related to OLS by making sure that the coefficients are small in size.

1205
01:25:54,000 --> 01:25:59,000
If you get a large coefficient for any value, say a large coefficient for the M, right?

1206
01:26:00,000 --> 01:26:05,000
If you ended up getting a large coefficient, that can skew things in potentially undesirable ways.

1207
01:26:06,000 --> 01:26:09,000
So Ridge tries to ensure that the coefficients are kept small.

1208
01:26:09,000 --> 01:26:10,000
And it has techniques to do that.

1209
01:26:11,000 --> 01:26:12,000
We're not going to get into.

1210
01:26:13,000 --> 01:26:17,000
Lasso tries to encourage very simple or sparse models, models with fewer variables.

1211
01:26:18,000 --> 01:26:22,000
Huber, it tries to look for those outliers and account for them in particular ways.

1212
01:26:23,000 --> 01:26:24,000
All right.

1213
01:26:25,000 --> 01:26:26,000
So let's get into it.

1214
01:26:27,000 --> 01:26:28,000
We'll start by prepping our data.

1215
01:26:29,000 --> 01:26:30,000
Typically, I start with standard imports.

1216
01:26:31,000 --> 01:26:33,000
I know that I'll probably want to plot something.

1217
01:26:34,000 --> 01:26:36,000
I know that I may want to plot something.

1218
01:26:36,000 --> 01:26:38,000
I know that I may or may not be using NumPy pandas.

1219
01:26:39,000 --> 01:26:41,000
I typically just dump those in right off the bat just in case.

1220
01:26:42,000 --> 01:26:44,000
And of course, we import scikit-learn.

1221
01:26:45,000 --> 01:26:48,000
I have a note, and this is in pretty much every one of these.

1222
01:26:49,000 --> 01:26:57,000
If you already know what model you're going to do or you're going to use, you could potentially just import it right here with the rest of them.

1223
01:26:58,000 --> 01:27:01,000
I have purposely broken that step out to a later component.

1224
01:27:01,000 --> 01:27:04,000
But there's no reason you can't do it first if you wanted to.

1225
01:27:05,000 --> 01:27:06,000
All right.

1226
01:27:07,000 --> 01:27:09,000
So we've got this skin cancer text data.

1227
01:27:10,000 --> 01:27:11,000
Let's go look at it real quick.

1228
01:27:12,000 --> 01:27:14,000
It should be in my universal data set.

1229
01:27:15,000 --> 01:27:16,000
Skin cancer.

1230
01:27:17,000 --> 01:27:26,000
And theoretically, we have skin cancer data, latitudes, the number of people who passed away.

1231
01:27:27,000 --> 01:27:28,000
That's the mortality column, the mort.

1232
01:27:28,000 --> 01:27:30,000
I don't even know what ocean meant in that particular case.

1233
01:27:31,000 --> 01:27:32,000
We have a longitude.

1234
01:27:33,000 --> 01:27:36,000
Those are the latitude and longitude, the centers of each state.

1235
01:27:37,000 --> 01:27:39,000
And we have all of the states.

1236
01:27:40,000 --> 01:27:53,000
And so theoretically, you may be able to look at this data and get some sense that maybe at higher or lower latitudes, maybe there is more of an influence on skin cancer that might potentially result in more deaths.

1237
01:27:53,000 --> 01:28:02,000
There are a lot of, if you do any reading about this particular data set, there's a lot of questions about whether this even makes any sense at all.

1238
01:28:03,000 --> 01:28:16,000
And are there other types of influences that might be impacting things in ways that maybe override the strength of latitude and longitude, maybe the availability of health care and those kind of things in various areas.

1239
01:28:17,000 --> 01:28:19,000
So this is simply for demo purposes.

1240
01:28:19,000 --> 01:28:27,000
I am not saying that we should draw any conclusions about latitude and longitude on mortality and skin cancer.

1241
01:28:28,000 --> 01:28:29,000
All right.

1242
01:28:30,000 --> 01:28:31,000
I'm going to read in the CSV.

1243
01:28:32,000 --> 01:28:33,000
This CSV is a little wonky.

1244
01:28:34,000 --> 01:28:37,000
It uses some different delimiters than normal CSVs.

1245
01:28:38,000 --> 01:28:43,000
And I'm going to feed in names for each column manually.

1246
01:28:44,000 --> 01:28:45,000
All right.

1247
01:28:46,000 --> 01:28:47,000
So there we go.

1248
01:28:47,000 --> 01:28:48,000
I'm using head.

1249
01:28:48,000 --> 01:28:50,000
Head shows five rows by default.

1250
01:28:51,000 --> 01:28:52,000
So there's our data.

1251
01:28:57,000 --> 01:28:59,000
So the data is all 50 states.

1252
01:29:00,000 --> 01:29:03,000
And yet I get 49 rows and five columns.

1253
01:29:09,000 --> 01:29:11,000
Yeah. Washington DC is in there.

1254
01:29:11,000 --> 01:29:12,000
Alaska is not.

1255
01:29:12,000 --> 01:29:13,000
Hawaii is not.

1256
01:29:13,000 --> 01:29:16,000
So this is continental US, including DC.

1257
01:29:16,000 --> 01:29:22,000
So again, when you get your data, it really helps take a look at it, figure out what's going on in there.

1258
01:29:22,000 --> 01:29:24,000
Right. Is this what you expected?

1259
01:29:24,000 --> 01:29:25,000
Yay.

1260
01:29:25,000 --> 01:29:34,000
Nay. I'm going to change the latitude into a features matrix, change Y into a or I'll use Y as an array.

1261
01:29:34,000 --> 01:29:36,000
Look at each of those.

1262
01:29:36,000 --> 01:29:40,000
So there's latitudes, there's longitudes or sorry, latitudes and mortality.

1263
01:29:41,000 --> 01:29:42,000
OK.

1264
01:29:42,000 --> 01:29:44,000
I'm going to split this data up.

1265
01:29:44,000 --> 01:29:45,000
I've got 49 records.

1266
01:29:45,000 --> 01:29:50,000
Let's kind of break it up into parts so we can do a little training and testing.

1267
01:29:52,000 --> 01:29:58,000
I'm using a sand random seed of 42 and I'm using a dot three three for my split.

1268
01:29:59,000 --> 01:30:01,000
And there we go.

1269
01:30:02,000 --> 01:30:03,000
All right. And we'll do a quick scatter plot.

1270
01:30:05,000 --> 01:30:10,000
This scatter plot shows just my training data.

1271
01:30:10,000 --> 01:30:15,000
So it should be about 30 to 33 records, I think.

1272
01:30:18,000 --> 01:30:23,000
I chose to put a label for X's and Y's to show latitudes, number of deaths, that kind of stuff.

1273
01:30:25,000 --> 01:30:26,000
All right.

1274
01:30:26,000 --> 01:30:31,000
And, you know, there's maybe a trend here.

1275
01:30:31,000 --> 01:30:36,000
Get to different latitudes, fewer number of deaths, go to lower latitudes, you get more deaths.

1276
01:30:36,000 --> 01:30:41,000
So maybe we can get a cool line out of this.

1277
01:30:41,000 --> 01:30:43,000
Next step, we choose our model.

1278
01:30:43,000 --> 01:30:50,000
Again, we could have done this right at the beginning when we did imports, but I wanted to put it here because this is the step of the process, right?

1279
01:30:50,000 --> 01:30:53,000
We're going to import the linear regression.

1280
01:30:53,000 --> 01:30:55,000
We do this guy.

1281
01:30:55,000 --> 01:30:56,000
OK.

1282
01:30:56,000 --> 01:30:58,000
All right.

1283
01:30:58,000 --> 01:31:02,000
I mentioned that with linear regression, it's generally pretty straightforward.

1284
01:31:03,000 --> 01:31:07,000
You don't have to fuss too much with the hyperparameters.

1285
01:31:07,000 --> 01:31:10,000
There are four of them.

1286
01:31:10,000 --> 01:31:23,000
If I wanted to know more about some of these hyperparameters, you know, I can do shift tab on this and it'll bring them up and I can scroll down to start to look at some of them.

1287
01:31:23,000 --> 01:31:30,000
So if I wanted to read a little bit more about them, I can maybe try and dive into what does fit intercept mean.

1288
01:31:30,000 --> 01:31:35,000
What should happen if it's set to false, if it's set to true.

1289
01:31:35,000 --> 01:31:39,000
I can learn more about what does it mean to normalize this data.

1290
01:31:39,000 --> 01:31:43,000
What happens behind the scenes when I do a copy X.

1291
01:31:43,000 --> 01:31:47,000
What happens when I change number of jobs.

1292
01:31:47,000 --> 01:31:53,000
You know, say I want to do this on a distributed network or something and have this computed in various places.

1293
01:31:53,000 --> 01:31:58,000
So.

1294
01:31:58,000 --> 01:32:02,000
But even reading quote unquote the manual, right?

1295
01:32:02,000 --> 01:32:07,000
Hyperparameters can still be really tricky to understand.

1296
01:32:07,000 --> 01:32:12,000
I like to start at the documentation to see what they say.

1297
01:32:12,000 --> 01:32:13,000
Good places also.

1298
01:32:13,000 --> 01:32:18,000
People will ask this question or similar questions on Stack Overflow.

1299
01:32:18,000 --> 01:32:23,000
So.

1300
01:32:23,000 --> 01:32:29,000
I had a question about how the fit intercept parameter impacts linear regression.

1301
01:32:29,000 --> 01:32:32,000
Someone else out there had the exact same question.

1302
01:32:32,000 --> 01:32:34,000
And so we asked this thing.

1303
01:32:34,000 --> 01:32:35,000
Blah, blah, blah, blah, right.

1304
01:32:35,000 --> 01:32:45,000
And there were two answers here that were fairly robust and walked through and provided some examples of what would happen.

1305
01:32:45,000 --> 01:32:55,000
If I used fit intercept equals true or false and provided both code to demo it and the results that came out of it.

1306
01:32:55,000 --> 01:32:59,000
Had notes here about notice these two concepts that are going on, et cetera.

1307
01:32:59,000 --> 01:33:01,000
It's got a little summation, right?

1308
01:33:01,000 --> 01:33:09,000
So as you run into questions that the documentation doesn't answer for you, feel free to check your friend Google.

1309
01:33:09,000 --> 01:33:15,000
Your friend Google will often be able to point you to people just like you who are asking the same kinds of questions.

1310
01:33:15,000 --> 01:33:28,000
I will say when I started going down the journey of getting into machine learning, in many cases it took several readings of things.

1311
01:33:28,000 --> 01:33:33,000
I would read some piece of documentation and like, yeah, no, I don't understand that.

1312
01:33:33,000 --> 01:33:38,000
And I might read multiple sources and I go, oh, wow, this one says this and this one says this.

1313
01:33:38,000 --> 01:33:39,000
And this one says this and this one says this.

1314
01:33:39,000 --> 01:33:42,000
And I still don't understand.

1315
01:33:42,000 --> 01:33:51,000
Might take some tests, run the code in various ways, not get it to work, struggle for a lot, cry a little, and then eventually it works.

1316
01:33:51,000 --> 01:33:55,000
Take people's examples and tests and run those myself.

1317
01:33:55,000 --> 01:34:02,000
And that can be a really long and frustrating, annoying process before you start to wrap your head around your expected outcomes.

1318
01:34:02,000 --> 01:34:11,000
And that's totally OK because if that happens to you, you're just like the rest of us.

1319
01:34:11,000 --> 01:34:15,000
I feel like this most days.

1320
01:34:15,000 --> 01:34:17,000
All right.

1321
01:34:17,000 --> 01:34:20,000
So we've broken our data into parts.

1322
01:34:20,000 --> 01:34:23,000
We've got it separated into training and testing.

1323
01:34:23,000 --> 01:34:25,000
Let's fit the model.

1324
01:34:25,000 --> 01:34:30,000
Let's put the training data in and see what we get.

1325
01:34:30,000 --> 01:34:46,000
And if I if I want to open up a new cell in Jupiter so that I can maybe play with this a little bit, I can put a cursor in a cell and hit escape and then press the B key.

1326
01:34:46,000 --> 01:34:53,000
So cursors in the cell, I press escape and then I press the B key and it opens up a new cell below the one I'm in.

1327
01:34:53,000 --> 01:34:56,000
If I pressed a would open a cell above the one I'm in.

1328
01:34:56,000 --> 01:35:00,000
So I've got model dot tab.

1329
01:35:00,000 --> 01:35:08,000
Whenever you have a object in Jupiter and it has attributes or behaviors or functions associated with it, you can type the name of the object.

1330
01:35:08,000 --> 01:35:09,000
You hit the dot.

1331
01:35:09,000 --> 01:35:14,000
And if you press the tab key, it'll open up a list of these options that you have available to you.

1332
01:35:14,000 --> 01:35:19,000
And so with this model, I have a coefficient available.

1333
01:35:19,000 --> 01:35:20,000
I have an intercept.

1334
01:35:20,000 --> 01:35:23,000
I have some functions like fit or predict.

1335
01:35:23,000 --> 01:35:26,000
I'm going to go look at the coefficient.

1336
01:35:26,000 --> 01:35:29,000
Oops.

1337
01:35:29,000 --> 01:35:33,000
And so this thing has a slope of negative six.

1338
01:35:33,000 --> 01:35:37,000
And if we go back, so sure enough, it has a negative slope.

1339
01:35:37,000 --> 01:35:43,000
OK, so that sounds cool.

1340
01:35:43,000 --> 01:35:47,000
We had an intercept as well.

1341
01:35:47,000 --> 01:35:49,000
Intercept.

1342
01:35:49,000 --> 01:35:55,000
And it intercepts somewhere around three hundred and eighty eight.

1343
01:35:55,000 --> 01:36:03,000
So if this thing were to continue to go over to zero and I was to look on the y axis, it would probably intersect somewhere around three hundred eighty eight.

1344
01:36:03,000 --> 01:36:08,000
So that's good.

1345
01:36:08,000 --> 01:36:11,000
All right. So this thing can produce a line.

1346
01:36:11,000 --> 01:36:17,000
If I provide it with new data points, it should be able to predict where on the line those fit.

1347
01:36:17,000 --> 01:36:23,000
And it should produce 17 of them, which is about 33 percent of 49.

1348
01:36:23,000 --> 01:36:34,000
And so these are all the expected number of folks who may die from skin cancer at various latitudes.

1349
01:36:34,000 --> 01:36:39,000
So let's graph these.

1350
01:36:39,000 --> 01:36:41,000
All the purple were the original training data.

1351
01:36:41,000 --> 01:36:48,000
All of the pink ones are our predicted outcomes.

1352
01:36:48,000 --> 01:36:55,000
If you don't like the dots, we can use plot and we can make a nice red line.

1353
01:36:55,000 --> 01:36:59,000
And you can always pull up your coefficients and your intercepts.

1354
01:36:59,000 --> 01:37:02,000
All right.

1355
01:37:02,000 --> 01:37:07,000
So let's talk about gotchas.

1356
01:37:07,000 --> 01:37:09,000
I kept messing up the formats of my data.

1357
01:37:09,000 --> 01:37:14,000
I kept trying to feed it the one D arrays instead of the two D feature matrices.

1358
01:37:14,000 --> 01:37:18,000
I'd pull up some tutorial online and I try and follow it.

1359
01:37:18,000 --> 01:37:20,000
And it was a fail. Right.

1360
01:37:20,000 --> 01:37:28,000
So just keep in your heads that when you're doing this and you're testing your own models, make sure that you're feeding it the right types of data.

1361
01:37:28,000 --> 01:37:31,000
Couple of thoughts to help you as you go down the road of learning stuff.

1362
01:37:31,000 --> 01:37:35,000
Some tips and hints. Read the outputs.

1363
01:37:35,000 --> 01:37:48,000
When Jupiter provides you with some form of output, say, for example, right here, when I do a fit, Jupiter spits out this text string.

1364
01:37:48,000 --> 01:37:51,000
Take a deep breath. Check it out.

1365
01:37:51,000 --> 01:37:56,000
Some of the things you'll recognize, like you might see fit intercept in multiple models.

1366
01:37:56,000 --> 01:37:59,000
You might see end jobs in multiple places.

1367
01:37:59,000 --> 01:38:03,000
The things that you don't yet recognize, be a little curious about those.

1368
01:38:03,000 --> 01:38:12,000
You can go look some of those up and using a thing like, you know, shift tab to bring up the help documentation can allow you to start to scroll through these.

1369
01:38:12,000 --> 01:38:20,000
I will say that in many cases when I scroll through these and I come across a term or phrase, I often don't know what it means right off the bat.

1370
01:38:20,000 --> 01:38:26,000
And sometimes the documentation is sparse enough that I still don't know what it means even after I've read the documentation.

1371
01:38:26,000 --> 01:38:30,000
But that's cool because your vocabulary will start to increase.

1372
01:38:30,000 --> 01:38:36,000
And the next time, the third time, the eighth time you see that word, it will click and things will start to gel.

1373
01:38:36,000 --> 01:38:41,000
You'll see that word used with other words. And now concepts will kind of come together.

1374
01:38:41,000 --> 01:38:44,000
So read what it spits out for you.

1375
01:38:44,000 --> 01:38:50,000
I work with students all the time and Python and Jupiter spit stuff out on the screen and they don't read it.

1376
01:38:50,000 --> 01:38:53,000
They don't read it at all. And it blows my mind.

1377
01:38:53,000 --> 01:38:57,000
Take a deep breath. Even if you don't understand it, just take a deep breath. Read it.

1378
01:38:57,000 --> 01:39:04,000
All right. It's that regular exposure to arguments that you might end up using in one model or another.

1379
01:39:04,000 --> 01:39:11,000
Regular exposure to terms, the vocabulary that will help to build an understanding of the inner workings.

1380
01:39:11,000 --> 01:39:17,000
Totally recommend that you do outside research. When you find a new word that you don't know, go hunt that word down.

1381
01:39:17,000 --> 01:39:19,000
Wikipedia, other places.

1382
01:39:19,000 --> 01:39:25,000
The statistics descriptions in Wikipedia are very robust, probably too robust.

1383
01:39:25,000 --> 01:39:30,000
But again, you're going to start to learn new vocabulary and that's a good way to do it.

1384
01:39:30,000 --> 01:39:36,000
Case in point. The word regression has been a thorn in my side since the very first time I saw it.

1385
01:39:36,000 --> 01:39:41,000
I've not quite grok'd what the word regression meant.

1386
01:39:41,000 --> 01:39:50,000
Like, what is a regression? In my head, I kept thinking like, you know, if something regresses, it goes backwards or it decreases or something.

1387
01:39:50,000 --> 01:39:56,000
And I was trying to figure out why is a line drawn through a bunch of points called a regression.

1388
01:39:56,000 --> 01:39:59,000
And this really, really, really stressed me out.

1389
01:39:59,000 --> 01:40:10,000
And I kept looking and eventually I came across an article that describes the term regression was actually coined by a guy named Francis Galton in the 19th century.

1390
01:40:10,000 --> 01:40:14,000
And it described a biological phenomenon.

1391
01:40:14,000 --> 01:40:20,000
He looked at the descendants of ancestors who were tall and he saw that they tended to regress and get shorter.

1392
01:40:20,000 --> 01:40:25,000
So if you're a tall person and you have kids, eventually your kids basically come back to normal.

1393
01:40:25,000 --> 01:40:36,000
And this word has now entered the lexicon to describe changes in data points over time, even though it doesn't really mean changes in data points over time, if you will.

1394
01:40:36,000 --> 01:40:39,000
So there you go.

1395
01:40:39,000 --> 01:40:44,000
All right. Before we get into the exercises, we've covered a lot of stuff about linear regression.

1396
01:40:44,000 --> 01:41:01,000
What questions do you guys have? What you got?

1397
01:41:01,000 --> 01:41:11,000
Is there a way to look at errors? I'm not 100 percent sure I'm tracking the question.

1398
01:41:11,000 --> 01:41:26,000
Oh, OK. So scikit learn has a wide variety of techniques to allow you to kind of evaluate what we'll call scores or metrics for your your models.

1399
01:41:26,000 --> 01:41:33,000
And API reference. So score.

1400
01:41:33,000 --> 01:41:38,000
So score. So you can quantify the quality of your predictions.

1401
01:41:38,000 --> 01:41:43,000
They have an article here that they discuss that you can look at things like your accuracy score.

1402
01:41:43,000 --> 01:41:48,000
They have particular types of scoring mechanisms for many of the models.

1403
01:41:48,000 --> 01:41:53,000
If we were to look at.

1404
01:41:53,000 --> 01:41:59,000
OK, we were to look at our model.

1405
01:41:59,000 --> 01:42:09,000
Most models will come with some sort of a scoring method and the scoring method will give you some sort of a coefficient related to your prediction.

1406
01:42:09,000 --> 01:42:18,000
And in this case, it looks at, I think, OLS and tries to define how big of a deviation do we have.

1407
01:42:18,000 --> 01:42:24,000
Gives you some ranges for the best possible score is going to look like talks those things through.

1408
01:42:24,000 --> 01:42:29,000
Right. And various models, you just different types of scoring.

1409
01:42:29,000 --> 01:42:34,000
Some of the techniques to get the scoring to work right may vary between models.

1410
01:42:34,000 --> 01:42:39,000
We are not going to dive into looking at scoring in this particular conversation.

1411
01:42:39,000 --> 01:42:47,000
We're not going to have enough time by the end of the day, but it's there and it's there for every model.

1412
01:42:47,000 --> 01:42:55,000
Any other questions? What you got?

1413
01:42:55,000 --> 01:43:00,000
Fit the data.

1414
01:43:00,000 --> 01:43:03,000
So you're applying a model, right?

1415
01:43:03,000 --> 01:43:08,000
I mean, I don't know that I've ever had a whole lot of concern about the word fit.

1416
01:43:08,000 --> 01:43:13,000
You have data, you fit this algorithm and this model to the data, and now it matches that data.

1417
01:43:13,000 --> 01:43:20,000
We will look at under fitting and over fitting in a few minutes and we'll see what happens when the fit is not right.

1418
01:43:20,000 --> 01:43:24,000
But fit has always kind of made sense to me, I guess.

1419
01:43:24,000 --> 01:43:27,000
So what you got?

1420
01:43:27,000 --> 01:43:31,000
My question is, you know, we get the example of the test.

1421
01:43:31,000 --> 01:43:36,000
You know, at the end of the day, you get a bunch of numbers, but when do you know that you can actually use that?

1422
01:43:36,000 --> 01:43:43,000
When you take that prediction or that model and you use it on other predictions, so a lot of things are possible.

1423
01:43:43,000 --> 01:43:45,000
When do you know you're done?

1424
01:43:45,000 --> 01:43:47,000
When do you know you're done? All right.

1425
01:43:47,000 --> 01:43:56,000
So that is going to be understanding the validity of the data that you're getting out of it and using scores.

1426
01:43:56,000 --> 01:44:01,000
So here is my input data. Here's my predictions.

1427
01:44:01,000 --> 01:44:07,000
For my test data, I know what the prediction should have said and I know what it does say.

1428
01:44:07,000 --> 01:44:11,000
And so you can score that and see how far off your model is.

1429
01:44:11,000 --> 01:44:17,000
If your model is really, really good, you'll get maybe a high score, point nine, something like that.

1430
01:44:17,000 --> 01:44:19,000
Don't quote me on that number, dear Lord.

1431
01:44:19,000 --> 01:44:26,000
High score means different things in different scenarios where you might get a really, really bad score.

1432
01:44:26,000 --> 01:44:30,000
There are what they call learning validation curves and learning curves for models.

1433
01:44:30,000 --> 01:44:35,000
And depending on how you tune those hyperparameters, you can get your model to be more and more accurate.

1434
01:44:35,000 --> 01:44:40,000
It'll almost never be 100 percent accurate.

1435
01:44:40,000 --> 01:44:47,000
But there's an entire, we've probably talked four or five hours on just that topic alone of how do you score your model

1436
01:44:47,000 --> 01:44:53,000
and how do you confirm to your liking that the model is accurate enough to use.

1437
01:44:53,000 --> 01:44:58,000
So there's no good easy answer, but it is a process.

1438
01:44:58,000 --> 01:45:03,000
And there's a lot of documentation in the API references and the user guide about how to do that and what to look for.

1439
01:45:03,000 --> 01:45:06,000
There's a lot of tutorials that cover things like that as well.

1440
01:45:06,000 --> 01:45:09,000
I don't know if that answered the question, but...

1441
01:45:09,000 --> 01:45:14,000
So like, short answer is you wouldn't base the ones that the students...

1442
01:45:20,000 --> 01:45:23,000
I wouldn't use that because I don't know what the data is.

1443
01:45:23,000 --> 01:45:24,000
I don't know where the data came from.

1444
01:45:24,000 --> 01:45:32,000
And I think there's a lot of really weird things behind the scenes that might influence skin mortality.

1445
01:45:32,000 --> 01:45:33,000
So, but anyway.

1446
01:45:33,000 --> 01:45:40,000
But yeah, really is diving into the scoring and diving into how well does your predictions track your known good data.

1447
01:45:40,000 --> 01:45:42,000
But when you get into clustering, there is no known good data.

1448
01:45:42,000 --> 01:45:44,000
So that makes that opens a whole nother can of worms.

1449
01:45:44,000 --> 01:45:46,000
And we'll look at clustering in a little bit.

1450
01:45:46,000 --> 01:45:48,000
So it's not easy.

1451
01:45:48,000 --> 01:45:50,000
All right.

1452
01:45:50,000 --> 01:45:51,000
307.

1453
01:45:51,000 --> 01:45:53,000
OK.

1454
01:45:53,000 --> 01:45:55,000
Let's see here.

1455
01:45:55,000 --> 01:45:56,000
Got you.

1456
01:45:56,000 --> 01:45:57,000
We did that.

1457
01:45:57,000 --> 01:45:58,000
We did that outside research.

1458
01:45:58,000 --> 01:45:59,000
Good to go.

1459
01:45:59,000 --> 01:46:00,000
All right.

1460
01:46:00,000 --> 01:46:03,000
So I'm going to turn the time over to you to do a little research here.

1461
01:46:03,000 --> 01:46:07,000
Next model we're going to look at is a model called Naive Bayes.

1462
01:46:07,000 --> 01:46:10,000
In particular, we're going to look at Gaussian Naive Bayes.

1463
01:46:10,000 --> 01:46:12,000
There is a link here to take you to that.

1464
01:46:12,000 --> 01:46:15,000
I'd like you to answer a couple questions.

1465
01:46:15,000 --> 01:46:20,000
There is a method in Naive Bayes that you can call that allows you to update your models over time.

1466
01:46:20,000 --> 01:46:22,000
What is that method called?

1467
01:46:22,000 --> 01:46:25,000
And then there is a link to the user guide in there.

1468
01:46:25,000 --> 01:46:28,000
Find that link and then skim the overview.

1469
01:46:28,000 --> 01:46:35,000
They mentioned two very specific places where Naive Bayes classifiers have worked really well in real world situations.

1470
01:46:35,000 --> 01:46:38,000
What are those two cases?

1471
01:46:38,000 --> 01:46:42,000
And we'll come back in about three minutes.

1472
01:46:58,000 --> 01:47:01,000
Okay.

1473
01:48:58,000 --> 01:49:01,000
Okay.

1474
01:49:01,000 --> 01:49:04,000
Okay.

1475
01:49:04,000 --> 01:49:07,000
Okay.

1476
01:49:07,000 --> 01:49:10,000
Okay.

1477
01:49:10,000 --> 01:49:13,000
Okay.

1478
01:49:13,000 --> 01:49:16,000
Okay.

1479
01:49:16,000 --> 01:49:19,000
Okay.

1480
01:49:19,000 --> 01:49:22,000
Okay.

1481
01:49:22,000 --> 01:49:25,000
Okay.

1482
01:49:25,000 --> 01:49:28,000
Okay.

1483
01:49:28,000 --> 01:49:31,000
Okay.

1484
01:49:31,000 --> 01:49:34,000
Okay.

1485
01:49:34,000 --> 01:49:37,000
Okay.

1486
01:49:37,000 --> 01:49:40,000
Okay.

1487
01:49:40,000 --> 01:49:43,000
Okay.

1488
01:49:43,000 --> 01:49:46,000
Okay.

1489
01:49:46,000 --> 01:49:49,000
Okay.

1490
01:49:49,000 --> 01:49:52,000
Okay.

1491
01:49:52,000 --> 01:49:55,000
Okay.

1492
01:49:55,000 --> 01:49:58,000
Okay.

1493
01:49:58,000 --> 01:50:01,000
Okay.

1494
01:50:01,000 --> 01:50:04,000
Okay.

1495
01:50:04,000 --> 01:50:07,000
Okay.

1496
01:50:07,000 --> 01:50:10,000
Okay.

1497
01:50:10,000 --> 01:50:13,000
Okay.

1498
01:50:13,000 --> 01:50:16,000
Okay.

1499
01:50:16,000 --> 01:50:19,000
Okay.

1500
01:50:19,000 --> 01:50:22,000
Okay.

1501
01:50:22,000 --> 01:50:25,000
Okay.

1502
01:50:25,000 --> 01:50:28,000
Okay.

1503
01:50:28,000 --> 01:50:31,000
Okay.

1504
01:50:31,000 --> 01:50:34,000
Okay.

1505
01:50:34,000 --> 01:50:37,000
Okay.

1506
01:50:37,000 --> 01:50:40,000
Okay.

1507
01:50:40,000 --> 01:50:43,000
Okay.

1508
01:50:43,000 --> 01:50:46,000
Okay.

1509
01:50:46,000 --> 01:50:49,000
Okay.

1510
01:50:49,000 --> 01:50:52,000
Okay.

1511
01:50:52,000 --> 01:50:55,000
Okay.

1512
01:50:55,000 --> 01:50:58,000
Okay.

1513
01:50:58,000 --> 01:51:01,000
Okay.

1514
01:51:01,000 --> 01:51:04,000
Okay.

1515
01:51:04,000 --> 01:51:07,000
Okay.

1516
01:51:07,000 --> 01:51:10,000
Okay.

1517
01:51:10,000 --> 01:51:13,000
Okay.

1518
01:51:13,000 --> 01:51:16,000
All right.

1519
01:51:16,000 --> 01:51:19,000
Let's get started.

1520
01:51:19,000 --> 01:51:22,000
Turn on the mic.

1521
01:51:22,000 --> 01:51:25,000
Okay.

1522
01:51:25,000 --> 01:51:28,000
All right.

1523
01:51:28,000 --> 01:51:31,000
So, I'm about ready to roll.

1524
01:51:31,000 --> 01:51:34,000
So, we had a couple questions.

1525
01:51:34,000 --> 01:51:37,000
Okay.

1526
01:51:37,000 --> 01:51:40,000
Okay.

1527
01:51:40,000 --> 01:51:43,000
Okay.

1528
01:51:43,000 --> 01:51:56,000
What method in the Gaussian naive Bayes classifier can you use if you have a data set that's too big to fit into memory all at once or you have a data set that you get new data periodically?

1529
01:51:56,000 --> 01:51:58,000
Partial fit.

1530
01:51:58,000 --> 01:51:59,000
Partial fit.

1531
01:51:59,000 --> 01:52:00,000
So, we've seen fit.

1532
01:52:00,000 --> 01:52:01,000
They have partial fit.

1533
01:52:01,000 --> 01:52:02,000
You get new data.

1534
01:52:02,000 --> 01:52:06,000
You can essentially layer that in to your naive Bayes classifier.

1535
01:52:06,000 --> 01:52:09,000
And a number of the models have partial fit capabilities.

1536
01:52:09,000 --> 01:52:10,000
Not all of them do.

1537
01:52:10,000 --> 01:52:12,000
All right.

1538
01:52:12,000 --> 01:52:14,000
There was a link to the user guide.

1539
01:52:14,000 --> 01:52:19,000
There were two very specific cases that they called out as naive Bayes worked really well in real world situations.

1540
01:52:19,000 --> 01:52:20,000
What were those two cases?

1541
01:52:20,000 --> 01:52:25,000
See it again?

1542
01:52:25,000 --> 01:52:28,000
Document classification and spam filtering.

1543
01:52:28,000 --> 01:52:30,000
Document classification and spam filtering.

1544
01:52:30,000 --> 01:52:31,000
All right.

1545
01:52:31,000 --> 01:52:32,000
Cool.

1546
01:52:32,000 --> 01:52:35,000
So, let's go look at naive Bayes.

1547
01:52:35,000 --> 01:52:40,000
All right.

1548
01:52:40,000 --> 01:52:44,000
So, naive Bayes classifiers, they're also very popular.

1549
01:52:44,000 --> 01:52:46,000
They're crazy fast.

1550
01:52:46,000 --> 01:52:48,000
They're generally simple.

1551
01:52:48,000 --> 01:52:50,000
They have very few tunable hyperparameters.

1552
01:52:50,000 --> 01:52:59,000
If you have data sets that have lots of columns, lots of variables, they're suitable for those types of data sets with high dimensions.

1553
01:52:59,000 --> 01:53:09,000
And again, they can make a really nice baseline to help you kind of get a sense of where's your data going before you spend the time, energy, and money of coming up with some really sophisticated models.

1554
01:53:09,000 --> 01:53:22,000
As I was putting this together, like the day after I started writing some of this stuff, this tweet showed up in my feed.

1555
01:53:22,000 --> 01:53:25,000
And Serada mentions that, you know, she made a bet at work.

1556
01:53:25,000 --> 01:53:36,000
Her company does neural nets, and she bet somebody, I guess, that, hey, they could do humor recognition, document recognition, if you will, just as well as this very complicated neural net.

1557
01:53:36,000 --> 01:53:38,000
And she won.

1558
01:53:38,000 --> 01:53:46,000
So, a lot of times the solutions we're trying to engineer do not require incredible amount of sophistication.

1559
01:53:46,000 --> 01:53:50,000
You often want to start simple, see if that is good enough.

1560
01:53:50,000 --> 01:53:55,000
We talked about a question like, how do I score things and determine is my model good enough?

1561
01:53:55,000 --> 01:54:02,000
Well, if you do something that's fast and dirty and it's good enough, you can stop there and you can spend your valuable time and effort on something more important.

1562
01:54:02,000 --> 01:54:04,000
All right.

1563
01:54:04,000 --> 01:54:09,000
So, naive Bayes classifiers, they rely upon a thing called the Bayes theorem.

1564
01:54:09,000 --> 01:54:16,000
And what Bayes theorem allows you to do is predict the probability that a label will apply to something if you're given a set of features.

1565
01:54:16,000 --> 01:54:19,000
So, the probability of a label given some features.

1566
01:54:19,000 --> 01:54:22,000
And that's where we're going to stop with the math.

1567
01:54:22,000 --> 01:54:29,000
If you want to dive into the math, I do have a link to my tutorial on statistics and probability.

1568
01:54:29,000 --> 01:54:31,000
It was in PyCon last year.

1569
01:54:31,000 --> 01:54:40,000
We talk about a wide variety of things related to statistics and probability, but we do mention naive Bayes, and we go through an example of how you can use naive Bayes.

1570
01:54:40,000 --> 01:54:43,000
All right.

1571
01:54:43,000 --> 01:54:49,000
There are about four different naive Bayes classifiers in Scikit-learn.

1572
01:54:49,000 --> 01:54:55,000
They use the word naive because they make certain assumptions about the data.

1573
01:54:55,000 --> 01:55:02,000
If your data doesn't fit within the confines of those assumptions, you're not going to get good results.

1574
01:55:02,000 --> 01:55:08,000
If they do fit nicely in the confines of those assumptions, then everything's going to work out fine.

1575
01:55:08,000 --> 01:55:16,000
Each of these different classifiers has different assumptions.

1576
01:55:16,000 --> 01:55:18,000
We'll look at Gaussian.

1577
01:55:18,000 --> 01:55:26,000
It's going to presume that the likelihood of any given feature follows a Gaussian curve.

1578
01:55:26,000 --> 01:55:30,000
And for those who are not into statistics, you may go, what the heck is a Gaussian curve?

1579
01:55:30,000 --> 01:55:32,000
You might have heard a bell curve.

1580
01:55:32,000 --> 01:55:35,000
Your grades fit on a nice bell curve in school.

1581
01:55:35,000 --> 01:55:38,000
A Gaussian curve is essentially a bell curve.

1582
01:55:38,000 --> 01:55:40,000
Most frequent things will fall right in the middle.

1583
01:55:40,000 --> 01:55:44,000
Less frequent things will fall out on the outskirts of the curve.

1584
01:55:44,000 --> 01:55:55,000
And so if your data fits that kind of a profile, like I might have lots of things that fit in the middle, but certain characteristics of my data will be fairly rare.

1585
01:55:55,000 --> 01:55:59,000
If you have that parameters for your data, Gaussian may be a good fit for you.

1586
01:55:59,000 --> 01:56:00,000
All right.

1587
01:56:00,000 --> 01:56:02,000
So let's go through the process.

1588
01:56:02,000 --> 01:56:03,000
All right.

1589
01:56:03,000 --> 01:56:04,000
So we import the stuff.

1590
01:56:04,000 --> 01:56:07,000
I'm going to read in this bananas data set.

1591
01:56:07,000 --> 01:56:10,000
We'll look at the first couple of rows.

1592
01:56:10,000 --> 01:56:16,000
We see that the rows are provided with lengths, widths, and a category.

1593
01:56:16,000 --> 01:56:19,000
We're looking at two types of bananas.

1594
01:56:19,000 --> 01:56:22,000
There is the Cavendish banana and the apple banana.

1595
01:56:22,000 --> 01:56:25,000
Apple bananas are very common and popular in Hawaii.

1596
01:56:25,000 --> 01:56:30,000
The Cavendish is the one you folks buy.

1597
01:56:30,000 --> 01:56:36,000
Average length for Cavendish is going to be in some range.

1598
01:56:36,000 --> 01:56:38,000
Apple bananas will be in a different range.

1599
01:56:38,000 --> 01:56:43,000
I think the 190s is probably the apple bananas and the Cavendish is around the 200 somethings.

1600
01:56:43,000 --> 01:56:45,000
By the way, this is all fake data too.

1601
01:56:45,000 --> 01:56:47,000
I used a random algorithm to generate this data.

1602
01:56:47,000 --> 01:56:48,000
It's close.

1603
01:56:48,000 --> 01:56:54,000
I looked up how long an average apple banana is and I looked up how long an average Cavendish banana is.

1604
01:56:54,000 --> 01:56:59,000
Banana science is an amazing thing.

1605
01:56:59,000 --> 01:57:01,000
Absolutely amazing thing.

1606
01:57:01,000 --> 01:57:08,000
There's actually a Freakonomics podcast all about bananas and the Cavendish banana in particular.

1607
01:57:08,000 --> 01:57:16,000
If fruits are your jam, you should go after that.

1608
01:57:16,000 --> 01:57:17,000
It's good.

1609
01:57:17,000 --> 01:57:18,000
All right.

1610
01:57:18,000 --> 01:57:20,000
I'm going to make my features matrix.

1611
01:57:21,000 --> 01:57:27,000
I'm going to grab two columns out of the data frame because I want to consider both the length of the banana

1612
01:57:27,000 --> 01:57:31,000
and the diameter of the banana or the width of the banana.

1613
01:57:31,000 --> 01:57:36,000
This is going to be a two dimensional matrix.

1614
01:57:36,000 --> 01:57:42,000
The categories for the bananas, whether they are apple or Cavendish, that is just an array.

1615
01:57:42,000 --> 01:57:44,000
That worked out good.

1616
01:57:44,000 --> 01:57:45,000
Let's look at our data real quick.

1617
01:57:45,000 --> 01:57:47,000
Tick.

1618
01:57:47,000 --> 01:57:49,000
Talk.

1619
01:57:49,000 --> 01:57:50,000
Okay.

1620
01:57:50,000 --> 01:57:57,000
I've pulled out these guys and I've pulled out the categories, zeros and ones.

1621
01:57:57,000 --> 01:58:02,000
We're going to do the train test split, test size.

1622
01:58:02,000 --> 01:58:05,000
I'm using the same numbers I always use.

1623
01:58:05,000 --> 01:58:14,000
We do have a place where you guys will do some modifications to the code later and you'll see what some of the changes might yield in a couple of instances.

1624
01:58:14,000 --> 01:58:16,000
Let's see what we got there.

1625
01:58:16,000 --> 01:58:18,000
A couple of thoughts here.

1626
01:58:18,000 --> 01:58:25,000
I'm actually going to cycle through each of these outputs, X train, X test, Y train and Y test.

1627
01:58:25,000 --> 01:58:32,000
I'm going to look at just the first two rows of my training data, my test data.

1628
01:58:32,000 --> 01:58:37,000
I'm going to look at the size of that matrix.

1629
01:58:37,000 --> 01:58:42,000
There were 67 records out of 100 related to my training.

1630
01:58:42,000 --> 01:58:46,000
There's 33 records out of 100 related to testing.

1631
01:58:46,000 --> 01:58:48,000
Okay.

1632
01:58:48,000 --> 01:58:50,000
All right.

1633
01:58:50,000 --> 01:58:52,000
Let's map these on a scatter plot.

1634
01:58:52,000 --> 01:58:56,000
All right.

1635
01:58:56,000 --> 01:59:02,000
Everything in the upper left column part here is all blue.

1636
01:59:02,000 --> 01:59:04,000
That will be one type of banana.

1637
01:59:04,000 --> 01:59:07,000
Everything down here at the bottom, that's going to be all red.

1638
01:59:07,000 --> 01:59:10,000
It's a different kind of banana.

1639
01:59:10,000 --> 01:59:16,000
Cavendish, I believe, is category one and it's red because it's a really long banana.

1640
01:59:16,000 --> 01:59:21,000
Apple bananas are short and kind of fat.

1641
01:59:21,000 --> 01:59:23,000
They do taste a little like apples.

1642
01:59:23,000 --> 01:59:25,000
It's creepy.

1643
01:59:25,000 --> 01:59:27,000
Okay.

1644
01:59:27,000 --> 01:59:35,000
In the next plot, I'm going to plot just the test points and we're going to make them a little faded in color using alpha.

1645
01:59:35,000 --> 01:59:36,000
These are all bananas.

1646
01:59:36,000 --> 01:59:38,000
I don't yet know what they are.

1647
01:59:38,000 --> 01:59:40,000
I'm not sure if they are Cavendish or apple.

1648
01:59:40,000 --> 01:59:42,000
That's why we're doing this.

1649
01:59:42,000 --> 01:59:43,000
All right.

1650
01:59:43,000 --> 01:59:46,000
So I want to try and figure this out.

1651
01:59:46,000 --> 01:59:57,000
We're importing Gaussian naive Bayes and we're going to choose the appropriate hyperparameters.

1652
01:59:57,000 --> 01:59:59,000
You'll see that it only has two.

1653
01:59:59,000 --> 02:00:06,000
It has priors and it has variance smoothing.

1654
02:00:06,000 --> 02:00:13,000
It's set to this ridiculously, I can never do scientific notation, ridiculously small number.

1655
02:00:13,000 --> 02:00:15,000
All right.

1656
02:00:15,000 --> 02:00:18,000
So if you want to know more about that, you can read about that.

1657
02:00:18,000 --> 02:00:20,000
Okay.

1658
02:00:20,000 --> 02:00:27,000
But again, easy to apply, almost no hyperparameters, fast and dirty.

1659
02:00:27,000 --> 02:00:34,000
I've now fit my model and I'm going to now use my prediction capabilities to say,

1660
02:00:34,000 --> 02:00:38,000
here's those values that I don't yet have a category for.

1661
02:00:38,000 --> 02:00:41,000
Let's figure out what the categories are.

1662
02:00:41,000 --> 02:00:46,000
I had 33 that didn't have categories, so we should get 33 out, which we did.

1663
02:00:46,000 --> 02:00:48,000
Here's the first few.

1664
02:00:48,000 --> 02:00:51,000
And so it shows a bunch of zeros and ones.

1665
02:00:51,000 --> 02:00:53,000
It's all good.

1666
02:00:53,000 --> 02:00:57,000
Let's go see what we get.

1667
02:00:57,000 --> 02:01:01,000
I'm going to plot the training data that we fed into the model.

1668
02:01:01,000 --> 02:01:09,000
I'm going to plot the test data and the predictions that we got out of the model on the same graph.

1669
02:01:09,000 --> 02:01:11,000
And there you go.

1670
02:01:11,000 --> 02:01:14,000
And so everything that's pretty pale, we see it also showed up as blue

1671
02:01:14,000 --> 02:01:17,000
and everything that's pretty pale also showed up as red.

1672
02:01:17,000 --> 02:01:20,000
So this naive Bayes classifier was successfully able to identify

1673
02:01:20,000 --> 02:01:24,000
all of these things should be in this category versus that category.

1674
02:01:24,000 --> 02:01:27,000
There you go.

1675
02:01:27,000 --> 02:01:29,000
Nice.

1676
02:01:29,000 --> 02:01:31,000
All right.

1677
02:01:31,000 --> 02:01:37,000
So let me tell you about some of the gotchas that I had when I was going down this road.

1678
02:01:37,000 --> 02:01:39,000
I remember this distinctly.

1679
02:01:39,000 --> 02:01:42,000
There was a lot of crying around this as well.

1680
02:01:42,000 --> 02:01:45,000
Naming conventions.

1681
02:01:45,000 --> 02:01:49,000
I got into a lot of trouble with my naming conventions.

1682
02:01:49,000 --> 02:01:54,000
I spent a great deal of time earlier, I think, probably when we were in one of the earlier conversations

1683
02:01:54,000 --> 02:01:58,000
where I talked about some people will call this y, some people call it x,

1684
02:01:58,000 --> 02:02:04,000
some people call it capital X, capital Y, some people call it independent variables, et cetera.

1685
02:02:04,000 --> 02:02:11,000
In my head, when I was looking at this, I was thinking, hey, these are all x values.

1686
02:02:11,000 --> 02:02:15,000
These are all y values because that's how you plot stuff when you're in high school.

1687
02:02:15,000 --> 02:02:19,000
You have an x, y coordinate system and you plot x's and y's.

1688
02:02:19,000 --> 02:02:26,000
And so I'm picturing in Chalmers head x, y coordinates.

1689
02:02:26,000 --> 02:02:37,000
But the x values are the feature matrix and the x values actually were the length of the banana

1690
02:02:37,000 --> 02:02:40,000
and the width of the banana.

1691
02:02:40,000 --> 02:02:46,000
That x features matrix actually had, quote unquote, the x and y coordinates in it.

1692
02:02:46,000 --> 02:02:49,000
And so everything deteriorated from there.

1693
02:02:49,000 --> 02:02:53,000
And so I'm trying to figure out how I get my x and y coordinates into here and here

1694
02:02:53,000 --> 02:02:58,000
when really my x, y coordinates were this thing and my y inputs were actual labels.

1695
02:02:58,000 --> 02:03:00,000
They were categories.

1696
02:03:00,000 --> 02:03:02,000
This is banana A, banana B.

1697
02:03:02,000 --> 02:03:06,000
And so that really messed things up for me a lot.

1698
02:03:06,000 --> 02:03:11,000
If instead I had approached this with, oh, well, here's a latitude and a longitude,

1699
02:03:11,000 --> 02:03:16,000
a price and quantity sold or passing yards and the number of wins for a sports team,

1700
02:03:16,000 --> 02:03:20,000
I could picture those as being graphed on the graph.

1701
02:03:20,000 --> 02:03:25,000
And I would not have confused either of these with x's or y's the way statisticians

1702
02:03:25,000 --> 02:03:27,000
or mathematicians use.

1703
02:03:27,000 --> 02:03:32,000
So sometimes as you go down this road, you might end up kind of messing yourself up

1704
02:03:32,000 --> 02:03:37,000
because your algebra teacher or whatever got certain things stuck in your head.

1705
02:03:37,000 --> 02:03:40,000
So don't let that scare you.

1706
02:03:40,000 --> 02:03:42,000
It happens to most of us.

1707
02:03:42,000 --> 02:03:45,000
All right.

1708
02:03:45,000 --> 02:03:50,000
We keep drawing a bunch of graphs and we do a lot of data handling and stuff.

1709
02:03:50,000 --> 02:03:54,000
And I don't want that to distract you from the machine learning.

1710
02:03:54,000 --> 02:03:59,000
We have the database steps to help us kind of confirm that the machine learning is working

1711
02:03:59,000 --> 02:04:02,000
or to see what the data looks like before it's going in.

1712
02:04:02,000 --> 02:04:10,000
And we, by default, have to interweave that with our process and our scripts.

1713
02:04:10,000 --> 02:04:16,000
But quite literally, these four lines are the machine learning.

1714
02:04:16,000 --> 02:04:20,000
All that other junk on either end is just getting the data ready to go in

1715
02:04:20,000 --> 02:04:22,000
or looking at the data that came out.

1716
02:04:22,000 --> 02:04:25,000
Those four lines are your machine learning and you're done.

1717
02:04:25,000 --> 02:04:27,000
We can all go home, right?

1718
02:04:27,000 --> 02:04:34,000
So don't let the fact that there's 28 lines of code here distract you from this part is the machine learning.

1719
02:04:34,000 --> 02:04:38,000
And as you are doing your machine learning studies, this is what you need to focus on.

1720
02:04:38,000 --> 02:04:42,000
Now, when you're doing your data viz studies, there's a different part of our code that you look at.

1721
02:04:42,000 --> 02:04:45,000
When you're doing your data prep or your data engineering, there's a different part of the code to look at.

1722
02:04:45,000 --> 02:04:48,000
But that part, those four lines are the machine learning.

1723
02:04:48,000 --> 02:04:50,000
All right.

1724
02:04:50,000 --> 02:04:56,000
Another thing, another kind of gotcha, sometimes starting too big is too confusing.

1725
02:04:56,000 --> 02:05:05,000
In some of my early forays into this, I'm like, oh, I have this really great 12 gig data file that I downloaded off the internets.

1726
02:05:05,000 --> 02:05:08,000
And I'm going to solve all the machine learnings.

1727
02:05:08,000 --> 02:05:09,000
Yeah.

1728
02:05:09,000 --> 02:05:13,000
You have no idea what that data contains and you have no idea what the relationships might be.

1729
02:05:13,000 --> 02:05:17,000
And you've got 89 different variables and some of them are important, some of them are not.

1730
02:05:17,000 --> 02:05:19,000
You don't even know where that's...

1731
02:05:19,000 --> 02:05:22,000
Start small.

1732
02:05:22,000 --> 02:05:33,000
For example, if I go back and look at my bananas and I look at row 83, I would see that the length of that banana is 187 and the width of that banana is 36.

1733
02:05:33,000 --> 02:05:38,000
And that banana happened to be category zero, which is an apple banana.

1734
02:05:38,000 --> 02:05:43,000
You can actually predict a single value at a time.

1735
02:05:43,000 --> 02:05:51,000
And so if I took something real close to 187 and real close to 36, let's go see.

1736
02:05:51,000 --> 02:05:59,000
If I took something close to 187 and close to 36, that should fit nicely in this blue zone and I should get a blue categorization out of that.

1737
02:05:59,000 --> 02:06:02,000
So let's go try it.

1738
02:06:02,000 --> 02:06:06,000
Let's put one single prediction in here.

1739
02:06:06,000 --> 02:06:10,000
187 to 36 and see if we get back a category of zero.

1740
02:06:10,000 --> 02:06:13,000
And sure enough, we do.

1741
02:06:13,000 --> 02:06:15,000
OK, let's let's up the ante a bit.

1742
02:06:15,000 --> 02:06:17,000
Let's put two items in to predict.

1743
02:06:17,000 --> 02:06:23,000
This one is clearly in the long skinny banana range, which is down there, the cabin dishes.

1744
02:06:23,000 --> 02:06:28,000
This one is pretty close to the fat apple banana categories.

1745
02:06:28,000 --> 02:06:32,000
I pulled those two rough estimates out of my data.

1746
02:06:32,000 --> 02:06:34,000
This is row 28, row 93.

1747
02:06:34,000 --> 02:06:37,000
You notice I kind of rounded them a little bit.

1748
02:06:37,000 --> 02:06:40,000
And sure enough, I should get back a one and a zero.

1749
02:06:40,000 --> 02:06:42,000
And I do.

1750
02:06:42,000 --> 02:06:48,000
So feel free to kind of predict a thing or two at a time to help you wrap your head around what's going on behind the scenes.

1751
02:06:48,000 --> 02:06:51,000
What is this really doing?

1752
02:06:51,000 --> 02:06:54,000
Check your data. Do a sanity check.

1753
02:06:54,000 --> 02:07:05,000
If this had come up with a zero and a one or come up with something like a five and a six, then you know something's gone crazy wrong and you want to figure out what's going on.

1754
02:07:05,000 --> 02:07:07,000
So sanity checks. Pretty good.

1755
02:07:07,000 --> 02:07:10,000
All right.

1756
02:07:10,000 --> 02:07:13,000
So how do we learn more?

1757
02:07:13,000 --> 02:07:20,000
You saw me kind of poke and play just a little bit with the prediction process.

1758
02:07:20,000 --> 02:07:23,000
That quite literally is how you learn any programming concept.

1759
02:07:23,000 --> 02:07:30,000
You change stuff and you see what happens.

1760
02:07:30,000 --> 02:07:33,000
Notice this is in the essential O'Reilly series.

1761
02:07:33,000 --> 02:07:35,000
This is essential for your learning process.

1762
02:07:35,000 --> 02:07:40,000
Change stuff and see what happens.

1763
02:07:40,000 --> 02:07:48,000
Even though I just mentioned, hey, those four lines are the machine learning, all that other stuff is the data handling or the data vis.

1764
02:07:48,000 --> 02:07:53,000
Get crazy familiar with your favorite graphing library.

1765
02:07:53,000 --> 02:08:01,000
You being able to visualize your results is going to help you understand if your model is doing what you want it to do.

1766
02:08:01,000 --> 02:08:07,000
So I hate to tell you, hey, you can learn machine machine learning, et cetera, but you really need to learn graphing too.

1767
02:08:07,000 --> 02:08:09,000
Right. Now it's two things you got to go home and study.

1768
02:08:09,000 --> 02:08:19,000
But quite literally, you being able to see results in some sort of visualization is going to help and is going to help you better succeed at the goal of data science.

1769
02:08:19,000 --> 02:08:24,000
The goal of data science is meant to inform and thus enable action.

1770
02:08:24,000 --> 02:08:31,000
And most people are not going to understand the math and all the craziness that you spew out at them.

1771
02:08:31,000 --> 02:08:35,000
They're going to want a pretty picture or a brief description.

1772
02:08:35,000 --> 02:08:48,000
And so if you have a good handle on your graphing library, you can provide them with a well tuned display of what the meaning of the data is so they can choose to take action.

1773
02:08:48,000 --> 02:08:51,000
Next thing, read the docs.

1774
02:08:51,000 --> 02:08:56,000
When I first started getting into this, I didn't like reading the docs, not even a little bit.

1775
02:08:56,000 --> 02:09:03,000
And I'll say I love math. But even for me, right, my eyes would glaze over when I say row after row of equations come across.

1776
02:09:03,000 --> 02:09:09,000
But as I mentioned before, the more you read the docs, the more the terminology will come to you.

1777
02:09:09,000 --> 02:09:15,000
We'll talk about this in a bit, but don't just copy paste other people's models.

1778
02:09:15,000 --> 02:09:20,000
You'll find a tutorial and have a model and you copy paste it into a notebook and it runs and it works.

1779
02:09:20,000 --> 02:09:24,000
Good. Don't do that.

1780
02:09:24,000 --> 02:09:33,000
Either create find a data set and use a model that you might find a trial to look at your own data set.

1781
02:09:33,000 --> 02:09:36,000
Put in some test values and play with that.

1782
02:09:36,000 --> 02:09:38,000
See if it predicts properly.

1783
02:09:38,000 --> 02:09:48,000
There's another technique we'll walk through in a little bit that can be really useful to kind of learning how the models are working behind the scenes when you're looking at other people's content.

1784
02:09:48,000 --> 02:09:52,000
All right. So we'll pause there for a sec.

1785
02:09:52,000 --> 02:09:57,000
Covered a lot. What questions you got? Yes.

1786
02:09:57,000 --> 02:10:07,000
Does SKLearn let you use categories as labels for your features rather than numbers which are really categories?

1787
02:10:07,000 --> 02:10:12,000
Zero and one really mean Cavendish and Apple or something like that.

1788
02:10:12,000 --> 02:10:19,000
Yeah. Does it does SKLearn or does Scikit-learn allow you to actually use say a label use the word Cavendish instead of zero?

1789
02:10:19,000 --> 02:10:24,000
I do not believe that it does.

1790
02:10:24,000 --> 02:10:27,000
And I have a head nod or not a head nod a head shake in the back.

1791
02:10:27,000 --> 02:10:28,000
I don't believe it does.

1792
02:10:28,000 --> 02:10:34,000
They have actual libraries or methods built into Scikit-learn that will do the opposite.

1793
02:10:34,000 --> 02:10:42,000
You handed a bunch of Cavendish and Apples and it'll immediately convert them into ones and zeros for you in an automatic fashion.

1794
02:10:42,000 --> 02:10:54,000
The other kind of nuance here and why that might not be a good a good path to go down is that typically strings will take up a lot more space in memory than a simple one or two or three or four.

1795
02:10:54,000 --> 02:10:56,000
You know, small integers take up less space in memory.

1796
02:10:56,000 --> 02:11:11,000
So if you're operating on extremely large data sets and you feel like you've entered in some sort of memory constrained zone, having things reduced down to the smallest amount of memory can sometimes be useful, might speed up processes, etc.

1797
02:11:11,000 --> 02:11:16,000
So it depends on the amount of data you're using and the constraints of your system.

1798
02:11:16,000 --> 02:11:21,000
I've been stuck on systems where we had small systems, we had lots of data and we were just stuck.

1799
02:11:21,000 --> 02:11:24,000
So we had to look at different ways to handle things.

1800
02:11:24,000 --> 02:11:27,000
All right. What other questions?

1801
02:11:27,000 --> 02:11:29,000
What you got?

1802
02:11:29,000 --> 02:11:33,000
Sure.

1803
02:12:00,000 --> 02:12:04,000
Give me a second.

1804
02:12:04,000 --> 02:12:09,000
Shucks.

1805
02:12:09,000 --> 02:12:13,000
It's not the one I wanted, but we'll get there.

1806
02:12:13,000 --> 02:12:32,000
So for example, for some of the naive Bayes, they have techniques to score the accuracy.

1807
02:12:32,000 --> 02:12:39,000
If you already know the labels and you have the given test data, they have techniques to score some of these.

1808
02:12:39,000 --> 02:12:41,000
They got the score method, etc.

1809
02:12:41,000 --> 02:12:49,000
If certain samples have different sample weights, they're strongly categorized in a particular way, you might be able to influence that in various ways.

1810
02:12:49,000 --> 02:13:00,000
But there are some warnings in some of these cases where they will tell you that some of these things do not.

1811
02:13:00,000 --> 02:13:10,000
I can't remember where I read it, but one of them mentioned particularly that the predict probability aspect of one of these was not good.

1812
02:13:10,000 --> 02:13:16,000
It was great for classifying, but predicting the probability of that thing being a thing was a weakness.

1813
02:13:16,000 --> 02:13:27,000
So what you have to kind of do is just go through each of the different models and get a sense of what's going on with that particular model and really dive into it and figure out, is this model the right one for me?

1814
02:13:27,000 --> 02:13:38,000
But yeah, I mean, just this morning I was reading about something where it mentioned that the prediction capability was a little weird, but the classifying probability was really high.

1815
02:13:38,000 --> 02:13:46,000
And I think the folks who wrote the documentation even said something to the effect of, this is great for classifying, but don't ever rely on the prediction capabilities.

1816
02:13:46,000 --> 02:13:50,000
I'm like, what? That sounds so bizarre, but that's just the way it was.

1817
02:13:50,000 --> 02:13:54,000
What other questions? Anything?

1818
02:13:54,000 --> 02:13:58,000
Okay. All right.

1819
02:13:58,000 --> 02:14:05,000
And I totally wish we had time to dive into a whole section on scoring, but we did not.

1820
02:14:05,000 --> 02:14:07,000
Okay.

1821
02:14:07,000 --> 02:14:10,000
So next, I'm going to cut you loose.

1822
02:14:10,000 --> 02:14:13,000
You're going to read a little bit about clustering.

1823
02:14:13,000 --> 02:14:19,000
This will be our first foray into an unsupervised learning model.

1824
02:14:19,000 --> 02:14:21,000
We will not have a classification for any of these things.

1825
02:14:21,000 --> 02:14:28,000
We're going to ask the model to help kind of group and lump things, but we don't have a known answer for any of these.

1826
02:14:28,000 --> 02:14:30,000
So got two different links.

1827
02:14:30,000 --> 02:14:34,000
Each link has a question associated with that link.

1828
02:14:34,000 --> 02:14:39,000
So take a couple minutes and see what we get.

1829
02:14:39,000 --> 02:14:44,000
I'm going to turn off the thing here and I'll walk around.

1830
02:15:00,000 --> 02:15:04,000
Okay.

1831
02:15:30,000 --> 02:15:34,000
Okay.

1832
02:16:00,000 --> 02:16:04,000
Okay.

1833
02:16:30,000 --> 02:16:34,000
Okay.

1834
02:17:00,000 --> 02:17:04,000
Okay.

1835
02:17:04,000 --> 02:17:08,000
Okay.

1836
02:17:08,000 --> 02:17:12,000
Okay.

1837
02:17:12,000 --> 02:17:16,000
Okay.

1838
02:17:16,000 --> 02:17:20,000
Okay.

1839
02:17:20,000 --> 02:17:24,000
Okay.

1840
02:17:24,000 --> 02:17:28,000
Okay.

1841
02:17:28,000 --> 02:17:32,000
Okay.

1842
02:17:32,000 --> 02:17:36,000
Okay.

1843
02:17:36,000 --> 02:17:40,000
Okay.

1844
02:17:40,000 --> 02:17:44,000
Check, check.

1845
02:17:44,000 --> 02:17:48,000
Check, check.

1846
02:17:48,000 --> 02:17:52,000
Check, check.

1847
02:17:52,000 --> 02:17:56,000
Check, check.

1848
02:17:56,000 --> 02:18:00,000
Check, check.

1849
02:18:00,000 --> 02:18:04,000
Check, check.

1850
02:18:04,000 --> 02:18:08,000
Check, check.

1851
02:18:08,000 --> 02:18:12,000
Check, check.

1852
02:18:12,000 --> 02:18:16,000
Check, check.

1853
02:18:16,000 --> 02:18:20,000
Check, check.

1854
02:18:20,000 --> 02:18:24,000
Check, check.

1855
02:18:24,000 --> 02:18:28,000
Check, check.

1856
02:18:28,000 --> 02:18:32,000
Check, check.

1857
02:18:32,000 --> 02:18:34,000
So we are about to go live.

1858
02:18:34,000 --> 02:18:38,000
Three, two, one.

1859
02:18:38,000 --> 02:18:40,000
So tell me.

1860
02:18:40,000 --> 02:18:46,000
There were four cases where K-means clustering might struggle to produce some accurate clusters.

1861
02:18:46,000 --> 02:18:50,000
Can anybody describe to me some of those characteristics?

1862
02:18:50,000 --> 02:18:56,000
Say that again.

1863
02:18:56,000 --> 02:19:02,000
So if you give the wrong number of clusters.

1864
02:19:02,000 --> 02:19:06,000
Okay.

1865
02:19:06,000 --> 02:19:10,000
Too many what?

1866
02:19:10,000 --> 02:19:14,000
Too many dimensions.

1867
02:19:14,000 --> 02:19:18,000
Where is?

1868
02:19:18,000 --> 02:19:22,000
Shucks.

1869
02:19:22,000 --> 02:19:26,000
I lost my link, folks.

1870
02:19:26,000 --> 02:19:30,000
Documentation. Go to the API.

1871
02:19:30,000 --> 02:19:34,000
There we go.

1872
02:19:34,000 --> 02:19:38,000
There's always control F.

1873
02:19:38,000 --> 02:19:42,000
My battery is about to die, right?

1874
02:19:42,000 --> 02:19:46,000
All right. How bad is the battery?

1875
02:19:46,000 --> 02:19:50,000
Look at that. Five percent. Sweet.

1876
02:19:50,000 --> 02:19:54,000
Okay.

1877
02:19:54,000 --> 02:19:58,000
User guide. There's that bit.

1878
02:19:58,000 --> 02:20:02,000
Okay.

1879
02:20:02,000 --> 02:20:06,000
So here's a case, right?

1880
02:20:06,000 --> 02:20:10,000
We had a bunch of blobs. We got the wrong number of

1881
02:20:10,000 --> 02:20:14,000
clusters associated with this thing.

1882
02:20:14,000 --> 02:20:18,000
We've got data that is not nice round

1883
02:20:18,000 --> 02:20:22,000
clusters. They're actually kind of these arrays

1884
02:20:22,000 --> 02:20:26,000
of, I don't know what you call them.

1885
02:20:26,000 --> 02:20:30,000
They're lines of data points, but they're not round. We've got

1886
02:20:30,000 --> 02:20:34,000
some data points here where the variance or how far

1887
02:20:34,000 --> 02:20:38,000
they spread away from kind of the center varies. And then we've got

1888
02:20:38,000 --> 02:20:42,000
some data sets that are unevenly sized. And so that might

1889
02:20:42,000 --> 02:20:46,000
impact certain things. So as you're looking at, should I use K-means

1890
02:20:46,000 --> 02:20:50,000
clustering here or should I use a different clustering algorithm? Some of these things

1891
02:20:50,000 --> 02:20:54,000
will come up. All right. Next.

1892
02:20:54,000 --> 02:20:58,000
For the

1893
02:20:58,000 --> 02:21:02,000
K-means API page.

1894
02:21:02,000 --> 02:21:06,000
What answer did you folks get there while I plug in my laptop?

1895
02:21:06,000 --> 02:21:10,000
Well, what

1896
02:21:10,000 --> 02:21:14,000
answer did we get? Ten.

1897
02:21:14,000 --> 02:21:18,000
All right. And we'll talk about what that ten means.

1898
02:21:18,000 --> 02:21:22,000
There we go. In just a second.

1899
02:21:22,000 --> 02:21:26,000
All right. And we'll talk about why

1900
02:21:26,000 --> 02:21:30,000
that ten is there.

1901
02:21:30,000 --> 02:21:34,000
So, naive base.

1902
02:21:34,000 --> 02:21:38,000
Okay. We have electricity.

1903
02:21:38,000 --> 02:21:42,000
It's all good.

1904
02:21:42,000 --> 02:21:46,000
So keep that ten in mind because we'll come back to it.

1905
02:21:46,000 --> 02:21:50,000
Wait. We're not doing naive base. What are we doing?

1906
02:21:50,000 --> 02:21:54,000
Are we doing the next one? K-means.

1907
02:21:54,000 --> 02:21:58,000
There we go. Somebody's got to keep me on track.

1908
02:21:58,000 --> 02:22:02,000
Okay.

1909
02:22:02,000 --> 02:22:06,000
There we go.

1910
02:22:06,000 --> 02:22:10,000
So the goal of these clustering algorithms, right, is to

1911
02:22:10,000 --> 02:22:14,000
find data points. If they're similar, put them together in the same group. If

1912
02:22:14,000 --> 02:22:18,000
they're dissimilar, put them into different groups. Part of the

1913
02:22:18,000 --> 02:22:22,000
reason they're crazy popular is that they're unsupervised. You don't have to have predetermined

1914
02:22:22,000 --> 02:22:26,000
labels. So that simplifies certain things. It sometimes

1915
02:22:26,000 --> 02:22:30,000
makes it more difficult to figure out did this really work?

1916
02:22:30,000 --> 02:22:34,000
They can accommodate multi-dimensional data sets. So if you have lots of columns,

1917
02:22:34,000 --> 02:22:38,000
you can use these sometimes very successfully.

1918
02:22:38,000 --> 02:22:42,000
And for simple cases, especially if you can put it on a two-dimensional,

1919
02:22:42,000 --> 02:22:46,000
three-dimensional chart, they can be fairly easy to interpret. You can kind of

1920
02:22:46,000 --> 02:22:50,000
look at things and say, yeah, that makes sense or no, that doesn't make

1921
02:22:50,000 --> 02:22:54,000
sense. Let's see.

1922
02:22:54,000 --> 02:22:58,000
Like this one right here. We can look at the graph up in the upper

1923
02:22:58,000 --> 02:23:02,000
right-hand side and say, yeah, that maybe doesn't make much sense.

1924
02:23:02,000 --> 02:23:06,000
So some of these can be fairly easy to interpret if you can

1925
02:23:06,000 --> 02:23:10,000
visually display them well. We're going to focus on

1926
02:23:10,000 --> 02:23:14,000
the K-means clustering algorithm. This thing looks for an arithmetic mean,

1927
02:23:14,000 --> 02:23:18,000
looks for a mathematical average

1928
02:23:18,000 --> 02:23:22,000
of all the points in a particular cluster to identify

1929
02:23:22,000 --> 02:23:26,000
what they'll call the cluster center. So I got a bunch of dots and it says,

1930
02:23:26,000 --> 02:23:30,000
all right, the distance from each of the dots to this point takes

1931
02:23:30,000 --> 02:23:34,000
an average and pinpoints a cluster center.

1932
02:23:34,000 --> 02:23:38,000
Then what it does when it has these cluster centers, this is, all right,

1933
02:23:38,000 --> 02:23:42,000
which dots on my graph or which dots in my space are closest

1934
02:23:42,000 --> 02:23:46,000
to that one and which ones are closest to other centers and

1935
02:23:46,000 --> 02:23:50,000
it will then lump things together based on their affinity or their closeness

1936
02:23:50,000 --> 02:23:54,000
to the cluster centers that it determined.

1937
02:23:54,000 --> 02:23:58,000
We'll talk more in detail about what happens behind the scenes with that, but we'll

1938
02:23:58,000 --> 02:24:02,000
leave it at that for now. All right. K-means is just one way of

1939
02:24:02,000 --> 02:24:06,000
doing this clustering. They have things like affinity propagation,

1940
02:24:06,000 --> 02:24:10,000
dbScan, mean shift, etc. I'll leave it up to you guys to read about some of those.

1941
02:24:10,000 --> 02:24:14,000
Many of them work similarly, but some of them have nuances where

1942
02:24:14,000 --> 02:24:18,000
this applies better in this case than another.

1943
02:24:18,000 --> 02:24:22,000
All right. I'm going to go ahead and import train test split

1944
02:24:22,000 --> 02:24:26,000
right off the bat because we're going to do that, we know.

1945
02:24:26,000 --> 02:24:30,000
In this case, I'm going to show you how you can make your own data.

1946
02:24:30,000 --> 02:24:34,000
I mentioned, you know, sometimes you want to kind of play with this

1947
02:24:34,000 --> 02:24:38,000
and test things out. Well, if you could make your own data

1948
02:24:38,000 --> 02:24:42,000
and you could make it within certain confines or constraints,

1949
02:24:42,000 --> 02:24:46,000
then you have a known good data set, if you will.

1950
02:24:46,000 --> 02:24:50,000
SKLearn has data sets that are

1951
02:24:50,000 --> 02:24:54,000
downloaded with Scikit-learn when you get it. It also has data sets where they

1952
02:24:54,000 --> 02:24:58,000
have a small script here that you can use to download much larger

1953
02:24:58,000 --> 02:25:02,000
data sets, and then they also have the capability to make samples. And so we're

1954
02:25:02,000 --> 02:25:06,000
going to use makeBlobs in the samples generator module to make

1955
02:25:06,000 --> 02:25:10,000
some blobs of dots. All right.

1956
02:25:10,000 --> 02:25:14,000
The cool thing about their data set generator is it preformats your features into a

1957
02:25:14,000 --> 02:25:18,000
matrices and into target arrays.

1958
02:25:18,000 --> 02:25:22,000
What I'm going to get out of here is a matrix of X and Y vectors, X and Y dots

1959
02:25:22,000 --> 02:25:26,000
that can be plotted on a chart, and then I will get

1960
02:25:26,000 --> 02:25:30,000
an array of cluster labels. So let's fire this off.

1961
02:25:30,000 --> 02:25:34,000
In this case, I'm going to have a total

1962
02:25:34,000 --> 02:25:38,000
of four centers, four clusters.

1963
02:25:38,000 --> 02:25:42,000
We're going to have a standard deviation of 0.7,

1964
02:25:42,000 --> 02:25:46,000
and we're going to look later at what happens if we change that. And I chose for this case

1965
02:25:46,000 --> 02:25:50,000
to use a random state of 13, and so let's see what we get.

1966
02:25:52,000 --> 02:25:56,000
And so this thing has 400 samples, and

1967
02:25:56,000 --> 02:26:00,000
it has two features. Basically what's going to be an X and a Y

1968
02:26:00,000 --> 02:26:04,000
variable on a coordinate chart.

1969
02:26:04,000 --> 02:26:08,000
This is the first five records that were randomly generated.

1970
02:26:08,000 --> 02:26:12,000
Six and a nine, a two and a five.

1971
02:26:12,000 --> 02:26:16,000
If we were to plot this on a set of coordinates, these would all be a bunch of dots.

1972
02:26:16,000 --> 02:26:20,000
If I look at the Y,

1973
02:26:20,000 --> 02:26:24,000
if I look at just the first five,

1974
02:26:24,000 --> 02:26:28,000
I have three categories, but I had

1975
02:26:28,000 --> 02:26:32,000
four clusters. So I was a little confused, like, hey,

1976
02:26:32,000 --> 02:26:36,000
did I actually get four clusters? I only have three categories.

1977
02:26:36,000 --> 02:26:40,000
So I expanded that a little bit.

1978
02:26:40,000 --> 02:26:44,000
I only have three categories. That's pretty crazy. Let's go take a look at some more.

1979
02:26:44,000 --> 02:26:48,000
Ah, sure enough. And so I actually do

1980
02:26:48,000 --> 02:26:52,000
have four different categories to go with the four

1981
02:26:52,000 --> 02:26:56,000
cluster centers that I asked. If you've ever run into a snag

1982
02:26:56,000 --> 02:27:00,000
like this where maybe your data feels like it is not uniformly distributed in some way,

1983
02:27:00,000 --> 02:27:04,000
and you want to see what all your unique values are, numpy has a

1984
02:27:04,000 --> 02:27:08,000
numpy unique function to drop those in and it'll say, oh yeah, look Chalmer,

1985
02:27:08,000 --> 02:27:12,000
here's all your unique values, zero through three. And I can confirm that I did

1986
02:27:12,000 --> 02:27:16,000
actually get four different categories. We're going to

1987
02:27:16,000 --> 02:27:20,000
do a train test split. Notice I opted not to use

1988
02:27:20,000 --> 02:27:24,000
my own categorization for how big a split

1989
02:27:24,000 --> 02:27:28,000
to make. I'm going to use the default. Do you remember what the default

1990
02:27:28,000 --> 02:27:32,000
is for train test split?

1991
02:27:32,000 --> 02:27:36,000
.25

1992
02:27:36,000 --> 02:27:40,000
Test size default is .25. So one quarter of

1993
02:27:40,000 --> 02:27:44,000
my data will be segregated off for testing purposes. The other 75% will be

1994
02:27:44,000 --> 02:27:48,000
used for training.

1995
02:27:48,000 --> 02:27:52,000
Let's do that. And sure enough, it made four really awesome blobs.

1996
02:27:52,000 --> 02:27:56,000
A couple things to notice. The blobs are a little

1997
02:27:56,000 --> 02:28:00,000
egg shaped or oval shaped. They're not actually fully round.

1998
02:28:00,000 --> 02:28:04,000
If you were

1999
02:28:04,000 --> 02:28:08,000
to look at these, odds are

2000
02:28:08,000 --> 02:28:12,000
there's probably a little bit of a Gaussian kind of curve here. Certain dots

2001
02:28:12,000 --> 02:28:16,000
appear more frequently in the center and fewer dots appear on the edge.

2002
02:28:16,000 --> 02:28:20,000
So let's choose

2003
02:28:20,000 --> 02:28:24,000
the model. I created four blobs.

2004
02:28:24,000 --> 02:28:28,000
I'm going to tell my k-means model to look for four clusters.

2005
02:28:28,000 --> 02:28:32,000
We'll see what we get. Notice

2006
02:28:32,000 --> 02:28:36,000
these are all of the hyperparameters

2007
02:28:36,000 --> 02:28:40,000
that I could set. We're not going to go into what all these mean. We'll talk about

2008
02:28:40,000 --> 02:28:44,000
in and in later. That's that 10 that you folks looked up.

2009
02:28:44,000 --> 02:28:48,000
When we do talk in more detail

2010
02:28:48,000 --> 02:28:52,000
about what happens behind the scenes, more of these will start to make more sense.

2011
02:28:52,000 --> 02:28:56,000
Alright. Let's do our fit. Let's train it on the data that we already have.

2012
02:28:56,000 --> 02:29:00,000
Okay. And

2013
02:29:00,000 --> 02:29:04,000
let's do a prediction. Let's look at the shape of the prediction.

2014
02:29:04,000 --> 02:29:08,000
It predicted a hundred categories for us.

2015
02:29:08,000 --> 02:29:12,000
Hopefully the categories will work out well.

2016
02:29:12,000 --> 02:29:16,000
Again, looking at just

2017
02:29:16,000 --> 02:29:20,000
five maybe doesn't show all the categories. Ah, okay.

2018
02:29:20,000 --> 02:29:24,000
It did come up with four categories. That's good. Let's do the

2019
02:29:24,000 --> 02:29:28,000
scatter plot. So out of the hundred data points

2020
02:29:28,000 --> 02:29:32,000
that I handed it to test, it was able to

2021
02:29:32,000 --> 02:29:36,000
segregate these hundred data points into four different categories.

2022
02:29:36,000 --> 02:29:40,000
And I colored each of the categories based on whether it was a 0, 1, 2, 3, or 4.

2023
02:29:40,000 --> 02:29:44,000
Alright. So that part works.

2024
02:29:44,000 --> 02:29:48,000
So I

2025
02:29:48,000 --> 02:29:52,000
mentioned that it does this calculation of the big

2026
02:29:52,000 --> 02:29:56,000
data set. It tries to find a center.

2027
02:29:56,000 --> 02:30:00,000
Theoretically, we should be able to get those centers back. We should be able to look at them.

2028
02:30:00,000 --> 02:30:04,000
And sure enough, you can. Notice that this

2029
02:30:04,000 --> 02:30:08,000
attribute ends in an underscore. You're going to see several attributes

2030
02:30:08,000 --> 02:30:12,000
in scikit-learn that end in underscores. And there's a purpose for that. Anytime

2031
02:30:12,000 --> 02:30:16,000
you see an attribute that has an underscore at the end of it, it means

2032
02:30:16,000 --> 02:30:20,000
that was generated by the model. It was a product or an output of the model.

2033
02:30:20,000 --> 02:30:24,000
If you have an attribute that does not have an underscore, it means that

2034
02:30:24,000 --> 02:30:28,000
was an input to the model. So let's go see one of the outputs. Let's see

2035
02:30:28,000 --> 02:30:32,000
what the center cluster, or the cluster centers were. These were four points that

2036
02:30:32,000 --> 02:30:36,000
represent the middles of each of those clusters.

2037
02:30:36,000 --> 02:30:40,000
So I'm going to grab those four values

2038
02:30:40,000 --> 02:30:44,000
and we're going to feed them into a new map.

2039
02:30:44,000 --> 02:30:48,000
This will be all of our data

2040
02:30:48,000 --> 02:30:52,000
that we just looked at a second ago. But now I'm going to overlay on that another scatter plot with just

2041
02:30:52,000 --> 02:30:56,000
those four centers. And I'm going to color them white. I'll make the edges black.

2042
02:30:56,000 --> 02:31:00,000
And I'm going to make them a little bit bigger.

2043
02:31:00,000 --> 02:31:04,000
And so for each one of my clusters, these black circles

2044
02:31:04,000 --> 02:31:08,000
are all the places where the centers are located.

2045
02:31:08,000 --> 02:31:12,000
So that in a nutshell

2046
02:31:12,000 --> 02:31:16,000
is a basic clustering algorithm. Many of them work in a very similar way.

2047
02:31:16,000 --> 02:31:20,000
But how do they work under the hood? Let's talk about that.

2048
02:31:20,000 --> 02:31:24,000
The K-Means model works on a process called

2049
02:31:24,000 --> 02:31:28,000
expectation maximization.

2050
02:31:28,000 --> 02:31:32,000
In that process, the model starts off by randomly

2051
02:31:32,000 --> 02:31:36,000
picking a cluster center. It literally pulls out of the air,

2052
02:31:36,000 --> 02:31:40,000
you know what, I think the center of this cluster is over here. Boom. Just randomly.

2053
02:31:40,000 --> 02:31:44,000
It will then go through and it'll say, okay,

2054
02:31:44,000 --> 02:31:48,000
let me look

2055
02:31:48,000 --> 02:31:52,000
at all of the points that seem to be closest to this randomly chosen center.

2056
02:31:52,000 --> 02:31:56,000
It goes, okay, these points all seem close. These points seem close

2057
02:31:56,000 --> 02:32:00,000
to this other thing that I randomly picked. And once it figures out who's closest

2058
02:32:00,000 --> 02:32:04,000
to that, it will then stop, look at just those points, and it'll say,

2059
02:32:04,000 --> 02:32:08,000
where's the center for each of these now that I've picked them?

2060
02:32:08,000 --> 02:32:12,000
Where's the center for them? And it'll choose a new center and it'll have four new centers.

2061
02:32:12,000 --> 02:32:16,000
It will then do that same process over again. Now that I've picked

2062
02:32:16,000 --> 02:32:20,000
four new cluster centers, which points happen to be closest to the new

2063
02:32:20,000 --> 02:32:24,000
cluster centers? Okay, now that I've figured out those, let me see

2064
02:32:24,000 --> 02:32:28,000
if I can zoom in or tighten up my estimation of where

2065
02:32:28,000 --> 02:32:32,000
the true cluster center is. And it'll keep doing this

2066
02:32:32,000 --> 02:32:36,000
ten times.

2067
02:32:36,000 --> 02:32:40,000
Sorry. Roll that back.

2068
02:32:40,000 --> 02:32:44,000
Rewind the tape. It will keep doing this until

2069
02:32:44,000 --> 02:32:48,000
it stops getting changes in its answers.

2070
02:32:48,000 --> 02:32:52,000
It will converge onto an answer and then it'll stop. Then what it does is it

2071
02:32:52,000 --> 02:32:56,000
starts all over again. It will start with four completely different

2072
02:32:56,000 --> 02:33:00,000
randomly picked cluster centers and it'll try and converge a second time, converge

2073
02:33:00,000 --> 02:33:04,000
a third time, fourth time, and it'll do that process ten times.

2074
02:33:04,000 --> 02:33:08,000
That's what the N and NIT ten is. So this thing actually

2075
02:33:08,000 --> 02:33:12,000
churns through its learning process ten whole sequences

2076
02:33:12,000 --> 02:33:16,000
of times. How long it takes to converge may depend

2077
02:33:16,000 --> 02:33:20,000
on the data and how well it picks a random number.

2078
02:33:20,000 --> 02:33:24,000
So not only could it have a long convergence cycle, but once it

2079
02:33:24,000 --> 02:33:28,000
finishes that first convergence cycle, it's going to start all over again from scratch. And it kind of takes a nice average of that.

2080
02:33:28,000 --> 02:33:32,000
But you can fine tune that if you wanted to.

2081
02:33:32,000 --> 02:33:36,000
All right. So let's go back and look at some of these parameters.

2082
02:33:36,000 --> 02:33:40,000
If instead of doing that process ten times, you wanted

2083
02:33:40,000 --> 02:33:44,000
to do it fewer or more times, you can change that. The maximum

2084
02:33:44,000 --> 02:33:48,000
number of iterations, those kind of things can be all fine tuned.

2085
02:33:48,000 --> 02:33:52,000
You can set a random state which

2086
02:33:52,000 --> 02:33:56,000
will impact what random locations it might check

2087
02:33:56,000 --> 02:34:00,000
to start. Those kind of things happen. You can try

2088
02:34:00,000 --> 02:34:04,000
and set this up to be handled in different jobs if you want to have a distributed

2089
02:34:04,000 --> 02:34:08,000
system handle this because it can be labor-some and

2090
02:34:08,000 --> 02:34:12,000
compute-intensive, et cetera. So there's a lot of hyperparameters

2091
02:34:12,000 --> 02:34:16,000
we can tune there. And I'm going to stop talking because I don't know anything about any of the others.

2092
02:34:16,000 --> 02:34:20,000
All right. So what are the gotchas?

2093
02:34:20,000 --> 02:34:24,000
I mentioned this idea that it tries to converge. It's like, hey,

2094
02:34:24,000 --> 02:34:28,000
I picked a random spot. Here are things that are close to it. Here's the center

2095
02:34:28,000 --> 02:34:32,000
of those. And it's supposed to converge. And it converges really well

2096
02:34:32,000 --> 02:34:36,000
for a small given cluster of dots. But there is no

2097
02:34:36,000 --> 02:34:40,000
guarantee that it actually finds

2098
02:34:40,000 --> 02:34:44,000
holistically where the right clusters are.

2099
02:34:44,000 --> 02:34:48,000
When it throws out a random

2100
02:34:48,000 --> 02:34:52,000
selection of cluster centers, if it's completely out of

2101
02:34:52,000 --> 02:34:56,000
whack, it may converge in ways that you don't expect, especially

2102
02:34:56,000 --> 02:35:00,000
if the data overlaps in various ways. Our clusters were really

2103
02:35:00,000 --> 02:35:04,000
well separated. So you have no global guarantee that it's

2104
02:35:04,000 --> 02:35:08,000
going to get the right centers.

2105
02:35:08,000 --> 02:35:12,000
That is part of why it tries to do a multiple

2106
02:35:12,000 --> 02:35:16,000
run. The odds of it 10 times in a row finding

2107
02:35:16,000 --> 02:35:20,000
really crappy random starting points is pretty low.

2108
02:35:20,000 --> 02:35:24,000
Another weakness of K-means is you have to decide on the number of clusters.

2109
02:35:24,000 --> 02:35:28,000
In this case, I made the data and I'm like, yeah, I want four clusters.

2110
02:35:28,000 --> 02:35:32,000
So I knew to expect four. But if your

2111
02:35:32,000 --> 02:35:36,000
data overlaps in various ways, you may not have a good sense of how

2112
02:35:36,000 --> 02:35:40,000
many clusters there are. There might actually be, if you were to

2113
02:35:40,000 --> 02:35:44,000
drill into your data five clusters and you tell it to look for four, and so

2114
02:35:44,000 --> 02:35:48,000
it may essentially split one of those clusters up in ways that you might not

2115
02:35:48,000 --> 02:35:52,000
want. There are different models that can give you a measure of

2116
02:35:52,000 --> 02:35:56,000
fitness to the number of clusters. Hey, you know what?

2117
02:35:56,000 --> 02:36:00,000
You're looking at the wrong number of clusters here. Gaussian mixture is one of those.

2118
02:36:00,000 --> 02:36:04,000
There are some models that have

2119
02:36:04,000 --> 02:36:08,000
mechanisms built into them to try and determine a suitable number of clusters automatically.

2120
02:36:08,000 --> 02:36:12,000
As far as speed, clustering

2121
02:36:12,000 --> 02:36:16,000
algorithms can sometimes be really slow on large data sets, especially if they have to redo the

2122
02:36:16,000 --> 02:36:20,000
process multiple times. So those are some

2123
02:36:20,000 --> 02:36:24,000
kind of gotchas that you want to consider.

2124
02:36:24,000 --> 02:36:28,000
Here's a thing, and this confused me early on.

2125
02:36:28,000 --> 02:36:32,000
What do those

2126
02:36:32,000 --> 02:36:36,000
category labels really mean? If I say that

2127
02:36:36,000 --> 02:36:40,000
if we were to look under

2128
02:36:40,000 --> 02:36:44,000
the hood and look at the labels, we might see that this is cluster zero, cluster one,

2129
02:36:44,000 --> 02:36:48,000
cluster two, cluster three, just pretending.

2130
02:36:48,000 --> 02:36:52,000
What do those labels really mean? Well, they don't mean anything.

2131
02:36:52,000 --> 02:36:56,000
They're just numbers. They don't carry magic.

2132
02:36:56,000 --> 02:37:00,000
It just means that anything that is labeled with a zero

2133
02:37:00,000 --> 02:37:04,000
is related to all the things labeled with a zero. During a subsequent

2134
02:37:04,000 --> 02:37:08,000
run of the algorithm, what used to be labeled zero might

2135
02:37:08,000 --> 02:37:12,000
now be labeled a one. You'll probably have mostly the same

2136
02:37:12,000 --> 02:37:16,000
dots in it, and since they're all labeled one,

2137
02:37:16,000 --> 02:37:20,000
you know that they're all related to each other. But don't plant any sort

2138
02:37:20,000 --> 02:37:24,000
of magical association with, oh,

2139
02:37:24,000 --> 02:37:28,000
everything in this cluster is labeled zero, so

2140
02:37:28,000 --> 02:37:32,000
next week when I rerun the same model, maybe with a slightly altered data set,

2141
02:37:32,000 --> 02:37:36,000
I should look for zeros to narrow in on this cluster, because

2142
02:37:36,000 --> 02:37:40,000
when you get random starts and convergence, next

2143
02:37:40,000 --> 02:37:44,000
week's run of this model might say these are all labeled category one now.

2144
02:37:44,000 --> 02:37:48,000
They are randomly determined, so you cannot rely on

2145
02:37:48,000 --> 02:37:52,000
them always having the same category.

2146
02:37:52,000 --> 02:37:56,000
To kind of demo this principle, I'm actually going to plot the test data,

2147
02:37:56,000 --> 02:38:00,000
and I'll plot the training data on the same chart, and we're going to see they're going to have different colors,

2148
02:38:00,000 --> 02:38:04,000
because in calculating the test data

2149
02:38:04,000 --> 02:38:08,000
and running the training data, they got categorized in different ways.

2150
02:38:08,000 --> 02:38:12,000
One of them came up as a zero, one of them came up as a one, but they were

2151
02:38:12,000 --> 02:38:16,000
pointing at opposite points on the graph.

2152
02:38:16,000 --> 02:38:20,000
So to help highlight this,

2153
02:38:20,000 --> 02:38:24,000
in scatter plots, all our C values are based on whatever labels we

2154
02:38:24,000 --> 02:38:28,000
provide, zero, one, two, three. The C map is going to map a color to each value.

2155
02:38:28,000 --> 02:38:32,000
This is the seismic color mapping. It starts at

2156
02:38:32,000 --> 02:38:36,000
dark blackish, bluish, reddish, and then

2157
02:38:36,000 --> 02:38:40,000
dark maroon. So I'm going to get four different colors out,

2158
02:38:40,000 --> 02:38:44,000
and that's not what we wanted.

2159
02:38:44,000 --> 02:38:48,000
So let's plot both of these on the same graph.

2160
02:38:48,000 --> 02:38:52,000
These are all clearly clustered together, the training

2161
02:38:52,000 --> 02:38:56,000
and the test set, but they ended up having different numbers assigned

2162
02:38:56,000 --> 02:39:00,000
to them. This is probably

2163
02:39:00,000 --> 02:39:04,000
all, maybe ones and zeros kind of ended up here,

2164
02:39:04,000 --> 02:39:08,000
threes and fours ended up there or something, right? So don't let

2165
02:39:08,000 --> 02:39:12,000
the categorization, oh this is a category zero, throw you off. That's all.

2166
02:39:12,000 --> 02:39:16,000
Alright.

2167
02:39:16,000 --> 02:39:20,000
Lastly, so a little bit on

2168
02:39:20,000 --> 02:39:24,000
how to learn some more. And we're coming

2169
02:39:24,000 --> 02:39:28,000
quickly to the end of this show.

2170
02:39:28,000 --> 02:39:32,000
When you get an error message,

2171
02:39:32,000 --> 02:39:36,000
take the time to read the error message, even though they are typically scarier than the

2172
02:39:36,000 --> 02:39:40,000
documentation is. Pay attention to those.

2173
02:39:40,000 --> 02:39:44,000
While I was putting the lesson together, I actually

2174
02:39:44,000 --> 02:39:48,000
cobbled together some stuff from different cells in a different Jupiter

2175
02:39:48,000 --> 02:39:52,000
notebook, and I forgot to copy and paste the .fit.

2176
02:39:52,000 --> 02:39:56,000
And when I tried to run the notebook, I got this error. It says

2177
02:39:56,000 --> 02:40:00,000
hey, you tried to do a prediction, but you never actually fitted it. And it says this k

2178
02:40:00,000 --> 02:40:04,000
means instance was not fitted yet. And it even gave me a little bit of a helpful

2179
02:40:04,000 --> 02:40:08,000
hint. He's like call the fit method.

2180
02:40:08,000 --> 02:40:12,000
Which is quite nice of it. But

2181
02:40:12,000 --> 02:40:16,000
having taught hundreds and hundreds of students over the years,

2182
02:40:16,000 --> 02:40:20,000
the habit that I see with students is that they often do not read

2183
02:40:20,000 --> 02:40:24,000
the error message at all. And there's a variety of reasons why they don't.

2184
02:40:24,000 --> 02:40:28,000
Sometimes the error messages are not written by humans it seems like. They're written

2185
02:40:28,000 --> 02:40:32,000
in some gobbledygook that you just can't understand.

2186
02:40:32,000 --> 02:40:36,000
Some of the error messages may be well written. They might be clear. They might

2187
02:40:36,000 --> 02:40:40,000
be concise. But you as a beginner are not ready for the data it provided to you.

2188
02:40:40,000 --> 02:40:44,000
If you were more advanced, it would make sense. But you're a noob, right?

2189
02:40:44,000 --> 02:40:48,000
So it doesn't, even though it was well written. And sometimes

2190
02:40:48,000 --> 02:40:52,000
students might do three, four, five changes to a snippet of code.

2191
02:40:52,000 --> 02:40:56,000
Like I'll change this, and I'll change this, and I'll change this, and I'll change this thing.

2192
02:40:56,000 --> 02:41:00,000
And they run it. And the error message

2193
02:41:00,000 --> 02:41:04,000
might be about change one, but they think it's about change

2194
02:41:04,000 --> 02:41:08,000
four because that was the most recent thing they changed. So obviously that's what broke it.

2195
02:41:08,000 --> 02:41:12,000
And so they'll dive right back at whatever the last thing it was that they changed.

2196
02:41:12,000 --> 02:41:16,000
Because obviously that must have been what I screwed up.

2197
02:41:16,000 --> 02:41:20,000
And so

2198
02:41:20,000 --> 02:41:24,000
they miss in that case a pointer that points them at, oh wait, wait, wait.

2199
02:41:24,000 --> 02:41:28,000
Your inclination is to change whatever you

2200
02:41:28,000 --> 02:41:32,000
messed with on line 50, but this error occurred on line three, right?

2201
02:41:32,000 --> 02:41:36,000
So get used to reading those error messages.

2202
02:41:36,000 --> 02:41:40,000
It will give you that verbiage terminology, the grammar,

2203
02:41:40,000 --> 02:41:44,000
if you will, that you need to know. It will often point you at where the

2204
02:41:44,000 --> 02:41:48,000
error might lie. It'll help fine tune you in on

2205
02:41:48,000 --> 02:41:52,000
oh, this is on line 20, not line 50 kind of things.

2206
02:41:52,000 --> 02:41:56,000
I get a

2207
02:41:56,000 --> 02:42:00,000
lot of error messages even after programming for years and

2208
02:42:00,000 --> 02:42:04,000
years that I look at them and I go, I have no idea what this person is trying to say.

2209
02:42:04,000 --> 02:42:08,000
No clue. This makes zero sense to me. So I will

2210
02:42:08,000 --> 02:42:12,000
copy paste the error message, put it in my friend Google's app, and Google will then come

2211
02:42:12,000 --> 02:42:16,000
back and say, hey, this is what this means.

2212
02:42:16,000 --> 02:42:20,000
For those

2213
02:42:20,000 --> 02:42:24,000
of you who want to explore a few more things, we talked about this a couple

2214
02:42:24,000 --> 02:42:28,000
times. If you are really curious about scoring and validating your models, I've got two links

2215
02:42:28,000 --> 02:42:32,000
here for you. One on kind of the learning curve and what that means,

2216
02:42:32,000 --> 02:42:36,000
and another one on cross validation.

2217
02:42:36,000 --> 02:42:40,000
So looking at the clock, we're at about 17 minutes

2218
02:42:40,000 --> 02:42:44,000
out. I'm going to let you play with a snippet of code here.

2219
02:42:44,000 --> 02:42:48,000
I've got a bit of code at the bottom. We'll scroll down and see it in a minute.

2220
02:42:48,000 --> 02:42:52,000
We're going to make some blobs.

2221
02:42:52,000 --> 02:42:56,000
These blobs are all set with a standard deviation

2222
02:42:56,000 --> 02:43:00,000
of .7.

2223
02:43:00,000 --> 02:43:04,000
It's the same .7 that we had a few minutes ago. But what I want you to do is I want you to

2224
02:43:04,000 --> 02:43:08,000
change each of these. You're going to change it to a

2225
02:43:08,000 --> 02:43:12,000
one and then you're going to rerun the code and change it to a two and then rerun the code and change it to

2226
02:43:12,000 --> 02:43:16,000
three. So you just change this variable right here. A .7 is going to

2227
02:43:16,000 --> 02:43:20,000
give you really tight clusters. As you start to get closer and closer to three, your clusters

2228
02:43:20,000 --> 02:43:24,000
are going to spread out. They're going to start to overlap each other and craziness will ensue.

2229
02:43:24,000 --> 02:43:28,000
The code is set up to already do

2230
02:43:28,000 --> 02:43:32,000
the train test split and it will produce a scatter plot

2231
02:43:32,000 --> 02:43:36,000
of just the training data. You will then run the machine

2232
02:43:36,000 --> 02:43:40,000
learning and then it's got another scatter plot

2233
02:43:40,000 --> 02:43:44,000
to show the outcomes of the tests. And I just want you to take

2234
02:43:44,000 --> 02:43:48,000
a quick glance, if you will, at using

2235
02:43:48,000 --> 02:43:52,000
make blobs and how that impacts what the model does.

2236
02:43:52,000 --> 02:43:56,000
Does this have an impact on the model's ability to do its thing? Does it not? Those kind of things.

2237
02:43:56,000 --> 02:44:00,000
So I'll give you two or three minutes to play with that.

2238
02:44:00,000 --> 02:44:04,000
And then

2239
02:44:04,000 --> 02:44:08,000
dear lord, special topics and conclusion.

2240
02:44:08,000 --> 02:44:12,000
Crap. Nice.

2241
02:44:12,000 --> 02:44:16,000
Wait, 440 or 420?

2242
02:44:16,000 --> 02:44:20,000
Oh, shh. Amen. Alright. For some reason I was thinking

2243
02:44:20,000 --> 02:44:24,000
420. Okay, we're good.

2244
02:44:24,000 --> 02:44:28,000
Yeah.

2245
02:44:28,000 --> 02:44:32,000
So random. Okay.

2246
02:44:32,000 --> 02:44:36,000
Go ahead. Play with the code a little bit. Poke and prod at it.

2247
02:44:36,000 --> 02:44:40,000
See what you get. Gracias.

2248
02:44:40,000 --> 02:44:44,000
Do you want coffee water or do you don't drink coffee?

2249
02:44:44,000 --> 02:44:48,000
There is a water

2250
02:44:48,000 --> 02:44:52,000
thing right there. If you could get me a bit of water that would be amazingly good.

2251
02:44:56,000 --> 02:45:00,000
Okay. I'm going to turn the mic off.

2252
02:45:10,000 --> 02:45:14,000
Okay.

2253
02:45:30,000 --> 02:45:34,000
By the way, whoever said that this ends at 440? Screw you.

2254
02:45:34,000 --> 02:45:38,000
I was like, yes.

2255
02:45:38,000 --> 02:45:42,000
It's a lot of work.

2256
02:46:08,000 --> 02:46:12,000
Okay.

2257
02:46:38,000 --> 02:46:42,000
Alright.

2258
02:46:42,000 --> 02:46:46,000
Seeing a bunch of stickies.

2259
02:46:46,000 --> 02:46:50,000
I also see a hand.

2260
02:47:08,000 --> 02:47:12,000
Okay.

2261
02:47:38,000 --> 02:47:42,000
Okay.

2262
02:48:08,000 --> 02:48:12,000
Okay.

2263
02:48:38,000 --> 02:48:42,000
Okay.

2264
02:48:42,000 --> 02:48:46,000
Okay.

2265
02:48:46,000 --> 02:48:50,000
Okay.

2266
02:48:50,000 --> 02:48:54,000
Okay.

2267
02:48:54,000 --> 02:48:58,000
Okay.

2268
02:48:58,000 --> 02:49:02,000
Okay.

2269
02:49:02,000 --> 02:49:06,000
Okay.

2270
02:49:06,000 --> 02:49:10,000
Okay.

2271
02:49:10,000 --> 02:49:14,000
Okay.

2272
02:49:14,000 --> 02:49:18,000
Okay.

2273
02:49:18,000 --> 02:49:22,000
Okay.

2274
02:49:22,000 --> 02:49:26,000
Okay.

2275
02:49:26,000 --> 02:49:30,000
Okay.

2276
02:49:30,000 --> 02:49:34,000
Okay.

2277
02:49:34,000 --> 02:49:38,000
Okay.

2278
02:49:38,000 --> 02:49:42,000
Okay.

2279
02:49:42,000 --> 02:49:46,000
Okay.

2280
02:49:46,000 --> 02:49:50,000
Okay.

2281
02:49:50,000 --> 02:49:54,000
Okay.

2282
02:49:54,000 --> 02:49:58,000
Okay.

2283
02:49:58,000 --> 02:50:02,000
Okay.

2284
02:50:02,000 --> 02:50:06,000
Okay.

2285
02:50:06,000 --> 02:50:10,000
Okay.

2286
02:50:10,000 --> 02:50:14,000
Okay.

2287
02:50:14,000 --> 02:50:18,000
Okay.

2288
02:50:18,000 --> 02:50:22,000
Okay.

2289
02:50:22,000 --> 02:50:26,000
Okay.

2290
02:50:26,000 --> 02:50:30,000
Okay.

2291
02:50:30,000 --> 02:50:34,000
Okay.

2292
02:50:34,000 --> 02:50:38,000
Okay.

2293
02:50:38,000 --> 02:50:42,000
Okay.

2294
02:50:42,000 --> 02:50:46,000
Okay.

2295
02:50:46,000 --> 02:50:50,000
Okay.

2296
02:50:50,000 --> 02:50:54,000
Okay.

2297
02:50:54,000 --> 02:50:58,000
Okay.

2298
02:50:58,000 --> 02:51:02,000
Okay.

2299
02:51:02,000 --> 02:51:06,000
Okay.

2300
02:51:06,000 --> 02:51:10,000
Okay.

2301
02:51:10,000 --> 02:51:14,000
Okay.

2302
02:51:14,000 --> 02:51:18,000
Okay.

2303
02:51:18,000 --> 02:51:22,000
Okay.

2304
02:51:52,000 --> 02:52:21,180
All right, folks, I'm gonna turn on the little magic thing.

2305
02:52:21,180 --> 02:52:25,460
All right.

2306
02:52:25,460 --> 02:52:30,460
Okay.

2307
02:52:30,460 --> 02:52:40,460
So, we've got to get started.

2308
02:52:40,460 --> 02:52:44,540
Good.

2309
02:52:44,540 --> 02:52:50,380
So, I'm going to run this real quick with a dot 7.

2310
02:52:50,380 --> 02:52:51,900
Very nicely separated.

2311
02:52:51,900 --> 02:52:57,260
The standard deviation is fairly tight, so the clusters are small.

2312
02:52:57,260 --> 02:52:58,740
It's very easy to train this.

2313
02:52:58,740 --> 02:53:00,400
Everything's good.

2314
02:53:00,400 --> 02:53:01,940
Let's jump right to three.

2315
02:53:01,940 --> 02:53:05,940
I asked you to do all of them, but we'll, in the interest of time, we'll just do three.

2316
02:53:05,940 --> 02:53:07,260
This one's crazy overlapped.

2317
02:53:07,260 --> 02:53:13,300
There's things, when I made the groups, when I purposely made the groups, I said,

2318
02:53:13,300 --> 02:53:17,940
create a grouping here, and it's going to have category one, and it did.

2319
02:53:17,980 --> 02:53:21,340
But that group is fairly spread out.

2320
02:53:21,340 --> 02:53:25,060
I said, make another group to category two, and that group happened to be spread out,

2321
02:53:25,060 --> 02:53:26,540
and there's plenty of overlap.

2322
02:53:26,540 --> 02:53:30,100
So, that makes it more difficult for us to figure out.

2323
02:53:30,100 --> 02:53:34,300
Well, should this dot right here, should this one be

2324
02:53:34,300 --> 02:53:37,700
in what we'll call the red category or the blue category?

2325
02:53:37,700 --> 02:53:40,580
I don't know.

2326
02:53:40,580 --> 02:53:46,100
And so, as you look at your input data and you look at your output data,

2327
02:53:46,100 --> 02:53:55,620
you as the data scientist, right, have to try and get a sense for,

2328
02:53:55,620 --> 02:53:59,380
do I feel comfortable with these results?

2329
02:53:59,380 --> 02:54:02,780
Maybe do I need to do more testing?

2330
02:54:02,780 --> 02:54:04,620
Do I need to look at more data?

2331
02:54:04,620 --> 02:54:10,320
Do I need to find some sort of characteristic of my data that might help me fine tune

2332
02:54:10,320 --> 02:54:12,580
which things fall into which categories?

2333
02:54:12,620 --> 02:54:17,980
If I was to add another column to my data set, might it help me to discern between things

2334
02:54:17,980 --> 02:54:21,700
that should be in the blue category versus the red category?

2335
02:54:21,700 --> 02:54:22,820
Those kind of things.

2336
02:54:22,820 --> 02:54:25,700
And so, you might have to change your data.

2337
02:54:25,700 --> 02:54:29,900
You might have to do some sort of transformation to your data to allow you to look

2338
02:54:29,900 --> 02:54:31,740
at things in different ways.

2339
02:54:31,740 --> 02:54:37,020
So, there's options, but you're going to run into cases where, hmm,

2340
02:54:37,020 --> 02:54:38,820
I'm not sure what this means.

2341
02:54:38,860 --> 02:54:42,940
And that'll take time, effort, and research.

2342
02:54:42,940 --> 02:54:46,460
Questions, essentially, that came up during the conversation here, you know,

2343
02:54:46,460 --> 02:54:49,300
how reliable are some of these things?

2344
02:54:49,300 --> 02:54:56,700
The Scikit-learn folks have a process for accepting new algorithms into the library.

2345
02:54:56,700 --> 02:55:00,180
Those libraries have to go through kind of an evaluation process.

2346
02:55:00,180 --> 02:55:02,380
Have those libraries shown up in the literature?

2347
02:55:02,380 --> 02:55:05,780
If so, do they have, you know, citations associated with them

2348
02:55:05,780 --> 02:55:09,140
where people are using them, referencing them, et cetera?

2349
02:55:09,140 --> 02:55:11,500
Do they have a history, et cetera?

2350
02:55:11,500 --> 02:55:15,060
You can't just be, you know, random Joe, I made an algorithm, it's cool.

2351
02:55:15,060 --> 02:55:18,020
You guys should include it, and they just suck it in.

2352
02:55:18,020 --> 02:55:22,220
They look for algorithms that have some runtime behind them

2353
02:55:22,220 --> 02:55:25,300
and some level of study and rigor.

2354
02:55:25,300 --> 02:55:30,620
So, they're fairly reliable, but you also have to use good judgment.

2355
02:55:30,620 --> 02:55:31,780
All right.

2356
02:55:31,780 --> 02:55:33,100
So, that's that.

2357
02:55:34,100 --> 02:55:36,620
Let's go look at special topics.

2358
02:55:40,620 --> 02:55:45,340
We're going to look at a thing called the polynomials feature method.

2359
02:55:45,340 --> 02:55:50,140
We're going to look at a technique to create workflows called pipelines.

2360
02:55:50,140 --> 02:55:53,260
And then we're going to drop polynomial features into a pipeline

2361
02:55:53,260 --> 02:55:57,340
to look at a topic called underfitting and overfitting.

2362
02:55:57,340 --> 02:56:00,940
So, we'll start off with polynomial features.

2363
02:56:01,180 --> 02:56:03,900
You can import the polynomial feature class.

2364
02:56:03,900 --> 02:56:07,620
It has a method associated with it called fit transform.

2365
02:56:07,620 --> 02:56:10,940
We've seen fit, we've seen predict.

2366
02:56:10,940 --> 02:56:12,540
I mentioned the word transform before.

2367
02:56:12,540 --> 02:56:14,580
This one is fit and transform.

2368
02:56:14,580 --> 02:56:19,820
Okay. It takes input values and it gives you a new series of output values.

2369
02:56:19,820 --> 02:56:24,220
And a lot of times, people will take these output values that they get from here

2370
02:56:24,220 --> 02:56:27,020
and they will immediately drop them into other models.

2371
02:56:27,020 --> 02:56:28,380
All right.

2372
02:56:28,420 --> 02:56:32,260
What polynomial features does is it will give you a new feature matrix

2373
02:56:32,260 --> 02:56:36,620
that has all of the polynomial combinations of your original features

2374
02:56:36,620 --> 02:56:39,420
with a degree less than or equal to the specified degree,

2375
02:56:39,420 --> 02:56:43,500
which sounds really mathy and it is, so sorry.

2376
02:56:43,500 --> 02:56:50,220
As an example, say I had two features, feature A, feature B,

2377
02:56:50,220 --> 02:56:56,820
and I wanted to get the degree two polynomial features out of this.

2378
02:56:56,860 --> 02:57:06,100
When I run my fit transform, I would get back feature A, I'd get back feature B,

2379
02:57:06,100 --> 02:57:09,780
but I also get back A times A, which is A squared.

2380
02:57:09,780 --> 02:57:12,500
I get back B times B, which is B squared.

2381
02:57:12,500 --> 02:57:19,140
I get A times B and unless I turn it off, I get a column called one,

2382
02:57:19,140 --> 02:57:20,500
or I get a value called one.

2383
02:57:20,500 --> 02:57:21,980
We'll talk about that in a second.

2384
02:57:21,980 --> 02:57:24,860
Okay. You say, well, what the heck?

2385
02:57:24,860 --> 02:57:31,540
What this is going to allow me to do is to allow me to do some evaluations of data

2386
02:57:31,540 --> 02:57:37,300
and create curves that may fit my data more succinctly,

2387
02:57:37,300 --> 02:57:40,260
and we'll talk more about this.

2388
02:57:40,260 --> 02:57:43,100
I'll show you examples of how polynomial features works

2389
02:57:43,100 --> 02:57:46,180
and then we'll see some graphs later in the conversation.

2390
02:57:46,180 --> 02:57:46,980
All right.

2391
02:57:46,980 --> 02:57:50,260
So polynomial features is in the pre-processing module,

2392
02:57:50,260 --> 02:57:54,300
so from pre-processing we import polynomial features.

2393
02:57:54,340 --> 02:57:56,380
I'm going to create a very simple matrix.

2394
02:57:56,380 --> 02:57:58,100
It's only got three things in it.

2395
02:57:58,100 --> 02:57:59,660
It's got a zero, one, and a two.

2396
02:57:59,660 --> 02:58:03,700
I'm going to drop that matrix into polynomial features

2397
02:58:03,700 --> 02:58:06,420
and ask it to give me something back.

2398
02:58:06,420 --> 02:58:09,820
I'm going to start off by asking for degree one,

2399
02:58:09,820 --> 02:58:14,940
and it'll give me back a degree one polynomial feature set.

2400
02:58:14,940 --> 02:58:16,740
All right.

2401
02:58:16,740 --> 02:58:20,300
Unless I turn it off, it's always going to give me a column of ones,

2402
02:58:20,300 --> 02:58:24,620
and it will give me back just the A's that I put into it.

2403
02:58:24,620 --> 02:58:26,340
Pretty straightforward.

2404
02:58:26,340 --> 02:58:28,740
If I want to get rid of that column of A's,

2405
02:58:28,740 --> 02:58:34,620
there is an argument that I can include called includeBias equals false.

2406
02:58:34,620 --> 02:58:36,100
I can make that go away.

2407
02:58:36,100 --> 02:58:37,540
There you go.

2408
02:58:37,540 --> 02:58:42,500
And so polynomial features with a degree one and bias turned off

2409
02:58:42,500 --> 02:58:44,620
gives me back just the numbers I put in.

2410
02:58:44,620 --> 02:58:47,300
Easy peasy.

2411
02:58:47,300 --> 02:58:50,060
But if I go to degree two,

2412
02:58:53,540 --> 02:58:57,100
in this case, I only had zero, one, and two.

2413
02:58:57,100 --> 02:58:58,740
If I square zero, I get zero.

2414
02:58:58,740 --> 02:58:59,900
If I square one, I get one.

2415
02:58:59,900 --> 02:59:01,500
If I square two, I get a four.

2416
02:59:01,500 --> 02:59:03,060
And so I get back a one column.

2417
02:59:03,060 --> 02:59:06,820
I get back my original inputs, and I get back A squared.

2418
02:59:06,820 --> 02:59:10,220
If I move on to, say, polynomial features four,

2419
02:59:11,220 --> 02:59:15,300
I'm going to get back the ones,

2420
02:59:15,300 --> 02:59:19,420
my original A squared, A cubed, and A quad.

2421
02:59:19,420 --> 02:59:25,100
Right? There's a method to the madness. Bear with me.

2422
02:59:25,100 --> 02:59:27,980
All right. Let's make a new matrix.

2423
02:59:27,980 --> 02:59:30,220
It's got two columns now,

2424
02:59:30,220 --> 02:59:32,540
zero, one, two, three, four, five.

2425
02:59:32,540 --> 02:59:38,740
And let's just do a simple one degree polynomial features.

2426
02:59:39,100 --> 02:59:41,460
We basically get back the ones column,

2427
02:59:41,460 --> 02:59:44,220
and we get back all of my A column and my B column.

2428
02:59:46,260 --> 02:59:51,260
If I do a degree two, we'll get back A column, B column,

2429
02:59:52,380 --> 02:59:55,580
A squared, A times B, and B squared.

2430
02:59:57,500 --> 03:00:01,740
And if I do this, it starts to get crazy,

2431
03:00:01,740 --> 03:00:04,020
and it's all hard to keep track of.

2432
03:00:04,020 --> 03:00:05,620
So I wrote it out there for you.

2433
03:00:06,620 --> 03:00:08,940
All right. Again, why does this matter?

2434
03:00:11,020 --> 03:00:14,740
When I made a simple straight line, just a nice slope,

2435
03:00:14,740 --> 03:00:18,020
I can say, hey, if I have a slope,

2436
03:00:18,020 --> 03:00:21,180
and I multiply by my X input, and I have a Y intercept,

2437
03:00:21,180 --> 03:00:23,780
this B, I can tell you what Y value is.

2438
03:00:24,860 --> 03:00:27,220
Well, to create different types of curves,

2439
03:00:28,220 --> 03:00:32,820
you can create different types of equations,

2440
03:00:32,820 --> 03:00:35,100
and equations can have different degrees.

2441
03:00:35,100 --> 03:00:40,100
You can use X squared, X, X cubed, quad,

2442
03:00:40,100 --> 03:00:42,620
and each of these might have different coefficients,

2443
03:00:42,620 --> 03:00:44,900
a coefficient of A or coefficient of B

2444
03:00:44,900 --> 03:00:47,300
that essentially equates to slopes, if you will.

2445
03:00:48,300 --> 03:00:53,300
And so if I can feed in different slopes into this formula,

2446
03:00:54,260 --> 03:00:56,900
I can craft different types of curves,

2447
03:00:56,900 --> 03:00:58,540
and if my data is not a straight line,

2448
03:00:58,540 --> 03:01:02,660
I can fit a curve to curved data.

2449
03:01:03,020 --> 03:01:06,180
The tricky part is what should be A,

2450
03:01:06,180 --> 03:01:07,660
what should be C, what should be D?

2451
03:01:07,660 --> 03:01:09,460
What should be those coefficients?

2452
03:01:09,460 --> 03:01:12,140
What should be the intersects, et cetera?

2453
03:01:13,660 --> 03:01:17,660
But to get my data in a format where I have

2454
03:01:19,180 --> 03:01:21,940
X squared, X cubed, et cetera,

2455
03:01:21,940 --> 03:01:23,460
you maybe don't want to do that manually,

2456
03:01:23,460 --> 03:01:26,420
so you use a tool like polynomial features

2457
03:01:26,420 --> 03:01:31,340
to take a couple of inputs and craft

2458
03:01:31,340 --> 03:01:33,260
a broader set of inputs.

2459
03:01:34,060 --> 03:01:35,380
And once you've done that,

2460
03:01:35,380 --> 03:01:37,140
you can theoretically make some magic happen,

2461
03:01:37,140 --> 03:01:38,460
and we'll see it.

2462
03:01:38,460 --> 03:01:39,300
Okay.

2463
03:01:40,820 --> 03:01:43,780
That probably didn't make much sense to most folks,

2464
03:01:43,780 --> 03:01:45,180
because there's no visuals.

2465
03:01:45,180 --> 03:01:47,420
There's a visual at the bottom of this conversation,

2466
03:01:47,420 --> 03:01:49,700
so just hang on to your hats with that.

2467
03:01:49,700 --> 03:01:50,860
We're gonna move into pipelines,

2468
03:01:50,860 --> 03:01:53,300
and we're gonna come back to polynomial features in a bit.

2469
03:01:55,980 --> 03:01:57,080
So what is a pipeline?

2470
03:01:58,080 --> 03:02:03,080
A pipeline is a way to take a scikit-learn estimator,

2471
03:02:04,000 --> 03:02:07,480
classifier, model,

2472
03:02:07,480 --> 03:02:12,240
and essentially connect it to another classifier,

2473
03:02:12,240 --> 03:02:15,960
transformer, estimator, so that you put inputs in one side,

2474
03:02:15,960 --> 03:02:17,920
the outputs come out, and they go immediately

2475
03:02:17,920 --> 03:02:21,280
into the next model, and you can chain these together.

2476
03:02:23,360 --> 03:02:25,200
If you wanted to do that manually, you can.

2477
03:02:25,200 --> 03:02:26,680
You can say, hey, here's an input.

2478
03:02:26,680 --> 03:02:27,760
Cool, I got an output.

2479
03:02:27,760 --> 03:02:30,220
Let me now drop that variable into the inputs

2480
03:02:30,220 --> 03:02:34,720
of my next model, but that becomes potentially cumbersome.

2481
03:02:34,720 --> 03:02:37,480
You have to kind of manually handle some of those things,

2482
03:02:37,480 --> 03:02:40,360
and we can simplify stuff by creating a pipeline.

2483
03:02:42,240 --> 03:02:44,440
You say, well, what would you need to do with a pipeline?

2484
03:02:44,440 --> 03:02:46,080
Well, when you're looking at your data,

2485
03:02:46,080 --> 03:02:48,280
sometimes you need to scale it,

2486
03:02:48,280 --> 03:02:51,680
and scikit-learn has a scalar to change things

2487
03:02:51,680 --> 03:02:56,440
from maybe a wide set of values to a smaller set of values.

2488
03:02:56,440 --> 03:02:59,520
Scale things from random values down to values

2489
03:02:59,520 --> 03:03:01,660
between one and zero, or between negative one

2490
03:03:01,660 --> 03:03:03,080
and positive one, those kind of things.

2491
03:03:03,080 --> 03:03:04,560
So you might need to scale it.

2492
03:03:04,560 --> 03:03:07,260
You might need to change it into a new format.

2493
03:03:07,260 --> 03:03:09,940
We talked about changing stuff into a new format

2494
03:03:09,940 --> 03:03:11,900
using polynomial features.

2495
03:03:11,900 --> 03:03:14,460
If you're looking at maybe text,

2496
03:03:14,460 --> 03:03:16,220
you might want to use a count vectorizer

2497
03:03:16,220 --> 03:03:19,260
to change things into counts and vectors.

2498
03:03:20,280 --> 03:03:22,580
You might want to normalize them using this

2499
03:03:22,620 --> 03:03:25,660
TFI-DF transformer, which is commonly used

2500
03:03:25,660 --> 03:03:29,580
in text analytics, I think.

2501
03:03:29,580 --> 03:03:31,740
We're not gonna say anything else about that.

2502
03:03:34,180 --> 03:03:35,020
Let's see.

2503
03:03:36,380 --> 03:03:40,460
So let's make a quick, trivial pipeline example,

2504
03:03:40,460 --> 03:03:41,860
and I'm gonna use polynomial features,

2505
03:03:41,860 --> 03:03:43,780
and I'm gonna use a linear regression.

2506
03:03:43,780 --> 03:03:45,220
There we go.

2507
03:03:45,220 --> 03:03:48,040
All right, we're gonna use the skin cancer data again,

2508
03:03:48,040 --> 03:03:49,500
because we've seen it, and we kind of know

2509
03:03:49,500 --> 03:03:52,980
what reality looks like there.

2510
03:03:52,980 --> 03:03:55,580
We change everything into the right formats.

2511
03:03:55,580 --> 03:03:59,060
Feature matrix, we'll split the data, blah, blah, blah, blah.

2512
03:03:59,060 --> 03:04:01,740
All right, so everything we just blew through,

2513
03:04:01,740 --> 03:04:02,780
we've already done.

2514
03:04:02,780 --> 03:04:03,940
This is the new stuff.

2515
03:04:05,620 --> 03:04:07,500
I import my linear regression model,

2516
03:04:07,500 --> 03:04:10,220
my polynomial features model, which we haven't used yet,

2517
03:04:10,220 --> 03:04:12,960
and I import a pipeline model.

2518
03:04:15,540 --> 03:04:17,980
Because I just want to kind of show

2519
03:04:17,980 --> 03:04:19,420
that the pipeline actually works,

2520
03:04:20,340 --> 03:04:21,700
and if you feed stuff in, you get stuff out,

2521
03:04:21,700 --> 03:04:24,060
I'm not going to use polynomial features

2522
03:04:24,060 --> 03:04:26,620
to make more features.

2523
03:04:26,620 --> 03:04:29,340
I'm gonna use degree one, and if you remember,

2524
03:04:29,340 --> 03:04:31,440
degree one takes whatever you feed in

2525
03:04:31,440 --> 03:04:33,700
and gives you exactly that back out.

2526
03:04:33,700 --> 03:04:35,180
I'm gonna include bias equals false

2527
03:04:35,180 --> 03:04:37,200
so I don't get a column of ones.

2528
03:04:37,200 --> 03:04:39,700
So in this case, because it's trivial,

2529
03:04:39,700 --> 03:04:41,980
I just feed in my data,

2530
03:04:41,980 --> 03:04:44,180
and I get the exact same data right back out.

2531
03:04:45,520 --> 03:04:47,400
We'll see a case where we use different degrees,

2532
03:04:47,400 --> 03:04:51,480
but for now, data in will equal the exact same data out.

2533
03:04:51,480 --> 03:04:53,780
Linear regression, what did I not do?

2534
03:04:55,120 --> 03:04:55,960
All right.

2535
03:04:58,280 --> 03:05:00,280
Error message showed up.

2536
03:05:00,280 --> 03:05:01,480
You should always read the error message.

2537
03:05:01,480 --> 03:05:04,960
It said, hey, Chalmer, you didn't define linear regression.

2538
03:05:04,960 --> 03:05:07,320
That's always my cue that I have screwed up

2539
03:05:07,320 --> 03:05:10,980
as an instructor and I have forgotten to execute a cell.

2540
03:05:10,980 --> 03:05:12,380
So let's execute it.

2541
03:05:13,600 --> 03:05:14,600
There we go, okay.

2542
03:05:17,560 --> 03:05:22,560
To make the pipeline, you call the pipeline method,

2543
03:05:23,160 --> 03:05:25,480
and you're gonna feed it a list,

2544
03:05:25,480 --> 03:05:27,540
and the list is gonna be full of tuples.

2545
03:05:27,540 --> 03:05:29,440
Every tuple will have two parts.

2546
03:05:29,440 --> 03:05:32,500
The first part of the tuple will be a name that you type in.

2547
03:05:32,500 --> 03:05:37,500
I chose to call my first step of my pipeline poly F,

2548
03:05:37,960 --> 03:05:42,960
and then you tell it what model you wanna use for that step.

2549
03:05:42,960 --> 03:05:47,960
For the next step, I give it a name.

2550
03:05:47,960 --> 03:05:49,400
I'm gonna call it linear R,

2551
03:05:49,400 --> 03:05:52,620
and I tell it which model to use for that step.

2552
03:05:54,320 --> 03:05:56,680
So I created a polynomial features model,

2553
03:05:56,680 --> 03:05:58,920
and I created a linear regression model.

2554
03:05:58,920 --> 03:06:01,220
I'm gonna feed both of those in the pipeline,

2555
03:06:02,200 --> 03:06:07,200
and the neat thing here is the pipeline

2556
03:06:07,440 --> 03:06:09,140
will go into polynomial features,

2557
03:06:09,140 --> 03:06:12,180
and it'll call fit transform for me.

2558
03:06:12,180 --> 03:06:13,780
It'll go into linear regression,

2559
03:06:13,780 --> 03:06:15,880
and it'll call fit for me.

2560
03:06:15,880 --> 03:06:17,220
I call fit one time,

2561
03:06:17,220 --> 03:06:20,380
and it'll do all of those steps on my behalf,

2562
03:06:20,380 --> 03:06:22,660
and it doesn't matter if I have two steps

2563
03:06:22,660 --> 03:06:24,220
or if I have 20 steps.

2564
03:06:24,220 --> 03:06:26,980
It'll call polynomial features dot fit transform.

2565
03:06:26,980 --> 03:06:29,860
It'll call the next vectorizer fit,

2566
03:06:29,860 --> 03:06:30,820
and these types of things.

2567
03:06:30,820 --> 03:06:32,340
It'll do them one by one,

2568
03:06:32,340 --> 03:06:34,260
and as it gets data out of one model,

2569
03:06:34,260 --> 03:06:36,780
it feeds it directly into the next model.

2570
03:06:36,780 --> 03:06:38,240
All right, so let's do this.

2571
03:06:39,240 --> 03:06:40,080
Fit.

2572
03:06:41,080 --> 03:06:42,320
It spits out, right?

2573
03:06:42,320 --> 03:06:44,040
And I told you you should always read these things,

2574
03:06:44,040 --> 03:06:45,120
and sure enough, it reminds me,

2575
03:06:45,120 --> 03:06:47,880
hey Chalmer, here's the steps you asked me to take,

2576
03:06:47,880 --> 03:06:49,640
poly F and linear R.

2577
03:06:49,640 --> 03:06:51,420
It says, hey Chalmer, here's the characteristics

2578
03:06:51,420 --> 03:06:54,160
that you fed into polynomial features.

2579
03:06:54,160 --> 03:06:55,720
You asked me to be degree one.

2580
03:06:55,720 --> 03:06:58,280
You told me to include bias equals false,

2581
03:06:58,280 --> 03:06:59,400
et cetera, et cetera.

2582
03:07:01,080 --> 03:07:02,920
And since I fit the model,

2583
03:07:02,920 --> 03:07:04,720
I can do a predict,

2584
03:07:04,720 --> 03:07:06,160
and we'll do a scatter plot.

2585
03:07:07,120 --> 03:07:11,800
And sure enough, it did match that skin cancer graph

2586
03:07:11,800 --> 03:07:12,840
the way we expected.

2587
03:07:14,080 --> 03:07:15,720
Fairly trivial, I didn't change anything.

2588
03:07:15,720 --> 03:07:18,520
Didn't really use the power of polynomial features yet,

2589
03:07:18,520 --> 03:07:19,920
but it works.

2590
03:07:19,920 --> 03:07:21,440
You make a list.

2591
03:07:21,440 --> 03:07:23,040
The list has tuples.

2592
03:07:23,040 --> 03:07:24,000
The tuple has a name,

2593
03:07:24,000 --> 03:07:25,720
and it has the thing you wanted to do.

2594
03:07:25,720 --> 03:07:28,400
You put them in order, and magic happens.

2595
03:07:29,520 --> 03:07:31,880
All right, so let's do the magic

2596
03:07:32,940 --> 03:07:34,480
on essentially a real world problem

2597
03:07:34,480 --> 03:07:35,760
that you might want to look at

2598
03:07:35,760 --> 03:07:37,460
as you're doing machine learning.

2599
03:07:38,760 --> 03:07:42,440
Okay, so there is a concern

2600
03:07:42,440 --> 03:07:45,880
when you do machine learning

2601
03:07:45,880 --> 03:07:48,880
that you may so tightly

2602
03:07:49,980 --> 03:07:52,280
constrain the characteristics of your model,

2603
03:07:52,280 --> 03:07:55,760
and you may force it to follow the data so closely

2604
03:07:55,760 --> 03:07:57,280
that it basically is worthless.

2605
03:07:57,280 --> 03:07:59,360
It only follows the training data.

2606
03:07:59,360 --> 03:08:02,400
When I throw new random test data at it,

2607
03:08:02,400 --> 03:08:03,920
it doesn't work very well.

2608
03:08:03,920 --> 03:08:05,360
That's called overfitting.

2609
03:08:06,240 --> 03:08:08,760
And what that looks like in the real world is this.

2610
03:08:10,000 --> 03:08:11,920
This mattress is clearly overfit

2611
03:08:11,920 --> 03:08:14,560
for the human that's going to sleep on it.

2612
03:08:14,560 --> 03:08:16,660
If the human deviates in any way

2613
03:08:16,660 --> 03:08:18,300
from how this mattress is laid out,

2614
03:08:18,300 --> 03:08:20,440
this is going to be amazingly uncomfortable.

2615
03:08:21,960 --> 03:08:23,280
So you don't want that.

2616
03:08:23,280 --> 03:08:26,160
Underfitting is essentially the opposite.

2617
03:08:26,160 --> 03:08:27,920
You maybe draw a line through a set of data.

2618
03:08:27,920 --> 03:08:30,320
Maybe the data curves down in a big U like this,

2619
03:08:30,320 --> 03:08:32,640
but you run a line right through the middle of it

2620
03:08:32,640 --> 03:08:33,760
when you really want to have a line

2621
03:08:33,760 --> 03:08:36,040
that kind of curves and follows the data.

2622
03:08:36,040 --> 03:08:38,200
All right, so underfitting and overfitting.

2623
03:08:39,280 --> 03:08:40,760
All right.

2624
03:08:40,760 --> 03:08:43,280
There is a link in the documentation here.

2625
03:08:43,280 --> 03:08:45,360
You can go to a very nice article in Scikit-Learn

2626
03:08:45,360 --> 03:08:47,120
about underfitting and overfitting.

2627
03:08:48,160 --> 03:08:50,880
What we're going to do is we're going to make a pipeline.

2628
03:08:50,880 --> 03:08:55,880
We will use the polynomial features methodology

2629
03:08:56,200 --> 03:09:00,040
to do a degree one analysis,

2630
03:09:00,040 --> 03:09:02,000
which is going to be a straight line,

2631
03:09:02,000 --> 03:09:05,400
a degree four analysis, which will be a curve,

2632
03:09:05,400 --> 03:09:07,720
and then a degree 15 analysis,

2633
03:09:07,720 --> 03:09:10,680
which is going to be some really bizarre curve.

2634
03:09:12,680 --> 03:09:15,660
So let's start by importing these three things.

2635
03:09:17,120 --> 03:09:19,560
This function, don't worry about the math.

2636
03:09:19,560 --> 03:09:21,480
This is basically just going to make a bunch of dots

2637
03:09:21,480 --> 03:09:24,280
and they're going to be kind of any U-shaped curve.

2638
03:09:24,280 --> 03:09:25,120
All right.

2639
03:09:27,500 --> 03:09:29,600
So I create the function.

2640
03:09:29,600 --> 03:09:33,720
I take 30 random values, da-da-da-da-da,

2641
03:09:33,720 --> 03:09:35,840
and there we go.

2642
03:09:35,840 --> 03:09:39,360
So I have a bunch of random dots in kind of a cosine shape.

2643
03:09:39,360 --> 03:09:40,400
It's a nice little U.

2644
03:09:41,520 --> 03:09:42,360
All right.

2645
03:09:43,480 --> 03:09:48,480
I'm going to produce 100 test values between 0.5 and one.

2646
03:09:50,040 --> 03:09:51,880
They're going to be, or 0.05 and one.

2647
03:09:51,880 --> 03:09:53,720
They're going to be nicely evenly spaced,

2648
03:09:53,720 --> 03:09:55,920
and those will be my test values.

2649
03:09:55,920 --> 03:09:59,560
I'm going to see whether or not I can fit those test values

2650
03:10:00,560 --> 03:10:05,160
to the input data appropriately once my model is built.

2651
03:10:05,160 --> 03:10:06,000
All right.

2652
03:10:07,120 --> 03:10:12,120
So I'm going to take my input data.

2653
03:10:13,360 --> 03:10:14,640
Degree one means I'm going to leave it

2654
03:10:14,640 --> 03:10:17,400
exactly the way it was, no change.

2655
03:10:17,400 --> 03:10:20,260
Include bias, false means don't have the ones column.

2656
03:10:21,360 --> 03:10:24,640
We will then feed the raw input data,

2657
03:10:24,640 --> 03:10:27,120
which hasn't changed, into linear regression,

2658
03:10:27,160 --> 03:10:30,720
and we'll get a linear regression out of this.

2659
03:10:30,720 --> 03:10:31,960
And so I've made a pipeline,

2660
03:10:31,960 --> 03:10:34,800
and all I have to do is call fit on the pipeline,

2661
03:10:34,800 --> 03:10:36,840
and it'll call fit on all the other parts for me,

2662
03:10:36,840 --> 03:10:38,700
behind the scenes, there we go.

2663
03:10:40,360 --> 03:10:41,860
Okay, so we fit.

2664
03:10:41,860 --> 03:10:46,360
We now predict using my 100 variables, or 100 values,

2665
03:10:46,360 --> 03:10:47,280
and let's plot it.

2666
03:10:48,680 --> 03:10:53,680
So the blue dots are the raw data that came in.

2667
03:10:54,240 --> 03:10:57,080
The orange line is what we should see.

2668
03:10:57,080 --> 03:11:00,840
That follows a very nice cosine curve,

2669
03:11:00,840 --> 03:11:03,840
and my linear regression gave me this crappy blue line

2670
03:11:03,840 --> 03:11:05,400
that doesn't quite really work.

2671
03:11:06,560 --> 03:11:08,040
Right?

2672
03:11:08,040 --> 03:11:09,420
Doesn't quite follow things.

2673
03:11:09,420 --> 03:11:12,080
It misses this, it misses this belly,

2674
03:11:12,080 --> 03:11:14,720
it misses this spike at the end, no bueno.

2675
03:11:17,960 --> 03:11:22,200
The magic of polynomial features is that it allows me

2676
03:11:22,200 --> 03:11:24,800
to make more nuanced curves.

2677
03:11:24,800 --> 03:11:29,440
And by telling it that hey, take the original input data,

2678
03:11:29,440 --> 03:11:33,800
and now expand the number of columns in a particular way,

2679
03:11:33,800 --> 03:11:35,780
using the polynomial features method,

2680
03:11:37,040 --> 03:11:39,300
we're going to have a lot more columns,

2681
03:11:39,300 --> 03:11:40,920
which is gonna allow us to hopefully get

2682
03:11:40,920 --> 03:11:43,880
a little bit better looking curve.

2683
03:11:43,880 --> 03:11:48,880
So run that through, predict, and there we go.

2684
03:11:50,880 --> 03:11:51,720
Nice.

2685
03:11:53,200 --> 03:11:55,280
Deviates a little bit from the orange up here,

2686
03:11:55,280 --> 03:11:56,880
deviates from the orange a little bit here,

2687
03:11:56,880 --> 03:11:58,920
but it's pretty daggum close.

2688
03:11:58,920 --> 03:12:01,160
Nice, this is probably a pretty good fit.

2689
03:12:01,160 --> 03:12:03,320
So in this case, by choosing to have done

2690
03:12:03,320 --> 03:12:06,780
a polynomial features kind of transformation on my data,

2691
03:12:06,780 --> 03:12:10,080
I managed to find a curve that fits better.

2692
03:12:10,080 --> 03:12:13,780
The linear regression model was able to determine

2693
03:12:13,780 --> 03:12:17,160
what coefficients should go into those polynomial features

2694
03:12:17,160 --> 03:12:19,140
to make a beautiful looking curve.

2695
03:12:20,000 --> 03:12:21,560
But what if I was a little bit over eager,

2696
03:12:21,560 --> 03:12:23,200
and I'm like, well, hey, shucks,

2697
03:12:23,200 --> 03:12:27,040
degree four was great, degree 15 must be amazingly awesome.

2698
03:12:28,280 --> 03:12:31,960
All right, so we'll go there, degree 15, and...

2699
03:12:34,800 --> 03:12:38,640
Now this crazy curve is trying to fit really close

2700
03:12:38,640 --> 03:12:40,340
to as many of the dots as it can,

2701
03:12:40,340 --> 03:12:42,460
and obviously this is like crazy, right?

2702
03:12:42,460 --> 03:12:44,580
So that's overfitting in a nutshell.

2703
03:12:46,720 --> 03:12:47,560
Okay.

2704
03:12:48,560 --> 03:12:49,880
All right.

2705
03:12:49,880 --> 03:12:52,720
We're definitely getting close to the end of the show now.

2706
03:12:53,720 --> 03:12:55,800
I'll give you a last couple little bits,

2707
03:12:55,800 --> 03:12:57,160
and we'll do the conclusion.

2708
03:12:58,200 --> 03:13:01,040
Those examples I just showed you of the

2709
03:13:03,880 --> 03:13:05,840
degree one, degree four, degree 15,

2710
03:13:07,920 --> 03:13:12,920
they came from this example in the documentation.

2711
03:13:17,240 --> 03:13:18,720
They produced this little article,

2712
03:13:18,720 --> 03:13:20,720
they have essentially the same graphs.

2713
03:13:20,720 --> 03:13:22,440
I blatantly plagiarized it.

2714
03:13:25,680 --> 03:13:29,480
This is all of the code that generated that, right?

2715
03:13:29,480 --> 03:13:32,360
But what they did was they ended up producing

2716
03:13:32,360 --> 03:13:35,720
a matplotlib figure so they could put all three pictures

2717
03:13:35,720 --> 03:13:40,400
on the same canvas, if you will.

2718
03:13:40,400 --> 03:13:43,060
They added certain things in here

2719
03:13:43,060 --> 03:13:48,060
that maybe weren't necessarily critical to the message.

2720
03:13:48,700 --> 03:13:51,900
They put the number of degrees into a loop,

2721
03:13:51,900 --> 03:13:54,900
and they ran this through some sort of a for loop here,

2722
03:13:55,820 --> 03:13:57,260
and so they kind of produced things

2723
03:13:57,260 --> 03:14:00,180
in this nice systematic way, et cetera.

2724
03:14:00,180 --> 03:14:03,380
They added a lot of cruft to do the machine learning

2725
03:14:03,380 --> 03:14:04,780
and the graphing, et cetera.

2726
03:14:05,700 --> 03:14:09,860
So what I ended up doing when I was looking at that example

2727
03:14:09,860 --> 03:14:12,460
and trying to figure out how I could hopefully

2728
03:14:12,460 --> 03:14:14,940
present it here in this venue,

2729
03:14:15,500 --> 03:14:20,500
I took that entire process and I looked at it line by line.

2730
03:14:21,860 --> 03:14:23,860
I'm like, what is that line doing?

2731
03:14:23,860 --> 03:14:26,100
And do I need that line?

2732
03:14:26,100 --> 03:14:26,980
What is this line doing?

2733
03:14:26,980 --> 03:14:27,900
Do I need this line?

2734
03:14:27,900 --> 03:14:30,300
And I basically pulled out individual lines

2735
03:14:30,300 --> 03:14:32,940
and I kind of crafted this on my own.

2736
03:14:32,940 --> 03:14:35,460
And as I proceeded, I ran every couple of lines,

2737
03:14:35,460 --> 03:14:37,180
like, okay, now it's still working, it's still working,

2738
03:14:37,180 --> 03:14:39,180
I haven't broken anything too badly yet.

2739
03:14:40,700 --> 03:14:42,500
And I stripped away as many of those kind of

2740
03:14:42,500 --> 03:14:44,860
extraneous complications as I could

2741
03:14:45,780 --> 03:14:50,180
to really get down to what's the data formatting piece,

2742
03:14:50,180 --> 03:14:51,420
what's the machine learning piece,

2743
03:14:51,420 --> 03:14:53,420
what's the visualization piece?

2744
03:14:53,420 --> 03:14:55,780
And then I crafted that.

2745
03:14:55,780 --> 03:14:56,900
And I do that regularly.

2746
03:14:56,900 --> 03:15:00,700
When I played with Bokeh and some other types

2747
03:15:00,700 --> 03:15:04,180
of libraries, right, you see these great examples,

2748
03:15:04,180 --> 03:15:06,260
the code is really cool, it works,

2749
03:15:07,220 --> 03:15:10,420
but there's often kind of these complications

2750
03:15:10,420 --> 03:15:11,500
that they put in there.

2751
03:15:11,500 --> 03:15:13,700
So if you take a line at a time and you type them in

2752
03:15:13,700 --> 03:15:15,740
one by one and make sure they're working,

2753
03:15:15,740 --> 03:15:19,300
that can help you to fine tune on what's going on here

2754
03:15:19,300 --> 03:15:21,900
versus, I mean, I easily could have copied and pasted

2755
03:15:21,900 --> 03:15:23,740
all of it and said, yeah, let's just run it.

2756
03:15:23,740 --> 03:15:25,660
But then you're kind of figuring out what's going on

2757
03:15:25,660 --> 03:15:28,740
in all the other parts that are not critical to the task.

2758
03:15:28,740 --> 03:15:32,060
So I love to tear apart examples,

2759
03:15:32,900 --> 03:15:35,380
and I highly recommend it to students.

2760
03:15:35,380 --> 03:15:37,660
All right, we've got four minutes.

2761
03:15:40,580 --> 03:15:41,860
This is the conclusion.

2762
03:15:42,020 --> 03:15:44,340
Section seven.

2763
03:15:45,860 --> 03:15:47,420
Just a reminder, right?

2764
03:15:47,420 --> 03:15:49,580
This is the trademarked process.

2765
03:15:49,580 --> 03:15:52,820
You prep your data, you choose your model,

2766
03:15:52,820 --> 03:15:55,460
you choose the hyperparameters, you fit the model,

2767
03:15:55,460 --> 03:15:58,980
you apply the model, and then you examine your results.

2768
03:15:58,980 --> 03:16:00,900
If you do all of these things,

2769
03:16:00,900 --> 03:16:03,020
the models will start to make sense.

2770
03:16:03,020 --> 03:16:04,260
If you miss any of these steps,

2771
03:16:04,260 --> 03:16:06,020
things start to go a little wonky.

2772
03:16:07,540 --> 03:16:08,820
Where should you go next?

2773
03:16:09,500 --> 03:16:13,780
Machine learning is one part

2774
03:16:13,780 --> 03:16:16,620
of a very sophisticated data analysis system.

2775
03:16:18,180 --> 03:16:19,380
There's a lot of steps to this.

2776
03:16:19,380 --> 03:16:20,980
I broke out like six of them.

2777
03:16:20,980 --> 03:16:22,420
The first part is frame the problem.

2778
03:16:22,420 --> 03:16:24,700
What question am I trying to answer?

2779
03:16:24,700 --> 03:16:26,860
The next part is data acquisition.

2780
03:16:26,860 --> 03:16:28,100
How do I get the data?

2781
03:16:30,620 --> 03:16:32,380
In some cases, you may not have the data

2782
03:16:32,380 --> 03:16:35,220
or you might have to set up some system to collect the data.

2783
03:16:35,220 --> 03:16:36,900
Oh, I want to better understand my customers.

2784
03:16:36,900 --> 03:16:38,060
Well, are you collecting the right data

2785
03:16:38,060 --> 03:16:38,900
about your customers?

2786
03:16:38,900 --> 03:16:40,460
Oh, no, I'm not, okay.

2787
03:16:40,460 --> 03:16:42,500
Well, now I need to find a way to collect that.

2788
03:16:42,500 --> 03:16:44,140
Once you actually acquire the data,

2789
03:16:44,140 --> 03:16:46,140
then you go into the data wrangling piece,

2790
03:16:46,140 --> 03:16:48,580
cleansing your data, filtering stuff out,

2791
03:16:48,580 --> 03:16:51,540
deduplicating records, normalizing records, all that.

2792
03:16:52,420 --> 03:16:56,660
Typically, somewhere in there, often kind of intermingled,

2793
03:16:56,660 --> 03:16:58,260
you have some data exploration,

2794
03:16:58,260 --> 03:16:59,420
look at what the data says,

2795
03:16:59,420 --> 03:17:01,860
what's going on behind the scenes.

2796
03:17:01,860 --> 03:17:06,100
Eventually, once you kind of understand your data,

2797
03:17:06,100 --> 03:17:07,940
you can do some in-depth analysis

2798
03:17:08,780 --> 03:17:09,620
and that's where machine learning

2799
03:17:09,620 --> 03:17:10,860
typically comes into play.

2800
03:17:10,860 --> 03:17:12,100
Once you've done your machine learning,

2801
03:17:12,100 --> 03:17:13,740
you communicate your results out.

2802
03:17:14,940 --> 03:17:17,940
If you are part of a team, you may be lucky enough

2803
03:17:17,940 --> 03:17:21,300
to get somebody who is adept at some of these steps,

2804
03:17:21,300 --> 03:17:24,660
like the data acquisition piece or the data wrangling piece.

2805
03:17:24,660 --> 03:17:29,260
Those are typically ugly, difficult, and hard to manage.

2806
03:17:29,260 --> 03:17:32,220
They're often also tedious and time-consuming.

2807
03:17:32,220 --> 03:17:33,860
So if you're on a team and you've got somebody

2808
03:17:33,860 --> 03:17:35,100
who can do that, that's great.

2809
03:17:35,100 --> 03:17:37,540
For the rest of us, you're gonna have to do

2810
03:17:37,540 --> 03:17:39,180
some or all of that yourself.

2811
03:17:42,100 --> 03:17:43,940
You say, what does that mean?

2812
03:17:43,940 --> 03:17:45,580
So Forbes Magazine had an article

2813
03:17:45,580 --> 03:17:47,880
on how data scientists spend their time.

2814
03:17:49,180 --> 03:17:52,980
So this big C over here on this side,

2815
03:17:52,980 --> 03:17:56,020
this thing is green, it says 60% of your time.

2816
03:17:56,020 --> 03:17:57,740
It's meant cleaning and organizing your data.

2817
03:17:57,740 --> 03:18:00,700
That does not sound like machine learning to me.

2818
03:18:00,700 --> 03:18:03,180
This section right here, this kind of dark green,

2819
03:18:03,180 --> 03:18:06,900
that's 19% of your time, that's basically getting your data,

2820
03:18:06,940 --> 03:18:08,460
acquiring your data sets.

2821
03:18:09,340 --> 03:18:11,940
9% is kind of that data exploration,

2822
03:18:11,940 --> 03:18:14,380
maybe mining data for stuff, right?

2823
03:18:16,460 --> 03:18:18,980
Some machine learning might occur in there as well.

2824
03:18:20,540 --> 03:18:22,620
Building trading sets.

2825
03:18:22,620 --> 03:18:24,860
Four is gonna be refining your very cool

2826
03:18:24,860 --> 03:18:26,900
machine learning algorithms, and then five is gonna be

2827
03:18:26,900 --> 03:18:28,660
other miscellaneous stuff.

2828
03:18:29,740 --> 03:18:31,500
But the long story short here is,

2829
03:18:31,500 --> 03:18:33,500
you're gonna spend very little time doing machine learning.

2830
03:18:33,500 --> 03:18:35,300
You're gonna spend a heck of a lot of time

2831
03:18:36,300 --> 03:18:38,620
cleaning and refining your data.

2832
03:18:38,620 --> 03:18:41,740
So when I covered data handling in the beginning of this,

2833
03:18:41,740 --> 03:18:42,740
that was kind of a precursor

2834
03:18:42,740 --> 03:18:46,380
to this particular screen, right?

2835
03:18:46,380 --> 03:18:47,700
You wanna understand your tools,

2836
03:18:47,700 --> 03:18:50,180
NumPy, Pandas, those types of things.

2837
03:18:50,180 --> 03:18:52,980
Understand them well, because if you have to spend

2838
03:18:52,980 --> 03:18:56,580
60 to 80% of your time playing with those tools

2839
03:18:56,580 --> 03:18:58,340
before you ever get to machine learning,

2840
03:18:58,340 --> 03:19:00,260
you don't wanna be wasting precious valuable time

2841
03:19:00,260 --> 03:19:01,780
not knowing your tools.

2842
03:19:01,780 --> 03:19:03,900
So looking for that.

2843
03:19:03,940 --> 03:19:06,740
The data visualization piece, right?

2844
03:19:06,740 --> 03:19:08,500
That comes after the machine learning,

2845
03:19:08,500 --> 03:19:09,860
but it helps you with your machine learning.

2846
03:19:09,860 --> 03:19:11,660
It helps you with refining your algorithms.

2847
03:19:11,660 --> 03:19:12,900
So that's a critical component

2848
03:19:12,900 --> 03:19:14,860
that you wanna kind of focus on.

2849
03:19:14,860 --> 03:19:16,060
And so that might leave you with,

2850
03:19:16,060 --> 03:19:18,020
yeah, but what about machine learning?

2851
03:19:19,940 --> 03:19:21,940
When you walk out of this room,

2852
03:19:21,940 --> 03:19:25,180
many of you have the data and the libraries

2853
03:19:25,180 --> 03:19:28,100
installed on your laptops.

2854
03:19:28,100 --> 03:19:33,100
My machine learning content is up on GitHub.

2855
03:19:33,100 --> 03:19:35,660
Go through each of these different notebooks.

2856
03:19:35,660 --> 03:19:37,220
Do them again and again.

2857
03:19:37,220 --> 03:19:39,980
Read the text, refresh your memory on what was said.

2858
03:19:41,420 --> 03:19:44,700
Look for a learning environment that you learn best in.

2859
03:19:44,700 --> 03:19:46,060
Chalmers is a huge fan of books.

2860
03:19:46,060 --> 03:19:47,780
I love books.

2861
03:19:47,780 --> 03:19:49,620
Some people are not bookish.

2862
03:19:49,620 --> 03:19:50,980
They prefer videos.

2863
03:19:52,060 --> 03:19:54,260
They prefer being in a classroom, et cetera.

2864
03:19:55,260 --> 03:19:57,660
But find the environment that you're comfortable in.

2865
03:19:57,660 --> 03:19:58,500
Go to meetups.

2866
03:19:58,500 --> 03:20:01,100
A lot of meetups have data scientists there, et cetera.

2867
03:20:01,940 --> 03:20:04,380
I highly recommend a pet project.

2868
03:20:04,380 --> 03:20:06,460
Find some sort of data that you would like to model

2869
03:20:06,460 --> 03:20:07,380
in machine learn.

2870
03:20:08,460 --> 03:20:09,980
That'll keep your attention.

2871
03:20:09,980 --> 03:20:11,700
Just doing what's in a tutorial,

2872
03:20:11,700 --> 03:20:13,340
that kind of sucks, that's boring.

2873
03:20:13,340 --> 03:20:15,060
And the answer's already there.

2874
03:20:15,060 --> 03:20:18,300
But man, if you can find a relationship between data

2875
03:20:18,300 --> 03:20:23,300
and categorizations in data you care about, that's cool.

2876
03:20:23,700 --> 03:20:24,900
Coffee I don't care about.

2877
03:20:24,900 --> 03:20:28,300
I don't know why it ended up in my lesson, but it did.

2878
03:20:28,300 --> 03:20:30,060
Write code every day.

2879
03:20:30,060 --> 03:20:31,140
And I really mean that.

2880
03:20:31,140 --> 03:20:32,900
You're learning a brand new language

2881
03:20:32,900 --> 03:20:35,300
with new vocabulary and new terminology.

2882
03:20:35,300 --> 03:20:37,380
If you're not using this language every day,

2883
03:20:37,380 --> 03:20:38,980
much like a linguist,

2884
03:20:38,980 --> 03:20:40,900
you are not gonna get it very quickly.

2885
03:20:42,660 --> 03:20:47,540
And lastly, stuff's gonna break.

2886
03:20:47,540 --> 03:20:49,020
You're gonna look at wall of code and you're like,

2887
03:20:49,020 --> 03:20:50,980
oh, I have no idea what's going on.

2888
03:20:50,980 --> 03:20:52,580
Take a deep breath.

2889
03:20:52,580 --> 03:20:54,940
Break that code up into parts.

2890
03:20:54,940 --> 03:20:55,900
Build the code.

2891
03:20:55,900 --> 03:20:58,380
Type it in yourself line by line.

2892
03:20:58,380 --> 03:21:01,900
Figure out what step of the process you're working on.

2893
03:21:01,900 --> 03:21:03,340
Is this visualization?

2894
03:21:03,340 --> 03:21:04,860
Is this data wrangling?

2895
03:21:07,220 --> 03:21:10,060
Separate in a wall of text.

2896
03:21:10,060 --> 03:21:12,020
Must have code to do the machine learning

2897
03:21:12,020 --> 03:21:14,260
or the visualization from nice to have code.

2898
03:21:15,260 --> 03:21:16,780
Most importantly, don't give up.

2899
03:21:16,780 --> 03:21:18,500
Be curious, study hard.

2900
03:21:19,740 --> 03:21:20,900
This be curious part,

2901
03:21:20,900 --> 03:21:23,100
that's crazy important for data scientists.

2902
03:21:24,340 --> 03:21:27,220
And this is your last task of the day.

2903
03:21:27,220 --> 03:21:30,420
Tell yourself, I've got this and repeat.

2904
03:21:30,420 --> 03:21:31,940
I appreciate you folks coming out.

2905
03:21:31,940 --> 03:21:34,500
I'll be here the entire weekend if you run into me

2906
03:21:34,500 --> 03:21:36,340
and you wanna ask questions or talk,

2907
03:21:36,340 --> 03:21:38,660
I'll be happy to kind of chat with you.

2908
03:21:38,660 --> 03:21:41,180
I really appreciate you coming here and being here.

2909
03:21:42,860 --> 03:21:44,060
A nuance for you,

2910
03:21:45,980 --> 03:21:49,700
the funds that you expend here

2911
03:21:49,700 --> 03:21:52,180
go to support the mission of PSF.

2912
03:21:52,180 --> 03:21:53,460
They funnel off a little bit to me

2913
03:21:53,460 --> 03:21:55,500
and I'm eternally grateful to you guys for that.

2914
03:21:55,500 --> 03:21:57,740
But most of the funds that you guys spend

2915
03:21:57,740 --> 03:21:59,740
go to the PSF to help support the mission

2916
03:21:59,740 --> 03:22:01,660
of the Python Software Foundation.

2917
03:22:01,660 --> 03:22:04,300
So being here, you have done a great service

2918
03:22:04,300 --> 03:22:07,060
to helping keep things like PyCon running and on the rails.

2919
03:22:07,060 --> 03:22:09,220
So I appreciate you, thank you.

2920
03:22:09,220 --> 03:22:14,220
I'm gonna put a URL up on the screen.

2921
03:22:18,700 --> 03:22:21,100
They would much appreciate if you guys would take a moment

2922
03:22:21,100 --> 03:22:23,620
to answer a survey monkey.

2923
03:22:24,340 --> 03:22:26,180
With that, we'll call it good.

2924
03:22:26,180 --> 03:22:27,460
I'll stick around.

