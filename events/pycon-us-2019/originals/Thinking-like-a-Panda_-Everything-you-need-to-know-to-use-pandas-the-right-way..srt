1
00:00:00,000 --> 00:00:02,000
more chill.

2
00:00:06,180 --> 00:00:08,280
Â» Hello everyone and welcome to this afternoon's session of

3
00:00:08,280 --> 00:00:13,200
PyCon 2019. Just for the last few people who have walked into

4
00:00:13,200 --> 00:00:16,180
the room, please try to find an empty seat because we cannot

5
00:00:16,180 --> 00:00:18,180
have more people in the room than there are seats.

6
00:00:20,580 --> 00:00:22,140
Look for someone with their hand up and there will be an empty

7
00:00:22,140 --> 00:00:25,600
seat there. If you cannot find an empty seat, then you may have

8
00:00:25,600 --> 00:00:27,600
to find another session just due to fire code regulations.

9
00:00:30,000 --> 00:00:43,000
Okay, so I will be talking about pandas and that does not be in the panda bear.

10
00:00:43,000 --> 00:00:45,600
That means the panda's library.

11
00:00:45,600 --> 00:00:50,640
And the reason I am up here giving this talk today is because when I started working with

12
00:00:50,640 --> 00:01:03,640
pandas probably four years ago-ish now, I was a little lost.

13
00:01:03,640 --> 00:01:07,840
It kind of requires a shift in thinking that may not be always intuitive for somebody who's

14
00:01:07,840 --> 00:01:11,280
worked in Python but hasn't done a lot of data processing.

15
00:01:11,280 --> 00:01:18,240
So I'm hoping that you will get out of this talk today a good method for working with

16
00:01:18,240 --> 00:01:25,080
pandas and how to set yourself up for success.

17
00:01:25,080 --> 00:01:26,080
So who am I?

18
00:01:26,080 --> 00:01:27,080
I'm Hannah.

19
00:01:27,080 --> 00:01:34,520
I worked at Intel building a test library for post silicon validation and I did that

20
00:01:34,520 --> 00:01:41,240
for about six years and one and a half years of that was spent specifically working with

21
00:01:41,240 --> 00:01:43,820
pandas on a data analysis project.

22
00:01:43,820 --> 00:01:50,100
So we had a ton of silicon data and we wanted to do something with it.

23
00:01:50,100 --> 00:01:55,340
That something being like regressions or we got really fancy and we're like we're going

24
00:01:55,340 --> 00:01:59,340
to build models out of it and do some machine learning and stuff.

25
00:01:59,340 --> 00:02:03,700
So it ranged in technicality.

26
00:02:03,700 --> 00:02:10,300
But my non-relevant experience for this talk is I now work at a nonprofit startup called

27
00:02:10,300 --> 00:02:16,620
Hypothesis and we build a annotation tool which is a web application for annotating

28
00:02:16,620 --> 00:02:19,380
web pages.

29
00:02:19,380 --> 00:02:21,340
So what is pandas?

30
00:02:21,340 --> 00:02:26,660
This description quite long winded was pulled from the readme file.

31
00:02:26,660 --> 00:02:33,500
The gist of it is that it's for loading, transforming and dumping data.

32
00:02:33,500 --> 00:02:37,580
But the interesting part I found was actually at the end of this.

33
00:02:37,580 --> 00:02:42,460
So it says additionally it has the broader goal of becoming the most powerful and flexible

34
00:02:42,460 --> 00:02:46,860
open source data analysis manipulation tool available in any language.

35
00:02:46,860 --> 00:02:48,500
I was like huh.

36
00:02:48,500 --> 00:02:50,500
That's kind of interesting.

37
00:02:50,500 --> 00:02:53,700
Because I've only ever heard of it in Python.

38
00:02:53,700 --> 00:02:55,540
So yes indeed.

39
00:02:55,540 --> 00:02:58,660
It is available in other languages.

40
00:02:58,660 --> 00:03:02,900
The Ruby version is a bit of a cheat in my opinion because it's just kind of a wrapper

41
00:03:02,900 --> 00:03:03,980
around Python.

42
00:03:03,980 --> 00:03:05,980
But it's there.

43
00:03:05,980 --> 00:03:11,340
So this was my question when I was going to give this talk.

44
00:03:11,340 --> 00:03:12,460
I was like that's all great.

45
00:03:12,460 --> 00:03:15,100
But why is pandas called pandas?

46
00:03:15,100 --> 00:03:17,100
Like data, pandas.

47
00:03:17,100 --> 00:03:19,700
I don't see the connection here.

48
00:03:19,700 --> 00:03:24,500
But it comes from the term panel data which is like you can make panels out of your larger

49
00:03:24,500 --> 00:03:25,860
panel of data.

50
00:03:25,860 --> 00:03:27,860
And that still really didn't answer my question for me personally.

51
00:03:27,860 --> 00:03:29,180
I was like all right.

52
00:03:29,180 --> 00:03:30,180
Not getting it.

53
00:03:30,180 --> 00:03:35,300
But I guess if you say panel data really fast, it starts to kind of sound like pandas.

54
00:03:35,300 --> 00:03:39,300
That's my working theory.

55
00:03:39,300 --> 00:03:42,660
So what is the underlying data structure?

56
00:03:42,660 --> 00:03:46,020
So pandas is built on NumPy.

57
00:03:46,020 --> 00:03:52,740
And NumPy often operates in C. And if you know anything about C versus Python, C is

58
00:03:52,740 --> 00:03:54,220
generally very fast.

59
00:03:54,220 --> 00:03:57,380
And Python is slow.

60
00:03:57,380 --> 00:03:59,340
And let's talk about why that is a little bit.

61
00:04:00,100 --> 00:04:05,740
So Python has this great garbage collector memory management for you.

62
00:04:05,740 --> 00:04:07,340
But that doesn't come for free.

63
00:04:07,340 --> 00:04:15,220
That means that it takes more memory to store in data type in Python.

64
00:04:15,220 --> 00:04:18,500
And it also takes some CPU cycles.

65
00:04:18,500 --> 00:04:25,080
So how the garbage collector kind of works in a very broad sense is you keep track of

66
00:04:25,080 --> 00:04:27,220
references to your data.

67
00:04:27,220 --> 00:04:30,980
So you end up decrementing and incrementing these counters and references.

68
00:04:30,980 --> 00:04:36,060
And eventually when you no longer have any references to your data, we either can just

69
00:04:36,060 --> 00:04:41,260
destroy that object altogether because it's now like references are zero.

70
00:04:41,260 --> 00:04:44,300
Or if you have some kind of circular loop, that's where the garbage collector comes in

71
00:04:44,300 --> 00:04:49,380
and gets rid of, like knows to destroy that memory.

72
00:04:49,380 --> 00:04:52,780
Whereas C, lovely and fast.

73
00:04:52,780 --> 00:04:55,580
But you have to do all that memory management yourself.

74
00:04:56,580 --> 00:04:58,940
So anyways, moving on.

75
00:04:58,940 --> 00:05:00,740
So this is restaurant data.

76
00:05:00,740 --> 00:05:04,420
This is kind of the data set that I'll be working with throughout this talk.

77
00:05:04,420 --> 00:05:06,220
So we have the restaurant name.

78
00:05:06,220 --> 00:05:08,900
We have the restaurant location.

79
00:05:08,900 --> 00:05:13,180
And the location is supposed to be a geocode, but I kind of like BSed it a little bit because

80
00:05:13,180 --> 00:05:15,180
geocodes are really long.

81
00:05:15,180 --> 00:05:19,860
Then we have a date of the inspection and a score for the inspection.

82
00:05:19,860 --> 00:05:24,100
And if you don't know anything about health inspection data from restaurants, this is

83
00:05:24,100 --> 00:05:25,860
publicly available data.

84
00:05:25,860 --> 00:05:28,060
So the government or city provides it to you.

85
00:05:28,060 --> 00:05:32,140
And it comes in various fun forms to work with.

86
00:05:32,140 --> 00:05:36,380
And my boyfriend is actually the author of sketchyfood.com, which I thought was like

87
00:05:36,380 --> 00:05:40,540
a really cool use case, like restaurant data, because he actually does use pandas.

88
00:05:40,540 --> 00:05:46,660
And so you can see what are the most dirtiest disgusting restaurants in your area.

89
00:05:46,660 --> 00:05:50,380
So here we're importing this into pandas.

90
00:05:50,380 --> 00:05:52,260
We're making a data frame out of it.

91
00:05:52,260 --> 00:05:54,220
That's what it looks like.

92
00:05:54,220 --> 00:05:56,900
So what does this look like underneath?

93
00:05:56,900 --> 00:05:59,620
So it's a bunch of NumPy arrays, essentially.

94
00:05:59,620 --> 00:06:03,900
We have an index, which keeps track of the names of the columns.

95
00:06:03,900 --> 00:06:07,480
And then we have blocks, which is a NumPy array of NumPy arrays.

96
00:06:07,480 --> 00:06:10,500
And if you're thinking to yourself, oh, I've heard those terms in pandas.

97
00:06:10,500 --> 00:06:16,540
No, these are not the index and blocks that you may be familiar with through the API of

98
00:06:16,540 --> 00:06:17,540
pandas.

99
00:06:17,540 --> 00:06:20,620
These are actually the variables in the code that pandas use.

100
00:06:20,620 --> 00:06:21,660
Not related.

101
00:06:21,660 --> 00:06:26,100
So the most important part here that I want you to keep track of is that number up in

102
00:06:26,100 --> 00:06:27,860
the right-hand corner.

103
00:06:27,860 --> 00:06:33,380
This as it is right here takes up 353 bytes.

104
00:06:33,380 --> 00:06:36,380
So now we're going to look at a multi-index data frame.

105
00:06:36,380 --> 00:06:41,940
So what we've done here is we're just taking the restaurant and the location and shoving

106
00:06:41,940 --> 00:06:49,420
that into the index and kind of creating a unique restaurant as our index.

107
00:06:49,740 --> 00:06:52,140
How we do this is a two-step process.

108
00:06:52,140 --> 00:06:54,620
We create the index and then we attach it to the data.

109
00:06:54,620 --> 00:06:57,260
So that's what we're going to do.

110
00:06:57,260 --> 00:07:03,060
So here what this looks like underneath is we have levels, which are the names of the

111
00:07:03,060 --> 00:07:04,060
indexes.

112
00:07:04,060 --> 00:07:06,180
And then we have the values.

113
00:07:06,180 --> 00:07:12,240
The names variable, which keeps track of the values within each of those indexes.

114
00:07:12,240 --> 00:07:15,140
And then those correspond to the labels.

115
00:07:15,140 --> 00:07:18,140
So for example, diner is at index zero.

116
00:07:18,140 --> 00:07:25,060
So we put a zero where it should go in the data section of the index.

117
00:07:25,060 --> 00:07:31,300
And this, hopefully for some of you, you're going, aha, this is compression of data.

118
00:07:31,300 --> 00:07:35,140
Because we're not repeating the string, which takes up a lot more memory.

119
00:07:35,140 --> 00:07:37,940
We're simply repeating an integer.

120
00:07:37,940 --> 00:07:42,820
And also this is why it's very important to normalize your data.

121
00:07:42,820 --> 00:07:46,700
For example, if we have a capital case diner and we have a lower case diner and we also

122
00:07:46,700 --> 00:07:51,220
have this mixed case diner, you're not going to get as much compression.

123
00:07:51,220 --> 00:07:56,420
And so your data frame is going to be a lot bigger than it needs to be.

124
00:07:56,420 --> 00:07:57,540
So we have our index.

125
00:07:57,540 --> 00:08:00,260
So now we can attach this to our data.

126
00:08:00,260 --> 00:08:02,060
This should look familiar.

127
00:08:02,060 --> 00:08:03,060
We still have the index.

128
00:08:03,060 --> 00:08:04,060
We still have the blocks.

129
00:08:04,060 --> 00:08:06,180
We just have less columns now.

130
00:08:06,180 --> 00:08:13,060
But this, unlike the flat data frame, only takes up 261 bytes.

131
00:08:13,060 --> 00:08:20,500
So we've a thirded our memory here just by storing our data in a different format.

132
00:08:20,500 --> 00:08:26,140
And this is the first reason why you should use a multi-index data frame.

133
00:08:26,140 --> 00:08:27,960
So that's great.

134
00:08:27,960 --> 00:08:33,140
But we don't often get, when you load data in pandas, you generally have to load it as

135
00:08:33,140 --> 00:08:35,220
a flat data frame and then convert it.

136
00:08:35,220 --> 00:08:37,540
So this is kind of just how you might do that.

137
00:08:37,540 --> 00:08:41,340
You basically just called set index and you pass in the columns that you want to be indexes.

138
00:08:41,660 --> 00:08:46,460
And the important part here on this slide is the inplace equals true.

139
00:08:46,460 --> 00:08:48,740
And that is very important in pandas.

140
00:08:48,740 --> 00:08:52,780
Because if you don't do that, you will be duplicating your memory, which kind of defeats

141
00:08:52,780 --> 00:08:56,860
the purpose of using a multi-index data frame in the first place.

142
00:08:56,860 --> 00:09:02,220
So it occurs as inplace equals true or copy equals true.

143
00:09:02,220 --> 00:09:07,580
The copy equals false is a little finicky.

144
00:09:07,580 --> 00:09:12,660
I think the documentation says something like it avoids copying when possible, which is

145
00:09:12,660 --> 00:09:13,660
a little vague.

146
00:09:13,660 --> 00:09:16,300
So you have to be a little careful there.

147
00:09:16,300 --> 00:09:18,300
But it occurs.

148
00:09:18,300 --> 00:09:21,060
So loading and normalizing data.

149
00:09:21,060 --> 00:09:26,620
So I'm going to be talking about the read CSV function briefly today because I don't

150
00:09:26,620 --> 00:09:27,820
have a lot of time.

151
00:09:27,820 --> 00:09:34,740
But my recommendation when loading data into pandas is always read the loader's documentation

152
00:09:34,740 --> 00:09:35,980
for the loader that you're using.

153
00:09:36,460 --> 00:09:41,300
Because there are a lot of options in there for kind of like normalizing your data while

154
00:09:41,300 --> 00:09:46,900
you're loading it, which is fantastic because it means you don't have to load as much data

155
00:09:46,900 --> 00:09:48,900
and then sort and filter it later.

156
00:09:51,020 --> 00:09:52,260
All right. So loading data.

157
00:09:52,260 --> 00:09:57,300
So if we just load the raw inspection data from New York City, this is kind of what it

158
00:09:57,300 --> 00:09:59,660
looks like in its raw form.

159
00:09:59,660 --> 00:10:05,140
New York City luckily gives us CSV file, which is a lot nicer than some other cities.

160
00:10:06,060 --> 00:10:11,900
So here we have these not applicable values.

161
00:10:11,900 --> 00:10:19,180
And we also have, you notice, like an empty string for the inspection date.

162
00:10:19,180 --> 00:10:21,580
So raw data is kind of ugly.

163
00:10:21,580 --> 00:10:23,500
And it's important to filter through it.

164
00:10:23,500 --> 00:10:29,420
So in this scenario, what we can do is just mark those with like a none type, essentially.

165
00:10:29,420 --> 00:10:31,220
Be like, we don't have this information.

166
00:10:31,220 --> 00:10:32,940
And we can decide what to do with that later.

167
00:10:32,980 --> 00:10:35,820
Whether we drop this data or keep it and do something else.

168
00:10:35,820 --> 00:10:36,620
I don't know.

169
00:10:36,620 --> 00:10:39,900
But the point being, we want to convert those to NANDs.

170
00:10:39,900 --> 00:10:42,940
And NANDs stands for not a number.

171
00:10:42,940 --> 00:10:45,780
It comes to us from NumPy.

172
00:10:45,780 --> 00:10:54,220
My high level interpretation of it is that it's a none type for pandas.

173
00:10:54,220 --> 00:10:56,620
And so here we're just converting them to NANDs.

174
00:10:56,620 --> 00:10:59,500
So we have this NA values.

175
00:10:59,500 --> 00:11:02,500
And then we pass in like which values we consider to be NANDs.

176
00:11:02,540 --> 00:11:03,700
When we load the data.

177
00:11:05,820 --> 00:11:10,660
It's always really important to understand the library that you're working with and its limitations.

178
00:11:11,500 --> 00:11:18,420
So in this scenario, the question is, OK, so what if we have a restaurant named NAND?

179
00:11:18,420 --> 00:11:19,300
What do you think happens?

180
00:11:21,140 --> 00:11:22,740
It's like, oh, crap.

181
00:11:22,740 --> 00:11:24,540
This is not good.

182
00:11:24,540 --> 00:11:27,140
So it converted to NAND.

183
00:11:27,140 --> 00:11:28,620
Which we don't want to do.

184
00:11:28,780 --> 00:11:32,340
So here we can just say keep default NA equals false.

185
00:11:32,340 --> 00:11:36,620
Which my interpretation is like pandas don't think you're smarter than you are.

186
00:11:37,900 --> 00:11:39,180
And don't do that.

187
00:11:39,180 --> 00:11:47,700
So lastly here, you'll notice that if you just load this raw data and you look at the score column,

188
00:11:47,700 --> 00:11:54,620
which is generally an integer from 0 to 100, at least we hope it is, the data type on it is an object.

189
00:11:54,620 --> 00:11:57,140
And an object in pandas is basically a string.

190
00:11:58,140 --> 00:12:01,060
And so strings are larger than integers.

191
00:12:01,060 --> 00:12:04,460
We would prefer if pandas made it an integer, it would probably save us some memory.

192
00:12:05,460 --> 00:12:06,980
But pandas didn't do that.

193
00:12:06,980 --> 00:12:09,140
And why does it think it's a string?

194
00:12:09,140 --> 00:12:11,420
Pandas tries to infer the data type.

195
00:12:11,420 --> 00:12:16,540
But since we had those not applicable values in that column, even though we converted them to NANDs,

196
00:12:16,540 --> 00:12:20,780
it still kind of interpreted the data type beforehand as a string type.

197
00:12:20,940 --> 00:12:27,620
So what we can do in order to kind of like save memory and get things in the right types to begin with,

198
00:12:27,620 --> 00:12:31,980
when we load the data is to pass in the D type for each of the columns.

199
00:12:31,980 --> 00:12:33,820
So that's what we're doing here.

200
00:12:33,820 --> 00:12:37,740
Notice we convert the date string into a date time.

201
00:12:37,740 --> 00:12:41,740
We're using an int date, which is like the smallest integer we can use.

202
00:12:41,740 --> 00:12:43,620
And the other one's string.

203
00:12:43,620 --> 00:12:48,900
Okay, so here is kind of a table of, sorry it's a little fuzzy,

204
00:12:49,580 --> 00:12:52,300
here's a table of the different data types.

205
00:12:52,300 --> 00:12:57,020
So we have pandas data types, python types, and then the NumPy types.

206
00:12:57,020 --> 00:13:01,780
So you're like, this is great, but why is this important?

207
00:13:01,780 --> 00:13:06,860
So I mentioned earlier that we go from python to NumPy to C.

208
00:13:06,860 --> 00:13:12,660
And ideally we'd like to run in C as much as possible, but we can only do that if our code is translatable into C,

209
00:13:12,660 --> 00:13:15,900
which means that date time is not really translatable into C.

210
00:13:15,900 --> 00:13:22,020
So what we should probably do if we have any date times is convert them to like a millisecond set the epoch or something,

211
00:13:22,020 --> 00:13:25,500
like an integer value, which is easily translatable into C.

212
00:13:25,500 --> 00:13:28,380
Thus our code will be more performant.

213
00:13:28,380 --> 00:13:33,100
So what does this look like in terms of the actual memory savings that we get?

214
00:13:33,100 --> 00:13:40,140
So if we look at that score, it was originally an object which took up 232 bytes.

215
00:13:40,140 --> 00:13:43,020
We made an integer and now we're taking up 108 bytes.

216
00:13:43,020 --> 00:13:45,580
So that's like we have to our memory, which is great.

217
00:13:45,580 --> 00:13:47,500
And this is only four rows.

218
00:13:47,500 --> 00:13:50,660
Keeping in mind, we're probably not dealing with four rows.

219
00:13:50,660 --> 00:13:51,460
We're working pandas.

220
00:13:51,460 --> 00:13:52,820
We're doing big data processing.

221
00:13:52,820 --> 00:13:54,660
We probably have thousands of rows.

222
00:13:54,660 --> 00:13:57,260
So this is a huge memory savings.

223
00:13:57,260 --> 00:13:57,860
All right.

224
00:13:57,860 --> 00:14:00,860
Analyzing and manipulating data.

225
00:14:00,860 --> 00:14:05,220
There are two documentation links here.

226
00:14:05,220 --> 00:14:06,980
The first one is basic indexing.

227
00:14:06,980 --> 00:14:09,460
Like you're a beginner, you know nothing about pandas.

228
00:14:09,460 --> 00:14:11,940
How do you get your data out of it?

229
00:14:11,980 --> 00:14:15,140
The second one is one that I refer to a lot.

230
00:14:15,140 --> 00:14:16,940
Merging, joining, concatenating, because I

231
00:14:16,940 --> 00:14:18,340
can't remember the difference.

232
00:14:18,340 --> 00:14:20,580
So I always have it open.

233
00:14:20,580 --> 00:14:22,140
So indexing.

234
00:14:22,140 --> 00:14:26,420
Pandas has this really nice what I like to call dictionary syntax, which you

235
00:14:26,420 --> 00:14:28,940
should be familiar with if you're familiar with Python.

236
00:14:28,940 --> 00:14:32,900
So you can basically treat the column name as a key.

237
00:14:32,900 --> 00:14:38,620
And if we want to get all the scores, we can say df of score,

238
00:14:38,620 --> 00:14:41,140
get all the scores back for the restaurants.

239
00:14:41,140 --> 00:14:44,540
We can also do something a lot fancier than that with Boolean expressions.

240
00:14:44,540 --> 00:14:53,020
So we can grab all of the restaurants with scores greater than or equal to 90

241
00:14:53,020 --> 00:14:54,980
after April 18.

242
00:14:58,140 --> 00:15:02,300
So there's also iloke and loke that you can use for grabbing data out

243
00:15:02,300 --> 00:15:04,060
of your data frame.

244
00:15:04,060 --> 00:15:05,940
A lot of people can't remember the difference.

245
00:15:05,940 --> 00:15:09,900
The way I remember it is i stands for integer.

246
00:15:09,900 --> 00:15:14,020
And that's kind of what we use to get the data out with iloke.

247
00:15:14,020 --> 00:15:19,500
So you can grab row zero out of the data.

248
00:15:19,500 --> 00:15:22,420
You can also grab even rows out of the data.

249
00:15:22,420 --> 00:15:24,220
You can also use a list syntax.

250
00:15:24,220 --> 00:15:28,300
So you can use a colon comma, sorry, like colon to 10.

251
00:15:28,300 --> 00:15:32,620
And it'll give you back zero to 10 rows of your data set.

252
00:15:32,620 --> 00:15:35,300
I don't really find myself using this a lot in practice,

253
00:15:35,300 --> 00:15:38,900
because I'm not usually after a particular row in the data.

254
00:15:38,900 --> 00:15:43,100
But here, maybe the inspection date is wrong,

255
00:15:43,100 --> 00:15:45,940
and you want to modify it to be correct.

256
00:15:45,940 --> 00:15:46,780
You can do that.

257
00:15:49,220 --> 00:15:50,740
All right.

258
00:15:50,740 --> 00:15:52,820
Oops, going in, cutting out again.

259
00:15:52,820 --> 00:15:59,140
So loke, it has two things here.

260
00:15:59,140 --> 00:16:03,140
You can specify the index and then comma the columns.

261
00:16:03,140 --> 00:16:05,020
So in the first scenario, and if you

262
00:16:05,020 --> 00:16:06,440
don't want to specify one of them,

263
00:16:06,440 --> 00:16:08,580
you can either use like colon comma.

264
00:16:08,580 --> 00:16:10,260
If you don't want to specify the index,

265
00:16:10,260 --> 00:16:14,020
or you can just leave off the columns altogether, which

266
00:16:14,020 --> 00:16:15,860
is what we're doing in the first scenario.

267
00:16:15,860 --> 00:16:20,660
So here, we're just grabbing all of the data for diner.

268
00:16:20,660 --> 00:16:25,700
Second example, we are grabbing a particular diner restaurant,

269
00:16:25,700 --> 00:16:28,300
assuming there are multiple locations for a diner,

270
00:16:28,300 --> 00:16:32,300
and all of the scores for it.

271
00:16:32,300 --> 00:16:35,060
Again, we can also do something super complicated and fancy

272
00:16:35,060 --> 00:16:36,500
with Boolean expressions.

273
00:16:36,500 --> 00:16:40,620
So here, we're grabbing all of the restaurants

274
00:16:40,620 --> 00:16:43,580
with scores greater than or equal to 90.

275
00:16:43,580 --> 00:16:46,260
And I've turned something in here a little bit extra.

276
00:16:46,260 --> 00:16:48,420
So index, get level values, restaurant.

277
00:16:48,420 --> 00:16:50,960
So basically, we only care about the names of the restaurants.

278
00:16:50,960 --> 00:16:55,460
We don't want the whole entire sub data frame.

279
00:16:55,460 --> 00:16:56,100
OK, merging.

280
00:16:56,100 --> 00:17:00,900
So the merge for pandas pretty much works like the merge for,

281
00:17:01,140 --> 00:17:04,060
if you're familiar with database merges, same thing.

282
00:17:04,060 --> 00:17:06,460
It has inner, outer, left, and right.

283
00:17:06,460 --> 00:17:10,260
And I'm going to go over inner and outer today.

284
00:17:10,260 --> 00:17:14,300
So say we have old data and new data,

285
00:17:14,300 --> 00:17:21,500
and we want to get the data in common between those two sets.

286
00:17:21,500 --> 00:17:24,940
We can use an inner merge to do that.

287
00:17:24,940 --> 00:17:30,980
Alternatively, if we have the old data

288
00:17:30,980 --> 00:17:33,660
and we want to update it with a new data,

289
00:17:33,660 --> 00:17:37,380
and the new data may also contain the old data,

290
00:17:37,380 --> 00:17:43,140
we don't want to repeat that data in common.

291
00:17:43,140 --> 00:17:45,180
So we can use an outer merge for that.

292
00:17:48,220 --> 00:17:50,540
And one thing before I move on.

293
00:17:50,540 --> 00:17:52,980
So you'll notice that these data frames are not

294
00:17:52,980 --> 00:17:54,740
multi-indexed.

295
00:17:54,740 --> 00:17:58,020
Merge doesn't work with multi-indexed data frames.

296
00:17:58,020 --> 00:18:00,660
You have to have a flat data frame to use merge.

297
00:18:00,660 --> 00:18:04,500
So join does work with multi-indexed data frames.

298
00:18:04,500 --> 00:18:10,380
So here, we, and this is actually the case, generally,

299
00:18:10,380 --> 00:18:13,940
the cities don't provide the geolocations

300
00:18:13,940 --> 00:18:15,140
of the restaurants.

301
00:18:15,140 --> 00:18:17,140
You have to look at that information separately.

302
00:18:17,140 --> 00:18:20,420
So here we have the restaurant health inspection data,

303
00:18:20,420 --> 00:18:22,380
and then we have another table that

304
00:18:22,380 --> 00:18:24,620
contains the locations of all those restaurants,

305
00:18:24,620 --> 00:18:26,620
and we're just joining them together.

306
00:18:29,700 --> 00:18:31,300
And you'll notice that we don't have

307
00:18:31,300 --> 00:18:33,900
to specify the index to join on.

308
00:18:33,900 --> 00:18:37,300
The tables are already indexed, and join automatically

309
00:18:37,300 --> 00:18:39,380
determines that for us.

310
00:18:39,380 --> 00:18:44,460
The next line there is just when we do this join,

311
00:18:44,460 --> 00:18:50,060
the location ends up being a column in the data

312
00:18:50,060 --> 00:18:51,100
as opposed to an index.

313
00:18:51,100 --> 00:18:53,580
So we're just telling it after the fact

314
00:18:53,580 --> 00:18:54,540
to set it as an index.

315
00:18:57,180 --> 00:18:58,180
Concatenate.

316
00:18:58,180 --> 00:19:02,300
So we can also concatenate and repeat data if we need to.

317
00:19:02,300 --> 00:19:06,300
So here, we have inspection data from two different cities.

318
00:19:06,300 --> 00:19:10,060
And while the line is the same, chances

319
00:19:10,060 --> 00:19:13,340
are that's actually a different restaurant.

320
00:19:13,340 --> 00:19:15,620
So this is in a different city.

321
00:19:15,620 --> 00:19:17,860
So here, we're actually concatenating the data

322
00:19:17,860 --> 00:19:19,900
together because we do want the data to be repeated.

323
00:19:22,580 --> 00:19:23,140
Apply.

324
00:19:23,140 --> 00:19:27,620
So apply is probably one of the most overused and incorrectly

325
00:19:27,620 --> 00:19:31,220
used functions in pandas, I think.

326
00:19:31,220 --> 00:19:33,860
You really shouldn't be using apply ever

327
00:19:33,860 --> 00:19:37,420
unless you have to write a custom function,

328
00:19:37,420 --> 00:19:39,340
and you have to apply it to every single row

329
00:19:39,340 --> 00:19:41,860
or every single column in your data set.

330
00:19:41,860 --> 00:19:44,260
And this is an example of such a case.

331
00:19:44,260 --> 00:19:50,460
So we're trying to just convert the score into a letter grade

332
00:19:50,460 --> 00:19:52,460
for each of the restaurants.

333
00:19:52,460 --> 00:19:54,380
And so here, we can use apply to do that.

334
00:19:54,380 --> 00:19:58,180
So we literally want to take every single value in the score

335
00:19:58,180 --> 00:20:01,500
column and convert it to a letter.

336
00:20:01,500 --> 00:20:04,380
And so here, we make a grade function,

337
00:20:04,380 --> 00:20:07,220
and then we pass in that function into the apply.

338
00:20:07,220 --> 00:20:09,860
And then we say access equals 1.

339
00:20:09,860 --> 00:20:13,700
So you can say access equals 0 or access equals 1.

340
00:20:13,700 --> 00:20:15,380
Access equals 1 in this case because we

341
00:20:15,380 --> 00:20:17,580
want to apply it to every single value in the row.

342
00:20:21,340 --> 00:20:26,580
Every single value in the column, meaning we want to go per row.

343
00:20:26,580 --> 00:20:30,740
So here, this is a great example of when not to use apply.

344
00:20:30,740 --> 00:20:34,340
And unfortunately, this isn't exactly in the documentation

345
00:20:34,340 --> 00:20:34,860
anymore.

346
00:20:34,860 --> 00:20:39,740
I think they have like mp.sum, but this is pretty much

347
00:20:39,740 --> 00:20:42,420
from the documentation.

348
00:20:42,420 --> 00:20:43,980
But you don't want to do this.

349
00:20:43,980 --> 00:20:45,720
So let's talk about why that is.

350
00:20:45,720 --> 00:20:50,740
So here, we can do a df apply and pass in the mp.mean function.

351
00:20:50,740 --> 00:20:52,660
And specify the access.

352
00:20:52,660 --> 00:20:56,500
And we can calculate the mean score

353
00:20:56,500 --> 00:20:57,820
for each of the restaurants.

354
00:20:57,820 --> 00:20:59,380
And that's fine.

355
00:20:59,380 --> 00:21:00,020
It works.

356
00:21:00,020 --> 00:21:02,660
It's probably pretty slow, but it works.

357
00:21:02,660 --> 00:21:04,780
Alternatively, you may notice that there

358
00:21:04,780 --> 00:21:08,820
are functions that exist off the data frame as well.

359
00:21:08,820 --> 00:21:10,660
They're kind of built into pandas.

360
00:21:10,660 --> 00:21:13,980
So we also have a data frame.mean that we can call.

361
00:21:13,980 --> 00:21:15,220
And again, it works.

362
00:21:15,220 --> 00:21:18,140
They do the same thing, ultimately.

363
00:21:18,140 --> 00:21:21,580
But one of them is much slower.

364
00:21:21,580 --> 00:21:24,500
The first case is two times as slow.

365
00:21:24,500 --> 00:21:26,380
And let's talk about why that is.

366
00:21:26,380 --> 00:21:33,300
So the df.apply several layers down

367
00:21:33,300 --> 00:21:38,260
ends up ultimately calling into what df.mean calls

368
00:21:38,260 --> 00:21:42,500
into almost instantly, which is ultimately like a numpy mean.

369
00:21:45,060 --> 00:21:50,660
But the difference is that df.apply loops in Python

370
00:21:50,660 --> 00:21:53,820
and calls into mp.mean on every single value,

371
00:21:53,820 --> 00:21:57,900
whereas df.mean calls into numpy with all the values

372
00:21:57,900 --> 00:22:01,220
and loops in C, and then returns the result.

373
00:22:01,220 --> 00:22:04,460
So it's like looping in Python versus looping in C.

374
00:22:04,460 --> 00:22:06,100
Which one is faster?

375
00:22:06,100 --> 00:22:07,860
I'm just shocked.

376
00:22:07,860 --> 00:22:13,740
So here, again, this isn't 2x slower forever.

377
00:22:13,740 --> 00:22:17,060
This is like n number of rows slower,

378
00:22:17,060 --> 00:22:18,700
which is pretty substantial.

379
00:22:18,700 --> 00:22:19,900
So don't do that.

380
00:22:19,900 --> 00:22:24,820
If you have a function off the data frame, use it.

381
00:22:24,820 --> 00:22:25,660
Groupby.

382
00:22:25,660 --> 00:22:28,980
So chances are, if you're working with a large data set,

383
00:22:28,980 --> 00:22:30,300
at some point, you're going to have

384
00:22:30,300 --> 00:22:33,580
to run a groupby, which is basically put our data

385
00:22:33,580 --> 00:22:36,900
into groups and run some kind of aggregation function over it.

386
00:22:36,940 --> 00:22:42,460
So in this example, we're grouping our restaurants

387
00:22:42,460 --> 00:22:45,780
uniquely, and then we're counting up

388
00:22:45,780 --> 00:22:47,700
the number of scores that we are,

389
00:22:47,700 --> 00:22:49,220
a number of inspections that have

390
00:22:49,220 --> 00:22:50,580
been done on each restaurant.

391
00:22:53,780 --> 00:22:55,540
And ta-da, it works.

392
00:22:55,540 --> 00:22:57,580
So we have our groupby.

393
00:22:57,580 --> 00:22:59,620
We pass in the indexes, which happen

394
00:22:59,620 --> 00:23:02,900
to be the same indexes as we've indexed our multi-index data

395
00:23:02,900 --> 00:23:04,180
frame by.

396
00:23:04,180 --> 00:23:06,420
And then you have an aggregation function at the end.

397
00:23:06,420 --> 00:23:07,980
In this case, count.

398
00:23:07,980 --> 00:23:09,500
And then I'm just doing something

399
00:23:09,500 --> 00:23:12,180
really subtle at the end here.

400
00:23:12,180 --> 00:23:14,260
Because we're counting the scores,

401
00:23:14,260 --> 00:23:18,700
it's going to output the name score for the data instead

402
00:23:18,700 --> 00:23:21,660
of what we'd like, ideally, now, to describe our data

403
00:23:21,660 --> 00:23:22,540
as the total.

404
00:23:22,540 --> 00:23:24,300
It's no longer a score.

405
00:23:24,300 --> 00:23:25,500
So I'm just renaming it.

406
00:23:28,060 --> 00:23:31,020
Here, this is something that I see often people

407
00:23:31,020 --> 00:23:34,020
do a lot, which is a mistake.

408
00:23:34,020 --> 00:23:37,100
The groupby will also return an iterable,

409
00:23:37,100 --> 00:23:39,740
which in turn will let you iterate over the groups

410
00:23:39,740 --> 00:23:43,500
and call a function on them.

411
00:23:43,500 --> 00:23:44,780
You can do that.

412
00:23:44,780 --> 00:23:47,300
It's really slow, but you can do it.

413
00:23:47,300 --> 00:23:49,540
The difference between this slide and the last slide,

414
00:23:49,540 --> 00:23:52,220
though, is the last slide actually returned a series

415
00:23:52,220 --> 00:23:54,160
to us, or a smaller data frame.

416
00:23:54,160 --> 00:23:56,380
It didn't put the data back into the original data set,

417
00:23:56,380 --> 00:23:58,180
whereas here, we are putting the data back

418
00:23:58,180 --> 00:23:59,220
into the original data set.

419
00:23:59,220 --> 00:24:01,500
So there is a subtle difference there.

420
00:24:01,500 --> 00:24:03,940
But in terms of time, and of course,

421
00:24:03,940 --> 00:24:06,300
we didn't have to put the data back into the original data

422
00:24:06,300 --> 00:24:06,800
set.

423
00:24:06,800 --> 00:24:09,900
We could have made a mini one as well.

424
00:24:09,900 --> 00:24:12,820
But this is two times as slow.

425
00:24:12,820 --> 00:24:15,580
And it's, again, because we're looping over those groups

426
00:24:15,580 --> 00:24:19,100
in Python, as opposed to calculating the groups

427
00:24:19,100 --> 00:24:21,220
and calculating the data and then sending that

428
00:24:21,220 --> 00:24:24,580
to the count function in C land.

429
00:24:24,580 --> 00:24:27,020
So again, this isn't too excellent forever,

430
00:24:27,020 --> 00:24:28,980
depending on how many groups you have,

431
00:24:28,980 --> 00:24:32,180
this will increase accordingly.

432
00:24:32,180 --> 00:24:32,900
Pivot tables.

433
00:24:32,900 --> 00:24:36,740
So a pivot table is another one of those,

434
00:24:36,740 --> 00:24:40,780
what I would call overused functions in pandas.

435
00:24:40,780 --> 00:24:43,060
My recommendation is you really shouldn't be using it

436
00:24:43,060 --> 00:24:45,020
unless you're doing it as an initial first step.

437
00:24:45,020 --> 00:24:48,780
So it's really great if you have a huge set of flat data

438
00:24:48,780 --> 00:24:53,060
and you want to just completely reformat it and aggregate it

439
00:24:53,060 --> 00:24:54,860
and do a bunch of crazy stuff initially.

440
00:24:54,860 --> 00:24:56,660
I think it's great for that.

441
00:24:56,660 --> 00:24:59,020
It gets the job done a lot faster than doing all those

442
00:24:59,020 --> 00:25:01,260
steps incrementally.

443
00:25:01,260 --> 00:25:05,860
But if you are using it as a substitute for a group

444
00:25:05,860 --> 00:25:08,020
by in your code, don't do that.

445
00:25:08,020 --> 00:25:09,460
And I will show you why.

446
00:25:09,460 --> 00:25:13,180
So here, we're just essentially performing

447
00:25:13,180 --> 00:25:15,260
that same group by operation that we looked at earlier,

448
00:25:15,260 --> 00:25:17,420
where we're counting up the number of inspections

449
00:25:17,420 --> 00:25:19,420
per restaurant.

450
00:25:19,420 --> 00:25:21,980
And here, again, we can do this with a group by as well.

451
00:25:21,980 --> 00:25:25,220
This is the same group by scenario.

452
00:25:25,220 --> 00:25:27,620
But one of these is really slow.

453
00:25:27,620 --> 00:25:29,820
So pivot table is two times as slow.

454
00:25:29,820 --> 00:25:32,580
And I'm cheating a little bit here

455
00:25:32,580 --> 00:25:35,060
because pivot table is a flat data frame.

456
00:25:35,060 --> 00:25:38,660
And the group by case is a multi-index data frame.

457
00:25:38,660 --> 00:25:42,020
But that's the advantage of using a multi-index data frame,

458
00:25:42,020 --> 00:25:44,100
is that you can do group bys a lot more quickly.

459
00:25:44,100 --> 00:25:48,500
So this 2x comes because the multi-index data frame is

460
00:25:48,500 --> 00:25:49,380
already indexed.

461
00:25:49,380 --> 00:25:52,980
We've already calculated those unique pairs and stuff.

462
00:25:52,980 --> 00:25:56,280
Whereas in this scenario, with a flat data frame,

463
00:25:56,280 --> 00:25:57,460
we haven't done that.

464
00:25:57,460 --> 00:26:02,780
And so pivot table has to do that on top of it.

465
00:26:02,780 --> 00:26:07,580
So and this leads me to my next topic, which is choosing

466
00:26:07,580 --> 00:26:09,260
the right data frame.

467
00:26:09,260 --> 00:26:11,740
So depending on what you're going

468
00:26:11,740 --> 00:26:13,980
to be doing with the data, you may

469
00:26:13,980 --> 00:26:16,860
choose a different format to represent your data in

470
00:26:16,860 --> 00:26:20,060
and pass it around your code.

471
00:26:20,060 --> 00:26:24,540
So if we look at the flat data frame scenario,

472
00:26:24,540 --> 00:26:28,340
this is what it would take to get our data

473
00:26:28,340 --> 00:26:30,820
to calculate the number of inspections per restaurant

474
00:26:30,820 --> 00:26:33,580
and get it back into the original data set.

475
00:26:33,580 --> 00:26:34,940
Slot code.

476
00:26:34,940 --> 00:26:36,380
We got to do a group by.

477
00:26:36,380 --> 00:26:39,100
And then you'll notice we have this as index equals false

478
00:26:39,100 --> 00:26:39,700
in there.

479
00:26:39,700 --> 00:26:43,740
That basically just means that when group by returns

480
00:26:43,740 --> 00:26:45,820
the data frame, it doesn't return it

481
00:26:45,820 --> 00:26:47,060
as a multi-index data frame.

482
00:26:47,060 --> 00:26:48,940
It returns it as a flat data frame, which just makes it

483
00:26:48,940 --> 00:26:52,700
easier for us to merge it back into the original data set.

484
00:26:52,700 --> 00:26:54,420
But again, we're renaming the columns.

485
00:26:54,420 --> 00:26:56,940
And then we're merging them back together.

486
00:26:56,940 --> 00:27:00,380
We can get our results.

487
00:27:00,380 --> 00:27:04,460
However, if we chose this format instead,

488
00:27:04,460 --> 00:27:08,420
which is a multi-indexed, multi-level column data frame,

489
00:27:08,420 --> 00:27:10,700
sounds very fancy.

490
00:27:10,700 --> 00:27:14,540
So here, again, we have the unique restaurants in the index.

491
00:27:14,540 --> 00:27:18,140
And we have multi-level columns as well

492
00:27:18,140 --> 00:27:21,380
as multi-level indexes.

493
00:27:21,380 --> 00:27:25,860
So here, we've split out each inspection

494
00:27:25,860 --> 00:27:29,100
into its own column by inspection date.

495
00:27:29,100 --> 00:27:32,380
And then the score is represented as the data point

496
00:27:32,380 --> 00:27:33,260
in each of those.

497
00:27:33,260 --> 00:27:34,780
And you'll notice that we have some nans in there,

498
00:27:34,780 --> 00:27:38,260
because not every restaurant was inspected at the same time.

499
00:27:38,260 --> 00:27:39,940
But these are integers.

500
00:27:39,940 --> 00:27:44,100
They don't really take up a lot of space.

501
00:27:44,100 --> 00:27:46,540
It's not really too big of a deal.

502
00:27:46,620 --> 00:27:50,660
So in order to get that same result of counting

503
00:27:50,660 --> 00:27:53,420
the number of inspections per restaurant,

504
00:27:53,420 --> 00:27:55,780
this is really simple now.

505
00:27:55,780 --> 00:27:57,820
We don't have to run a group by at all.

506
00:27:57,820 --> 00:27:59,700
We can just run a df.count.

507
00:27:59,700 --> 00:28:01,460
And you'll notice that that function is right

508
00:28:01,460 --> 00:28:02,900
off the data frame.

509
00:28:02,900 --> 00:28:05,580
So it means we're, again, looping in C.

510
00:28:05,580 --> 00:28:10,820
And we only have really two rows to loop over here anyway.

511
00:28:10,820 --> 00:28:12,900
So it's really fast.

512
00:28:12,900 --> 00:28:14,300
How much faster is it?

513
00:28:14,300 --> 00:28:21,140
So here, there's a difference of 4x in this particular example.

514
00:28:21,140 --> 00:28:24,340
Obviously, the more groups you have, the more rows you have,

515
00:28:24,340 --> 00:28:27,620
the more that's going to grow, which

516
00:28:27,620 --> 00:28:30,220
is why it's very important to choose

517
00:28:30,220 --> 00:28:35,580
the appropriate format for your data frame initially.

518
00:28:35,580 --> 00:28:38,340
Don't be afraid to spend some time thinking about that

519
00:28:38,340 --> 00:28:42,540
and the ways you're going to need to transform your data

520
00:28:42,540 --> 00:28:46,260
and how your data frame will affect that.

521
00:28:46,260 --> 00:28:54,060
So lastly, we looked at that apply case earlier.

522
00:28:54,060 --> 00:28:57,760
And there are some scenarios, like that example

523
00:28:57,760 --> 00:29:01,540
that I showed, where you just have to use apply.

524
00:29:01,540 --> 00:29:03,740
You have this custom function.

525
00:29:03,740 --> 00:29:08,920
You have to iterate over every row in your data set.

526
00:29:08,920 --> 00:29:09,900
You have to apply it.

527
00:29:09,900 --> 00:29:11,180
It's just the way it is.

528
00:29:11,180 --> 00:29:13,340
So how can we improve the performance of that?

529
00:29:13,340 --> 00:29:14,920
Because that ends up being really slow

530
00:29:14,920 --> 00:29:16,460
for large sets of data.

531
00:29:16,460 --> 00:29:21,140
So this code is actually the code in Pandas,

532
00:29:21,140 --> 00:29:25,060
the implementation for apply deep down.

533
00:29:25,060 --> 00:29:26,340
So we have this loop.

534
00:29:26,340 --> 00:29:28,620
It iterates over series generator.

535
00:29:28,620 --> 00:29:32,940
So that's either iterating over columns or rows.

536
00:29:32,940 --> 00:29:37,260
And it grabs our value out of that iterator.

537
00:29:37,260 --> 00:29:40,180
And then we have this self.f, which

538
00:29:40,180 --> 00:29:42,580
is the function that you passed into apply.

539
00:29:42,580 --> 00:29:43,860
And it passes in the value.

540
00:29:43,860 --> 00:29:44,900
And it gets the results.

541
00:29:44,900 --> 00:29:47,860
And then does some other appending whatever.

542
00:29:47,860 --> 00:29:51,060
But the point is, it's a very simple implementation.

543
00:29:51,060 --> 00:29:53,860
It's not really that hard to implement yourself.

544
00:29:53,860 --> 00:29:56,660
So that's what we're going to do.

545
00:29:56,660 --> 00:29:58,180
We know that C is fast.

546
00:29:58,180 --> 00:30:01,620
We know that Pandas has some built-in functions that

547
00:30:01,620 --> 00:30:04,620
essentially implement apply for us.

548
00:30:04,620 --> 00:30:06,700
And so we're just going to implement our own custom

549
00:30:06,700 --> 00:30:08,780
apply that's super fast.

550
00:30:08,780 --> 00:30:12,220
So here we are going to use Cython to do that.

551
00:30:12,220 --> 00:30:15,420
So we're going to write Python code.

552
00:30:15,420 --> 00:30:18,060
And then we're going to have Cython compile it into C.

553
00:30:18,060 --> 00:30:22,020
And then we're going to call our C function from Python.

554
00:30:22,020 --> 00:30:27,660
So here we've implemented our C function.

555
00:30:27,660 --> 00:30:29,620
So we're looping for i in range.

556
00:30:29,620 --> 00:30:31,080
And we're keeping it really simple.

557
00:30:31,080 --> 00:30:33,500
Because the more Python data types

558
00:30:33,500 --> 00:30:37,760
that we try to pass into C land, the more complicated

559
00:30:37,760 --> 00:30:41,120
and convoluted and potentially not C it's going to be.

560
00:30:41,120 --> 00:30:46,840
So we're just passing in essentially an array of values.

561
00:30:46,840 --> 00:30:49,480
And we're looping over that array of values.

562
00:30:49,480 --> 00:30:50,800
And for each one of those values,

563
00:30:50,800 --> 00:30:56,640
we're changing it to be a letter instead of a number.

564
00:30:56,640 --> 00:31:00,760
So here how we do this from the Python land of things

565
00:31:00,760 --> 00:31:03,200
is we take our score column.

566
00:31:03,200 --> 00:31:05,120
We convert it to a list.

567
00:31:05,120 --> 00:31:07,080
We call our grade function and note

568
00:31:07,080 --> 00:31:08,440
that we're doing this in place.

569
00:31:08,440 --> 00:31:10,160
So we're not actually getting anything back.

570
00:31:10,160 --> 00:31:12,120
We're just modifying the scores in place.

571
00:31:12,120 --> 00:31:16,520
And then we create a series out of it.

572
00:31:16,520 --> 00:31:20,400
We connect the index that we were using before to it.

573
00:31:20,400 --> 00:31:25,160
And then we update our data frame with our new data.

574
00:31:25,160 --> 00:31:28,200
So how much faster is this?

575
00:31:28,200 --> 00:31:30,760
This is an exciting moment.

576
00:31:30,760 --> 00:31:34,320
So this is literally how much faster it is.

577
00:31:34,320 --> 00:31:37,600
With 40,000 rows, it takes about four minutes

578
00:31:37,600 --> 00:31:39,400
to run the Panda supply.

579
00:31:39,400 --> 00:31:41,160
And it takes like five milliseconds

580
00:31:41,160 --> 00:31:45,000
to run the custom apply.

581
00:31:45,000 --> 00:31:49,200
So highly recommend doing this if you

582
00:31:49,200 --> 00:31:52,120
run into a case where you have to use applying your code.

583
00:31:52,120 --> 00:31:55,280
So in summary, avoid looping.

584
00:31:55,280 --> 00:31:56,960
Normalize your data.

585
00:31:56,960 --> 00:31:58,880
Use multi-index data frames.

586
00:31:58,880 --> 00:32:00,440
Use in place operations.

587
00:32:00,440 --> 00:32:04,560
And take advantage of Cython when you need to.

588
00:32:04,560 --> 00:32:06,000
And that is it.

589
00:32:06,000 --> 00:32:18,440
Thank you very much, Hannah.

590
00:32:18,440 --> 00:32:20,640
If anyone has questions, there are microphones located

591
00:32:20,640 --> 00:32:22,640
in the aisles here.

592
00:32:22,640 --> 00:32:23,400
Please queue up.

593
00:32:23,400 --> 00:32:26,760
Please be asking questions, not raising comments or telling

594
00:32:26,760 --> 00:32:27,360
stories, please.

595
00:32:30,440 --> 00:32:46,240
Question over here.

596
00:32:51,480 --> 00:32:56,520
Instead of using .apply, would you be able to use .map?

597
00:32:56,520 --> 00:32:58,880
And how much faster would .map be

598
00:32:58,920 --> 00:33:02,840
if it's only iterating through a particular column?

599
00:33:02,840 --> 00:33:04,640
That's a good question.

600
00:33:04,640 --> 00:33:05,440
I'm not sure.

601
00:33:05,440 --> 00:33:08,760
I'd have to look at .map in the implementation.

602
00:33:08,760 --> 00:33:10,640
Yeah.

603
00:33:10,640 --> 00:33:11,280
I don't know.

604
00:33:20,080 --> 00:33:24,640
So for that particular example, I mean, for one,

605
00:33:24,640 --> 00:33:26,800
.apply when you use access is equal to 1

606
00:33:26,800 --> 00:33:30,320
is essentially an iterose.

607
00:33:30,320 --> 00:33:32,880
In that particular example, you could have just used the

608
00:33:32,880 --> 00:33:37,720
numpy.where and just created a series of mass and replaced

609
00:33:37,720 --> 00:33:41,600
the values where it was true and where it was false without

610
00:33:41,600 --> 00:33:42,840
having to go into Cython.

611
00:33:45,640 --> 00:33:49,320
So you're saying looping over the columns instead?

612
00:33:49,320 --> 00:33:52,480
So you don't need a loop at all, honestly, in Python.

613
00:33:52,480 --> 00:33:54,440
All you needed to do in that situation, if you had that

614
00:33:54,480 --> 00:33:57,480
example, you said less than 70 and greater than 80 or

615
00:33:57,480 --> 00:33:58,600
something like that.

616
00:33:58,600 --> 00:34:02,760
So if you use a numpy.where, you can replace, or even Pandas

617
00:34:02,760 --> 00:34:04,440
has it, it's a little slightly slower.

618
00:34:04,440 --> 00:34:08,480
But if you do a numpy.where and first condition with the

619
00:34:08,480 --> 00:34:11,760
condition, it returns you an array of Booleans.

620
00:34:11,760 --> 00:34:13,000
Yeah.

621
00:34:16,120 --> 00:34:20,760
I suppose, why would you ever use .apply?

622
00:34:20,760 --> 00:34:23,880
So I mean, there are definitely, if you have a

623
00:34:23,880 --> 00:34:28,720
custom function, can you, in your scenario, can you pass in

624
00:34:28,720 --> 00:34:29,800
a custom function?

625
00:34:29,800 --> 00:34:33,680
Even though I know you're mapping the Booleans, but are

626
00:34:33,680 --> 00:34:36,320
you applying a custom Python function?

627
00:34:40,880 --> 00:34:43,560
I'm just trying to think of any way where you're actually

628
00:34:43,560 --> 00:34:46,240
looking at the particular, any situation where you really

629
00:34:46,240 --> 00:34:48,960
have to look at the row.

630
00:34:48,960 --> 00:34:50,400
Yeah.

631
00:34:50,400 --> 00:34:53,160
Each an individual row.

632
00:34:53,160 --> 00:35:00,720
I mean, I think in that scenario, it's a conversion of

633
00:35:00,720 --> 00:35:05,760
a value that's in every row of your data.

634
00:35:05,760 --> 00:35:13,400
So I think in that scenario, yeah, you just have to do that.

635
00:35:13,400 --> 00:35:17,280
But yeah, it really depends on your data set.

636
00:35:17,280 --> 00:35:19,040
Maybe you don't have to do that for your data set.

637
00:35:19,840 --> 00:35:20,320
OK.

638
00:35:20,320 --> 00:35:22,320
Thank you.

639
00:35:22,320 --> 00:35:23,360
We're still taking questions.

640
00:35:23,360 --> 00:35:25,480
If people could take any conversations they want to have

641
00:35:25,480 --> 00:35:27,640
out into the hallway, that would be most appreciated.

642
00:35:27,640 --> 00:35:28,840
Thank you very much.

643
00:35:28,840 --> 00:35:30,480
Next question here.

644
00:35:30,480 --> 00:35:33,200
So I might be doing something wrong.

645
00:35:33,200 --> 00:35:36,160
And this is hopefully a good question.

646
00:35:36,160 --> 00:35:40,240
I'm currently using apply to create, effectively, a

647
00:35:40,240 --> 00:35:43,440
compound key between two other columns in a data frame,

648
00:35:43,440 --> 00:35:45,200
because they also need to be able to take that

649
00:35:45,200 --> 00:35:46,640
value and pass it around.

650
00:35:46,640 --> 00:35:51,240
The function that I'm using is a SHA-256 function so that I

651
00:35:51,240 --> 00:35:54,000
can obfuscate the data because of the nature of it.

652
00:35:54,000 --> 00:35:56,080
Is that a OK?

653
00:35:56,080 --> 00:35:58,400
Or should I be doing something different?

654
00:35:58,400 --> 00:35:58,920
I'm sorry.

655
00:35:58,920 --> 00:36:01,400
I was having trouble hearing what you were saying.

656
00:36:01,400 --> 00:36:02,240
OK.

657
00:36:02,240 --> 00:36:02,840
I apologize.

658
00:36:02,840 --> 00:36:04,320
Should I start at the beginning?

659
00:36:04,320 --> 00:36:04,680
Sorry.

660
00:36:04,680 --> 00:36:05,560
Could I repeat, please?

661
00:36:05,560 --> 00:36:07,400
Anyone who's having a conversation in here, could

662
00:36:07,400 --> 00:36:09,320
they please move out into the hallway now, please?

663
00:36:09,320 --> 00:36:13,680
I can also just, if you want to come up here, I can answer

664
00:36:13,680 --> 00:36:15,240
questions in person.

665
00:36:15,240 --> 00:36:15,920
Afterwards?

666
00:36:15,920 --> 00:36:17,080
That might be easier.

667
00:36:17,080 --> 00:36:18,080
Yeah.

668
00:36:18,080 --> 00:36:19,560
Yeah, just because there's a lot of echoing.

669
00:36:19,560 --> 00:36:20,800
I'm sorry.

670
00:36:24,680 --> 00:36:25,920
Are there any more questions?

671
00:36:28,200 --> 00:36:29,440
Thank you again, Hannah.

