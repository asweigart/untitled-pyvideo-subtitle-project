1
00:00:00,000 --> 00:00:05,200
Welcome for the second talk in this session.

2
00:00:05,200 --> 00:00:08,280
Our speaker is Fridolin Procorny,

3
00:00:08,280 --> 00:00:11,000
and he's going to talk about, thus,

4
00:00:11,000 --> 00:00:13,560
how to recommend the best possible libraries

5
00:00:13,560 --> 00:00:14,840
for your application.

6
00:00:14,840 --> 00:00:16,440
Please give him a hand of applause.

7
00:00:16,440 --> 00:00:16,940
Thank you.

8
00:00:21,800 --> 00:00:23,440
So thank you very much.

9
00:00:23,440 --> 00:00:25,400
Welcome to this presentation about TOD,

10
00:00:25,400 --> 00:00:27,300
and we will talk about how to recommend

11
00:00:27,820 --> 00:00:30,740
the best possible packages for your applications.

12
00:00:30,740 --> 00:00:33,540
Before we start, let me introduce myself.

13
00:00:33,540 --> 00:00:35,340
My name is Fridolin.

14
00:00:35,340 --> 00:00:37,460
I'm a software engineer at Red Hat.

15
00:00:37,460 --> 00:00:40,700
My focus is mostly on disability systems, machine learning,

16
00:00:40,700 --> 00:00:42,580
and of course, I love Python.

17
00:00:42,580 --> 00:00:46,260
I find Python like programming language that is a pseudocode,

18
00:00:46,260 --> 00:00:48,140
and you write it, and it just runs.

19
00:00:48,140 --> 00:00:49,420
It's awesome.

20
00:00:49,420 --> 00:00:52,580
I started my career as a reverse engineer in a research group.

21
00:00:52,580 --> 00:00:55,020
Then I slightly shifted.

22
00:00:55,020 --> 00:00:56,620
I did some kernel development.

23
00:00:56,620 --> 00:01:01,260
Then I did distributed systems.

24
00:01:01,260 --> 00:01:02,820
I developed Selenon.

25
00:01:02,820 --> 00:01:05,140
That is some kind of enhancement for Celery.

26
00:01:05,140 --> 00:01:08,140
And right now, I work on a project that is called TOD,

27
00:01:08,140 --> 00:01:12,060
and that's the project I would like to introduce to you.

28
00:01:12,060 --> 00:01:15,340
So project TOD is, as of now, developed

29
00:01:15,340 --> 00:01:17,620
by six engineers in total.

30
00:01:17,620 --> 00:01:20,860
But initially, we started two engineers,

31
00:01:20,860 --> 00:01:24,300
and the project is inside a bigger team that

32
00:01:24,340 --> 00:01:28,180
is called AICOE, which stands for Artificial Intelligence

33
00:01:28,180 --> 00:01:29,860
Center of Excellence.

34
00:01:29,860 --> 00:01:34,620
This team is part of office of the CTO, so the Red Hat CTO,

35
00:01:34,620 --> 00:01:37,080
and our focus is on emerging technologies

36
00:01:37,080 --> 00:01:42,420
and coming up with solutions for these emerging technologies.

37
00:01:42,420 --> 00:01:45,220
You can find project TOD on the GitHub

38
00:01:45,220 --> 00:01:48,580
under TODstation organization, and we have also a website.

39
00:01:48,580 --> 00:01:52,500
It is todstation.ninja.

40
00:01:52,540 --> 00:01:55,540
So that would be something about the project.

41
00:01:55,540 --> 00:01:59,100
And now, let's talk about what actually TOD is

42
00:01:59,100 --> 00:02:01,900
and why we developed TOD.

43
00:02:01,900 --> 00:02:05,820
If you take a look at PyPI, the Python Package Index,

44
00:02:05,820 --> 00:02:10,860
you can find something like 170,000 projects.

45
00:02:10,860 --> 00:02:13,300
Each of these projects have releases,

46
00:02:13,300 --> 00:02:19,420
and last time I checked, it was 1.3 million releases.

47
00:02:19,420 --> 00:02:21,820
You can say it's quite a huge code base.

48
00:02:21,820 --> 00:02:25,800
So let's use some libraries that are available out there,

49
00:02:25,800 --> 00:02:28,780
and let's develop an application.

50
00:02:28,780 --> 00:02:30,940
What we will do, we will develop an application

51
00:02:30,940 --> 00:02:33,580
using TensorFlow, so a machine learning library that

52
00:02:33,580 --> 00:02:36,060
is quite popular, especially given the number of stars

53
00:02:36,060 --> 00:02:37,980
that it has on the GitHub.

54
00:02:37,980 --> 00:02:41,220
And we will use Flask for serving our machine learning

55
00:02:41,220 --> 00:02:42,260
model.

56
00:02:42,260 --> 00:02:44,860
If you take a look, what we do, we basically

57
00:02:44,860 --> 00:02:46,860
pick TensorFlow and Flask.

58
00:02:46,860 --> 00:02:48,260
We glue them together.

59
00:02:48,300 --> 00:02:54,180
We write our own code to provide our desired functionality.

60
00:02:54,180 --> 00:02:55,460
How do we do that?

61
00:02:55,460 --> 00:02:58,540
We simply issue pip install tensorflow,

62
00:02:58,540 --> 00:03:03,020
then pip install flask, and we run our application.

63
00:03:03,020 --> 00:03:05,060
Unfortunately, there is some error,

64
00:03:05,060 --> 00:03:09,220
like tensorflow requires some other version of NumPy.

65
00:03:09,220 --> 00:03:12,300
I found this error on GitHub, but in project TOD,

66
00:03:12,300 --> 00:03:15,940
we found very similar issues when running tensorflow,

67
00:03:15,940 --> 00:03:18,060
especially with NumPy.

68
00:03:18,140 --> 00:03:21,100
There were issues with API incompatibilities.

69
00:03:21,100 --> 00:03:22,020
So you see the point.

70
00:03:22,020 --> 00:03:26,900
You install some library, and it does not work as expected.

71
00:03:26,900 --> 00:03:30,740
If we take a look what pip did on the background,

72
00:03:30,740 --> 00:03:33,500
we issued pip install tensorflow.

73
00:03:33,500 --> 00:03:35,780
Pip searched by API and found out

74
00:03:35,780 --> 00:03:38,260
there are 59 releases of tensorflow.

75
00:03:38,260 --> 00:03:41,540
And as we did not provide any version range specification,

76
00:03:41,540 --> 00:03:44,140
it automatically picked the latest one.

77
00:03:44,140 --> 00:03:46,060
The same was for Flask.

78
00:03:46,100 --> 00:03:49,700
It searched by API, and it found 28 releases,

79
00:03:49,700 --> 00:03:55,620
and it picked the latest one based on the current status.

80
00:03:55,620 --> 00:03:59,260
If we give some math to it, we see

81
00:03:59,260 --> 00:04:03,980
that there are at least 1,600 combinations

82
00:04:03,980 --> 00:04:06,900
how you can install different version of tensorflow

83
00:04:06,900 --> 00:04:09,300
with different version of Flask.

84
00:04:09,300 --> 00:04:13,020
Unfortunately, when you do pip install Flask,

85
00:04:13,020 --> 00:04:15,980
it's not just Flask that it's being installed.

86
00:04:15,980 --> 00:04:18,260
There are dependencies of Flask.

87
00:04:18,260 --> 00:04:21,500
These dependencies are air direct or indirect,

88
00:04:21,500 --> 00:04:24,300
so dependencies of dependencies of Flask.

89
00:04:24,300 --> 00:04:27,580
I will refer to them as transitive dependencies.

90
00:04:27,580 --> 00:04:29,340
So Flask depends on click.

91
00:04:29,340 --> 00:04:30,140
It's dangerous.

92
00:04:30,140 --> 00:04:32,300
Jinja to markup-safe, very quick.

93
00:04:32,300 --> 00:04:34,420
Air directly or indirectly.

94
00:04:34,420 --> 00:04:37,620
So when you install Flask, it's also these libraries

95
00:04:37,620 --> 00:04:39,180
that are installed.

96
00:04:39,180 --> 00:04:45,540
I did rough estimate, and there are at least 54 million

97
00:04:45,540 --> 00:04:51,540
combinations how you can install Flask with dependencies,

98
00:04:51,540 --> 00:04:56,620
meaning any Flask version with any dependency given

99
00:04:56,620 --> 00:05:00,500
the version range specification for these libraries.

100
00:05:00,500 --> 00:05:04,020
This number is estimation, and the actual number

101
00:05:04,020 --> 00:05:07,500
varies based on dependency graph traversal.

102
00:05:07,500 --> 00:05:09,540
If you take a look at tensorflow,

103
00:05:09,540 --> 00:05:12,500
then things might get even worse.

104
00:05:12,500 --> 00:05:13,900
TensorFlow is a bigger project.

105
00:05:13,900 --> 00:05:16,420
It depends on much more libraries,

106
00:05:16,420 --> 00:05:19,300
and these libraries have more versions.

107
00:05:19,300 --> 00:05:22,020
So when I did estimation, how many combinations

108
00:05:22,020 --> 00:05:25,420
are there for different tensorflow releases

109
00:05:25,420 --> 00:05:28,540
with different versions of its dependencies,

110
00:05:28,540 --> 00:05:32,860
I found that there is approximately 1.3

111
00:05:32,860 --> 00:05:36,220
multiplied by 10 to the power of 20 combinations

112
00:05:36,220 --> 00:05:41,380
how you can install a tensorflow release with its dependencies.

113
00:05:41,380 --> 00:05:44,140
I will refer to this as a tensorflow stack,

114
00:05:44,140 --> 00:05:52,300
and for Flask case, I will refer to it as for Flask stack.

115
00:05:52,300 --> 00:05:55,700
Now, if you take a look at how many combinations are there

116
00:05:55,700 --> 00:06:00,060
to install tensorflow and Flask, then it's quite a huge number.

117
00:06:00,060 --> 00:06:05,500
It's like 7.6 multiplied by 10 to the power of 27.

118
00:06:05,500 --> 00:06:09,340
That's a really huge number, and we haven't finished it.

119
00:06:09,340 --> 00:06:12,540
Our application runs in some operating system.

120
00:06:12,540 --> 00:06:17,300
This operating system provides some kind of environment

121
00:06:17,300 --> 00:06:19,220
for running our applications.

122
00:06:19,220 --> 00:06:22,060
So it provides Python interpreter that can, again,

123
00:06:22,060 --> 00:06:24,620
be in different versions, and there

124
00:06:24,620 --> 00:06:27,500
can be different versions of operating system, meaning,

125
00:06:27,500 --> 00:06:34,340
for example, Fedora 30, Fedora 29, CentOS, and so on.

126
00:06:34,340 --> 00:06:38,340
This operating system provides also native dependencies.

127
00:06:38,340 --> 00:06:40,780
So it provides, for example, glibc.

128
00:06:40,780 --> 00:06:44,740
And if you are familiar with Python wheel package format,

129
00:06:44,740 --> 00:06:47,340
you know that there is many Linux standard that

130
00:06:47,340 --> 00:06:52,340
tries to standardize or try to provide requirements

131
00:06:52,340 --> 00:06:57,300
for operating system in order to run your Python applications.

132
00:06:57,300 --> 00:07:00,500
It, of course, cannot handle all these dependencies.

133
00:07:00,500 --> 00:07:04,460
So if you run tensorflow and you use GPU,

134
00:07:04,460 --> 00:07:08,420
then there is a CUDA library that is not in many Linux

135
00:07:08,420 --> 00:07:10,180
standard, but you still require it.

136
00:07:10,180 --> 00:07:12,900
And these have also different versions.

137
00:07:12,900 --> 00:07:15,620
So there are different versions of CUDA.

138
00:07:15,620 --> 00:07:19,180
Now, the operating system runs on some hardware.

139
00:07:19,180 --> 00:07:22,820
So you have different types of CPU, different types of GPU,

140
00:07:22,820 --> 00:07:26,860
and you remember that huge number.

141
00:07:26,860 --> 00:07:32,100
So we are basically multiplying with more and more combinations.

142
00:07:32,100 --> 00:07:34,020
If you take a look at it in abstract,

143
00:07:34,020 --> 00:07:38,380
what I've just described, we are developing a Python application

144
00:07:38,380 --> 00:07:41,460
that has some direct dependencies.

145
00:07:41,460 --> 00:07:46,180
These direct dependencies have also some dependencies,

146
00:07:46,180 --> 00:07:48,140
meaning transitive dependencies.

147
00:07:48,140 --> 00:07:51,380
This lives in Python ecosystem.

148
00:07:51,380 --> 00:07:53,980
This is then run with Python interpreter

149
00:07:53,980 --> 00:07:56,840
that can be also in different versions.

150
00:07:56,840 --> 00:08:00,220
And these dependencies can use native dependencies

151
00:08:00,220 --> 00:08:03,980
or native packages that are provided by operating system.

152
00:08:03,980 --> 00:08:06,500
Operating system also has some kernel modules.

153
00:08:06,500 --> 00:08:11,340
So if you use CUDA, there needs to be aligned CUDA version.

154
00:08:11,340 --> 00:08:13,860
And this runs on hardware.

155
00:08:13,860 --> 00:08:16,380
So you have a lot of combinations.

156
00:08:16,380 --> 00:08:19,380
And you can imagine that if you make one single mistake

157
00:08:19,380 --> 00:08:21,820
in any item in this stack, your application

158
00:08:21,820 --> 00:08:23,820
will simply not work.

159
00:08:23,820 --> 00:08:26,100
If you make mistake in a Python application,

160
00:08:26,100 --> 00:08:27,060
you simply fix it.

161
00:08:27,060 --> 00:08:30,100
You wrote, you own the code, you have the knowledge,

162
00:08:30,100 --> 00:08:31,180
you fix it.

163
00:08:31,180 --> 00:08:34,100
But if there is an issue with direct dependencies,

164
00:08:34,100 --> 00:08:36,780
transitive dependencies, Python interpreter,

165
00:08:36,780 --> 00:08:40,260
native dependencies, or kernel modules,

166
00:08:40,260 --> 00:08:42,420
it's much harder to fix it.

167
00:08:42,420 --> 00:08:45,940
And that's basically the main idea behind ToT.

168
00:08:45,940 --> 00:08:49,420
In ToT, we are creating a knowledge base,

169
00:08:49,420 --> 00:08:52,540
how applications run, if they run correctly,

170
00:08:52,540 --> 00:08:56,540
and what is their overall performance.

171
00:08:56,540 --> 00:08:59,840
So we are interested in building applications,

172
00:08:59,840 --> 00:09:03,080
meaning if the given Python package installs correctly

173
00:09:03,080 --> 00:09:06,120
into your runtime environment, if it runs correctly,

174
00:09:06,120 --> 00:09:10,040
so you don't see issues that can be seen in the previous slide

175
00:09:10,040 --> 00:09:11,780
with TensorFlow and NumPy.

176
00:09:11,780 --> 00:09:15,040
And we also are interested in whether applications

177
00:09:15,040 --> 00:09:18,800
behave correctly and if they give reasonable results,

178
00:09:18,800 --> 00:09:20,760
so they perform well.

179
00:09:20,760 --> 00:09:24,320
So the idea is to create advanced Python resolver that

180
00:09:24,320 --> 00:09:27,960
uses knowledge base to resolve software stack not

181
00:09:27,960 --> 00:09:32,760
to latest versions, but to the best possible versions

182
00:09:32,760 --> 00:09:35,200
of libraries that you use.

183
00:09:35,200 --> 00:09:39,160
OK, so now let's take a look at how we are building

184
00:09:39,160 --> 00:09:41,520
ToT's knowledge base.

185
00:09:41,520 --> 00:09:44,720
We have right now at Red Hat two instances of ToT.

186
00:09:44,720 --> 00:09:47,520
One is test, one is stage environment,

187
00:09:47,520 --> 00:09:51,620
and we are provisioning also production environment.

188
00:09:51,620 --> 00:09:54,720
And in these environments, we are aggregating knowledge,

189
00:09:54,720 --> 00:09:59,480
like if the given package or if given software stack runs.

190
00:09:59,480 --> 00:10:01,000
If there are some build failures,

191
00:10:01,000 --> 00:10:02,200
we aggregate this knowledge.

192
00:10:02,200 --> 00:10:03,880
If there are some installation failures,

193
00:10:03,880 --> 00:10:05,400
we aggregate this knowledge.

194
00:10:05,400 --> 00:10:08,040
And we use for this purpose, for example,

195
00:10:08,040 --> 00:10:11,120
Jupyter Hub that is deployed internally at Red Hat,

196
00:10:11,120 --> 00:10:13,680
and data scientists use it.

197
00:10:13,680 --> 00:10:16,560
So we aggregate each time there is a build of container image,

198
00:10:16,560 --> 00:10:19,480
we aggregate information into ToT's knowledge base.

199
00:10:19,480 --> 00:10:21,980
We analyze also ToT's container images.

200
00:10:21,980 --> 00:10:25,940
So ToT is analyzed by itself.

201
00:10:25,940 --> 00:10:28,820
These are, I would say, real world use cases.

202
00:10:28,820 --> 00:10:33,500
We also have other sources of data for our knowledge base.

203
00:10:33,500 --> 00:10:37,300
But we have a service that is called Amoon.

204
00:10:37,300 --> 00:10:39,840
And what this service does, it automatically

205
00:10:39,840 --> 00:10:43,420
verifies that the given software runs correctly,

206
00:10:43,420 --> 00:10:47,260
that it builds and behaves correctly.

207
00:10:47,260 --> 00:10:50,220
There is a tool called Dependence Monkey that

208
00:10:50,220 --> 00:10:52,940
can switch different versions of libraries

209
00:10:52,940 --> 00:10:57,740
and can run specific software on some specific hardware.

210
00:10:57,740 --> 00:11:01,020
And then we aggregate data into our knowledge base.

211
00:11:01,020 --> 00:11:03,260
So we burn some CPU for you in order

212
00:11:03,260 --> 00:11:07,020
to come up with the best possible software stack.

213
00:11:07,020 --> 00:11:09,900
We also do performance-related analysis.

214
00:11:09,900 --> 00:11:13,100
For example, TensorFlow is quite a huge project,

215
00:11:13,100 --> 00:11:15,380
as you saw with the number of dependencies

216
00:11:15,380 --> 00:11:19,260
and number of combinations that can be installed.

217
00:11:19,260 --> 00:11:25,580
And the core part of TensorFlow is built in C, C++.

218
00:11:25,580 --> 00:11:28,980
So what we do, we analyze the performance of TensorFlow

219
00:11:28,980 --> 00:11:31,420
by switching different versions of TensorFlow.

220
00:11:31,420 --> 00:11:34,740
But we also have our own builds of TensorFlow.

221
00:11:34,740 --> 00:11:36,860
And these builds are automatically

222
00:11:36,860 --> 00:11:39,940
verified in our CI system, that is Amoon.

223
00:11:39,940 --> 00:11:43,580
And we gather performance-related characteristics

224
00:11:43,580 --> 00:11:46,740
of these machine learning libraries.

225
00:11:46,740 --> 00:11:49,340
So we have optimized TensorFlow builds.

226
00:11:49,340 --> 00:11:52,080
You can find them on our index.

227
00:11:52,080 --> 00:11:55,300
There are also instructions how you can set up your environment

228
00:11:55,300 --> 00:11:57,500
in order to use that index.

229
00:11:57,500 --> 00:12:02,060
And we simply found out that by recompiling TensorFlow

230
00:12:02,060 --> 00:12:04,700
on some specific CPU architecture,

231
00:12:04,700 --> 00:12:08,180
so enabling CPU-specific compiler options of the core parts

232
00:12:08,180 --> 00:12:11,020
of TensorFlow that are written in C, C++,

233
00:12:11,020 --> 00:12:14,300
you can gain performance.

234
00:12:14,300 --> 00:12:18,460
This work was also in cooperation with Google.

235
00:12:18,460 --> 00:12:21,580
And there is a TensorFlow on-demand project.

236
00:12:21,580 --> 00:12:24,460
So if you are familiar with TensorFlow,

237
00:12:24,460 --> 00:12:27,700
you see our community section of TensorFlow,

238
00:12:27,700 --> 00:12:31,540
you will see what's going on in this area.

239
00:12:31,540 --> 00:12:33,980
OK, so we aggregated knowledge.

240
00:12:33,980 --> 00:12:36,020
We have our knowledge base.

241
00:12:36,020 --> 00:12:39,780
And now let's come up with recommendations.

242
00:12:39,780 --> 00:12:42,340
So how do we do that?

243
00:12:42,340 --> 00:12:45,220
Let's imagine that we have a software.

244
00:12:45,220 --> 00:12:46,860
So we run our application.

245
00:12:46,860 --> 00:12:50,140
And this application has only two dependencies

246
00:12:50,140 --> 00:12:52,580
called simple-lip and another-lip.

247
00:12:52,580 --> 00:12:56,300
These dependencies do not have any other dependencies.

248
00:12:56,300 --> 00:12:58,780
So what we are looking for in recommendations,

249
00:12:58,780 --> 00:13:02,580
we are looking for really good score, whatever that means.

250
00:13:02,580 --> 00:13:03,500
It can be performance.

251
00:13:03,500 --> 00:13:06,220
It can be security vulnerabilities.

252
00:13:06,220 --> 00:13:09,340
So it can be a single score, or it can be also

253
00:13:09,340 --> 00:13:12,300
compound of multiple scores.

254
00:13:12,300 --> 00:13:16,780
And if we dive into mathematics a little,

255
00:13:16,780 --> 00:13:19,100
then we can plot this function.

256
00:13:19,100 --> 00:13:21,140
So we have different versions of simple-lip.

257
00:13:21,140 --> 00:13:23,180
We have different versions of another-lip.

258
00:13:23,180 --> 00:13:26,080
And what we are looking for, we are looking for really high

259
00:13:26,080 --> 00:13:29,740
spike in this case, so very high score.

260
00:13:29,740 --> 00:13:33,180
And we would like to recommend simple-lip and another-lip

261
00:13:33,180 --> 00:13:34,180
in some versions.

262
00:13:34,180 --> 00:13:35,700
So we are looking for combinations

263
00:13:35,700 --> 00:13:41,700
of different versions, which have very good overall score.

264
00:13:41,700 --> 00:13:44,460
Now let's consider that these libraries

265
00:13:44,460 --> 00:13:46,740
have other dependencies.

266
00:13:46,740 --> 00:13:49,620
If we would plot that, we would just

267
00:13:49,620 --> 00:13:54,220
introduce a new dimension that's not that easy to plot.

268
00:13:54,220 --> 00:13:57,740
But now let's consider that simple-lip in version V1

269
00:13:57,740 --> 00:14:01,140
depends on dependency 1 in version V1 or V2.

270
00:14:01,140 --> 00:14:06,020
So dependency 1 is installed in version V1 or V2.

271
00:14:06,020 --> 00:14:10,020
Then let's consider another-lip has dependency 2.

272
00:14:10,020 --> 00:14:12,980
And if we install another-lip in version V2,

273
00:14:12,980 --> 00:14:16,620
we need to install dependency 2 in version V1 or V2.

274
00:14:16,620 --> 00:14:18,740
That's the dependency.

275
00:14:18,740 --> 00:14:22,420
OK, so let's take a look how PIP, PIPenv, Poetry,

276
00:14:22,420 --> 00:14:25,460
or other tools would install your software stack.

277
00:14:25,460 --> 00:14:27,980
They will automatically try to resolve

278
00:14:27,980 --> 00:14:29,540
to the latest possible versions.

279
00:14:29,540 --> 00:14:32,100
So there will be install simple-lip in version V2,

280
00:14:32,100 --> 00:14:36,900
another-lip in version V2, and dependency 2 in version V2.

281
00:14:36,900 --> 00:14:41,540
Now take a look at how TOD approaches this problem.

282
00:14:41,540 --> 00:14:43,100
So we have this dependency graph.

283
00:14:43,100 --> 00:14:45,380
And before the actual resolution,

284
00:14:45,380 --> 00:14:47,600
we take a look into our knowledge base.

285
00:14:47,600 --> 00:14:50,940
And we find out that dependency 2 in version 2

286
00:14:50,940 --> 00:14:51,900
has some issues.

287
00:14:51,900 --> 00:14:55,140
For example, it does not install into your runtime environment

288
00:14:55,140 --> 00:14:57,500
because of different Python version.

289
00:14:57,500 --> 00:15:00,580
So what we do, we simply remove the dependency

290
00:15:00,580 --> 00:15:02,740
from our dependency graph.

291
00:15:02,740 --> 00:15:05,740
Now we take a look into our knowledge base again.

292
00:15:05,740 --> 00:15:08,780
And we see that simple-lip in version V1

293
00:15:08,780 --> 00:15:12,820
performs very well with dependency 1 in version V1.

294
00:15:12,820 --> 00:15:16,620
So what we will do, we shift these versions

295
00:15:16,620 --> 00:15:20,900
to take precedence when there is done actual resolution.

296
00:15:20,900 --> 00:15:25,700
Now let's take a look at how TOD would resolve this example.

297
00:15:25,700 --> 00:15:29,100
So simple-lip is resolved in version V1 with dependency 1

298
00:15:29,100 --> 00:15:33,940
in version V1 because these are well-performing libraries.

299
00:15:33,940 --> 00:15:36,300
And now another-lip is in version V2.

300
00:15:36,300 --> 00:15:39,580
But instead of installing dependency 2 in version V2,

301
00:15:39,580 --> 00:15:44,180
we install V1 because that does not cause any issues.

302
00:15:44,180 --> 00:15:46,380
This is accomplished in TOD using

303
00:15:46,380 --> 00:15:48,660
Stack Generation Pipeline.

304
00:15:48,660 --> 00:15:50,100
That's the term that we use.

305
00:15:50,100 --> 00:15:51,820
The input is runtime environment.

306
00:15:51,820 --> 00:15:53,700
So there is information.

307
00:15:53,700 --> 00:15:54,620
Where do you run it?

308
00:15:54,620 --> 00:15:55,940
On what hardware?

309
00:15:55,940 --> 00:15:56,820
What is CPU?

310
00:15:56,820 --> 00:15:58,020
And stuff like that.

311
00:15:58,020 --> 00:15:59,700
Then there are provided requirements.

312
00:15:59,700 --> 00:16:01,580
So if you're familiar with requirements,

313
00:16:01,580 --> 00:16:04,080
the .txt file, that's basically the requirements

314
00:16:04,080 --> 00:16:05,660
that are provided.

315
00:16:05,660 --> 00:16:10,260
And then there is also input that is, as of now, not used.

316
00:16:10,260 --> 00:16:13,000
But we perform static analysis of your source code,

317
00:16:13,000 --> 00:16:14,500
of your application.

318
00:16:14,500 --> 00:16:19,040
So that's also the input of this Stack Generation Pipeline.

319
00:16:19,040 --> 00:16:21,420
Then this Stack Generation Pipeline

320
00:16:21,420 --> 00:16:23,860
adjusts the dependency graph, adjusts

321
00:16:23,860 --> 00:16:29,260
how the resolution algorithm will produce stacks.

322
00:16:29,260 --> 00:16:31,700
And then there is a stream of stacks

323
00:16:31,700 --> 00:16:36,460
that are subsequently scored and gated based on final score.

324
00:16:36,460 --> 00:16:38,820
The output of the Stack Generation Pipeline

325
00:16:38,820 --> 00:16:41,900
is a log file stating fully pinned down

326
00:16:41,900 --> 00:16:44,580
software stack, so specific versions that

327
00:16:44,580 --> 00:16:47,220
should be installed, together with justification

328
00:16:47,220 --> 00:16:50,620
why you should install these dependencies

329
00:16:50,620 --> 00:16:55,020
instead of using latest ones.

330
00:16:55,020 --> 00:16:59,500
To support this use case, we had to extend information

331
00:16:59,500 --> 00:17:04,340
that is provided by PyPI, the Python Package Index.

332
00:17:04,340 --> 00:17:06,760
If you take a look at setuppy or metadata that

333
00:17:06,760 --> 00:17:10,980
are provided together with Python packages,

334
00:17:10,980 --> 00:17:14,700
you can see that there are, for example, license, information,

335
00:17:14,700 --> 00:17:16,020
classifiers.

336
00:17:16,020 --> 00:17:20,500
And then we had to know some purpose of package.

337
00:17:20,500 --> 00:17:24,980
You can use keywords for stating the purpose.

338
00:17:24,980 --> 00:17:27,640
But a lot of packages do not simply put anything

339
00:17:27,640 --> 00:17:29,220
into keyword section.

340
00:17:29,220 --> 00:17:36,380
Or there are sometimes really weird strings.

341
00:17:36,380 --> 00:17:39,380
So we wanted to build a model that would automatically

342
00:17:39,380 --> 00:17:41,820
create or answer simple questions,

343
00:17:41,820 --> 00:17:45,100
like is the given package affecting performance somehow?

344
00:17:45,100 --> 00:17:47,700
So we can score these packages separately

345
00:17:47,700 --> 00:17:52,220
in different steps of Stack Generation Pipeline.

346
00:17:52,220 --> 00:17:54,900
So what we did, we created a model

347
00:17:54,900 --> 00:17:56,700
that is vector space model.

348
00:17:56,700 --> 00:17:58,460
And we analyzed each and every package

349
00:17:58,460 --> 00:18:00,500
that is available on PyPI.

350
00:18:00,500 --> 00:18:04,420
And we aggregated information such as reading files,

351
00:18:04,420 --> 00:18:07,340
descriptions, so any free text that

352
00:18:07,340 --> 00:18:09,420
is describing the project.

353
00:18:09,420 --> 00:18:11,900
We extracted relevant information,

354
00:18:11,900 --> 00:18:15,780
such as is that project related to Django,

355
00:18:15,780 --> 00:18:18,380
is that machine learning application, and stuff

356
00:18:18,380 --> 00:18:19,340
like that.

357
00:18:19,340 --> 00:18:22,100
So we extracted this information and constructed a vector

358
00:18:22,100 --> 00:18:25,460
that describes this Python package.

359
00:18:25,460 --> 00:18:29,620
This allows to create or form this vector space model.

360
00:18:29,620 --> 00:18:33,660
And what you can do, you can perform feature-based queries.

361
00:18:33,660 --> 00:18:36,580
So for example, you can ask, what

362
00:18:36,580 --> 00:18:44,320
is package that is machine learning and provides some UI?

363
00:18:44,320 --> 00:18:46,840
So this is feature-based query.

364
00:18:46,840 --> 00:18:49,000
You can also browse similar packages.

365
00:18:49,000 --> 00:18:53,440
So what you can do, you can, based on these features

366
00:18:53,440 --> 00:18:56,200
and looking into this vector space,

367
00:18:56,200 --> 00:18:58,520
simply by computing a collidian distance,

368
00:18:58,520 --> 00:19:00,560
you can find similar packages.

369
00:19:00,560 --> 00:19:02,000
There is visualization.

370
00:19:02,000 --> 00:19:07,480
If you go to a .station GitHub organization, you can see,

371
00:19:07,480 --> 00:19:12,580
this is t-SNE, so dimensionality space reduction.

372
00:19:12,580 --> 00:19:15,980
And this is the vector space that I spoke about.

373
00:19:15,980 --> 00:19:20,300
You can see clusters that are formed.

374
00:19:20,300 --> 00:19:22,380
These are typically very similar packages.

375
00:19:22,380 --> 00:19:25,060
So you can see, for example, MicroPython,

376
00:19:25,060 --> 00:19:29,540
MicroPython cluster, and stuff like that.

377
00:19:29,540 --> 00:19:34,380
This vector space model is served by API.

378
00:19:34,380 --> 00:19:36,900
And we would like to provide it for community.

379
00:19:37,820 --> 00:19:41,820
Project.todd can be found on the website, .station.ninja.

380
00:19:41,820 --> 00:19:43,340
You can find us also on Twitter.

381
00:19:43,340 --> 00:19:45,180
You can follow us for updates.

382
00:19:45,180 --> 00:19:49,340
As stated, we have two environments right now.

383
00:19:49,340 --> 00:19:51,260
The third one is provisioned.

384
00:19:51,260 --> 00:19:57,060
And what we plan to do, we plan to offer .todd to community

385
00:19:57,060 --> 00:20:03,180
so anyone can use it and anyone can benefit out of it.

386
00:20:03,180 --> 00:20:04,500
We sit on GitHub.

387
00:20:04,540 --> 00:20:06,700
That's our organization that is called .station.

388
00:20:06,700 --> 00:20:08,500
Everything is open source.

389
00:20:08,500 --> 00:20:11,340
You can even provision your own .instance,

390
00:20:11,340 --> 00:20:14,860
and you can run it locally.

391
00:20:14,860 --> 00:20:17,740
If you're a developer, this is the website.

392
00:20:17,740 --> 00:20:23,340
Right now, we have API that is accessible only

393
00:20:23,340 --> 00:20:24,180
at that internally.

394
00:20:24,180 --> 00:20:26,300
So we did internal release.

395
00:20:26,300 --> 00:20:28,140
We would like to make it more stable

396
00:20:28,140 --> 00:20:29,420
and provide it to community.

397
00:20:29,460 --> 00:20:34,100
If you want to integrate, we have a CLI that is called TAMOS.

398
00:20:34,100 --> 00:20:37,540
It's a simple CLI tool that you install.

399
00:20:37,540 --> 00:20:40,580
You go to your repository directory,

400
00:20:40,580 --> 00:20:43,620
and you issue TAMOS config that performs automatic hardware

401
00:20:43,620 --> 00:20:48,900
discovery that is available on your laptop or on your machine.

402
00:20:48,900 --> 00:20:52,340
And it creates configuration file for .todd.

403
00:20:52,340 --> 00:20:55,260
And then you issue TAMOS config.

404
00:20:55,260 --> 00:20:57,380
And then you issue TAMOS config.

405
00:20:58,260 --> 00:21:01,540
And then you issue TAMOS advice, and it will automatically

406
00:21:01,540 --> 00:21:05,580
advise you on what software you should use.

407
00:21:05,580 --> 00:21:08,100
Right now, we follow PIPenv.

408
00:21:08,100 --> 00:21:12,500
So if you use PIPenv, meaning PIP file and PIP file log,

409
00:21:12,500 --> 00:21:15,660
the service will provide you PIP file log

410
00:21:15,660 --> 00:21:18,220
based on your PIP file.

411
00:21:18,220 --> 00:21:22,420
Another point of integration is our bot, the de-skebehat.

412
00:21:22,420 --> 00:21:24,820
And what de-skebehat does, it automatically

413
00:21:24,820 --> 00:21:27,460
manages your GitHub repositories.

414
00:21:27,460 --> 00:21:29,660
So it opens pull requests.

415
00:21:29,660 --> 00:21:31,620
It opens issues if there's anything

416
00:21:31,620 --> 00:21:34,900
wrong with your software in your application.

417
00:21:34,900 --> 00:21:38,860
So you can also use that bot.

418
00:21:38,860 --> 00:21:42,100
There is planned integration with GitHub API,

419
00:21:42,100 --> 00:21:44,900
so it will be a plug-in to GitHub.

420
00:21:44,900 --> 00:21:47,540
But that's like more future.

421
00:21:47,540 --> 00:21:53,340
Now we support, or we first will provide TAMOS CLI client.

422
00:21:53,340 --> 00:21:54,980
This way, I would like to thank you.

423
00:21:54,980 --> 00:21:57,460
Hopefully you liked thought.

424
00:21:57,460 --> 00:22:00,220
If you have any questions, feel free to reach out to me.

425
00:22:00,220 --> 00:22:01,740
I will be in conference.

426
00:22:01,740 --> 00:22:03,300
Everything is open source.

427
00:22:03,300 --> 00:22:07,140
Follow us on Twitter for more updates if you are interested.

428
00:22:07,140 --> 00:22:08,940
And thank you very much.

429
00:22:08,940 --> 00:22:10,700
Thank you.

