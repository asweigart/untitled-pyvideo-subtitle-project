1
00:00:00,000 --> 00:00:06,280
opportunities. So let's please welcome Lorena Mesa. She'll be giving the talk Esquincla

2
00:00:06,280 --> 00:00:11,640
Barbosa, Creating a Tele-Novela, Script and Three Deep Learning Frameworks.

3
00:00:11,640 --> 00:00:26,640
Excellent. Okay, can you all hear me all right? Very cool. I naturally am a loud person and

4
00:00:26,640 --> 00:00:30,880
use a lot of hand motion. So this will be great because I think it goes with the topic

5
00:00:30,880 --> 00:00:34,720
pretty well. So yes, if you were staring at the name of this talk and you're like, I don't

6
00:00:34,720 --> 00:00:37,920
think there's English in some of this, you are correct. You're in the right place. So

7
00:00:37,920 --> 00:00:44,000
Esquincla Barbosa, Creating a Tele-Novela, Script and Three Python, Deep Learning Frameworks.

8
00:00:44,000 --> 00:00:48,560
Side note, I will be actually on some of the slides, there is a URL that you can actually

9
00:00:48,560 --> 00:00:52,940
follow along with the slides. And I do actually have all my content linked to the GitHub repository

10
00:00:53,020 --> 00:00:58,500
that has this code. And there's a surprise of another GitHub repository at the tail end.

11
00:00:58,500 --> 00:01:04,340
So who am I? Hola, soy Lorena Mesa. Puedo hablar en inglés o español. But I will obviously

12
00:01:04,340 --> 00:01:11,620
be talking in English for this talk. It is not PyConcharlas. So the topic, I personally

13
00:01:11,620 --> 00:01:16,380
am just really interested in deep learning. I hear the term deep learning and I'm like,

14
00:01:16,380 --> 00:01:20,840
what's so deep about this learning? So I'm really just kind of curious about this topic.

15
00:01:20,840 --> 00:01:24,840
And I also really loved El Novellas. So when I was thinking about a topic, it felt like

16
00:01:24,840 --> 00:01:32,960
a very big win-win for me. So why not? Why not? But actually, more practically, I actually

17
00:01:32,960 --> 00:01:38,400
work as a data engineer at GitHub and I sit between two teams in the data org. My team's

18
00:01:38,400 --> 00:01:45,320
name is Software Intelligence Systems, which is a little bit of a mouthful. SIS for short.

19
00:01:45,320 --> 00:01:49,640
And the teams that I work with are our semantic code team and our machine learning team, our

20
00:01:49,640 --> 00:01:53,920
machine learning engineers are increasingly talking about different deep learning techniques

21
00:01:53,920 --> 00:01:59,060
and using GPUs and a lot of these kind of techniques I hadn't used before. So why not?

22
00:01:59,060 --> 00:02:02,560
Why not marry my interests and try a little toy session and see what I can learn about

23
00:02:02,560 --> 00:02:08,080
deep learning and share with you all in a PyCon talk. I do some other work in the Python

24
00:02:08,080 --> 00:02:13,240
space as well. I'm in Chicago. I help run PyLady Chicago and I'm also on the Python

25
00:02:14,240 --> 00:02:23,240
of directors. Yay! If you do have any questions about those things, again, you can find me

26
00:02:23,240 --> 00:02:28,940
later. So I'm a bit curious how many of you may have seen some of these headlines floating

27
00:02:28,940 --> 00:02:36,100
around. Oops. Sorry about that. Let me go to the next slide. Okay. Excellent. Did anyone

28
00:02:36,100 --> 00:02:40,800
see this headline floating around about this AI generated script starring David Hasselhoff?

29
00:02:41,440 --> 00:02:47,080
No? Okay. You should watch it. It's this really interesting and absurdist eight-minute short.

30
00:02:47,080 --> 00:02:52,080
And the creator of this, Ross Goodwin, who is a creative technologist in Google's artists

31
00:02:52,080 --> 00:02:57,400
and machine learning program, actually used a R and N, a recurring neural network, to

32
00:02:57,400 --> 00:03:03,680
actually generate all the lines that were in the short for David Hasselhoff's script.

33
00:03:03,680 --> 00:03:10,680
The short is quite interesting. It's very ephemeral. It's not quite clear what's happening.

34
00:03:10,880 --> 00:03:14,560
When I first saw this headline, it got me thinking about the intersection of art and

35
00:03:14,560 --> 00:03:17,800
technology and this was something I never really had thought about before. What happens

36
00:03:17,800 --> 00:03:23,000
if we can automate the idea, the creativity process? What does that actually look like?

37
00:03:23,000 --> 00:03:26,840
Another place that I kind of started seeing this floating around, as one does, lots of

38
00:03:26,840 --> 00:03:33,480
conversations on Twitter. Specifically, this is a screenshot from a chat that was March

39
00:03:33,480 --> 00:03:40,000
2018 wherein someone alleged that they took all of the scripts from all the scripts of

40
00:03:40,080 --> 00:03:45,120
the various Saw movies, and no, I do not know how many there are, and apparently developed

41
00:03:45,120 --> 00:03:49,520
a neural network and allegedly trained it for a thousand hours, whatever that means,

42
00:03:49,520 --> 00:03:53,280
to generate this script. And this script is supposed to be a horror script. And in the

43
00:03:53,280 --> 00:03:59,920
script, that first line, it says, a sexy woman, Becky, sex woman, is covered by a bed. She's

44
00:03:59,920 --> 00:04:05,920
in a whale but doesn't know she's in a whale. And it's a very confusing piece of text. The

45
00:04:05,920 --> 00:04:09,680
part on the right-hand side that kind of continues to go into this a little bit starts talking

46
00:04:09,680 --> 00:04:14,640
about how this is a really interesting application of neural networks to try to create an idea

47
00:04:14,640 --> 00:04:19,360
with text generation of movie scripts. However, they do start asking questions about why are

48
00:04:19,360 --> 00:04:25,120
the words in the vocabulary used in the script including such a language as Trump and whale?

49
00:04:25,120 --> 00:04:29,580
The person then goes on to critique to say, actually, nowhere in the Saw movies are these

50
00:04:29,580 --> 00:04:33,760
two words appearing. And if you know anything about neural networks, and by the end of this

51
00:04:33,760 --> 00:04:37,480
talk, you will a little bit, that can be a little problematic because as the neural network

52
00:04:37,480 --> 00:04:41,840
is learning, it learns from the corpus of text that you provide it. So if you're allegedly

53
00:04:41,840 --> 00:04:46,440
providing it the Saw scripts and it's coming up with words it had not ever seen before,

54
00:04:46,440 --> 00:04:51,360
can we say this is actually an AI-generated script? I don't know. Either way, I think

55
00:04:51,360 --> 00:04:54,840
this thread is very interesting and it started poking a lot of questions and made me want

56
00:04:54,840 --> 00:04:59,080
to actually try my own example. The whale did that. It became a meme. You can go find

57
00:04:59,080 --> 00:05:02,440
out more about this script. But there's a lot of people that are starting to think about

58
00:05:02,440 --> 00:05:09,280
what is the intersectionality of creativity and technology? So you might be asking why

59
00:05:09,280 --> 00:05:15,480
telenovelas? Also, what in the world is a telenovela? Well, you came to the right place.

60
00:05:15,480 --> 00:05:22,240
I'm here to inform you about one of the most epic meltdowns in a telenovela that has ever

61
00:05:22,240 --> 00:05:27,920
happened. So do I have any telenovela fans in here? Yes. Okay. That's exciting for me.

62
00:05:27,920 --> 00:05:33,200
So maybe some of you remember this one. Mariela del Barrio, which is a beloved Mexican telenovela

63
00:05:33,200 --> 00:05:41,520
from the 1990s, I believe specifically 1995 to 1996 with like 195 episodes. But this scene

64
00:05:41,520 --> 00:05:45,720
and the images might be a little hard to see, but this scene includes some of the characters

65
00:05:45,720 --> 00:05:51,600
of where we get the beloved phrase, escuincalababosa, which is in the title of this talk. So we

66
00:05:51,600 --> 00:05:58,440
have at the top two, we've got a love struck couple. We have Alicia to the left and we

67
00:05:58,440 --> 00:06:02,480
have Nandito to the right. And the main character who's shown on the right hand side and also

68
00:06:02,480 --> 00:06:08,800
in that third panel at the bottom, that is Soraya. So she is our main character of this

69
00:06:08,800 --> 00:06:16,120
telenovela. And Soraya, she's got some complex history going on. So she's the stepmother

70
00:06:17,120 --> 00:06:27,360
of Alicia and then Nandito is her ex-husband's son. Right. And the thing here is Soraya is

71
00:06:27,360 --> 00:06:33,400
still desperately, desperately in love with Nandito's father. Like to the point of maybe

72
00:06:33,400 --> 00:06:38,360
there was like a restraining order. So she's desperately, she's desperately in love with

73
00:06:38,360 --> 00:06:45,120
Nandito's father and she's stuck in a marriage though where she's got this daughter, Alicia.

74
00:06:45,120 --> 00:06:49,680
So she happens to walk into the room where Alicia and Nandito are starting to express

75
00:06:49,680 --> 00:06:55,680
their love for one another. Nandito very tenderly says, may I, Alicia, please have a kiss. And

76
00:06:55,680 --> 00:07:01,280
as they're leaning into this moment, in comes Soraya. And Soraya is like, what is going

77
00:07:01,280 --> 00:07:06,600
on? I don't understand. Because for her, this shatters, I guess, her master plan to get

78
00:07:06,600 --> 00:07:12,880
back with Nandito's father. Essentially, she then screams and calls Alicia escuincalababosa,

79
00:07:12,880 --> 00:07:19,520
which means use boiled rotten bread in nice speak. And what then ensues is the most epic

80
00:07:19,520 --> 00:07:24,680
fight, I guess we could call it. There's a lot of kind of air slaps. You can very, very

81
00:07:24,680 --> 00:07:29,080
specifically tell that there's not any contact happening. There's people diving over beds

82
00:07:29,080 --> 00:07:33,840
to protect one another. Some random gentleman in a suit walks in and is trying to break

83
00:07:33,840 --> 00:07:38,600
up the fight. His character is in no shape or form explained as to why he's there. And

84
00:07:38,600 --> 00:07:43,280
then the nanny, the nanny here, she walks in. And what that says in Spanish here is,

85
00:07:43,280 --> 00:07:51,040
this woman is, this woman is, she's got problems. She's possessed by the devil. So this scene

86
00:07:51,040 --> 00:07:56,900
and all this kind of trope of like love and complex family relationships, and the idea

87
00:07:56,900 --> 00:08:01,640
that we all have this trauma that's near and dear to our heart, that leads us into what

88
00:08:01,640 --> 00:08:08,360
a telenovela is. So Jorge Gonzalez, in talking about telenovelas, he's a sociologist, kind

89
00:08:08,360 --> 00:08:16,440
of commenting on the impact and social phenomena of telenovelas, basically says they are melodramas.

90
00:08:16,440 --> 00:08:22,320
And these are serial melodramas. You get increments of 25, 30 minutes, maybe five times a week.

91
00:08:22,320 --> 00:08:27,080
But what happens here, because it is a stable relationship that you're making with the audience,

92
00:08:27,080 --> 00:08:34,280
people become very passionate and very connected with these narratives. To the extent that

93
00:08:34,400 --> 00:08:40,320
when we talk about telenovelas, there's about two billion people worldwide who consume telenovelas

94
00:08:40,320 --> 00:08:44,240
that are from Latin America. That's about like a third of the world population. So that's

95
00:08:44,240 --> 00:08:50,880
a pretty sizable number. And if you want to know more about why telenovelas are a big deal,

96
00:08:50,880 --> 00:08:57,160
here's some stats of some of the viewing and patterns of viewing of Latin American telenovelas.

97
00:08:57,160 --> 00:09:02,920
So Latin American telenovelas have been viewed in over 100 countries. They have generated

98
00:09:02,920 --> 00:09:08,840
over $800 million in marketing on various telenovelas in a five-year period. I believe

99
00:09:08,840 --> 00:09:17,400
that stat is 2010 to 2015. And just generally speaking, this is a global phenomenon.

100
00:09:17,400 --> 00:09:22,080
So in breaking down the arc of a telenovela, because if we want to try to emulate and create

101
00:09:22,080 --> 00:09:27,860
a telenovela with a neural network, we need to understand what it is we're trying to create.

102
00:09:27,860 --> 00:09:32,800
So back to kind of thinking about that kind of relationship and things that pop up. One

103
00:09:32,800 --> 00:09:37,240
of the things that's interesting about telenovelas is this idea that there's not really any

104
00:09:37,240 --> 00:09:42,440
cliffhangers or kind of open-ended stories. Dr. Rios, who talks about telenovelas and

105
00:09:42,440 --> 00:09:47,480
kind of understanding their social impact, specifically says that at the end of the telenovela,

106
00:09:47,480 --> 00:09:52,760
things have to be cleared up. So in our Maria de la Barrio, for example, if we have the

107
00:09:52,760 --> 00:09:56,320
character Maria and we want to know what's happening, she would hypothesize and say,

108
00:09:56,320 --> 00:09:59,560
our audience doesn't want to worry about Maria. We don't want to know if she didn't find her

109
00:09:59,560 --> 00:10:07,280
true love, her true mother, her true father. We want all those answers. So unlike English soap

110
00:10:07,280 --> 00:10:12,680
operas, which I think some have been going for more than 15, 20 years, what happens with

111
00:10:12,680 --> 00:10:17,360
telenovelas is you have a fixed melodramatic plot line. And yes, it's going to be lost loves. It's

112
00:10:17,360 --> 00:10:21,080
going to be mothers and daughters fighting. It's going to be long-lost relatives. It's going to

113
00:10:21,080 --> 00:10:24,760
be love found. And maybe as quickly as you found that love, something happens where that person is

114
00:10:24,760 --> 00:10:28,360
shipped away and we don't know what happened to them. Maybe it's the guy in the suit. He's from

115
00:10:28,360 --> 00:10:34,040
El Teatro Novela. I don't know. But our telenovelas have a finite beginning and end, and they're

116
00:10:34,040 --> 00:10:38,680
generally tied up with a very big happy element at the end. So for example, like the world's

117
00:10:38,680 --> 00:10:43,000
largest wedding. If you watch Casa de las Flores, there's kind of some of that maybe that's being

118
00:10:43,000 --> 00:10:48,360
built up too. But yes, there's always this idea we've got a fixed arc, a lot of intricate kind of

119
00:10:48,360 --> 00:10:53,240
heart-wrenching moments, but it's going to wrap up with something happy that we can all feel good

120
00:10:53,240 --> 00:10:58,360
about and move on with our lives and not wonder what happens to these characters. So if we're

121
00:10:58,360 --> 00:11:02,040
thinking about examples of telenovelas, let's take a look at some of the ones that are available to

122
00:11:02,040 --> 00:11:07,880
us in Spanish. So this is Kate del Castillo, who I swear, I think she's like my role model in life.

123
00:11:07,880 --> 00:11:14,040
She's fantastic. If any of you like watched that documentary about talking with Chapo, who's a

124
00:11:14,040 --> 00:11:19,240
narcotraficante, she was doing that with Sean Penn because that's what she does in her free time.

125
00:11:19,240 --> 00:11:24,280
But she's very famous. She's been in many telenovelas and one of the ones that is most

126
00:11:24,280 --> 00:11:30,920
known is La Reina del Sur. So that's an example of one that is really, really popular in Mexico.

127
00:11:30,920 --> 00:11:36,520
And then many of us may know Yo Soy Betty La Fea, because actually some of these telenovelas have

128
00:11:36,520 --> 00:11:40,760
crossovers in English. So the ones that I looked at, because again, we're predominantly an English

129
00:11:40,760 --> 00:11:46,280
speaking population, I looked at popular telenovelas in Spanish, looked for their English crossovers,

130
00:11:46,280 --> 00:11:51,640
and then worked with that text. So Queen of the South, which is actually on USA, talks to us a

131
00:11:51,640 --> 00:12:04,920
little bit about that narrative with Kate del Castillo. In this one, in Queen of the South,

132
00:12:04,920 --> 00:12:13,320
essentially what we have is the character who becomes the most prominent drug lord trafficker

133
00:12:13,320 --> 00:12:17,800
in South Spain after she has to flee Mexico because her beloved is actually captured by

134
00:12:17,800 --> 00:12:22,520
the cartel that's pushing drugs in that area. So of course, as one does, you relocate to another

135
00:12:22,520 --> 00:12:28,040
country and then you start your vast drug empire. So that one's going to have love lost, it's going

136
00:12:28,040 --> 00:12:33,400
to be a thriller, it's going to be a drama, and it's going to be high emotional kind of action

137
00:12:33,400 --> 00:12:39,800
impact. And then with Ugly Betty, what we have is more or less a rom-com. This actually started in

138
00:12:39,800 --> 00:12:43,480
Colombia and it's had, I think, three or four crossovers to different Spanish-speaking markets

139
00:12:44,680 --> 00:12:52,840
starting in 1999 in Colombia. But essentially what we have is a character, Beatriz, who works

140
00:12:52,840 --> 00:12:57,160
in the fashion industry and is considered ugly because stereotypical things like she has braces

141
00:12:57,160 --> 00:13:01,640
and she has glasses and things that I have all worn in my life. But anyways, it's all about the

142
00:13:01,640 --> 00:13:05,960
story about her struggles working in the fashion industry and trying to find love. And then Jane

143
00:13:05,960 --> 00:13:11,640
the Virgin actually, I know I said I looked for popular crossovers from Spanish to English,

144
00:13:11,640 --> 00:13:14,840
but for this one, because I think there's enough comfort and knowledge for people of what this one

145
00:13:14,840 --> 00:13:20,360
is, Jane the Virgin is actually a satirical melodrama, but it's fantastic because essentially

146
00:13:20,360 --> 00:13:25,080
we follow Jane, who is a virgin, who after visiting a doctor's office somehow becomes

147
00:13:25,080 --> 00:13:29,880
artificially inseminated and becomes pregnant. So she's a pregnant virgin and then various things

148
00:13:29,880 --> 00:13:34,680
that unfold afterwards and how she finds love or how she doesn't find love leads us into this

149
00:13:34,680 --> 00:13:40,840
complex story that is Jane the Virgin. So in thinking about telenovelas, we understand a

150
00:13:40,840 --> 00:13:45,720
little bit of the arc. We understand that's a melodrama. We understand that it has finite end.

151
00:13:45,720 --> 00:13:49,080
We understand it's going to wrap up with something happy and we understand some of the trips that

152
00:13:49,080 --> 00:13:56,920
may be employed. So now how can we start applying this to deep learning? When we think about machine

153
00:13:56,920 --> 00:14:00,760
learning, and I'm not sure if many of us in here do machine learning, some of the things that you

154
00:14:00,760 --> 00:14:05,640
may think of are some of the language that's provided to us by the founders of this discipline.

155
00:14:05,640 --> 00:14:10,520
So Arthur Samuel, who actually coined the term machine learning, actually provides us with this

156
00:14:10,520 --> 00:14:16,600
language saying that machine learning is a field of study where computers have the ability to learn

157
00:14:16,600 --> 00:14:20,760
without being explicitly programmed. So there's this idea that there's something that's not

158
00:14:20,760 --> 00:14:25,960
requiring us to actually have some intent or some purpose. So that is where we kind of can start

159
00:14:25,960 --> 00:14:31,480
with machine learning. Now, as we see here at PyCon, there's many various talks that may talk

160
00:14:31,480 --> 00:14:36,600
about data science or machine learning, and this is a very, very, very, very vast space. So if we

161
00:14:36,600 --> 00:14:41,240
were looking at, for example, technology and tools, we may see things like Python. We may be talking

162
00:14:41,240 --> 00:14:47,240
about things like Spark or Julia or other scientific programming languages. If we're talking about the

163
00:14:47,240 --> 00:14:52,120
kind of problems we can solve with machine learning, we'll see the problem categories on the bottom

164
00:14:52,120 --> 00:14:56,840
left-hand side. We might be doing classification, clustering, optimization kind of problems.

165
00:14:56,840 --> 00:15:00,840
However, what we want to focus on is this idea of a subfield. So when we talk about deep learning,

166
00:15:00,840 --> 00:15:09,080
deep learning is a subfield. It's a type of machine learning and is alongside other categories such as

167
00:15:09,080 --> 00:15:15,400
maybe ones you may have heard of, supervised, unsupervised, semi-supervised. So to broaden on

168
00:15:15,400 --> 00:15:19,080
that definition of machine learning and kind of challenging us to think about how we can actually

169
00:15:19,160 --> 00:15:25,400
have a deep learning model that can actually generate a manuscript, we probably need to

170
00:15:25,400 --> 00:15:30,280
reconceptualize what machine learning means. So this is a great definition given to us by Tom

171
00:15:30,280 --> 00:15:34,680
Mitchell, and Tom Mitchell, the former chair of the computer science department at Carnegie Mellon,

172
00:15:34,680 --> 00:15:39,800
who wrote, I think, the quintessential book on machine learning published in 1997. It stands

173
00:15:39,800 --> 00:15:44,280
up really well. I highly recommend reading it. Basically says, when we think about machine learning,

174
00:15:44,280 --> 00:15:49,560
it's not so much that it's a program that isn't explicitly programmed, but instead it's that we

175
00:15:49,560 --> 00:15:53,720
want to – there's something that we want to accomplish. So if we're thinking about a computer

176
00:15:53,720 --> 00:15:57,800
program that's said to learn, let's think about it in this framework. We have some objectives,

177
00:15:57,800 --> 00:16:02,840
so we have a task that we want to accomplish, and in order to accomplish that task, we need to be

178
00:16:02,840 --> 00:16:07,800
able to provide it with some ability to understand what that task is. So we have a task, and then we

179
00:16:07,800 --> 00:16:13,880
provide it some ability to understand what that task is, which is the experience. And then as

180
00:16:13,880 --> 00:16:18,760
that program continues to attempt to do that task, we can develop a performance measurement

181
00:16:18,760 --> 00:16:22,920
to understand how it's improving over time. So this language gets us a little bit closer

182
00:16:22,920 --> 00:16:26,600
to thinking about what machine learning is when applied to deep learning, but it doesn't get us

183
00:16:26,600 --> 00:16:30,920
out of the way. So the trend with machine learning over time – and it's kind of wild to think that

184
00:16:30,920 --> 00:16:34,760
in the 1950s and some of these algorithms that we're talking about today, you know, naive Bayes,

185
00:16:34,760 --> 00:16:38,200
for example, this is not new math. This is not new algorithms. These are things that have been

186
00:16:38,520 --> 00:16:44,600
that have been out there for 20-plus years. But what we're seeing in the trend of the world of

187
00:16:44,600 --> 00:16:48,520
machine learning and what does it mean for a machine to learn, we start with this realm of

188
00:16:48,520 --> 00:16:53,320
artificial intelligence. We move into machine learning, which is, again, that idea of a program

189
00:16:53,320 --> 00:16:58,440
not having to be explicitly programmed to do that task. And then we move into this world of deep

190
00:16:58,440 --> 00:17:02,840
learning, and we start hearing about these things called neural networks, which sounds very sci-fi

191
00:17:02,840 --> 00:17:10,280
and very unclear what that is. To frame it a little bit more simply, if we were to think about

192
00:17:10,280 --> 00:17:15,320
what it means that this evolution from AI to deep learning, we can say that an example of an

193
00:17:15,320 --> 00:17:20,280
artificial intelligence program is a door sensor because it's able to adapt to its environment

194
00:17:20,280 --> 00:17:24,760
around it. With a machine learning, we can say an example of a machine learning program is our spam

195
00:17:24,760 --> 00:17:30,440
filters. We all know it. We see it in our various email inboxes. The idea that we know spam when we

196
00:17:30,440 --> 00:17:35,240
see it, can we teach an algorithm how to do that? Yes, we can. And we've done that with machine

197
00:17:35,240 --> 00:17:42,360
learning. Now, if we're moving into the realm of deep learning, what we start getting into is this

198
00:17:42,360 --> 00:17:47,240
idea of using these neural networks that drives its own learning. That is the engine by which

199
00:17:48,280 --> 00:17:54,120
learning is empowered. And if you see on this right-hand side here, this project is called

200
00:17:54,360 --> 00:18:03,000
the deep dream, which came from Google research, wherein they are providing an image of an object

201
00:18:03,000 --> 00:18:09,800
to an algorithm with the desired intent and output to say, tell us what you see in this image. So

202
00:18:09,800 --> 00:18:13,640
what we start seeing on the other side is if we provide, let's say, an image of an apple, it's

203
00:18:13,640 --> 00:18:17,880
like, okay, I understand that this is an apple, but what other images do I see in it and what can I

204
00:18:17,880 --> 00:18:22,040
infer from this image that I'm seeing? So kind of flipping it a little bit on its head and getting

205
00:18:22,520 --> 00:18:26,760
into the realm of do Androids dream of electric sheep? That's where we start saying, okay, we're

206
00:18:26,760 --> 00:18:30,920
going to give you some idea, some definition, but using this neural network, we want you to drive

207
00:18:30,920 --> 00:18:38,440
your own learning. So if we were thinking about an example of applying deep learning to, let's say,

208
00:18:38,440 --> 00:18:43,400
a classification problem, if I had something like a flashlight and I wanted it to respond

209
00:18:43,400 --> 00:18:50,280
and learn from audio cues to turn on, we could say, let's say I want to have a flashlight that

210
00:18:50,280 --> 00:18:56,520
if I say the word dark, it turns on. Well, with a deep learning implementation, it wouldn't just

211
00:18:56,520 --> 00:19:01,000
have that fixed understanding of saying, okay, dark maps to turn on because this is the task

212
00:19:01,000 --> 00:19:07,240
of bringing light to the world, but instead it might understand other cues, other verbal cues

213
00:19:07,240 --> 00:19:13,880
that I might be able to supply to this model. So I might say something like I can't see or

214
00:19:13,880 --> 00:19:18,600
the light switch won't work. With a deep learning kind of implementation, the neural network would

215
00:19:18,600 --> 00:19:24,280
be able to learn and infer these other kinds of words that may fill in to suggest that there's an

216
00:19:24,280 --> 00:19:31,080
absence of light, to suggest that the area around you is dark. So this learning, again, that happens,

217
00:19:31,080 --> 00:19:35,240
the brain, which is that neural network, happens with an activation method.

218
00:19:37,240 --> 00:19:43,240
So moving now back to telenovelas, there's a type of problem called text generation that we're going

219
00:19:43,240 --> 00:19:48,120
to be looking at. So text generation is sequence processing, and sequence processing has a lot of

220
00:19:48,120 --> 00:19:52,920
applications in the world of deep learning. We can use it for video processing, price modeling,

221
00:19:52,920 --> 00:20:00,200
and yes, text generation. Essentially what we want to do is there's two approaches that we can have.

222
00:20:00,200 --> 00:20:05,880
Using language, so actually using a body of text, we can say we want you to look at the individual

223
00:20:05,880 --> 00:20:12,280
words that are in this text to actually generate some text on the other side, or we can actually

224
00:20:12,280 --> 00:20:17,320
develop the deep learning model to actually look at the individual characters that are provided in

225
00:20:17,320 --> 00:20:24,680
the corpus that we're working with. Either way, given the text that we provide to the

226
00:20:26,920 --> 00:20:31,800
sequence processing model that we use, what it's going to start trying to do is approximate the

227
00:20:31,800 --> 00:20:38,280
relationship of what words mean, what the relationship between one word is to another,

228
00:20:38,280 --> 00:20:43,240
and then as it tries to understand the relationship between words, try to think about how it is that

229
00:20:43,240 --> 00:20:48,120
it can build its own patterns and generate its own vocabulary that models the relationships it

230
00:20:48,120 --> 00:20:55,480
sees in this corpus of words that we provided. So I've been using the words neural networks

231
00:20:55,480 --> 00:21:01,160
in deep learning a little bit interchangeably, and while I want you to understand that there are

232
00:21:01,160 --> 00:21:07,400
these artificial neural networks, what essentially deep learning is, it actually makes use of several

233
00:21:07,400 --> 00:21:12,520
neural networks. And when we talk about neural networks, the power that drives a neural network

234
00:21:12,520 --> 00:21:18,040
is the neuron. So the neuron here, it's got three parts. We have the inputs, we have the weights,

235
00:21:18,040 --> 00:21:22,760
and then we have the activation function. So essentially what's happening is our words are

236
00:21:22,760 --> 00:21:29,560
going to be the inputs, and we're going to represent those as either words or as either characters.

237
00:21:29,560 --> 00:21:34,200
We then apply an arbitrary weight to it, and then as it runs through each layer in our neural

238
00:21:34,200 --> 00:21:38,040
network, it's going to start trying to approximate that relationship to understand relationships

239
00:21:38,040 --> 00:21:41,960
either from word to word or character to character, and then it's going to go through

240
00:21:42,920 --> 00:21:47,640
this activation function where as it's learning these relationships, it starts adjusting those

241
00:21:47,640 --> 00:21:52,360
weights to try to find the optimal weight to use to actually start generating a thing,

242
00:21:53,000 --> 00:21:59,640
or in our case, generating text. So the activation function, you can think of something like a

243
00:21:59,640 --> 00:22:04,680
sigmoid function. So essentially what we have is some kind of function that is a nonlinear function

244
00:22:05,560 --> 00:22:10,840
that is going to allow us to actually start doing some math. So if we have a weight applied to a

245
00:22:10,840 --> 00:22:16,920
word, let's say it starts at 0.5, but we need to start shifting it either higher or lower based on

246
00:22:16,920 --> 00:22:21,160
the relationship between these words that the network is learning, we've got this activation

247
00:22:21,160 --> 00:22:25,480
function to help us do that. So when we're training a neuron, essentially what we need to do

248
00:22:25,480 --> 00:22:31,000
is these three steps. The neuron takes an initial guess at classifying the sample, and then updates

249
00:22:31,000 --> 00:22:35,880
the guess by adjusting the weights, and then it goes ahead and repeats and tries again.

250
00:22:35,880 --> 00:22:41,880
So this process is very iterative. And with neural networks, there's many kinds of neural

251
00:22:41,880 --> 00:22:47,560
networks that we can use. Feedforward, radial basis function, or an RNN. And RNNs are good

252
00:22:47,560 --> 00:22:52,440
for sequential processing problems like text generation. And what's really cool about this

253
00:22:52,440 --> 00:22:57,160
is that it actually allows us to propagate data back and forth. So as our neurons are all connected,

254
00:22:57,960 --> 00:23:02,440
so a deep neural network would be an example of what we have on the right-hand side. Basically,

255
00:23:02,440 --> 00:23:07,160
it's a directed graph. We have input going from one node, but also we can move backwards. So using

256
00:23:07,160 --> 00:23:11,880
an RNN, insights learned upstream or downstream can move back and forth across these neurons.

257
00:23:13,800 --> 00:23:18,440
So if we wanted to make an RNN in Python, how do we do it? Well, there's more than one way,

258
00:23:18,440 --> 00:23:25,080
as we like to do. So there's three really popular frameworks, Keras, TensorFlow, and PyTorch.

259
00:23:25,560 --> 00:23:31,400
And yes, I'm going to borrow the language from the 2017 keynote, but yes, Python is surprisingly

260
00:23:31,400 --> 00:23:36,520
super awesome, or maybe unsurprisingly super awesome at scientific programming. So these three

261
00:23:36,520 --> 00:23:39,800
frameworks, when you're thinking about which ones to use, I like to think about questions to help me

262
00:23:39,800 --> 00:23:44,680
think about what tool is best for the task at hand. How much technical expertise is needed

263
00:23:44,680 --> 00:23:49,160
to start using it? What are your requirements? For example, do you have a certain SLO you have to

264
00:23:49,160 --> 00:23:53,640
meet? What's the size of data you're working on, working with? And also, how easy is it to start

265
00:23:53,640 --> 00:24:01,080
using the framework? So if we're looking at making an RNN in Python using Keras,

266
00:24:02,280 --> 00:24:06,520
we've got these three steps that we're always going to be emulating. Basically, we have to take that

267
00:24:07,480 --> 00:24:12,760
text and we have to transform it either into characters or words. So we tokenize it. Then

268
00:24:12,760 --> 00:24:17,720
once we've tokenized it, we generate these hot encodings, which basically gives us, let's say,

269
00:24:18,440 --> 00:24:23,080
on the next slide, this actually gives us a nice example. Let's say our corpus is the word bad.

270
00:24:24,200 --> 00:24:31,560
And because we've got the letter B, what we want to do is we've got nine characters in our corpus.

271
00:24:31,560 --> 00:24:35,400
We have other words that introduce these other characters. What we want to do is basically be

272
00:24:35,400 --> 00:24:42,120
able to understand and predict the likelihood for each corpus, for each character in our corpus,

273
00:24:42,120 --> 00:24:46,680
to actually say what's the likelihood that this character will appear. So as we generate this hot

274
00:24:46,680 --> 00:24:52,120
character encoding, we're going to use that as input to our neural network, and we're then going

275
00:24:52,120 --> 00:24:58,280
to pass it through our RNN and basically say, hey, start learning the weights and start coming up

276
00:24:58,280 --> 00:25:04,520
with the optimal weight to represent the likelihood of what character you should be generating based

277
00:25:04,520 --> 00:25:08,840
on these, again, trying to approximate that relationship in the underlying text, and let's

278
00:25:08,840 --> 00:25:13,000
see what we get on the other side. And to train this, we're going to run it through some number

279
00:25:13,000 --> 00:25:20,760
of epochs. So with Keras, what we can notice here, for those of you who may have used scikit-learn,

280
00:25:21,400 --> 00:25:27,080
we've got a really, really nice high-level API we can use. Notice how I said sequential

281
00:25:27,080 --> 00:25:32,600
processing is a text generation is a sequential processing problem? Surprise, we're going to use

282
00:25:32,600 --> 00:25:39,640
a sequential model. So LSTM, long short-term memory, is a type of RNN propagating that data

283
00:25:39,640 --> 00:25:44,200
back and forth. This is pretty straightforward because it allows us to pull in the sequential

284
00:25:44,200 --> 00:25:50,600
model. We specifically say we want to use an RNN of this type. We then basically add in our data,

285
00:25:51,160 --> 00:25:54,840
and then we say, okay, let's go ahead and compile that model, and notice this fit. We're going to

286
00:25:54,840 --> 00:25:58,600
have 20 epochs that we're going to train over, and what we want to do is we want to find that

287
00:25:58,600 --> 00:26:03,240
optimal weight that we can generate. So Keras has a very nice and user-friendly interface that we

288
00:26:03,240 --> 00:26:11,000
can start using. And if you were kind of looking at what it may look like for that process of

289
00:26:11,000 --> 00:26:15,640
running through epochs, well, you can just basically think of a for loop. It's very iterative.

290
00:26:15,640 --> 00:26:19,800
If we have 20 loops or we have 1,000, we can start doing things like that. And then when we start

291
00:26:19,800 --> 00:26:27,400
wanting to actually use it, we've got that model.predict functionality. So Keras is pretty cool.

292
00:26:27,960 --> 00:26:34,200
Now let's look at TensorFlow. Mind you, I did some lifting of this data, so there's some logic

293
00:26:34,200 --> 00:26:39,960
that's kind of not captured, but it is actually in my GitHub repository. But again, when we have

294
00:26:40,440 --> 00:26:46,040
when we're using TensorFlow, again, we import TensorFlow, and we've actually got an RNN that

295
00:26:46,040 --> 00:26:51,800
we can pull in. Unlike in the last case where we've got the sequential model and we've got

296
00:26:51,800 --> 00:26:56,920
the LSTM that's kind of baked in, we're going to actually have to start making and creating our

297
00:26:56,920 --> 00:27:01,800
own object to represent the RNN. We're going to be working with tensors, which is, again,

298
00:27:01,800 --> 00:27:08,040
a representation of the vector data. And what we need to do then is start writing a definition of

299
00:27:08,040 --> 00:27:13,800
the RNN that we want. So notice that we've got the basic LSTM here that we can pull in.

300
00:27:14,360 --> 00:27:17,000
But essentially when we're using TensorFlow, we're going to have to have a little bit more

301
00:27:17,000 --> 00:27:22,280
knowledge about the underlying we're not given that high-level API interface where it just says,

302
00:27:22,280 --> 00:27:26,360
hey, here's the thing out of the box. You actually have to go and take the components and actually

303
00:27:26,360 --> 00:27:32,440
start building it yourself. And likewise, then when you're actually optimizing the weights,

304
00:27:32,440 --> 00:27:37,080
what you have to do is you actually have to run a session, a TensorFlow session, and notice again,

305
00:27:37,080 --> 00:27:41,240
we have this epoch loop. We're going to go ahead and do N iterations. And then when we find the

306
00:27:41,240 --> 00:27:46,200
optimal weight, what happens in both cases is you can export those optimal weights once you've found

307
00:27:46,200 --> 00:27:52,680
them after running so many epoch sessions. Those weights then, let's say they're in a flat file,

308
00:27:52,680 --> 00:27:57,640
you can then import those for later use. So TensorFlow, we don't have that high-level

309
00:27:57,640 --> 00:28:01,400
interface. We've actually got to kind of meld some of those components together ourselves.

310
00:28:01,400 --> 00:28:06,280
But we understand what we're doing. We've got to build the model. We've got to fit the model.

311
00:28:06,280 --> 00:28:14,600
We've got to train the model. And with PyTorch, PyTorch actually gets a lot more interesting.

312
00:28:14,600 --> 00:28:19,080
And you have a lot more flexibility to go in there and build things as you want. So notice here as

313
00:28:19,080 --> 00:28:26,680
we're building our RNN, with PyTorch, notice how we're having to actually build the interface for

314
00:28:26,680 --> 00:28:32,200
actually how forward propagation works. So remember how data can move forward and backward? Now you

315
00:28:32,200 --> 00:28:35,720
are able to actually go in and write that logic how that happens and how you're interacting with

316
00:28:35,720 --> 00:28:41,400
these other neurons. So the interface actually is much more low-level. And again, when we're

317
00:28:41,400 --> 00:28:47,080
training, we have that great epoch loop that we've come to know and love. And also when we

318
00:28:47,080 --> 00:28:55,160
get our output, we can then export that output to then import to our model for later use as well.

319
00:28:56,680 --> 00:29:03,240
So with those three neural networks examples, what actually can we start generating with

320
00:29:03,240 --> 00:29:09,800
an RNN using Python? This is some of the most coherent text I actually started developing,

321
00:29:09,800 --> 00:29:17,320
which is to say that it is a work in progress. So one of the things I noticed is as I started

322
00:29:17,320 --> 00:29:22,280
asking to make longer pieces of text, it just started getting really, really wild. Remember,

323
00:29:22,280 --> 00:29:28,360
the whale did that. So some of the more coherent units of text I found was actually getting

324
00:29:28,360 --> 00:29:34,920
character strings of about 200. So why is that? This kind of feels like it's in line that it could

325
00:29:34,920 --> 00:29:39,640
be in a telenovela. Don't threaten me. Do you not understand me? Well, well, well, well, right.

326
00:29:39,640 --> 00:29:45,960
Maybe we're making that famous fight scene all over. But in thinking about the tooling that we

327
00:29:45,960 --> 00:29:51,080
need to build, think about those three questions. How much technical expertise is needed to start

328
00:29:51,080 --> 00:29:56,120
with a framework? I've told you that I'm new to deep learning or relatively new, so I probably

329
00:29:56,120 --> 00:29:59,160
want something that's a little bit more beginner-friendly, something that allows me

330
00:29:59,160 --> 00:30:03,720
to build a small toy project quickly, and something that has, for me, this is super important,

331
00:30:03,720 --> 00:30:09,000
a very, very good robust user group that gives lots of documentation and lots of examples of

332
00:30:09,000 --> 00:30:15,480
how to use it. For my second question, what are my requirements? Well, actually, because I focused

333
00:30:15,480 --> 00:30:19,960
on just looking at those three English telenovelas, I actually did not have that much data. I had less

334
00:30:19,960 --> 00:30:26,440
than 100 megabytes. And it was actually just looking at the raw text spoken word data itself

335
00:30:26,440 --> 00:30:31,800
that wasn't including things like sometimes there's cues in scripts that say something like, oh,

336
00:30:31,800 --> 00:30:38,120
15-second pause with a dramatic stare to the side. So those weren't actually, those kind of nitty

337
00:30:38,120 --> 00:30:42,600
gritty pieces of information I actually didn't have in my text that I was working with. So

338
00:30:42,600 --> 00:30:49,560
thinking about that I had a small data set and something that I wanted to have some ability to

339
00:30:49,560 --> 00:30:53,560
debug a little bit more friendly, let's continue on to my third requirement, which is basically

340
00:30:53,560 --> 00:30:58,680
how easy is it to start using a framework? And essentially, what I wanted was to get something

341
00:30:58,680 --> 00:31:02,520
that allowed me just to be able to bootstrap and get going because I just wanted to know what it

342
00:31:02,520 --> 00:31:07,000
means to actually start working with text, start processing text, and actually start doing text

343
00:31:07,000 --> 00:31:19,160
generation. And all that to say that the one that won for me, notice my priorities? Caris? Yes! I

344
00:31:19,160 --> 00:31:23,400
opted to work with Caris. So what's really cool about TensorFlow is you can actually,

345
00:31:24,120 --> 00:31:29,160
Caris you can plug in and you can use with TensorFlow and you can do the, you can use

346
00:31:29,160 --> 00:31:36,360
TensorFlow and Caris quite nicely together. But on the side of how to make a telenovela script a bit

347
00:31:36,360 --> 00:31:41,960
more intelligible and more interesting, think of it this way. Maybe I can write along an RNN model

348
00:31:41,960 --> 00:31:46,120
for each character that I want in my plot. So maybe like the David Hasselhoff, maybe I have

349
00:31:46,120 --> 00:31:51,080
five characters in the fight scene and I want a model for each of them. Maybe I look for similar

350
00:31:51,080 --> 00:31:55,560
sources of inspiration. Maybe I'm like, okay, I want telenovelas that have strong women leads.

351
00:31:55,560 --> 00:32:01,080
And maybe that's what I can use as the input for my strong women lead LSTM model. Also, what we

352
00:32:01,080 --> 00:32:05,560
can start doing is weaving these models together or maybe even having models based on categories.

353
00:32:05,560 --> 00:32:16,200
So long lost love, rom com, things like that. So all that to say, and I know this is a little bit

354
00:32:16,200 --> 00:32:21,320
of a cop out, but yeah, text generation is hard. It's pretty hard. So creating a learned plot,

355
00:32:21,320 --> 00:32:25,240
you know, sorry, I don't know if I'm going to come up with a consultancy tomorrow to take away all

356
00:32:25,240 --> 00:32:34,040
of Hollywood's jobs, but I'm going to try. So how can we do better? There's no surprise here. One,

357
00:32:34,040 --> 00:32:39,640
always validating your data. As I kind of said, there's a lot of time spent processing and looking

358
00:32:39,640 --> 00:32:43,800
at that data. So notice there's some strange characters on the right-hand side. Notice on the

359
00:32:43,800 --> 00:32:49,000
left-hand side you get this character, the Latino lover who's the narrator, which is a very strange

360
00:32:49,000 --> 00:32:55,080
thing to read out of context. But notice how on the right-hand side we may include character names,

361
00:32:55,080 --> 00:32:58,920
but I had some text that actually didn't name the characters. That got a little bit more complex.

362
00:32:58,920 --> 00:33:02,840
So data validation is always a thing. And on that note, I'm just going to say to you all,

363
00:33:02,840 --> 00:33:07,160
if you want to add to my corpus of ever-growing telenovela data, you should add. I'm collecting

364
00:33:07,160 --> 00:33:14,520
it here, and it's a really fun process and really interesting. But yeah, text generation is difficult.

365
00:33:14,520 --> 00:33:19,800
We've probably all seen this one. The surprise here is deep learning is only as smart as you make

366
00:33:19,800 --> 00:33:25,640
it. So if you have more data to provide it, the better it is to use. So we've seen a lot of deep

367
00:33:25,640 --> 00:33:30,840
learning models get hacked out in the wild, for example, obscuring your face because if it's

368
00:33:30,840 --> 00:33:34,680
learning that a human face is oval and your hair is pulled back, but then you have this wild haircut,

369
00:33:35,480 --> 00:33:39,560
if the deep learning model is actually learning that the relationship to a human face is a certain

370
00:33:39,560 --> 00:33:43,640
shape and we're representing that data as such, again, we have to think about all these things and

371
00:33:43,640 --> 00:33:47,880
how the model is actually only learning the relationship that we provide it. So if you want

372
00:33:47,880 --> 00:33:53,000
to keep learning, I have examples of various resources here. Again, I will share these slides

373
00:33:53,000 --> 00:34:00,600
on Twitter. And that is the end of my talk. But as a total, total serious ask, if you do own

374
00:34:00,600 --> 00:34:07,160
Mariela del Barrio, I was looking everywhere for it. I ordered it on VHS. We'll see if it arrives.

375
00:34:07,160 --> 00:34:11,800
Also, that gets into the place that I now need to have something that works with VHS. But that

376
00:34:11,800 --> 00:34:16,200
would be great. But if you do have any questions about this or want to kind of look at the

377
00:34:16,200 --> 00:34:22,520
repository and dig in and maybe help me make a more robust telenovela, come talk with me. I would

378
00:34:22,520 --> 00:34:33,560
love to see what we can create. Thank you.

