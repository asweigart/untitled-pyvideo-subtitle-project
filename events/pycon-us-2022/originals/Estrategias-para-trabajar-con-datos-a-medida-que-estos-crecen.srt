1
00:00:00,000 --> 00:00:20,400
Buenas tardes. Vamos a comenzar. Recuerden ahí mandarle mensajito a todos los que puedan

2
00:00:20,400 --> 00:00:31,240
que vengan a la foto para el cierre. Entonces, Marcos Carranza tiene el honor de hacer el cierre

3
00:00:31,240 --> 00:00:39,960
de esta PyCon charlas edición 2022 y nos trae una charla sobre estrategias para trabajar con

4
00:00:39,960 --> 00:00:44,680
data a medida que esta crece. Entonces, démosle la bienvenida.

5
00:00:44,680 --> 00:00:56,520
Gracias. ¿Se escucha bien o hablo un poquito más fuerte? Perfecto. Bueno, el nombre de mi

6
00:00:56,520 --> 00:01:00,840
charla, como mencionaron, es estrategias para trabajar con la data a medida que esta crece.

7
00:01:00,840 --> 00:01:09,080
Bueno, hola a todos. Mi nombre es Marco Carranza. Yo soy un usuario de Python y también soy un

8
00:01:09,080 --> 00:01:15,080
emprendedor tecnológico. Soy el co-founder de la empresa Teamcore Solutions. Bueno, he tenido la

9
00:01:15,080 --> 00:01:20,240
suerte de poder utilizar Python al menos exhaustivamente los últimos 15 años y he logrado

10
00:01:20,240 --> 00:01:27,800
desarrollar soluciones tecnológicas en el sector retail que afortunadamente han llegado a tener una

11
00:01:27,800 --> 00:01:32,000
escala global, ¿no? Porque básicamente hemos desarrollado soluciones para toda la industria

12
00:01:32,000 --> 00:01:37,720
de consumo masivo en toda América. Bueno, como resumen cortito, en Teamcore Solutions nosotros

13
00:01:37,720 --> 00:01:43,080
procesamos la información de ventas a través de algoritmos de machine learning y gracias a eso

14
00:01:43,080 --> 00:01:48,200
nosotros le proporcionamos a las empresas de consumo masivo visibilidad sobre la ejecución

15
00:01:48,200 --> 00:01:53,960
y el estado de sus productos en cada una de sus tiendas, ¿no? Generando insights y acciones

16
00:01:53,960 --> 00:01:58,320
específicas que puedan ayudar a los equipos que están en la oficina y también visitando las tiendas

17
00:01:58,320 --> 00:02:05,640
a que solucionen los problemas de forma proactiva, ¿no? Bueno, el objetivo de esta charla es ver

18
00:02:06,360 --> 00:02:11,680
distintas alternativas para el procesamiento de datos utilizando distintos tipos de tecnología,

19
00:02:12,680 --> 00:02:18,360
básicamente enfocado en el uso de data frames. Si bien la charla va a estar enfocado un poco

20
00:02:18,360 --> 00:02:23,400
más en la librería Pandas que es la librería más popular para el procesamiento de datos,

21
00:02:23,400 --> 00:02:30,440
también el objetivo de la sesión es mostrar algunas otras tecnologías existentes que les

22
00:02:30,440 --> 00:02:40,080
puedan servir para procesar data a medida que estas crezcan, ¿no? Bueno, primero, la primera parte de

23
00:02:40,080 --> 00:02:44,120
la presentación está un poco más enfocada en Pandas y vamos a ver algunas técnicas de cómo

24
00:02:44,120 --> 00:02:53,720
optimizar los data frames y cómo controlar y mantener la memoria lo más baja posible y bueno,

25
00:02:53,720 --> 00:02:58,080
y después vamos a ir viendo algunas opciones para procesar archivos mucho más grandes, ¿no? Después

26
00:02:58,080 --> 00:03:04,240
vamos a ver algunas herramientas como Modin que es una herramienta que nos permite utilizar todos

27
00:03:04,240 --> 00:03:10,320
los núcleos del procesador para realizar ciertos cálculos sobre nuestros data frames y que al

28
00:03:10,320 --> 00:03:16,120
mismo tiempo requieren muy poca modificación del código de Python y luego vamos a dar un vistazo

29
00:03:16,120 --> 00:03:24,160
rápido por la librería Bikes que es una librería tal vez no es tan conocida pero permite procesar

30
00:03:24,160 --> 00:03:28,960
bloques de datos muy grandes independientes del tamaño de la memoria, ¿no? Y bueno,

31
00:03:28,960 --> 00:03:32,600
y finalmente haremos una pasada muy rápida por PicePark y veremos sus ventajas.

32
00:03:34,680 --> 00:03:41,520
Bueno, como ya sabemos, la data aumenta cada año, algunos estudios muestran que cada dos años

33
00:03:41,520 --> 00:03:48,160
está, está se va duplicando y hoy en día debido a que la data cada vez es más grande se hace casi

34
00:03:48,160 --> 00:03:53,960
imposible procesarlos en computadores de escritorio, ¿no? Entonces para resolver estos problemas

35
00:03:53,960 --> 00:03:59,640
durante los últimos años han surgido distintas nuevas tecnologías, por ejemplo, este, tecnologías

36
00:03:59,640 --> 00:04:04,480
que están enfocadas en procesar big data o procesar la data en clusters pueden ser tecnologías como

37
00:04:04,480 --> 00:04:12,640
Hadoop, como Dask, Presto, etcétera, etcétera y bueno y en la práctica cuando uno necesita resolver

38
00:04:12,640 --> 00:04:17,920
un problema en el mundo real necesita, se ve obligado a combinar distintas tecnologías, ¿no?

39
00:04:17,920 --> 00:04:26,760
Indirectamente uno va construyendo un pipeline de datos. Bueno, el problema es que muchas veces

40
00:04:26,760 --> 00:04:33,720
nosotros tal vez este somos científicos o trabajamos en una startup o queremos hacer alguna prueba de

41
00:04:33,720 --> 00:04:40,120
concepto y en el momento en particular no tenemos tiempo suficiente para, para configurar tal vez

42
00:04:40,120 --> 00:04:47,360
distintas herramientas, un pipe mucho más, más elaborado porque tenemos básicamente o restricciones

43
00:04:47,360 --> 00:04:53,480
de presupuesto o tenemos muy poco tiempo disponible porque bueno para los startups el time to market

44
00:04:53,480 --> 00:05:00,800
es es muy muy importante, ¿no? Y además otro otro problema clásico es que al inicio nosotros

45
00:05:00,800 --> 00:05:05,440
empezamos a trabajar con sets de datos muy pequeñitos, empezamos a procesar la data,

46
00:05:05,440 --> 00:05:11,240
todo funciona bien y a medida que esta data aumenta de tamaño y la empezamos a procesar

47
00:05:11,240 --> 00:05:16,240
empiezan a fallar ciertas cosas, ¿no? Nos empezamos a dar cuenta que se requiere un proceso de limpieza

48
00:05:16,240 --> 00:05:22,000
mucho más exhaustivo o simplemente nos damos cuenta de que los recursos que tenemos en nuestra

49
00:05:22,000 --> 00:05:27,320
máquina no son suficientes, ¿no? Bueno, en algunos casos con algunas optimizaciones muy sencillas es

50
00:05:27,320 --> 00:05:37,240
relativamente fácil, fácil reducir el consumo de la memoria o acelerar los procesos pero en otros

51
00:05:37,240 --> 00:05:44,120
casos se necesitan hacer unas cositas un poco más complejas, ¿no? Bueno, hablemos un poco de pandas,

52
00:05:44,120 --> 00:05:49,520
a continuación vamos a ver algunos pequeños trucos o algunas técnicas muy simples que se

53
00:05:49,520 --> 00:05:55,160
pueden aplicar en las cargas de trabajo para para para reducir el consumo de memoria o la mayoría

54
00:05:55,160 --> 00:06:00,920
de ustedes debe saber pandas es es la librería de procesamiento de datos más popular en el

55
00:06:00,920 --> 00:06:09,160
ecosistema de python. Bueno, el tip número uno es trabajar con los sparse data structures,

56
00:06:09,160 --> 00:06:13,920
básicamente cuando nosotros leemos la información por lo general leemos bloques de datos,

57
00:06:13,920 --> 00:06:20,080
llega llena de columnas vacías, llena de celdas vacías, entonces cuando la data se incrementa,

58
00:06:20,080 --> 00:06:25,360
las celdas que están vacías también ocupan un espacio de memoria, ¿no? Entonces este

59
00:06:26,920 --> 00:06:31,960
pandas tiene un tipo de dato especial, un tipo objeto especial que si se lo aplicamos a la columna

60
00:06:31,960 --> 00:06:39,160
podemos hacer una reducción significativa de la memoria, ¿no? Si vemos un ejemplo acá tenemos un

61
00:06:39,480 --> 00:06:46,080
pequeño data frame como pueden ver en la primera imagen está llena de valores nulos y si hacemos

62
00:06:46,080 --> 00:06:50,840
un análisis de cuánta memoria está consumiendo esa columna nos damos cuenta que consume aproximadamente

63
00:06:50,840 --> 00:06:58,840
19 megas, ¿no? Pero si cambiamos el tipo de datos al sparse type podemos reducirlo a 11 megas,

64
00:06:58,840 --> 00:07:04,640
¿no? Entonces con un solo comando podemos bajar el consumo en un 41%, pero si esto lo llevamos a

65
00:07:04,640 --> 00:07:11,600
un archivo mucho más grande podemos estar hablando de gigabytes de ahorro, ¿no? El segundo truco que

66
00:07:11,600 --> 00:07:18,200
es obvio para los que conocemos tal vez tenemos un mayor conocimiento de data science y de estadística

67
00:07:18,200 --> 00:07:23,200
es el muestreo, hay muchas formas de hacer muestreo, pandas tiene ciertas herramientas que nos permiten

68
00:07:23,200 --> 00:07:28,000
hacer el muestreo, entonces básicamente lo que busca el muestreo es que nosotros obtengamos un

69
00:07:28,000 --> 00:07:33,760
subconjunto de datos para que podamos analizarlo y encontrar la información y encontrar ciertas

70
00:07:33,760 --> 00:07:39,760
cosas, ciertos insights, pero básicamente el objetivo es de que el análisis se pueda hacer mucho

71
00:07:39,760 --> 00:07:48,000
más rápido, ¿no? Bueno acá tenemos un ejemplo súper sencillo, la función se llama sample,

72
00:07:48,000 --> 00:07:54,080
tiene una diversidad de parámetros y básicamente en el ejemplo hacemos un muestreo de los primeros

73
00:07:54,080 --> 00:08:00,880
mil registros y luego comparamos en la segunda imagen un histograma en el cual vemos cómo está

74
00:08:00,880 --> 00:08:06,320
distribuida la data en ambos casos y vemos que se mantiene, si el muestreo se hace de forma

75
00:08:06,320 --> 00:08:11,160
adecuada uno puede trabajar con menos data, pero hay que tener cuidado porque hay veces que hay

76
00:08:11,160 --> 00:08:16,800
set de datos que son tan complejos que el muestreo como que no funciona muy bien, en especial cuando

77
00:08:16,800 --> 00:08:21,080
tenemos demasiados casos borde, entonces hay que tener siempre un poquito cuidado con eso.

78
00:08:22,480 --> 00:08:30,720
El tercer tip es algo que tal vez no es tan obvio, muchas veces las fuentes de datos cuando

79
00:08:30,720 --> 00:08:35,520
descargamos información los data set vienen con cientos de columnas o decenas de columnas,

80
00:08:35,520 --> 00:08:40,280
entonces muchas de estas columnas son innecesarias, entonces con el simple hecho de que yo cargue

81
00:08:40,280 --> 00:08:49,280
menos columnas voy a ahorrar mucha más memoria, ¿no? Por ejemplo acá en este ejemplo estoy abriendo

82
00:08:49,280 --> 00:08:58,200
un archivo csv que es bastante extenso, si no me equivoco tiene como 25 millones, 24 millones de

83
00:08:58,200 --> 00:09:06,880
registros, consume aproximadamente 4.5 gigabytes de memoria lo cual es considerable, no es raro

84
00:09:06,880 --> 00:09:12,760
ver este tipo de data sets, entonces simplemente por optar por elegir y trabajar sólo con las

85
00:09:12,760 --> 00:09:17,320
columnas más importantes puedo reducir drásticamente, en este caso seleccionamos

86
00:09:17,320 --> 00:09:23,640
sólo un subconjunto de columnas y la memoria se pudo reducir a 300 megas, lo cual es un ahorro

87
00:09:23,640 --> 00:09:32,520
bastante significativo. El cuarto tip es los tipos de datos numéricos, hay que entender que Pandas

88
00:09:32,520 --> 00:09:41,840
cuando lee un set de datos, un archivo csv, por ejemplo que son un archivo en excel, siempre va a

89
00:09:41,840 --> 00:09:47,760
tratar de adivinar el tipo de datos y bueno y lo que yo he visto la mayoría de las veces es que a

90
00:09:47,760 --> 00:09:53,160
todos los números les pone IN64 que es el tipo numérico que consume mucho más memoria,

91
00:09:53,800 --> 00:10:02,600
entonces obviamente existen otros tipos de datos el 8, el 16 y cada uno si bien es capaz de almacenar

92
00:10:02,600 --> 00:10:10,640
mucho menos información obviamente consume menos memoria, entonces en este ejemplo podemos ver que

93
00:10:10,640 --> 00:10:16,440
tenemos un data frame donde tenemos una columna donde se almacena el data, entonces no tiene ningún

94
00:10:16,440 --> 00:10:23,080
sentido utilizar un tipo de datos IN64 para almacenar el data porque nadie vive más de 120

95
00:10:23,080 --> 00:10:27,800
130 años que creo que el récord, entonces simplemente por el cambiar el tipo de datos se

96
00:10:27,800 --> 00:10:33,320
puede hacer una reducción bastante significativa en este ejemplo con un par de líneas logramos

97
00:10:33,320 --> 00:10:41,680
reducir la memoria en un 87.5 por ciento. Bueno después de eso viene la data que no es numérica

98
00:10:41,680 --> 00:10:47,600
que es la data categórica, entonces estamos hablando de data que son categorías, entonces

99
00:10:47,680 --> 00:10:57,480
que yo tengo datos que traen no sé características o texto es muy probable que yo pueda agrupar eso

100
00:10:57,480 --> 00:11:02,920
con este tipo de datos y eso al mismo tiempo en vez de que el data frame tenga el texto repetido

101
00:11:02,920 --> 00:11:07,840
en todas las columnas como que internamente tiene un muntero, hace un diccionario donde

102
00:11:07,840 --> 00:11:16,760
tiene los tipos de datos exactos y con eso se logra reducir la memoria. Acá tenemos otro ejemplo

103
00:11:16,760 --> 00:11:26,200
y en este data frame tenemos la columna sexo donde básicamente hay dos valores, el carácter F y M,

104
00:11:26,200 --> 00:11:31,720
pero como el data frame es gigantesco, esa columna está ocupando 142 megas,

105
00:11:31,720 --> 00:11:36,320
entonces simplemente por aplicar el tipo de datos categórico podemos reducirlo a 2 megas,

106
00:11:36,320 --> 00:11:42,400
entonces estamos hablando de una reducción muy importante de una reducción de 98 por ciento que

107
00:11:42,400 --> 00:11:50,360
es algo bastante significativo. Y bueno el tip número 6 ya para ir cerrando la parte de pandas

108
00:11:50,360 --> 00:11:57,360
es leer la data por pedazos también conocido por los chunks, muchas veces tenemos archivos muy

109
00:11:57,360 --> 00:12:06,760
grandes y yo necesito solamente hacer un cálculo sencillo, entonces la estrategia es leer un grupo

110
00:12:06,760 --> 00:12:12,160
de líneas del archivo, hacer el proceso y luego pasar al siguiente bloque, entonces esto va a

111
00:12:12,160 --> 00:12:17,160
evitar que en pandas como saben cuando uno abre un archivo muy grande puede consumir,

112
00:12:17,160 --> 00:12:20,840
o sea trata a pandas siempre colocar el archivo, toda la data directo en memoria,

113
00:12:20,840 --> 00:12:24,040
entonces si el archivo es demasiado grande va a consumir toda la memoria, va a fallar,

114
00:12:24,040 --> 00:12:30,480
se va a caer el proceso y el sistema operativo va a matar el proceso, entonces esta es una estrategia

115
00:12:30,640 --> 00:12:39,120
bastante simple que nos permite procesar archivos grandes, la forma de implementarlo es bastante

116
00:12:39,120 --> 00:12:45,840
simple hay que iterar, se le pasa un parámetro al momento de leer los sbs directamente desde pandas,

117
00:12:45,840 --> 00:12:52,640
el parámetro chunk size yo le indico cuántas líneas quiero leer al mismo tiempo, entonces de esa

118
00:12:52,640 --> 00:12:57,800
manera yo puedo leer de medio millón en medio millón, hacer un proceso, guardar el resultado

119
00:12:57,800 --> 00:13:04,280
en alguna estructura de datos, iterar hasta que termino de leer todos los archivos y finalmente

120
00:13:04,280 --> 00:13:10,520
obtengo un resultado, pero ojo que hay que tener igual un poquito cuidado porque en algunas

121
00:13:10,520 --> 00:13:17,240
situaciones es necesario hacer tal vez un proceso un poco más complejo que requiere tal vez leer

122
00:13:17,240 --> 00:13:22,360
toda la data para sacar un indicador de algún tipo, entonces cuando se dan esas situaciones

123
00:13:22,440 --> 00:13:28,840
esta estructura temporal se puede volver medio complicada y tal vez no sea la mejor estrategia,

124
00:13:28,840 --> 00:13:36,960
entonces ahí un pequeño warning. Bueno ahora voy a hablar muy brevemente acerca de modding,

125
00:13:36,960 --> 00:13:46,600
¿qué cosa es modding? Es una librería multiproceso que su API es similar a la de pandas,

126
00:13:46,800 --> 00:13:55,400
esto quiere decir de que simplemente con una línea yo cambio el import de pandas,

127
00:13:55,400 --> 00:14:05,320
en vez de importar pandas importo la librería de modding, automáticamente no tengo que tocar

128
00:14:05,320 --> 00:14:12,520
nada más en el código, entonces esto es gracias a que la API es idéntica. ¿Qué es lo que me permite,

129
00:14:12,600 --> 00:14:17,520
qué es lo que gano al hacer esto? Bueno modding tiene algunas características interesantes,

130
00:14:17,520 --> 00:14:23,560
por ejemplo me permite que cuando tengo que hacer algún cálculo, algún procesamiento,

131
00:14:23,560 --> 00:14:29,680
algo con mi data frame lo puedo hacer mucho más rápido porque modding lo que hace es lo vuelve

132
00:14:29,680 --> 00:14:34,680
multiproceso y empieza a aprovechar todos los núcleos que tengo disponibles en la máquina.

133
00:14:34,680 --> 00:14:41,840
Bueno en la forma de instalar es bastante directo, pip install modding, bueno como lo mencioné hace un

134
00:14:41,840 --> 00:14:46,040
momento la implementación de pandas es single threaded, entonces significa que utiliza solo

135
00:14:46,040 --> 00:14:52,600
un core cuando está haciendo algún procesamiento, entonces al momento de utilizar la implementación

136
00:14:52,600 --> 00:14:59,880
que nos da modding esto me permite utilizar todos los núcleos que tengo disponible en la máquina,

137
00:14:59,880 --> 00:15:05,000
entonces si es que yo tengo un proceso que requiere mucho cálculo o tengo que sacar

138
00:15:05,000 --> 00:15:11,400
estadísticas bastante pesadas puedo acelerar bastante este proceso. ¿Cuáles son las ventajas

139
00:15:11,800 --> 00:15:19,400
de modding? Bueno lo que mencioné que la primera gran ventaja es que se desbloquean todos los núcleos,

140
00:15:19,400 --> 00:15:25,680
puedo utilizar todo el poder de mi procesador, también solamente tengo que hacer un import y

141
00:15:25,680 --> 00:15:30,280
empezar a utilizar la librería de forma muy sencilla, es bastante rápido al momento leer

142
00:15:30,280 --> 00:15:36,240
la data ya que paraleliza y bueno además si se mete un poquito al proyecto y empieza a revisar

143
00:15:36,240 --> 00:15:45,600
más en detalle uno va a ver que modding tiene distintos motores en los cuales yo puedo distribuir

144
00:15:45,600 --> 00:15:50,640
la carga, la modding no está hecho solo para que utilice todos los cores en pandas en un

145
00:15:50,640 --> 00:15:57,240
solo equipo sino solamente me permite distribuir la carga, es compatible con RAI y también con

146
00:15:57,240 --> 00:16:04,280
DASC. ¿Cuáles son las desventajas? Que obviamente si es que yo quiero trabajar con una carga

147
00:16:04,280 --> 00:16:08,520
distribuida, quiero trabajar con DASC o con RAI, requiere una configuración adicional que puede

148
00:16:08,520 --> 00:16:14,280
ser un poco más compleja y bueno prácticamente por definición los sistemas distribuidos son

149
00:16:14,280 --> 00:16:19,560
difíciles de debugar y puede ser un poco más difícil. Y otra gran desventaja que tiene modding

150
00:16:19,560 --> 00:16:27,560
es que al igual que pandas requiere mucha memoria, básicamente lo que hace es le da más capacidad de

151
00:16:27,560 --> 00:16:31,840
computar pandas pero sigue teniendo el problema de que sube todos los archivos a memoria.

152
00:16:34,600 --> 00:16:39,200
Bueno la siguiente librería que les quiero mostrar brevemente es una que se llama BAEX,

153
00:16:39,200 --> 00:16:49,640
no es muy conocida pero esta viene del mundo de la astronomía. Es una biblioteca en Python que es

154
00:16:49,640 --> 00:16:56,160
muy parecida a pandas, la sintaxis es completamente distinta cambian los nombres pero los conceptos

155
00:16:56,160 --> 00:17:03,680
de data frames, columnas, etcétera existen y son muy similares. El objetivo de esta librería

156
00:17:03,680 --> 00:17:12,400
es permitir que nosotros podamos procesar archivos, digamos SSB, parquet, apache arrow, etcétera,

157
00:17:12,400 --> 00:17:20,400
etcétera en máquinas con recursos limitados. Yo lo que puedo hacer con BAEX es tomar un archivo

158
00:17:20,400 --> 00:17:25,640
de 40 gigas y poder hacer algún tipo de cálculo muy fácil en una máquina que tenga de vez 8

159
00:17:25,640 --> 00:17:31,320
gigas de ram. Esa es como que la gran ventaja. ¿Cómo hace esto BAEX? Básicamente ellos tienen

160
00:17:31,320 --> 00:17:36,280
el memory mapping, el mapeo de memoria disco, entonces aprovechan las ventajas que tienen

161
00:17:36,280 --> 00:17:43,600
ciertos formatos de archivos como apache arrow, parquet o hdf5 y lo que hacen es solo leen el

162
00:17:43,600 --> 00:17:50,560
bloque de información que necesitan en el momento y se comporta digamos de modo lazy,

163
00:17:50,760 --> 00:17:56,160
los cálculos que va haciendo BAEX los va haciendo sobre la demanda a menos que nosotros en el código

164
00:17:56,160 --> 00:18:04,280
explícitamente le pidamos un dato. Además, BAEX tiene algunas características bien interesantes

165
00:18:04,280 --> 00:18:11,800
como columnas virtuales, tiene todo un set de librerías optimizadas de machine learning que

166
00:18:11,800 --> 00:18:19,360
se pueden utilizar y sacarle provecho a la librería. Para que funcione BAEX recomiendan

167
00:18:19,360 --> 00:18:27,920
que trabajemos con algunos de los formatos de datos o apache arrow o hdf5. Hdf5 es un

168
00:18:27,920 --> 00:18:34,080
formato de dato abierto, de especificación abierta que está diseñado específicamente

169
00:18:34,080 --> 00:18:39,720
para almacenar grandes volúmenes de datos. Una de las cosas interesantes que tienen estos formatos

170
00:18:39,720 --> 00:18:44,840
es que posee la metadata, entonces cierta información ya vive en la metadata, entonces no es necesario

171
00:18:44,840 --> 00:18:52,400
que se haga el cálculo directo sobre la información. Otra característica interesante que trae BAEX es

172
00:18:52,400 --> 00:18:57,560
trae una serie de herramientas en las cuales se puede convertir automáticamente mi información

173
00:18:57,560 --> 00:19:03,600
que está en CSV o en otro formato de texto y pasarlo directamente a hdf5 con una línea,

174
00:19:03,600 --> 00:19:09,880
entonces de esa forma le agrego a mi proceso un paso. Este proceso de conversión tampoco

175
00:19:09,880 --> 00:19:14,200
consume infinita memoria y luego de eso yo puedo empezar a agilizar mi trabajo mucho.

176
00:19:16,280 --> 00:19:22,280
Además de eso BAEX tiene algunas cositas interesantes, tiene un servidor en el cual

177
00:19:22,280 --> 00:19:27,080
yo le puedo enviar la carga a través de una API, tiene una API escrita en Python y también tiene

178
00:19:27,080 --> 00:19:35,560
una API escrita con un API REST, en el cual me puede permitir que yo a un servidor en la nube

179
00:19:35,600 --> 00:19:40,640
que también tiene BAEX le envíe la información y que el procesamiento se haga fuera de mi equipo,

180
00:19:40,640 --> 00:19:45,760
por ejemplo tú puedes trabajar directamente sobre buckets de C3 y cosas por el estilo.

181
00:19:46,880 --> 00:19:54,000
¿Cuáles son las ventajas de esta librería? Bueno nos ayuda a controlar la memoria ya que

182
00:19:54,000 --> 00:20:01,360
permite hacer el mapeo de memoria a disco, bueno si les interesa conocer un poquito más en la

183
00:20:01,360 --> 00:20:08,480
página oficial de BAEX, tienen muchos ejemplos y muchos datasets donde proporcionan files de 40

184
00:20:08,480 --> 00:20:14,360
gigas 120 gigas y tienen ejemplos en los cuales uno los puede descargar y procesarlos en menos

185
00:20:14,360 --> 00:20:18,280
de un minuto, lo que más demora es descargar la data pero como que las visualizaciones son muy

186
00:20:18,280 --> 00:20:29,160
rápidas. Bueno como mencioné este BAEX permite que sean los cálculos sobre la marcha ya que

187
00:20:29,160 --> 00:20:35,760
tiene un modelo Lazy y este bueno otra de las grandes ventajas que tiene BAEX es que

188
00:20:37,160 --> 00:20:44,840
está diseñado para que se pueda visualizar la data de forma muy rápida, entonces de esa manera yo

189
00:20:44,840 --> 00:20:51,000
puedo hacer este visualizaciones, mapas de calor, histogramas y estadísticas distintas con archivos

190
00:20:51,000 --> 00:20:58,960
muy grandes pero sin necesitar este mucho poder de cómputo. Otra ventaja es que la data es que yo

191
00:20:58,960 --> 00:21:04,160
decido trabajar con BAEX y en algún momento necesito utilizar alguna funcionalidad particular

192
00:21:04,160 --> 00:21:08,840
que sólo está disponible en pandas, tiene una opción que te permite exportar el data frame de

193
00:21:08,840 --> 00:21:17,080
BAEX a pandas y cuando está ya en el formato de pandas puedo hacer los nuevos cálculos, el único

194
00:21:17,080 --> 00:21:21,400
problema de hacer eso es que si tu data es muy grande al momento de exportarlo a pandas vas a

195
00:21:21,400 --> 00:21:26,600
utilizar toda la memoria, entonces hay que tener harto cuidado con eso. Cuáles son las desventajas

196
00:21:26,600 --> 00:21:32,000
de utilizar una librería como BAEX, bueno hay que modificar tu código, si tu código inicialmente

197
00:21:32,000 --> 00:21:37,640
estaba escrito en Python lo tienes que reescribir porque la sintaxis es distinta, es muy similar,

198
00:21:37,640 --> 00:21:43,480
no es difícil pero si es que es mucho código puede ser un poco pesado. Otra ventaja es que

199
00:21:43,480 --> 00:21:48,600
tal vez BAEX no es tan conocido y no es tan maduro como pandas pero en los últimos meses

200
00:21:48,600 --> 00:21:55,040
ha ido evolucionando y le siguen agregando, tiene un desarrollo muy activo así que cada dos,

201
00:21:55,040 --> 00:22:00,560
tres meses agregan funcionalidades que se ven muy interesantes y bueno y otro tema que puede ser un

202
00:22:00,560 --> 00:22:04,920
poquito difícil es el tema de los tipos de datos, algunos tipos de datos cuando se exportan a

203
00:22:04,920 --> 00:22:12,520
formatos como HDF5 o Parquet puede ser medio complejo, pueden aparecer errores extraños,

204
00:22:12,520 --> 00:22:23,920
entonces ahí también hay que ir con cuidado. Bueno finalmente la última librería que vamos a ver es

205
00:22:23,920 --> 00:22:31,960
Spicepark, Spicepark es una librería que trabaja directamente con Spark, es parte del proyecto

206
00:22:31,960 --> 00:22:38,520
Spark y bueno cuando nosotros necesitamos procesar mucha data a gran escala, estamos hablando en el

207
00:22:38,520 --> 00:22:48,080
orden de teras de información o de cientos de gigas, a veces es mejor trabajar con herramientas

208
00:22:48,080 --> 00:22:54,960
que están optimizadas para esto, una de estas es Spark y básicamente lo que nos permite hacer

209
00:22:54,960 --> 00:23:00,960
Spark es que me permite distribuir los datos y los cálculos a un cluster, hacer esto directamente

210
00:23:00,960 --> 00:23:08,240
con pandas no se puede, hay otras alternativas que funcionan bien hasta cierto tamaño como tal

211
00:23:08,240 --> 00:23:16,760
vez Dask o Ray con modding pero al final al final día la herramienta más potente suele ser Spark,

212
00:23:16,760 --> 00:23:21,720
es como que lo más usado en la actualidad cuando necesitas procesar mucha data, entonces este motor

213
00:23:21,720 --> 00:23:27,040
este motor este como mencioné este motor permite distribuir el cálculo y también la data a los

214
00:23:27,040 --> 00:23:35,240
clusters. Spicepark es una API que es parte del proyecto, básicamente en vez de que yo trabaje

215
00:23:35,240 --> 00:23:39,800
directo con si no me equivoco Spark se trabaja con escala, en vez de que trabaje directo con escala

216
00:23:39,800 --> 00:23:45,240
estoy trabajando con Python, entonces la ventaja es que obviamente es la simplicidad de trabajar

217
00:23:45,240 --> 00:23:50,040
directamente con Python y bueno otra de las características interesantes de Spicepark es

218
00:23:50,040 --> 00:23:54,640
que me proporciona un shell, una consola donde yo puedo empezar a realizar cálculos, de algunos

219
00:23:54,640 --> 00:24:00,000
estos cálculos se van ejecutando gradualmente en el cluster de forma distribuida y bueno y

220
00:24:00,000 --> 00:24:04,360
tengo acceso a todo el stack de herramientas de stack, Spark trae su propio tipo de SQL,

221
00:24:04,360 --> 00:24:10,960
también existe el concepto data frames, tiene ciertas librerías de machine learning, no es tan

222
00:24:10,960 --> 00:24:17,680
rica su ecosistema de machine learning como el que está disponible en Python, pero hay muchas

223
00:24:17,680 --> 00:24:22,920
cosas que se pueden hacer este para resolver problemas puntuales, entonces las principales

224
00:24:22,920 --> 00:24:29,200
ventajas de trabajar con algo como Spark es bueno vas a poder procesar los datos a una gran velocidad,

225
00:24:29,200 --> 00:24:37,760
si tengo mucha data puedo paralizarlo casi en la nube casi de forma infinita dado a que es la

226
00:24:37,760 --> 00:24:43,720
herramienta de elección para procesar volúmenes importantes de data, todos los proveedores cloud,

227
00:24:43,720 --> 00:24:51,680
AWS, GCP, Azure, etcétera, etcétera, todos tienen algún tipo de producto, algún pass que me permite

228
00:24:51,680 --> 00:24:56,840
en este trabajar con Spark, entonces también existe la alternativa de no tener que configurar

229
00:24:56,840 --> 00:25:04,400
toda la data para poder procesarlo, obviamente Spark tiene un ecosistema bastante rico, es muy

230
00:25:04,400 --> 00:25:11,080
maduro y bueno y también permite ejecutar algunas herramientas disponibles del ecosistema Hado,

231
00:25:11,080 --> 00:25:19,400
ya es otra historia, es otro stack de herramientas, cuál es la desventaja que si yo tengo escrito mi

232
00:25:19,400 --> 00:25:23,800
código en pandas tengo que prácticamente rehacerlo, tengo que modificar todas las

233
00:25:23,800 --> 00:25:27,240
sintaxis, los data frames son un poco distintos, los tipos de datos, entonces hay que hacer un

234
00:25:27,240 --> 00:25:34,360
trabajo previo, si es que voy a procesar poca data no vale la pena usar Spark, porque básicamente

235
00:25:34,360 --> 00:25:38,360
se demora demasiado, es mucho más fácil usar pandas o alguna de las herramientas que les mostré,

236
00:25:38,360 --> 00:25:44,160
que funcionan bien para data de cierto tamaño, pero Spark como que no vale la pena, va a ser

237
00:25:44,160 --> 00:25:50,680
mucho más lento inclusive que pandas, si es que necesito trabajar con ciertas librerías de

238
00:25:50,680 --> 00:25:54,040
machine learning, ahí hay que tener un poquito de cuidado porque no todo está disponible,

239
00:25:54,040 --> 00:26:02,040
hay formas de ejecutar ciertas librerías, pero hay que verificar eso primero, y otra desventaja

240
00:26:02,040 --> 00:26:10,920
es que Spark requiere mucha memoria RAM, entonces los clusters de Spark pueden ser un poco costosos,

241
00:26:10,920 --> 00:26:19,280
pero obviamente si es que tengo que procesar teras y teras de datos, posiblemente tenga

242
00:26:19,280 --> 00:26:26,640
recursos para poder invertir en un cluster de este tipo, bueno, hay algunas notas finales como

243
00:26:26,640 --> 00:26:34,360
para ir cerrando la presentación, bueno como les mostré existen diversas opciones para poder

244
00:26:34,360 --> 00:26:44,040
escalar las cargas de trabajo, siempre la forma más sencilla es optimizar mis data frames,

245
00:26:44,040 --> 00:26:48,920
como les mostré ahí algunos tips, existen más formas, también se puede trabajar con los índices

246
00:26:48,920 --> 00:26:53,640
y por ahí otras cositas más, pero si yo trabajo de forma adecuada a mis data frames puedo reducir

247
00:26:53,640 --> 00:26:59,880
mucho el consumo de memoria y de recursos, y después de que he hecho esto también una alternativa

248
00:26:59,880 --> 00:27:04,680
sería escalar verticalmente mi carga, hoy día es relativamente fácil acceder a un proveedor cloud

249
00:27:04,680 --> 00:27:14,040
donde yo puedo levantar un Jupyter Notebook en una máquina bastante grande y empezar a realizar mis

250
00:27:14,040 --> 00:27:19,200
procesos, pero obviamente la recomendación es, primero optimiza en vez de escalarla de forma

251
00:27:19,200 --> 00:27:24,760
vertical, porque si no al final vas a estar desperdiciando dinero, otra herramienta muy

252
00:27:24,760 --> 00:27:29,400
interesante que vimos es Vaix, que nos permite trabajar con sets de datos que son mucho más grandes

253
00:27:29,400 --> 00:27:36,440
que la memoria, y la ventaja que tiene Vaix es que mapea la memoria directo al disco, aprovecha

254
00:27:36,440 --> 00:27:44,560
los formatos como Apache Arrow y permite procesar y hacer resoluciones de forma muy rápido que

255
00:27:44,760 --> 00:27:52,640
con panda sería imposible. Otra alternativa interesante es Modin, que si bien tiene la opción de

256
00:27:52,640 --> 00:28:00,000
distribuir la carga a motores como RAI y como DASC, nos permite con un cambio sencillo de una

257
00:28:00,000 --> 00:28:04,600
sola línea de código utilizar todos los cores disponibles que tengo en mi máquina, si tengo una

258
00:28:04,600 --> 00:28:09,560
máquina con 16 cores puedo acelerar ciertos procesos de una manera muy sencilla sin tener

259
00:28:09,560 --> 00:28:17,720
que modificar mi código. Y finalmente la última alternativa que les presenté es trabajar con Spark

260
00:28:17,720 --> 00:28:24,320
y Pi Spark, que si bien es mucho más difícil y requiere prácticamente reescribir gran parte del

261
00:28:24,320 --> 00:28:31,200
código, es una opción válida si es que tengo sets de datos bastante grandes y también gracias a eso

262
00:28:31,200 --> 00:28:37,520
podría aprovechar todos los productos PAS que están disponibles hoy en día entre los proveedores

263
00:28:37,520 --> 00:28:46,600
cloud. Eso sería todo, muchísimas gracias, si es que alguien tiene alguna pregunta, alguna

264
00:28:46,600 --> 00:28:50,920
duda, si necesita que le dé algún consejo adicional o si tiene algún problema específico,

265
00:28:50,920 --> 00:28:56,960
yo feliz de responder cualquier duda, inclusive después de la charla. Muchísimas gracias.

266
00:29:04,160 --> 00:29:05,360
Y acá algunos links.

267
00:29:07,520 --> 00:29:31,160
Se supone que modding es más rápido en el momento de la lectura, entonces podrías estar leyendo los

268
00:29:31,160 --> 00:29:35,840
chunks mucho más rápido, internamente te lo paralelices, entonces si es un archivo bastante

269
00:29:35,840 --> 00:29:39,840
grande podrías leer muchos chunks chiquitos y podrías acelerar esa carta.

270
00:29:56,120 --> 00:30:04,080
Sí, algo interesante que no lo puse es en Bikes, Bikes tiene una opción de que tú puedes tener

271
00:30:04,080 --> 00:30:10,160
tu archivo separado en decenas de archivos pequeñitos y le pasas a Bikes, tiene un parámetro

272
00:30:10,160 --> 00:30:15,120
que se llama Open Mini, le pasas como que la lista y te lo lee de forma instantánea,

273
00:30:15,120 --> 00:30:20,160
entonces ahí es mucho más rápido inclusive porque no busca cargar toda la data de memoria,

274
00:30:20,160 --> 00:30:23,320
sino hace el mapeo y lo tienes en dos, tres segundos.

275
00:30:31,680 --> 00:30:32,480
Gracias.

276
00:30:34,080 --> 00:30:35,560
Muchísimas gracias.

