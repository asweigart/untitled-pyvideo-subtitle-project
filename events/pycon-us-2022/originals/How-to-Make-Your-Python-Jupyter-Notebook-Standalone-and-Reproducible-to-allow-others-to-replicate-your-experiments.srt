1
00:00:00,000 --> 00:00:11,360
The next talk is on how to make your Python Jupyter Notebook standalone and reproducible

2
00:00:11,360 --> 00:00:14,760
to allow other users to replicate your experiments.

3
00:00:14,760 --> 00:00:19,520
And the speaker is Maya Costantini.

4
00:00:19,520 --> 00:00:23,560
And as a reminder, the speaker won't be taking questions after the talk here.

5
00:00:23,560 --> 00:00:28,560
You can meet her in the hallway.

6
00:00:28,560 --> 00:00:37,560
Hi, thank you.

7
00:00:37,560 --> 00:00:40,840
Welcome to this talk.

8
00:00:40,840 --> 00:00:43,160
First to begin with a few words about me.

9
00:00:43,160 --> 00:00:44,160
My name is Maya.

10
00:00:44,160 --> 00:00:45,760
I'm a software engineer at Red Hat.

11
00:00:45,760 --> 00:00:50,000
I'm passionate about Python and open source, and I come from Paris in France.

12
00:00:50,000 --> 00:00:53,240
If you want, you can find me on Twitter and GitHub.

13
00:00:53,240 --> 00:00:57,520
And if you want to have those slides available to follow along, you can scan this code right

14
00:00:58,400 --> 00:01:01,400
here.

15
00:01:01,400 --> 00:01:05,400
If I am here today, it's to talk to you about a great project I've been working on at Red

16
00:01:05,400 --> 00:01:08,440
Hat for seven months called Tots.

17
00:01:08,440 --> 00:01:12,680
And I want to explain to you how you can use it to make your Jupyter Notebook standalone,

18
00:01:12,680 --> 00:01:17,560
reproducible, easier to maintain and to share with others, but also to radically improve

19
00:01:17,560 --> 00:01:22,280
the quality of your application software stack by choosing the best possible dependencies

20
00:01:22,280 --> 00:01:24,800
for your notebook.

21
00:01:24,880 --> 00:01:28,880
Before we start, I'm curious to know who of you has ever used a Jupyter Notebook?

22
00:01:28,880 --> 00:01:31,760
Okay, a lot of people.

23
00:01:31,760 --> 00:01:34,760
And who uses this day to day for work, for example?

24
00:01:34,760 --> 00:01:38,360
Okay, great.

25
00:01:38,360 --> 00:01:41,760
For those of you who are not very familiar with Jupyter Notebooks, I can start with a

26
00:01:41,760 --> 00:01:43,760
quick introduction.

27
00:01:43,760 --> 00:01:48,960
Jupyter Notebooks is an open source web-based application that allows you to conduct iterative

28
00:01:49,000 --> 00:01:55,000
experiments and to create documents with life codes, equations, images, graphs, et cetera,

29
00:01:55,720 --> 00:01:59,720
and that supports a wide variety of languages, over 40.

30
00:01:59,720 --> 00:02:04,720
They provide a rich and interactive output and allow to leverage some big data and data

31
00:02:04,720 --> 00:02:08,720
exploration tools such as Sparks, for example.

32
00:02:08,720 --> 00:02:12,960
As you may know, Jupyter Notebooks are in fact already quite popular, especially in

33
00:02:12,960 --> 00:02:14,880
the Python community.

34
00:02:14,880 --> 00:02:19,880
Data scientists use them for things such as visualization, data modeling, data analysis,

35
00:02:19,880 --> 00:02:22,880
and developing machine learning models.

36
00:02:22,880 --> 00:02:28,640
They're also used by academics and by Python developers for things such as rapid prototyping

37
00:02:28,640 --> 00:02:31,640
and proof of concepts.

38
00:02:31,640 --> 00:02:34,640
So, what is the issue with Python Jupyter Notebooks?

39
00:02:34,640 --> 00:02:37,640
What problems could we be trying to solve?

40
00:02:37,640 --> 00:02:42,640
To understand that, let me show you an example.

41
00:02:43,400 --> 00:02:47,400
Let's say I'm a data scientist and I want to create, for example, a computer vision

42
00:02:47,400 --> 00:02:49,400
application.

43
00:02:49,400 --> 00:02:53,400
And to do so, I would like to install the OpenCV library in my notebook.

44
00:02:53,400 --> 00:02:58,400
To install OpenCV, I will run the following command in my notebook, which is pip install

45
00:02:58,400 --> 00:03:02,400
OpenCV Python to get the OpenCV Python package.

46
00:03:02,400 --> 00:03:03,400
Okay.

47
00:03:03,400 --> 00:03:07,400
But what exactly is OpenCV Python?

48
00:03:07,400 --> 00:03:11,400
OpenCV Python is a direct dependency of my application.

49
00:03:12,160 --> 00:03:16,160
So, this is direct dependency from other dependencies, like for example NumPy.

50
00:03:16,160 --> 00:03:22,160
NumPy is a dependency of my dependency, which makes it a transitive dependency.

51
00:03:22,160 --> 00:03:26,160
So far, I have run pip install OpenCV Python.

52
00:03:26,160 --> 00:03:29,160
I got OpenCV Python and NumPy.

53
00:03:29,160 --> 00:03:32,160
But did I forget something?

54
00:03:32,160 --> 00:03:34,160
What about the versions?

55
00:03:34,160 --> 00:03:38,160
Maybe I would like to install some specific versions of my dependencies.

56
00:03:38,920 --> 00:03:43,920
Well, last time I checked, OpenCV Python had over 60 releases and NumPy something like

57
00:03:43,920 --> 00:03:45,920
80.

58
00:03:45,920 --> 00:03:49,920
And if you take a look here at the release history for a package like NumPy, if you look

59
00:03:49,920 --> 00:03:55,920
at the dates, you will see that releases happen approximately every month, which is very frequent.

60
00:03:55,920 --> 00:03:57,920
So, this means one thing.

61
00:03:57,920 --> 00:04:03,920
It means that if today I start developing my notebook by running pip install, for example,

62
00:04:04,680 --> 00:04:08,680
I will have the same version of NumPy as I start to install NumPy in my notebook.

63
00:04:08,680 --> 00:04:12,680
And I run this notebook once again in one month without specifying any version either.

64
00:04:12,680 --> 00:04:16,680
I will probably not have the same version of NumPy installed in my notebook.

65
00:04:16,680 --> 00:04:20,680
So, this is an issue, of course.

66
00:04:20,680 --> 00:04:22,680
And also, what about hashes?

67
00:04:22,680 --> 00:04:28,680
Hashes are a way to guarantee the provenance of a package and to make sure that an artifact

68
00:04:28,680 --> 00:04:32,680
that you use in your application has been installed from a trusted source of your choice,

69
00:04:32,940 --> 00:04:34,940
like, for example, an index.

70
00:04:34,940 --> 00:04:39,940
Specifying hashes to identify the provenance of your dependencies is essential if you want

71
00:04:39,940 --> 00:04:44,940
to share your notebook application in a more secure and reliable manner and allow users

72
00:04:44,940 --> 00:04:50,940
to install packages from the same sources as you did.

73
00:04:50,940 --> 00:04:54,440
But also, what about the Python interpreter version?

74
00:04:54,440 --> 00:04:58,440
Every Python interpreter version is different, and your notebook will not necessarily be

75
00:04:58,440 --> 00:05:00,440
compatible with all Python versions.

76
00:05:01,200 --> 00:05:05,200
So, this is an issue, for example, if you used your notebook, if you developed your

77
00:05:05,200 --> 00:05:09,200
notebook using Python 3.8 and you shared it with someone who uses Python 3.10,

78
00:05:09,200 --> 00:05:13,200
maybe this person will not be able to run it properly.

79
00:05:13,200 --> 00:05:18,200
So, we have issues with versions, with hashes, with Python interpreter versions,

80
00:05:18,200 --> 00:05:20,200
but can we think of something more?

81
00:05:22,700 --> 00:05:27,700
In fact, a Python application is composed of many layers, and each one of these layers

82
00:05:27,960 --> 00:05:29,960
has the power to potentially break your code.

83
00:05:29,960 --> 00:05:36,960
On the top layer, you have your Python application code, and this is really the only layer you

84
00:05:36,960 --> 00:05:38,960
really have some control on.

85
00:05:38,960 --> 00:05:43,960
For example, if your code is broken, you can still have a fix for it and propagate this

86
00:05:43,960 --> 00:05:47,960
fix to other users, and they will be able to run the code as well.

87
00:05:47,960 --> 00:05:53,960
But below this layer, as we saw earlier, you also have Python direct dependencies,

88
00:05:54,220 --> 00:06:00,220
transitive dependencies, the Python interpreter you use, but also other things such as native

89
00:06:00,220 --> 00:06:03,220
dependencies, like JLIPC or CUDA, for example.

90
00:06:03,220 --> 00:06:07,220
You have also the operating system you use to develop your application with different

91
00:06:07,220 --> 00:06:12,220
kernel modules, and finally, on the bottom layer, you have the hardware with different

92
00:06:12,220 --> 00:06:15,220
CPU and GPU architectures.

93
00:06:15,220 --> 00:06:21,220
This is why using the simple pip command to install your dependencies will not help you

94
00:06:21,560 --> 00:06:28,560
achieve the goal of reproducibility you would like to have for your notebook.

95
00:06:28,560 --> 00:06:32,960
To summarize, if you do something like this, which is pip install, your dependency without

96
00:06:32,960 --> 00:06:38,960
specifying any version or hashes will not guarantee any reproducibility for your notebook.

97
00:06:38,960 --> 00:06:43,960
Maybe you would think of doing something like that, which is specifying a list of requirements

98
00:06:43,960 --> 00:06:49,460
in a manifest file such as requirements.txt, but here again, you don't have any information

99
00:06:49,500 --> 00:06:55,500
about versions or hashes used, so this will not work and not guarantee either any reproducibility

100
00:06:55,900 --> 00:06:59,900
for your Jupyter notebook.

101
00:06:59,900 --> 00:07:03,400
My point is Jupyter notebooks are by default not standalone.

102
00:07:03,400 --> 00:07:07,100
You cannot share a Jupyter notebook with someone else.

103
00:07:07,100 --> 00:07:11,280
It considers that it will run on a different machine with different dependency versions

104
00:07:11,280 --> 00:07:15,120
and using a different Python version.

105
00:07:15,120 --> 00:07:19,500
In fact, dependency management in Jupyter notebooks puts constraints on the author of

106
00:07:19,500 --> 00:07:25,500
the notebooks to provide a decouple manifest file like a requirement.txt or a pip file.loc

107
00:07:25,500 --> 00:07:30,360
along with the notebook so that the notebook users can install the exact same software

108
00:07:30,360 --> 00:07:35,060
stack that you used to develop the notebook.

109
00:07:35,060 --> 00:07:38,660
Dependencies are also a problem when you want to containerize your notebook.

110
00:07:38,660 --> 00:07:42,580
If you want, for example, to ship your notebook into a container and share it with someone

111
00:07:42,600 --> 00:07:47,160
else, you have them to make sure that your Docker file or container file will be able

112
00:07:47,160 --> 00:07:51,160
to adjust the dependencies accordingly.

113
00:07:51,160 --> 00:07:55,400
Finally, the consumers that receive a shared notebook also have to set up manually the

114
00:07:55,400 --> 00:08:01,500
correct environments, which can be difficult if no manifest file or any information regarding

115
00:08:01,500 --> 00:08:05,800
the environments in which the author developed the notebook is provided with it.

116
00:08:05,800 --> 00:08:11,700
This is often the case with shared Jupyter notebooks.

117
00:08:11,700 --> 00:08:16,840
To summarize what would be the difficulties this creates for both authors and consumers,

118
00:08:16,840 --> 00:08:21,120
they should both create an environment to run the notebook in, ideally a virtual one

119
00:08:21,120 --> 00:08:24,500
that would not be polluted by any dependencies.

120
00:08:24,500 --> 00:08:28,880
They will have to install the dependencies to create a kernel and update it every time

121
00:08:28,880 --> 00:08:30,880
they have a new dependency.

122
00:08:30,880 --> 00:08:35,800
In addition to that, the author had to maintain a requirement file to allow others to get

123
00:08:35,940 --> 00:08:43,600
the exact same dependencies that the notebook was developed with.

124
00:08:43,600 --> 00:08:48,780
What we are looking for is a solution that would help authors keep their dependencies

125
00:08:48,780 --> 00:08:55,780
and manifest files up to date and thus achieve reproducibility of their notebook.

126
00:08:55,780 --> 00:08:58,420
Earlier I introduced you to Project Thoughts.

127
00:08:58,420 --> 00:09:02,980
Now we'll see how Thoughts can actually help you manage dependencies in your notebook and

128
00:09:02,980 --> 00:09:08,560
get an optimal set of dependencies for your application.

129
00:09:08,560 --> 00:09:10,440
First a few words about Thoughts.

130
00:09:10,440 --> 00:09:15,360
The main goal of Thoughts is to help developers and data scientists create and maintain healthy

131
00:09:15,360 --> 00:09:17,760
Python applications.

132
00:09:17,760 --> 00:09:22,640
This project was started in 2018 after it had the office of CTO and now it is maintained

133
00:09:22,640 --> 00:09:27,200
by a team of 10 engineers and has had over 50 contributors.

134
00:09:28,140 --> 00:09:33,460
Thoughts is a cloud-based recommendation engine for Python applications which is able to solve

135
00:09:33,460 --> 00:09:38,060
dependencies to their optimal versions using machine learning.

136
00:09:38,060 --> 00:09:42,500
Of course Thoughts is an open source project so you can contribute of course if you want

137
00:09:42,500 --> 00:09:50,420
to and involve yourself in the Thoughts community and share your ideas with us.

138
00:09:50,420 --> 00:09:54,860
What Thoughts does is it provides an extension to manage dependencies in Jupyter Notebooks.

139
00:09:55,520 --> 00:09:58,280
There is a JupyterLab extension.

140
00:09:58,280 --> 00:10:04,600
So JupyterLab if you don't know is an interactive and extensible web interface to work with

141
00:10:04,600 --> 00:10:08,120
Jupyter Notebooks.

142
00:10:08,120 --> 00:10:13,360
That extension for JupyterLab is called JupyterLab Requirements and it allows you to manage your

143
00:10:13,360 --> 00:10:19,200
dependencies without even leaving it by storing all of the necessary dependency information

144
00:10:19,200 --> 00:10:22,480
directly in the notebook metadata.

145
00:10:22,480 --> 00:10:27,980
It can provide an optimized environment unique to each notebook by solving dependencies using

146
00:10:27,980 --> 00:10:32,820
Thoughts Depotency Resolution Engine.

147
00:10:32,820 --> 00:10:36,820
So let's see what the user interface of JupyterLab Requirements looks like.

148
00:10:36,820 --> 00:10:41,580
So when you work with JupyterLab you have your Jupyter Notebook open and typically it

149
00:10:41,580 --> 00:10:46,580
looks like this and if you have JupyterLab Requirements enabled on JupyterLab you will

150
00:10:46,580 --> 00:10:49,940
see this button called Manage Dependencies.

151
00:10:49,940 --> 00:10:54,060
So if you click on this button you will have this menu where you will be able to add your

152
00:10:54,060 --> 00:10:57,720
dependencies with a button.

153
00:10:57,720 --> 00:11:03,980
It is not for example we import TensorFlow and NumPy and I want to say that I want TensorFlow

154
00:11:03,980 --> 00:11:10,380
for example in version 2.2.0 and NumPy in a range of version from 1.0.0 to the latest

155
00:11:10,380 --> 00:11:11,800
version possible.

156
00:11:11,800 --> 00:11:19,780
So I will put the state of packages and constraints in the UI and save those dependencies.

157
00:11:19,780 --> 00:11:24,000
Now I will have this menu where I will be able to choose a resolution engine.

158
00:11:24,000 --> 00:11:29,900
So you can use the Thoughts Resolution Engine or you can use P-Pen which is used as a backup

159
00:11:29,900 --> 00:11:31,820
to Thoughts.

160
00:11:31,820 --> 00:11:37,020
You can also choose if you have the Thoughts Resolution Engine a recommendation type and

161
00:11:37,020 --> 00:11:40,200
we will see later exactly what it is.

162
00:11:40,200 --> 00:11:48,700
But now we will say that we want our dependencies to be resolved to the latest possible versions.

163
00:11:48,700 --> 00:11:55,920
And finally you have this section with information about your environment which is automatically

164
00:11:55,920 --> 00:12:00,900
filled by the extension but if you want to you can also modify it by hand.

165
00:12:00,900 --> 00:12:11,140
So here for example I developed my notebook using Python 3.8 in a rel8 environment OS.

166
00:12:11,140 --> 00:12:15,880
So now that everything is specified we are conducting Thoughts backend for advice which

167
00:12:15,880 --> 00:12:19,480
take typically a few seconds or minutes.

168
00:12:19,480 --> 00:12:24,160
And Thoughts backend is going to compute this fully pinned out software stack here with

169
00:12:24,160 --> 00:12:29,200
the latest possible versions I have specified and send it back to us in the form of a log

170
00:12:29,200 --> 00:12:32,760
file.

171
00:12:32,760 --> 00:12:37,720
Now that we received the fully pinned out software stack we are installing these requirements

172
00:12:37,720 --> 00:12:42,600
in a virtual environment and this is done with a tool called MicropyPen.

173
00:12:42,720 --> 00:12:49,560
This tool has been developed within Project Thoughts to install dependencies and it can

174
00:12:49,560 --> 00:12:56,440
sort of wrap around PIP to convert requirements file like requirements.txt or PIP and poetry

175
00:12:56,440 --> 00:13:03,040
log files for example into a PIP tools compatible outputs.

176
00:13:03,040 --> 00:13:07,000
So now we have our requirements locked and saved, they are installed.

177
00:13:07,040 --> 00:13:13,400
We also created a new kernel and we can simply click to start working on our notebook.

178
00:13:13,400 --> 00:13:18,360
Everything is installed and ready to use.

179
00:13:18,360 --> 00:13:23,820
On the right side of your UI you will normally see an option to have the notebook metadata

180
00:13:23,820 --> 00:13:30,760
which is a JSON file and here as I said earlier you will be able to see that your dependencies

181
00:13:30,760 --> 00:13:35,080
as you specify them so with the right versions have been locked directly in the notebook

182
00:13:35,160 --> 00:13:37,160
metadata.

183
00:13:37,160 --> 00:13:41,600
If you scroll down you normally will see also your whole software stack with our direct

184
00:13:41,600 --> 00:13:46,600
and transitive dependencies and the corresponding hashes they have so everything has been saved

185
00:13:46,600 --> 00:13:50,920
there is directly in the notebook.

186
00:13:50,920 --> 00:13:57,560
You can also have access thoughts recommendation using Horace magic commands so maybe if you

187
00:13:57,560 --> 00:14:03,440
know a bit high Python you are already familiar with magic commands but if not it's okay.

188
00:14:03,440 --> 00:14:07,880
So these commands basically allow you to do the same but directly into notebook cells

189
00:14:07,880 --> 00:14:10,480
to speed up the development of your notebook.

190
00:14:10,480 --> 00:14:16,600
So this is an example of the command that helps you lock your dependencies and outputs

191
00:14:16,600 --> 00:14:23,680
but you can also perform those actions which are checking notebook metadata about dependencies

192
00:14:23,680 --> 00:14:28,200
with Horace check but also converting PIP commands to Horace commands to allow more

193
00:14:28,200 --> 00:14:31,120
reproducibility.

194
00:14:31,120 --> 00:14:36,280
You can also discover dependencies already exist in your notebook and create automatically

195
00:14:36,280 --> 00:14:42,880
a PIP file and do a lot of other actions such as adding or removing requirements in your

196
00:14:42,880 --> 00:14:45,240
PIP file also.

197
00:14:45,240 --> 00:14:49,600
If you take a look at the output for Horace check for example for checking for dependencies

198
00:14:49,600 --> 00:14:56,880
you will have the following table with information about your projects about the advisor ID so

199
00:14:57,720 --> 00:15:02,720
the advisor ID you send to thoughts but also for example here I left this warning which

200
00:15:02,720 --> 00:15:08,080
tells you that you forgot to set a kernel so it will warn you that you have to run Horace

201
00:15:08,080 --> 00:15:14,800
set kernel to set a kernel for your notebook before performing this action.

202
00:15:14,800 --> 00:15:19,200
If you want to install and run JupyterLab requirements you can get this extension from

203
00:15:19,200 --> 00:15:23,800
the Python package index by PIP by running PIP install JupyterLab requirements on your

204
00:15:23,840 --> 00:15:30,160
machine and if you want to use this extension you can do so by running the JupyterLab command

205
00:15:30,160 --> 00:15:36,400
to open a local JupyterLab instance and if you have JupyterLab requirements enabled you

206
00:15:36,400 --> 00:15:43,160
will be able to run it as we saw just now.

207
00:15:43,160 --> 00:15:47,480
And now what about containerized notebooks?

208
00:15:47,480 --> 00:15:49,560
Does any of you work with containers?

209
00:15:50,280 --> 00:15:54,600
Okay that's a few people.

210
00:15:54,600 --> 00:15:59,520
So besides this extension for JupyterLab we thought it is also possible to manage dependencies

211
00:15:59,520 --> 00:16:04,520
in containerized Jupyter notebooks using source to image.

212
00:16:04,520 --> 00:16:10,520
Source to image is also known as S2I is a tool for building reproducible images and

213
00:16:10,520 --> 00:16:15,760
it produces images which are already which are ready to run by ejecting an application

214
00:16:15,800 --> 00:16:21,440
source code into a container image which assembles it into a new image.

215
00:16:21,440 --> 00:16:26,440
And that provides minimal Jupyter notebooks container images which can be built on OpenShift

216
00:16:26,440 --> 00:16:32,320
which is a distribution of Kubernetes to manage dependencies using that recommendation but

217
00:16:32,320 --> 00:16:38,520
also container images for other types of application by extending the functionality of some base

218
00:16:38,520 --> 00:16:45,320
S2I Python images to use that services.

219
00:16:45,320 --> 00:16:49,440
Now that we have seen a few ways to manage dependencies in Jupyter notebooks and to

220
00:16:49,440 --> 00:16:54,760
get an appropriate software stack using TOTS you might be wondering well how does that

221
00:16:54,760 --> 00:16:55,760
work?

222
00:16:55,760 --> 00:16:59,080
How is this recommendation computed exactly?

223
00:16:59,080 --> 00:17:02,320
So that's what we will see.

224
00:17:02,320 --> 00:17:06,780
The core component behind TOTS recommendation is called the resolver and the philosophy

225
00:17:06,780 --> 00:17:12,000
behind the resolver is to recommend the greatest and not the latest possible versions for your

226
00:17:12,000 --> 00:17:14,480
dependencies.

227
00:17:14,480 --> 00:17:19,320
When you install dependencies using PIP or P-Pen or similar resolver the resolution

228
00:17:19,320 --> 00:17:25,200
process uses a backtracking algorithm which will systematically solve your dependencies

229
00:17:25,200 --> 00:17:29,120
to the latest possible versions.

230
00:17:29,120 --> 00:17:33,840
However this is not the case with TOTS resolver which uses reinforcement learning to solve

231
00:17:33,840 --> 00:17:38,720
dependencies in the cloud therefore using resources from a cluster and not from the

232
00:17:38,720 --> 00:17:42,040
user's local machine.

233
00:17:42,040 --> 00:17:46,680
TOTS resolver has five different recommendation types which can adapt the dependency resolution

234
00:17:46,680 --> 00:17:49,880
process to user's specific needs.

235
00:17:49,880 --> 00:17:54,880
Of course the resolver can solve dependencies to the latest possible versions but for example

236
00:17:54,880 --> 00:18:01,120
it is also possible to configure the resolver to get the most secure possible software stack.

237
00:18:01,120 --> 00:18:08,000
The security recommendation type is particularly useful if you want to manage dependencies

238
00:18:09,000 --> 00:18:13,400
which are deployed in the production environment for example because the associated set of

239
00:18:13,400 --> 00:18:19,560
dependencies will contain only versions that are free of non-vulnerabilities.

240
00:18:19,560 --> 00:18:24,080
But for users developing programs where the focus might be on performance for example

241
00:18:24,080 --> 00:18:30,400
the resolver can be adjusted to performance recommendation type to get dependency versions

242
00:18:30,400 --> 00:18:35,560
which will maximize the performance of an application according to a user's hardware

243
00:18:35,680 --> 00:18:38,880
and software environments.

244
00:18:38,880 --> 00:18:44,640
An example of how we evaluate the performance of libraries by performing benchmarks between

245
00:18:44,640 --> 00:18:51,640
different versions by using those versions of libraries in different kind of performance

246
00:18:51,640 --> 00:18:53,440
scripts.

247
00:18:53,440 --> 00:18:59,220
For example it can be matrix multiplication, convolutional filters or any kind of heavy

248
00:18:59,220 --> 00:19:02,200
computational operations.

249
00:19:02,280 --> 00:19:06,640
Then we can compare the speed for example of execution for different versions of I don't

250
00:19:06,640 --> 00:19:12,200
know tensor flow if you're using for matrix multiplication.

251
00:19:12,200 --> 00:19:16,440
The recommendation type for TOTS can also be set to stable if you want to get the most

252
00:19:16,440 --> 00:19:23,440
stable versions of packages or to testing if you want to run software that can possibly

253
00:19:24,080 --> 00:19:26,560
be unstable.

254
00:19:26,600 --> 00:19:32,760
This recommendation type testing is more suitable for experimenting and because other recommendation

255
00:19:32,760 --> 00:19:37,760
type can be more strict and not resolve any dependencies in this case.

256
00:19:37,760 --> 00:19:44,760
TOTS collects a set of requirements and constraints about dependencies as well as the recommendation

257
00:19:45,520 --> 00:19:51,920
type for the resolution and also information about the user's environments such as the

258
00:19:51,960 --> 00:19:57,200
operating system and Python version used but also if the application uses a base container

259
00:19:57,200 --> 00:20:02,920
image to run, has CUDA configured or what type of hardware is present and it sends back

260
00:20:02,920 --> 00:20:07,200
this information to a component that we call the advisor.

261
00:20:07,200 --> 00:20:12,640
The advisor is responsible for computing the optimal application software stack.

262
00:20:12,640 --> 00:20:19,640
It uses a knowledge database containing information about dependencies and it dynamically constructs

263
00:20:20,160 --> 00:20:26,520
a resolution pipeline which is made of different type of customizable pipeline units.

264
00:20:26,520 --> 00:20:31,440
This pipeline units makes the resolution process optimized for each application and its runtime

265
00:20:31,440 --> 00:20:34,120
environment.

266
00:20:34,120 --> 00:20:38,920
So the advisor sends back a fully pinned down software stack including the direct and the

267
00:20:38,920 --> 00:20:44,480
transitive dependencies in the form of a log file along with a justification to inform

268
00:20:44,520 --> 00:20:50,800
the user on why some dependency versions were added or removed from the resolution process

269
00:20:50,800 --> 00:20:57,540
and to warn or inform about some package specificities.

270
00:20:57,540 --> 00:21:03,440
So what we observe in our knowledge graph, this observation include among other things

271
00:21:03,440 --> 00:21:08,920
information about the application stack such as build time and runtime environments but

272
00:21:09,120 --> 00:21:14,120
also about dependencies and benchmark of packages version performances.

273
00:21:14,120 --> 00:21:21,120
We also have information about application binary interface, CVEs for libraries, different

274
00:21:21,120 --> 00:21:28,120
kind of security analyzers and also source code meta information.

275
00:21:29,040 --> 00:21:34,080
So now I want to talk to you about prescriptions and how they can help you heal your Python

276
00:21:34,080 --> 00:21:34,720
applications.

277
00:21:34,760 --> 00:21:40,440
So what we call prescriptions are in fact an example of configurable pipeline units

278
00:21:40,440 --> 00:21:43,360
that we use in touch resolution process.

279
00:21:43,360 --> 00:21:49,740
They are simply declarative YAML files stating how the resolution process should look like

280
00:21:49,740 --> 00:21:56,140
and they are a way to formalize developers knowledge about dependencies into units which

281
00:21:56,140 --> 00:22:02,840
are automatically consumed by the resolver to compute the software stack recommendations.

282
00:22:02,840 --> 00:22:08,160
So prescriptions are present on open source GitHub repository under the dot station organization

283
00:22:08,160 --> 00:22:13,800
so that every developer with any knowledge about dependencies can help improve dots resolution

284
00:22:13,800 --> 00:22:16,920
process.

285
00:22:16,920 --> 00:22:23,280
So now let's see an example of how a developer can create prescriptions from its knowledge.

286
00:22:23,280 --> 00:22:29,720
So here I took this issue from the pillow repository on GitHub of the pillow library

287
00:22:29,960 --> 00:22:36,960
and here we have this user who saw this issue which was that pillow 8.3 and NumPy are incompatible.

288
00:22:40,160 --> 00:22:44,760
So for example you have this error when you try to run an application with pillow 8.3

289
00:22:44,760 --> 00:22:49,760
and NumPy, type error array takes one positional arguments but two are given.

290
00:22:49,760 --> 00:22:55,320
So we see that pillow 8.3 and NumPy have API incompatibility.

291
00:22:55,320 --> 00:22:59,680
So what this developer can do for instance is create this prescription which is a little

292
00:22:59,720 --> 00:23:04,800
yaml file and if you want to know exactly how to create a prescription it's quite easy

293
00:23:04,800 --> 00:23:09,120
to do, it's something we have in our documentation.

294
00:23:09,120 --> 00:23:14,480
So basically in this prescription or yaml file you will find information about the packages

295
00:23:14,480 --> 00:23:19,800
that don't go together, the versions, the index that come from also and you will have

296
00:23:19,800 --> 00:23:23,520
a warning which will be returned as a justification to the user.

297
00:23:23,520 --> 00:23:28,520
For example pillow in version 8.3.0 does not work with NumPy and you also have the link

298
00:23:28,520 --> 00:23:34,200
to the justification which is the issue that was open on the pillow repository at the end

299
00:23:34,200 --> 00:23:41,200
so that users can check how white thoughts exactly didn't allow to solve pillow 8.3

300
00:23:42,160 --> 00:23:47,920
and NumPy during the resolution process.

301
00:23:47,920 --> 00:23:54,920
Some of the prescriptions we have are automatically created and refreshed on a weekly basis using

302
00:23:55,440 --> 00:24:00,640
Python package data from different sources such as the Python package index, GitHub open

303
00:24:00,640 --> 00:24:06,640
source repositories but also from publicly available data sets such as security scorecards

304
00:24:06,640 --> 00:24:11,720
data sets which is created by the open source security foundation.

305
00:24:11,720 --> 00:24:16,840
This prescription can contain data about package popularity, non-reliabilities for some package

306
00:24:16,840 --> 00:24:22,720
versions, artifact size for example and all of this can be used during the resolution

307
00:24:23,200 --> 00:24:28,800
process to assess the reliability of a given dependency.

308
00:24:28,800 --> 00:24:35,800
So finally let's see what are the conclusions we can draw from what we just saw.

309
00:24:36,200 --> 00:24:41,160
Managing dependencies in Jupyter Notebooks has become significantly easier for others

310
00:24:41,160 --> 00:24:46,480
because of requirements being locked and embedded directly into the notebook metadata with no

311
00:24:46,480 --> 00:24:49,440
additional file needed.

312
00:24:49,440 --> 00:24:53,560
Jupyter Notebooks with embedded dependencies can be built using Jupyter Notebook source

313
00:24:53,560 --> 00:24:57,800
to image without any additional files either.

314
00:24:57,800 --> 00:25:04,800
And finally Notebooks can be shared as standalone units with an environment ready in a few clicks.

315
00:25:06,040 --> 00:25:10,560
Using TOT to manage dependencies in your notebook both guarantees that your notebook will be

316
00:25:10,560 --> 00:25:15,080
shared in a consistent and reliable manner and that your application will benefit from

317
00:25:15,080 --> 00:25:20,440
the best possible software stack according to its purpose.

318
00:25:20,440 --> 00:25:24,640
Finally I would like to thank you and also encourage you to contribute to Project TOTS

319
00:25:24,640 --> 00:25:27,880
to help make Python dependency resolution better.

320
00:25:27,880 --> 00:25:30,600
If you have any questions you can ask them to me.

321
00:25:30,600 --> 00:25:32,800
I will be available right there.

322
00:25:32,800 --> 00:25:33,800
So thank you very much.

