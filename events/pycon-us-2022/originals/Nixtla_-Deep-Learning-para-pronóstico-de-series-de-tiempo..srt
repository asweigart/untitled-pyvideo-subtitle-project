1
00:00:00,000 --> 00:00:14,000
Buenas tardes. Tengo mucho gusto de introducir el siguiente presentador a Federico.

2
00:00:14,000 --> 00:00:26,000
Gracias por la invitación. Como les decía, mi compañero Federico no pudo venir.

3
00:00:26,000 --> 00:00:30,000
Él es muy movido en la comunidad open source y creo que se puso en contacto con Denis,

4
00:00:30,000 --> 00:00:38,000
que nos invitó a dar una charla sobre nuestro proyecto de predicciones de series de tiempo con Python.

5
00:00:38,000 --> 00:00:44,000
Ahora estamos armando una librería open source para dar acceso al público.

6
00:00:44,000 --> 00:00:50,000
Implementación bastante cool de estos modelos de series de tiempo.

7
00:00:50,000 --> 00:01:02,000
Tenemos esta empresa que se llama Nixla. Los principales contribuidores del proyecto somos Federico, que no pudo venir,

8
00:01:02,000 --> 00:01:06,000
y también Cristian, un amigo argentino, Max de México y Joaquín.

9
00:01:06,000 --> 00:01:16,000
Somos dos doctores de Machine Learning en CMU y un filósofo y un matemático de México trabajando en el proyecto.

10
00:01:16,000 --> 00:01:22,000
En el pasado tuvimos la oportunidad de trabajar en empresas haciendo esto,

11
00:01:22,000 --> 00:01:26,000
justo como parte de la forma en que nos ganamos la vida.

12
00:01:26,000 --> 00:01:32,000
Mi amigo Cristian ha estado ganando competencias en DARPA de predicción de series de tiempo.

13
00:01:32,000 --> 00:01:38,000
Tomamos la decisión de compartirle al mundo lo que sabemos.

14
00:01:38,000 --> 00:01:48,000
Perdón por el inglés, pero no tuve tiempo de cambiarlo al español, pero se las cuento en español.

15
00:01:48,000 --> 00:01:56,000
Voy a hacerles una pequeña introducción sobre qué son las series de tiempo y para qué sirven.

16
00:01:56,000 --> 00:02:04,000
Este es un pequeño overview, pero no importa.

17
00:02:04,000 --> 00:02:06,000
¿De qué se tratan las series de tiempo?

18
00:02:06,000 --> 00:02:16,000
Las predicciones de series de tiempo se trata de tener información del pasado y con información del pasado tratar de predecir el futuro.

19
00:02:16,000 --> 00:02:22,000
Con información del pasado quieres predecir eventos del futuro.

20
00:02:22,000 --> 00:02:24,000
Tienen muchos ejemplos de uso.

21
00:02:24,000 --> 00:02:36,000
En Estados Unidos, cuando tienes estos spikes COVID, te gustaría saber dónde van a hacer los spikes para poder enviar al personal médico o los materiales que necesitan para combatir la enfermedad.

22
00:02:36,000 --> 00:02:38,000
Ese es un ejemplo.

23
00:02:38,000 --> 00:02:44,000
Otro ejemplo muy bonito que a mí me gusta mucho porque soy un economista es el grid de energía eléctrica.

24
00:02:44,000 --> 00:03:04,000
En pocos lugares necesitas un equilibrio permanente entre oferta y demanda como en el sistema de energía eléctrica porque si hay más demanda de energía que lo que se produce de energía, ahorita estarían tintrillando las lámparas porque no habría un equilibrio en el grid y habría pagones en las ciudades.

25
00:03:04,000 --> 00:03:10,000
Las personas encargadas de producir la energía eléctrica necesitan saber por adelantado cuánto se va a demandar.

26
00:03:10,000 --> 00:03:14,000
Es un bonito ejemplo de uso de time series forecasting.

27
00:03:14,000 --> 00:03:20,000
Y por último, esta es parte de la experiencia que yo tuve en el pasado trabajando con Amazon.

28
00:03:20,000 --> 00:03:30,000
Necesitas, por ejemplo, saber cuántos items se van a demandar para distintas regiones para tu poder enviar los productos por adelantado a los warehouses y que te llegue lo que tu pides por internet en una o dos semanas.

29
00:03:30,000 --> 00:03:34,000
Esa es otra posible aplicación de time series forecasting.

30
00:03:34,000 --> 00:03:37,000
Bueno, entonces aquí ya es un poquito más técnico.

31
00:03:37,000 --> 00:03:41,000
Esta es la ecuación de análisis de regresión de qué es time series forecasting.

32
00:03:41,000 --> 00:03:47,000
Es predecir información del futuro, que es la variable Y, utilizando información del pasado.

33
00:03:47,000 --> 00:03:56,000
Donde la Y es esta variable que va de T más 1 en timestamps, T más 1, T más 2, T más 3, T más H, que es el forecast horizon que uno tiene.

34
00:03:56,000 --> 00:04:06,000
Y por mucho tiempo, antes de que machine learning se empezara a poner de moda, la forma más clásica de hacer time series forecasting era con un modelo de regresión.

35
00:04:06,000 --> 00:04:10,000
Y normalmente se hacen con modelos de regresiones lineales.

36
00:04:10,000 --> 00:04:16,000
En el dibujo de la izquierda, en el que se ve la parte de abajo, se ve la parte de abajo.

37
00:04:16,000 --> 00:04:46,000
En la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en

38
00:04:46,000 --> 00:05:16,000
la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la

39
00:05:16,000 --> 00:05:46,000
izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquier

40
00:05:46,000 --> 00:06:16,000
y en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda, en la izquierda,

41
00:06:16,000 --> 00:06:25,640
empezaron a triunfar en 2012, igual en temas de Natural Language Processing y por el momento en Time Series Forecasting apenas empezaron,

42
00:06:25,640 --> 00:06:32,000
pero la competencia M4 y M5 probaron que las redes neuronales pueden hacerlo bastante bien en Time Series Forecasting también.

43
00:06:32,000 --> 00:06:37,000
Pues ahora les voy a hablar un poquito sobre...

44
00:06:37,000 --> 00:06:56,000
existe una oportunidad para acortar la brecha entre los modelos de economía y los modelos de redes neuronales y poder hacer útiles, interpretables y mucho más amigables estos modelos para las personas.

45
00:06:56,000 --> 00:07:13,000
Este modelo se llama MBITS-X, lo construimos en base a una investigación hecha por el equipo de VENGIO de Element AI y es un modelo que hace predicciones de series de tiempo interpretables.

46
00:07:13,000 --> 00:07:29,000
Bueno, entonces parte de nuestra motivación es que los modelos de Machine Learning muchas veces funcionan como cajas negras y cuando uno toma una decisión en base a un modelo de Machine Learning le gustaría saber por qué ese modelo está diciendo lo que está diciendo o por qué te está prediciendo lo que te está prediciendo.

47
00:07:29,000 --> 00:07:40,000
Entonces existe esa necesidad de tener modelos interpretables para poderle ayudar a las personas que toman decisiones, tomar una decisión informada.

48
00:07:40,000 --> 00:07:47,000
Ok, entonces aquí justamente hablando sobre de qué nos sirven los polinomios y los senos y cocenos.

49
00:07:47,000 --> 00:07:57,000
En una forma clásica clásica de hacer regresiones lo que uno puede hacer es hacer proyecciones contra polinomios y contra armónicas.

50
00:07:57,000 --> 00:08:06,000
Esto nos sirve porque las armónicas normalmente sirven para capturar sisonalities y los polinomios nos ayudan a capturar tendencias.

51
00:08:06,000 --> 00:08:12,000
Entonces mucho del trabajo en Time Series Forecasting es alrededor de cachar tendencias y estacionalidades.

52
00:08:12,000 --> 00:08:17,000
Las tendencias con polinomios y las estacionalidades con armónicas.

53
00:08:17,000 --> 00:08:30,000
En el pasado muchos organismos importantes hacían estas descomposiciones de las series de tiempo, por ejemplo la FE del Banco Central de todos los países hace ese tipo de descomposiciones.

54
00:08:30,000 --> 00:08:37,000
Y existen estos modelos clásicos que son X11-Arima, X12-Arima, XSeeds y T-Bats por ejemplo.

55
00:08:37,000 --> 00:08:56,000
El equipo de Joshua Benjo en Element AI hizo este modelo MBITS que es un modelo interpretable que hace proyecciones contra armónicas y contra polinomios, más los factores autorregresivos.

56
00:08:56,000 --> 00:09:10,000
Existía entonces una necesidad de extender esto hacia variables exógenas porque muchas de las veces tú puedes tener información sobre lo que va a suceder en el futuro que puedes incluir en el modelo.

57
00:09:10,000 --> 00:09:17,000
Esas son las exógenas y nuestro trabajo fue completar este trabajo de Benjo para incluirle las variables exógenas.

58
00:09:18,000 --> 00:09:38,000
Lo que hicimos fue construir en base a este sistema de descomposición de series de tiempo e incluirle las variables exógenas y mostrar cómo este modelo puede alcanzar state of the art performance en un task bastante complicado que es el de predecir el precio de la electricidad.

59
00:09:39,000 --> 00:09:58,000
Esta es la arquitectura del modelo. Lo que hace este modelo es hacer una red neuronal profunda concatenando multilayer perceptrons que a través de la red lo que hacen es descomponer la señal en trend, seasonality y variables exógenas.

60
00:09:58,000 --> 00:10:13,000
Como les había dicho, la trend lo que hace es una proyección contra polinomios, la seasonality es una proyección contra senos y cosenos y por último las variables exógenas son una proyección contra las exógenas que uno quiere ponerle al modelo.

61
00:10:13,000 --> 00:10:25,000
Básicamente lo que hacemos con este tipo de modelo y de estas descomposiciones secuenciales en el modelo es tener un algoritmo que es capaz de decirte por qué las predicciones son como son.

62
00:10:25,000 --> 00:10:31,000
Te está diciendo cómo es la tendencia, cómo son las estacionalidades y te está diciendo qué depende de las exógenas que le mandaste.

63
00:10:31,000 --> 00:10:40,000
Básicamente una persona que toma una decisión puede tomar una decisión informada porque te está diciendo en qué parte está afectando qué cosa en la predicción.

64
00:10:40,000 --> 00:10:56,000
Para testear este modelo utilizamos precios de la electricidad. Ahorita es bastante interesante porque la electricidad en Europa depende muchísimo de energías renovables y también depende de la cantidad de energía demandada.

65
00:10:56,000 --> 00:11:13,000
El precio de electricidad se define en un equilibrio clásico de economía en donde tienes la supply que es la curva verde y demand que es la curva naranja y tienes estos dos factores, el de demanda y el de oferta.

66
00:11:13,000 --> 00:11:26,000
Por parte de la oferta uno quiere poner la producción de energías renovables y por parte de la demanda uno quiere ponerle la cantidad de demanda que va a haber en las siguientes horas para poder predecir el precio.

67
00:11:26,000 --> 00:11:48,000
Este es un ejemplo de cómo en Alemania las energías renovables se vuelven cada vez más importantes y poder predecir correctamente el precio y la cantidad de energía renovable producida es cada vez más importante porque introduce una volatilidad extra en el grid que los operadores necesitan poder estabilizar.

68
00:11:49,000 --> 00:12:12,000
Este es un ejemplo de cómo se pueden explicar los precios de energía eléctrica con este modelo. En el tope es el precio de electricidad a 24 horas, segundo es el efecto de la tendencia que es un polinomio cuadrático, tercero es la estacionalidad con senos y cocenos, cuarto son las variables exógenas que es la interacción de oferta y demanda y quinto es los residuales.

69
00:12:12,000 --> 00:12:20,000
Como pueden ver podemos explicar casi toda la información residual que el modelo anterior de Element AI y Joshua Benio no podía explicar.

70
00:12:22,000 --> 00:12:35,000
Aquí estamos comparando nuestro modelo que es el MBITS XG contra modelos clásicos por ejemplo autoregresivos, exponential smoothing, el modelo de Benio, otro tipo de autoregresivos que están especializados en el tema.

71
00:12:35,000 --> 00:12:56,000
En resumen midiendo nuestro performance con mean absolute error y mean absolute percentage error tenemos mejoras de 20% en la accuracy de los modelos y 5% contra modelos especializados en el tema.

72
00:12:57,000 --> 00:13:08,000
Siguiente es el modelo N-HITS que es un modelo muy bonito en donde lo que hacemos es extender sobre la transformada de Fourier.

73
00:13:09,000 --> 00:13:15,000
Voy a tratar de explicarles cuál es el problema de hacer predicciones y cuál es nuestra motivación para este modelo.

74
00:13:15,000 --> 00:13:33,000
Hacer predicciones de long horizon en donde lo que queremos hacer es hacer horizons muy amplios y ese es un problema porque los modelos autoregresivos clásicos lo que tienden a hacer es crean predicciones que alimentan a las predicciones, que alimentan a las predicciones, que alimentan a las predicciones.

75
00:13:34,000 --> 00:13:39,000
Entonces se concatenan los errores y los modelos recurrentes simplemente no pueden hacer predicciones de largo plazo.

76
00:13:39,000 --> 00:13:55,000
Una solución a este problema de concatenar errores en las predicciones es hacer un multi-step forecasting en donde en lugar de hacer predicciones que alimentan predicciones que alimentan predicciones creas todo el horizon al mismo tiempo.

77
00:13:55,000 --> 00:14:04,000
El caso es qué pasa cuando el horizon es no 3, 4, 5, 20 sino 900 timestamps adelante, qué es lo que podemos hacer para mejorar las predicciones.

78
00:14:05,000 --> 00:14:15,000
Y existe un problema clásico de machine learning en donde tienes modelos econométricos y estadísticos que tienen high bias, o sea tienen mucho sesgo y tienes modelos de machine learning que tienen high variance, que son demasiado volátiles.

79
00:14:15,000 --> 00:14:22,000
Y por último que son modelos costosísimos, o sea son modelos que tardan muchísimo en entrenar y muchísimo en predecir.

80
00:14:23,000 --> 00:14:33,000
En imágenes bonitas de explicarles cuál es el high bias y el high variance, en el lado izquierdo lo que hacemos es predecir con la línea 1, asume que es una tendencia lineal y esa es tu predicción.

81
00:14:33,000 --> 00:14:40,000
Bueno, entonces lo que termina pasando es que es un modelo de high bias, entonces las predicciones se rompen en el long horizon.

82
00:14:41,000 --> 00:14:52,000
En el otro caso dices ok, bueno asume que son simplemente senos y cosenos y trata de predecir y pretender que es un modelo estacional y estacionario y simplemente va a continuar haciendo un par de modelos.

83
00:14:52,000 --> 00:14:53,000
Es high bias de nuevo y el modelo se rompe.

84
00:14:54,000 --> 00:15:02,000
Y por el otro lado modelos de machine learning pueden hacer las predicciones long horizon, pero son demasiado volátiles y lo que queremos hacer es ese punto medio entre los dos casos.

85
00:15:03,000 --> 00:15:13,000
Tratar de hacer un modelo que sea suficientemente flexible pero que sea lo que sea, que sea lo que sea, que sea lo que sea, que sea lo que sea, que sea lo que sea.

86
00:15:13,000 --> 00:15:22,000
Bueno, adelantándome un poquito, los modelos de high bias con el horizon largo se rompen, entonces aquí en la gráfica del lado izquierdo es el performance del modelo.

87
00:15:23,000 --> 00:15:33,000
El modelo de high bias es un modelo que se rompe con el horizon largo, entonces aquí en la gráfica del lado izquierdo es el performance del modelo.

88
00:15:33,000 --> 00:15:43,000
El modelo naranja es un modelo de senos y cosenos matemática lineal, el performance se rompe, mientras que el modelo en hits que es el que propone unos otros tiene bastante buen performance en long horizon.

89
00:15:44,000 --> 00:15:53,000
Por el otro lado en cuanto a memoria y velocidad igual, logramos tener un modelo que es bastante rápido y bastante amigable con el horizon largo.

90
00:15:53,000 --> 00:16:05,000
Un poquito más técnico, ¿qué fue lo que hicimos? Ok, lo que hicimos fue hacer filtros en la información input porque básicamente tienes en lugar inputs largos, entonces lo que hacemos es filtrar y quedarnos información filtrada, esa es la primera aportación.

91
00:16:06,000 --> 00:16:18,000
Segunda aportación es utilizar interpretaciones de información que se presenten en el modelo de high bias, entonces aquí en la gráfica del modelo de high bias, lo que hacemos es filtrar y quedarnos información filtrada, esa es la primera aportación.

92
00:16:18,000 --> 00:16:30,000
Segunda aportación es utilizar interpolación, entonces lo que hacemos en long horizon es, tienes pequeñitos puntos y en lugar de predecir todos los puntos al mismo tiempo, predices poquitos y los interpolas, de tal manera que garantizas que las predicciones sean suaves y tienes un long horizon pero con suavidad in between.

93
00:16:30,000 --> 00:16:42,000
Aquí es la ecuación de la interpolación. Entonces imaginándose que el espacio entre los puntitos que uno predice en long horizon es bastante grande, lo que logramos con eso es que el espacio entre los puntos que uno predice en long horizon es bastante grande,

94
00:16:42,000 --> 00:16:54,000
lo que logramos con eso es que el modelo se fije en frecuencias bajas, entonces son cosas que cambian lentamente y lo que hacemos para completar de nuevo la predicción es interpolar in between.

95
00:16:54,000 --> 00:17:15,000
Siguiente, lo que hacemos es hacer que el espacio entre los puntos sea más chiquito y podemos enfocarnos en frecuencias de mediana velocidad pero lo que está muy interesante y por qué esto es tan importante, es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que logramos con eso es que el modelo se fije en frecuencias bajas, entonces son cosas que cambian lentamente y lo que hacemos para completar de nuevo la predicción es interpolar in between.

96
00:17:15,000 --> 00:17:45,000
Entonces, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los puntos que uno predice en long horizon es bastante grande, lo que hacemos es que el espacio entre los

97
00:17:45,000 --> 00:17:46,100
puntos dem stake en la聊的时候 que un

98
00:17:46,100 --> 00:17:48,340
Neo Horizon que unhardo y el n faut voir cómo

99
00:17:48,340 --> 00:18:02,540
se constituye en algochain en mí, es un

100
00:18:02,540 --> 00:18:05,340
besonders porque la idea del

101
00:18:05,340 --> 00:18:13,360
Atlanta a ellos es bien ji

102
00:18:13,360 --> 00:18:20,180
Entonces, eso hacemos con esta red neuronal profunda, especializamos las predicciones

103
00:18:20,180 --> 00:18:26,240
en distintas frecuencias y luego las volvemos a componer sumándolas.

104
00:18:26,240 --> 00:18:31,240
Este es un ejemplo de cómo el modelo puede aprender literal una señal de Fourier y la

105
00:18:31,240 --> 00:18:37,640
puede reconstruir perfectamente para horizons de 600 puntos de por adelantado.

106
00:18:37,640 --> 00:18:41,640
Luego lo aplicamos en datos etriales, por ejemplo en datos de electricidad, en datos

107
00:18:41,640 --> 00:18:51,360
de exchange rates en tráfico en el Bay Area en San Francisco y en el comportamiento de

108
00:18:51,360 --> 00:18:54,680
transformadores eléctricos.

109
00:18:54,680 --> 00:18:57,960
Este es un resumen de esta información.

110
00:18:57,960 --> 00:19:02,720
Siguiente es una tabla buena onda en donde vencemos a todos los modelos que están tratando

111
00:19:02,720 --> 00:19:04,320
de hacer long horizon forecasting.

112
00:19:04,400 --> 00:19:08,920
En particular interesante es que el modelo ARIMA, que es el que está hasta la derecha,

113
00:19:08,920 --> 00:19:14,400
se degrada justamente por el problema de concatenar predicciones con predicciones con predicciones

114
00:19:14,400 --> 00:19:16,760
que destruyen el forecasting.

115
00:19:16,760 --> 00:19:25,280
Entonces, midiendo nuestro performance con errores cuadráticos y con errores absolutos,

116
00:19:25,280 --> 00:19:33,520
logramos mejoras de 25% contra modelos state of the art, incluyendo un paper que ganó

117
00:19:33,520 --> 00:19:37,000
Best Paper Award en Triple AI el año pasado.

118
00:19:37,000 --> 00:19:46,040
Y encima hacemos que este tipo de computación sea 50 veces más rápido que las alternativas.

119
00:19:46,040 --> 00:19:50,600
Esta es una gráfica en donde mostramos el tiempo computacional, que está bastante dramático

120
00:19:50,600 --> 00:19:52,680
porque está en escala logarítmica.

121
00:19:52,680 --> 00:19:57,600
Sin la escala logarítmica no se vería la diferencia, estaríamos aplanadísimos.

122
00:19:57,600 --> 00:20:01,360
Y este es un ejemplo en donde comparamos la cantidad de parámetros que tenemos en el

123
00:20:01,360 --> 00:20:02,360
modelo.

124
00:20:02,400 --> 00:20:05,400
Entonces, básicamente somos bastante amigables con la memoria.

125
00:20:08,160 --> 00:20:14,280
En datos reales, cómo se ven las predicciones y la descomposición en las frecuencias?

126
00:20:14,280 --> 00:20:18,760
Pues este es un ejemplo contra los datos de los transformadores.

127
00:20:18,760 --> 00:20:22,600
Básicamente, esta es la temperatura de un transformador y vemos cómo podamos descomponer

128
00:20:22,600 --> 00:20:25,560
en distintas frecuencias la temperatura del transformador para predecirla.

129
00:20:25,560 --> 00:20:32,560
Y ahora hablando de Niksla y de open source y cómo pueden acceder a este tipo de modelos.

130
00:20:32,560 --> 00:20:34,560
Es súper buena onda.

131
00:20:34,560 --> 00:20:41,560
Hicimos este forecasting ecosystem que se llama Niksla, en donde tenemos modelos de

132
00:20:41,560 --> 00:20:45,560
neural forecast, modelos de stats forecast, en donde tenemos estos modelos clásicos,

133
00:20:45,560 --> 00:20:50,080
econometría y los modelos de redes neuronales que han sido parte de mi investigación del

134
00:20:50,080 --> 00:20:51,080
doctorado en CMU.

135
00:20:51,600 --> 00:20:58,600
Y los hicimos públicos, o sea, cualquier persona que quisiera entrar y verlos y usarlos

136
00:20:58,600 --> 00:20:59,600
están disponibles.

137
00:21:01,600 --> 00:21:06,600
Ok, voy a hablar un poquito más sobre la librería neural forecast.

138
00:21:06,600 --> 00:21:15,600
Aquí están por si quieren, si alguien le interesa tomarle una foto al código QR.

139
00:21:16,120 --> 00:21:22,400
Y esto los va a mandar a un Google collab hablando de Jupyter Notebooks, en donde pueden

140
00:21:22,400 --> 00:21:27,560
ustedes ejecutar el modelo N-Hits y conseguir este tipo de performance state of the art

141
00:21:27,560 --> 00:21:28,560
en Long Horizon.

142
00:21:28,560 --> 00:21:35,560
Y encima, esponsoriados por una GPU de Google, entonces está bastante cool.

143
00:21:35,560 --> 00:21:40,560
Voy a darme aquí una...

144
00:21:41,520 --> 00:21:48,520
Voy a ponerles el ejemplo y voy a enseñarles cómo corre este modelo N-Hits en la GPU.

145
00:21:48,520 --> 00:21:54,200
Esto es bastante cool porque antes, o sea, lo que terminaba pasando es que era bastante

146
00:21:54,200 --> 00:21:59,920
complicado correr modelos de redes neuronales porque uno tenía que tener una GPU, dos,

147
00:21:59,920 --> 00:22:05,720
ponerle los controladores, tres, instalarle Python 4, o sea, era imposible.

148
00:22:05,880 --> 00:22:11,960
Ahora con Google collab tienes todo eso pre-resuelto y puedes correr estos modelos state of the

149
00:22:11,960 --> 00:22:16,960
art en tus datos con hardware pasadísimo de lanza en segundos.

150
00:22:16,960 --> 00:22:21,960
Entonces, ahí está por si quieren ustedes aplicarlo en sus...

151
00:22:23,960 --> 00:22:28,800
Ok, entonces el modelo N-Hits y vamos... y sobre este forecasting pipeline.

152
00:22:28,800 --> 00:22:33,040
Lo primero que voy a hacer es instalar neural forecast.

153
00:22:33,040 --> 00:22:36,000
Está available en PIP y en Conda.

154
00:22:36,000 --> 00:22:39,000
Aquí estoy instalando directo contra...

155
00:22:48,000 --> 00:22:50,800
Lo primero es que se instalo con PIP.

156
00:22:50,800 --> 00:22:56,000
Uno puede instalar esto con PIP o con Conda y aquí está instalando directo del GitHub.

157
00:22:56,000 --> 00:23:00,880
Después de eso son todas las dependencias.

158
00:23:01,720 --> 00:23:05,520
Instalo Pandas, instalo Matplotlib, instalo neural forecast que es la librería que hemos

159
00:23:05,520 --> 00:23:06,520
estado trabajando.

160
00:23:06,520 --> 00:23:14,520
Después de eso, lo que hago es leer estos datos del transformador eléctrico.

161
00:23:14,520 --> 00:23:19,120
Están... ahorita están disponibles en un S3 que ustedes pueden descargar pero pueden

162
00:23:19,120 --> 00:23:20,320
ponerle los datos que quieran.

163
00:23:20,320 --> 00:23:24,320
De hecho es bastante cool porque pueden poner sus propios datos desde Google Drive.

164
00:23:24,320 --> 00:23:27,320
Vamos a ver los datos.

165
00:23:27,760 --> 00:23:30,760
Ahorita que termine de instalar...

166
00:23:30,760 --> 00:23:33,760
Ya, ya termino de instalar.

167
00:23:35,760 --> 00:23:38,760
Ahorita vamos a visualizar los datos.

168
00:23:42,760 --> 00:23:45,760
Ok, los está descargando.

169
00:23:46,200 --> 00:23:51,200
Ok, mientras está descargando y mientras les enseño los datos, de las cosas más importantes

170
00:23:51,200 --> 00:23:55,200
que hay que hacer en un modelo de Machine Learning, además de poderlo entrenar, es

171
00:23:55,200 --> 00:23:57,200
seleccionar hiperparámetros.

172
00:23:57,200 --> 00:24:03,200
Aquí lo que hicimos fue poner al alcance del público maneras automatizadas de seleccionar

173
00:24:03,200 --> 00:24:07,200
hiperparámetros de tal manera que una persona no se va a ver.

174
00:24:07,200 --> 00:24:12,200
Aquí lo que hicimos fue poner al alcance del público maneras automatizadas de seleccionar

175
00:24:12,640 --> 00:24:18,640
hiperparámetros de tal manera que una persona no necesita estar tuqueando y moviéndole

176
00:24:18,640 --> 00:24:25,640
a los hiperparámetros y si uno puede usar como el domain expertise de nuestro lado,

177
00:24:25,640 --> 00:24:30,640
ya tenemos un espacio preconfigurado de hiperparámetros y puede explorar ese espacio para conseguir

178
00:24:30,640 --> 00:24:31,640
muy buen performance.

179
00:24:31,640 --> 00:24:38,640
Entonces, tenemos acá en nuestros modelos, cachamos la forma de correr de PIP, perdón,

180
00:24:39,080 --> 00:24:46,080
de ScaleLearn, entonces definimos métodos fit y predict, de tal manera que cuando uno

181
00:24:46,080 --> 00:24:50,080
ejecuta el fit, lo que va a hacer es buscar automáticamente buenos hiperparámetros,

182
00:24:50,080 --> 00:24:51,080
una buena configuración.

183
00:24:51,080 --> 00:24:54,080
Esto se va a tardar como 5 minutos.

184
00:24:54,080 --> 00:25:00,080
Aquí voy a dejar corriendo esto en the background y seguir un poco contra la librería.

185
00:25:00,080 --> 00:25:06,080
Al final, lo que hacemos es mostrar cuáles son los mejores hiperparámetros del modelo.

186
00:25:06,520 --> 00:25:11,520
Y ver las predicciones.

187
00:25:13,520 --> 00:25:18,520
Me voy a pasar directamente hacia cómo se ven las predicciones de largo plazo.

188
00:25:18,520 --> 00:25:25,520
Está bastante cool porque justamente especializar el modelo en frecuencias de largo y mediano

189
00:25:25,520 --> 00:25:30,520
plazo es lo más importante cuando uno está haciendo predicciones de largo plazo.

190
00:25:30,960 --> 00:25:37,960
Al final, como les había mencionado, medimos el performance con mean absolute error y mean

191
00:25:37,960 --> 00:25:44,960
square error y aquí replicamos el performance de nuestros experimentos y lo comparamos con

192
00:25:44,960 --> 00:25:49,960
la tabla de los performance en los papers, del transformer based methods.

193
00:25:49,960 --> 00:25:56,960
Voy a regresar entonces a la tabla de la performance y la tabla de la performance.

194
00:26:00,960 --> 00:26:03,960
A la presentación.

195
00:26:03,960 --> 00:26:10,960
La siguiente parte que es muy cool es la librería stats forecast donde tenemos estos modelos

196
00:26:11,560 --> 00:26:18,560
econométricos clásicos, por ejemplo exponential smoothing, tenemos el ARIMA y tenemos una implementación

197
00:26:18,560 --> 00:26:25,560
en nuestro, bueno, probablemente tengo demasiado en alto consideración nuestro trabajo, pero

198
00:26:25,600 --> 00:26:31,600
creo que es la implementación más rápida que existe de los modelos clásicos de ARIMA

199
00:26:31,600 --> 00:26:37,480
y exponential smoothing porque lo que hicimos fue hacer llamadas a C desde Python de tal

200
00:26:37,480 --> 00:26:42,480
manera que el código corre en C y tú solo haces las calls en Python.

201
00:26:42,480 --> 00:26:47,080
Encima eso te ayuda porque puedes paralelizar en tus calls una cosa que Python no podía

202
00:26:47,080 --> 00:26:50,720
hacer muy bien, entonces tú puedes hacer que tu código se ejecute, si tienes una máquina

203
00:26:50,720 --> 00:26:57,200
de 96 calls puedes ejecutar todos tus calls al mismo tiempo y predecir los últimos experimentos

204
00:26:57,200 --> 00:27:01,880
que estábamos haciendo con esto era predecir millones de series en menos de una hora con

205
00:27:01,880 --> 00:27:04,880
este código de stats forecast.

206
00:27:04,880 --> 00:27:07,880
Otra, si les interesa también.

207
00:27:07,880 --> 00:27:12,880
¿Todo el proceso de hacer amor en vez de un amor por una onda de FF?

208
00:27:12,880 --> 00:27:19,880
Sí, obvio, siempre que uno quiera hacer, yo soy superpartidario de machine learning

209
00:27:20,040 --> 00:27:27,040
y sí, mil veces, porque una GPU tiene dos mil cores, pero mientras el problema con los

210
00:27:27,600 --> 00:27:32,600
modelos clásicos es que tienen muchos for loops, entonces se necesitan ejecutar en CPU.

211
00:27:32,600 --> 00:27:39,600
Pero sí, si alguien le interesa estos modelos clásicos y quiere probar estas implementaciones

212
00:27:40,680 --> 00:27:42,680
aquí también está el link.

213
00:27:42,680 --> 00:27:49,680
Y por último ya, este es el Slack, si les gustaría contactar con ustedes, si les gustaría

214
00:27:49,960 --> 00:27:55,680
contactarnos, nuestro GitHub que básicamente no tenemos página web, eso es bonito porque

215
00:27:55,680 --> 00:28:02,680
siempre nos preguntan, oye, tu página web, nosotros tenemos la creencia de que es mejor

216
00:28:03,200 --> 00:28:09,400
la street credibility que una página internet bonita, entonces por el momento nuestra página

217
00:28:09,400 --> 00:28:13,680
internet es nuestro GitHub y ahí nos pueden poner un GitHub issue, alguna cosa en una

218
00:28:13,680 --> 00:28:17,320
discusión y pues muchas gracias por su atención.

