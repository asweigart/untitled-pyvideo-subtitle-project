1
00:00:00,000 --> 00:00:10,800
But now we're going to hear about open source Python-based tools for data privacy by Kylian

2
00:00:10,800 --> 00:00:11,800
Kieran.

3
00:00:11,800 --> 00:00:12,800
Thank you so much, Mason.

4
00:00:12,800 --> 00:00:13,800
Thank you all for having me.

5
00:00:13,800 --> 00:00:18,080
I'm conscious I'm going to try and get through a huge amount in 30 minutes, so I apologize

6
00:00:18,080 --> 00:00:20,120
if I speak very, very quickly.

7
00:00:20,120 --> 00:00:23,560
All of the documentation, all of the repos I'm going to use for this are publicly available

8
00:00:23,560 --> 00:00:24,880
and there are links to everything.

9
00:00:24,880 --> 00:00:29,560
So if you're trying to furiously keep up, please don't try and just listen, I guess.

10
00:00:29,560 --> 00:00:31,080
So very quickly, I'm Kylian Kieran.

11
00:00:31,080 --> 00:00:33,240
I'm the founder and CEO of Ethica.

12
00:00:33,240 --> 00:00:35,240
We're a data privacy engineering company.

13
00:00:35,240 --> 00:00:38,040
I'll explain what that is in a moment more substantively.

14
00:00:38,040 --> 00:00:41,600
I'm here specifically to talk to you about a handful of things that we do.

15
00:00:41,600 --> 00:00:45,000
So very quickly, we have this sort of thesis about privacy we call Privacy as Code.

16
00:00:45,000 --> 00:00:50,720
I'm going to explain what that is very quickly, then an overview of Fidos, so what that is,

17
00:00:50,720 --> 00:00:54,880
and then show you examples of Privacy as Code using Fidos, which is an open source privacy

18
00:00:54,880 --> 00:00:56,200
engineering platform.

19
00:00:56,200 --> 00:00:57,560
So it really is information dense.

20
00:00:57,560 --> 00:00:59,080
I apologize in advance.

21
00:00:59,080 --> 00:01:02,240
I wanted to squeeze as much in for you guys as possible.

22
00:01:02,240 --> 00:01:08,000
So this might seem obvious, but at Ethica, we're about 35 or 40 people and we believe

23
00:01:08,000 --> 00:01:10,400
privacy is a fundamental human right.

24
00:01:10,400 --> 00:01:13,440
Everybody in this audience is probably going to agree to that privacy is a fundamental

25
00:01:13,440 --> 00:01:14,800
human right.

26
00:01:14,800 --> 00:01:19,400
But it turns out it's really, really complicated, particularly for engineers, right?

27
00:01:19,400 --> 00:01:20,600
It's a difficult thing to do well.

28
00:01:20,600 --> 00:01:24,480
So we care about it, but it's difficult.

29
00:01:24,480 --> 00:01:28,360
Privacy today in most businesses, not everyone's, but most businesses, sort of happens after

30
00:01:28,440 --> 00:01:29,640
you ship software, right?

31
00:01:29,640 --> 00:01:33,040
So you ship an application and then you have to address privacy problems because you've

32
00:01:33,040 --> 00:01:34,680
collected some data in production.

33
00:01:34,680 --> 00:01:36,400
You've got human beings information in it.

34
00:01:36,400 --> 00:01:41,440
Now you've got a privacy risk and it becomes a pain for developers and for lawyers.

35
00:01:41,440 --> 00:01:44,440
Often it's a pain for developers from lawyers, right?

36
00:01:44,440 --> 00:01:46,240
Like that's the reality of it.

37
00:01:46,240 --> 00:01:50,080
And it's things like you might have heard of like data discovery, data annotation, data

38
00:01:50,080 --> 00:01:54,640
mapping, privacy reviews, risk reviews, consent, rights propagation, et cetera.

39
00:01:54,640 --> 00:01:57,040
All of these sort of complicated legal concepts.

40
00:01:57,720 --> 00:02:03,760
Now, the cause of a lot of this are reasonably new or relatively new legal regulations like

41
00:02:03,760 --> 00:02:10,280
the GDPR in Europe, the CCP area, the CPRA in California, the LGPD in Brazil, the People

42
00:02:10,280 --> 00:02:11,280
Act in China.

43
00:02:11,280 --> 00:02:12,720
There are many, many privacy regulations, right?

44
00:02:12,720 --> 00:02:14,720
You should not expect it to know the acronyms.

45
00:02:14,720 --> 00:02:15,720
You're engineers.

46
00:02:15,720 --> 00:02:16,720
That's the point.

47
00:02:16,720 --> 00:02:18,920
The idea behind a lot of these, though, is privacy by design.

48
00:02:18,920 --> 00:02:22,000
Now, if you're not familiar with privacy by design, that's an entirely different topic

49
00:02:22,000 --> 00:02:23,720
that we won't have time to cover today.

50
00:02:23,720 --> 00:02:26,160
There are seven principles in privacy by design.

51
00:02:26,160 --> 00:02:31,720
The TLDR is essentially how do you make privacy a design consideration in technical implementation?

52
00:02:31,720 --> 00:02:32,720
So how do you do it sooner?

53
00:02:32,720 --> 00:02:37,720
How do you make sure that you don't ship a product having not thought about privacy?

54
00:02:37,720 --> 00:02:41,600
These are some of the acronyms and definitions that exist in the world of privacy, right?

55
00:02:41,600 --> 00:02:42,840
They're pretty complicated.

56
00:02:42,840 --> 00:02:43,840
Anyone here know what a DPIA is?

57
00:02:43,840 --> 00:02:44,840
You don't have to answer.

58
00:02:44,840 --> 00:02:46,600
But they're examples, right?

59
00:02:46,600 --> 00:02:49,200
That's a data protection impact assessment.

60
00:02:49,200 --> 00:02:52,120
What it really means in plain English is a privacy review.

61
00:02:52,120 --> 00:02:55,760
Evaluate the risk of the code that we're writing to make sure that we don't compromise the

62
00:02:55,760 --> 00:02:58,840
human rights of the individual's data in the system.

63
00:02:58,840 --> 00:03:01,720
So lots of these complex concepts for engineers.

64
00:03:01,720 --> 00:03:06,960
And what we think at Ethica essentially is that tools should be built to add privacy

65
00:03:06,960 --> 00:03:07,960
into software development.

66
00:03:07,960 --> 00:03:09,960
A bit like shift left for security, right?

67
00:03:09,960 --> 00:03:10,960
Not so different.

68
00:03:10,960 --> 00:03:12,280
And we call that privacy as code.

69
00:03:12,280 --> 00:03:17,120
It's this idea that we can provide tools to developers to make it easier to build safer,

70
00:03:17,120 --> 00:03:20,440
more respectful systems.

71
00:03:20,440 --> 00:03:22,380
The benefit of this is really obvious, right?

72
00:03:22,420 --> 00:03:25,820
If you could accomplish this, like future state panacea, right?

73
00:03:25,820 --> 00:03:28,500
You'd essentially not have to think so much about privacy, not because you as developers

74
00:03:28,500 --> 00:03:31,020
don't care about it, but you're busy doing other things, right?

75
00:03:31,020 --> 00:03:34,820
Like you're trying to ship applications, solve problems.

76
00:03:34,820 --> 00:03:36,020
Privacy is sort of a means to an end.

77
00:03:36,020 --> 00:03:38,740
It's a non-functional requirement almost, right?

78
00:03:38,740 --> 00:03:40,420
And so we think of it as a layer of our tech stack, right?

79
00:03:40,420 --> 00:03:44,700
We've got front-end, back-end data infrastructure, infrastructure as code, our source control

80
00:03:44,700 --> 00:03:45,700
environments.

81
00:03:45,700 --> 00:03:47,580
We should have a layer that deals with privacy.

82
00:03:47,580 --> 00:03:52,300
So that brings us to the project that we've been working on at Ethica for over three years.

83
00:03:52,380 --> 00:03:53,900
So the company's about three years old.

84
00:03:53,900 --> 00:03:56,220
And we've been working on this project we call Fidesz.

85
00:03:56,220 --> 00:03:59,420
Fidesz is named after the Roman goddess of trust.

86
00:03:59,420 --> 00:04:02,020
We think that's apt, given all things.

87
00:04:02,020 --> 00:04:03,140
And it does what it says, right?

88
00:04:03,140 --> 00:04:07,180
It's an open-source developer tool for privacy by design, right, in any tech stack.

89
00:04:07,180 --> 00:04:09,500
Now, it happens to be built in Python, and that's why we're here.

90
00:04:09,500 --> 00:04:11,340
Everything we do is built in Python.

91
00:04:11,340 --> 00:04:15,340
And the intention of this is to help developers and engineers of all kinds, right, software,

92
00:04:15,340 --> 00:04:18,820
product, data engineers, to more easily comply with regulations.

93
00:04:18,820 --> 00:04:22,060
So sort of remove the cognitive load so you don't have to think about these issues and

94
00:04:22,060 --> 00:04:25,300
ensure that you're building safe systems for everybody.

95
00:04:25,300 --> 00:04:27,860
So this is a very, very dense slide.

96
00:04:27,860 --> 00:04:29,340
We'll try and explain it very simply.

97
00:04:29,340 --> 00:04:31,940
The left side, you've got code management environments, right, the sort of software

98
00:04:31,940 --> 00:04:36,460
development lifecycle where we design, implement, test, and eventually ship stuff.

99
00:04:36,460 --> 00:04:39,060
On the right side, you've got your runtime environments, right, your sort of production

100
00:04:39,060 --> 00:04:40,340
infrastructure.

101
00:04:40,340 --> 00:04:44,940
The types of questions that the laws ask us to answer are things like, on the left, well,

102
00:04:44,940 --> 00:04:48,220
how can we prevent risk when we're building our products, right?

103
00:04:48,220 --> 00:04:52,280
Like how do we prevent ourselves from accidentally using phone numbers that were collected for

104
00:04:52,280 --> 00:04:57,300
security and exposing them in APIs for developers, for games?

105
00:04:57,300 --> 00:04:58,300
That's a real example, right?

106
00:04:58,300 --> 00:05:01,500
That's why Facebook got fined $5 billion for doing that.

107
00:05:01,500 --> 00:05:03,500
Meta, I guess.

108
00:05:03,500 --> 00:05:07,980
Runtime, production environment, questions like how do we dynamically define and enforce

109
00:05:07,980 --> 00:05:08,980
a user's rights?

110
00:05:08,980 --> 00:05:10,500
And what do we mean here, right?

111
00:05:10,500 --> 00:05:15,780
The GDPR and the CPRA and the LGPD, they provide human beings, all of us in the room and everybody,

112
00:05:16,620 --> 00:05:20,700
like I can be deleted from a system and developers need to be able to assure that we can guarantee

113
00:05:20,700 --> 00:05:24,020
that we can consistently delete someone from an entire distributed data infrastructure

114
00:05:24,020 --> 00:05:26,100
and that could be petabytes, right?

115
00:05:26,100 --> 00:05:30,500
So code management environment is how do we prevent risk?

116
00:05:30,500 --> 00:05:34,020
Runtime is how do we ensure we afford a user or a human their rights?

117
00:05:34,020 --> 00:05:36,820
So we have tools in Fidesz that solve these problems.

118
00:05:36,820 --> 00:05:40,380
Fidesz control is on the left and we'll talk about that in a moment and Fidesz ops is on

119
00:05:40,380 --> 00:05:41,500
the right for the runtime.

120
00:05:41,500 --> 00:05:46,140
So the way to think about Fidesz, and I apologize, I'm going fast, but I'm trying to cover a

121
00:05:46,140 --> 00:05:47,140
lot of ground.

122
00:05:47,140 --> 00:05:50,580
I haven't even got to the live demo where I fall flat on my face.

123
00:05:50,580 --> 00:05:55,220
So the core of Fidesz is the language, Fidesz lang, and I'll show you that in a moment.

124
00:05:55,220 --> 00:05:59,100
Think of it as a lightweight description language for privacy and I'll elaborate on what that

125
00:05:59,100 --> 00:06:00,100
means in a second.

126
00:06:00,100 --> 00:06:03,620
And again, we've got two applications or tools that sit on top of the language.

127
00:06:03,620 --> 00:06:07,980
Fidesz control, which I mentioned, is a server that runs in the software development lifecycle

128
00:06:07,980 --> 00:06:08,980
effectively.

129
00:06:08,980 --> 00:06:11,060
It does a few other nifty things, but think of it that way.

130
00:06:11,100 --> 00:06:15,180
And Fidesz ops is a containerized application that allows you to execute and manage tasks

131
00:06:15,180 --> 00:06:16,580
on behalf of users' rights.

132
00:06:16,580 --> 00:06:17,580
Okay.

133
00:06:17,580 --> 00:06:20,180
So, a little bit more about the language.

134
00:06:20,180 --> 00:06:21,980
This is very hard to see on screen.

135
00:06:21,980 --> 00:06:27,500
So if you are sitting at a computer, I'd encourage you to, let me get rid of that.

136
00:06:27,500 --> 00:06:32,100
I'd encourage you to go to FID.es slash taxonomy and you can explore the taxonomy in the open

137
00:06:32,100 --> 00:06:34,340
source docs directly.

138
00:06:34,340 --> 00:06:36,300
But so it's a description language.

139
00:06:36,300 --> 00:06:40,180
When we say description, we mean lightweight in the way the terraform is, right?

140
00:06:40,180 --> 00:06:42,020
So terraform describes infrastructure.

141
00:06:42,020 --> 00:06:44,700
Fidesz describes privacy characteristics.

142
00:06:44,700 --> 00:06:48,140
So its intention is to ensure that the engineer or developer doesn't need to know a whole

143
00:06:48,140 --> 00:06:49,140
lot about privacy.

144
00:06:49,140 --> 00:06:53,540
You can describe the personal data or behaviors of your system's data such that you can then

145
00:06:53,540 --> 00:06:56,180
enforce policy or mitigate risk.

146
00:06:56,180 --> 00:06:57,900
So we don't need to know about privacy.

147
00:06:57,900 --> 00:07:01,180
We just need to be able to describe what our system is doing and Fidesz takes care of the rest.

148
00:07:03,020 --> 00:07:06,980
So let's talk about the sort of four components of the core taxonomy in a little bit more detail.

149
00:07:06,980 --> 00:07:09,380
They're what we call data categories.

150
00:07:09,380 --> 00:07:12,180
So they're the what, like the what type of data am I processing?

151
00:07:12,180 --> 00:07:16,740
So is it email addresses, behavioral information, what kind of information am I dealing with?

152
00:07:16,740 --> 00:07:17,980
Then you've got data subjects, right?

153
00:07:17,980 --> 00:07:18,980
That's the who.

154
00:07:18,980 --> 00:07:19,980
So who owns the information?

155
00:07:19,980 --> 00:07:23,020
Now, ownership is a sort of tricky concept in privacy, right?

156
00:07:23,020 --> 00:07:27,340
Because an owner in this context is from whom the data came, right?

157
00:07:27,340 --> 00:07:30,340
So an individual like Mason, for example, right?

158
00:07:30,340 --> 00:07:33,900
Whereas an owner could also be somebody that has collected and has a legal right of ownership.

159
00:07:33,900 --> 00:07:36,620
Here we're talking about the human being behind the information though, right?

160
00:07:36,620 --> 00:07:38,180
Then you've got data uses.

161
00:07:38,700 --> 00:07:41,380
Think of that like what we're using the information for.

162
00:07:41,380 --> 00:07:43,300
Are we personalizing our system?

163
00:07:43,300 --> 00:07:45,860
Are we doing advertising?

164
00:07:45,860 --> 00:07:47,700
What are we doing with that data?

165
00:07:47,700 --> 00:07:49,580
And then finally, data qualifiers.

166
00:07:49,580 --> 00:07:56,620
So think of a qualifier in Fidesz as a way of declaring how identifiable an individual is in a given data set.

167
00:07:56,620 --> 00:07:58,140
So someone might be fully identified, right?

168
00:07:58,140 --> 00:08:00,860
That is to say you've got fully identifiable data.

169
00:08:00,860 --> 00:08:04,780
You might have partially pseudonymized or pseudonymized and unlinked data.

170
00:08:04,780 --> 00:08:07,220
You might have aggregated and partially anonymized data.

171
00:08:07,260 --> 00:08:11,460
And all of these create a different risk of re-identification in a data set.

172
00:08:11,460 --> 00:08:18,660
Now, what's important to understand is with these four very simple concepts, data categories, data subjects, data uses and data qualifiers,

173
00:08:18,660 --> 00:08:23,380
you essentially model most of the, let's call it data and privacy characteristics of a system.

174
00:08:23,380 --> 00:08:24,500
And that's the purpose of this, right?

175
00:08:24,500 --> 00:08:25,500
Do it very, very simply.

176
00:08:27,580 --> 00:08:31,580
So again, if you want to explore the taxonomy more, you can do that online.

177
00:08:31,580 --> 00:08:35,100
The one thing I would notice, it ships with the taxonomy that we've built over the last three years,

178
00:08:35,140 --> 00:08:41,460
working with standards for the GDPR, the CPRA, LGPD, also ISO 19944,

179
00:08:41,460 --> 00:08:45,980
which is the ISO standard related to cloud service provider data flows, essentially.

180
00:08:45,980 --> 00:08:50,420
So it models against all of the complex regulations that you'd need to consider if you think about privacy.

181
00:08:50,420 --> 00:08:51,740
You can extend it.

182
00:08:51,740 --> 00:08:56,780
So most of the business users that use our open source tools have extended it for their specific uses.

183
00:08:56,780 --> 00:08:58,100
And we can talk about that afterwards.

184
00:08:59,820 --> 00:09:02,340
So then we get into how you actually declare, like how you write this stuff.

185
00:09:02,340 --> 00:09:03,660
It's pretty simple.

186
00:09:03,700 --> 00:09:07,260
Like I mentioned, it's intentionally lightweight and declarative in style, right?

187
00:09:07,260 --> 00:09:08,740
It's dot notation mostly.

188
00:09:08,740 --> 00:09:11,460
You'll see there's some snake case in there in some places, which is not ideal.

189
00:09:11,460 --> 00:09:12,300
We're working on it.

190
00:09:13,300 --> 00:09:15,620
You declare it as YAMLs in your projects.

191
00:09:15,620 --> 00:09:19,020
We're currently working on a version that allows you to declare it inline

192
00:09:19,020 --> 00:09:21,340
in whatever language you're working.

193
00:09:21,340 --> 00:09:23,620
Obviously, first for us is Python, then Java and Scala.

194
00:09:24,580 --> 00:09:29,020
But so effectively today, you declare it in distinct YAML files that you check into your projects.

195
00:09:29,020 --> 00:09:32,380
So if I wanted to describe two types of data I was handling just as examples,

196
00:09:32,380 --> 00:09:34,820
if I wanted to say I was dealing with data that's non-personal, right?

197
00:09:34,820 --> 00:09:37,740
It's data related to systems operations, like a timestamp.

198
00:09:37,740 --> 00:09:41,220
I would simply declare it in that way, system.operations, right?

199
00:09:41,220 --> 00:09:43,780
If I wanted to declare that I was dealing with an email address,

200
00:09:43,780 --> 00:09:47,540
I would say to user provided, identifiable, contact email.

201
00:09:47,540 --> 00:09:52,340
Now, I can be very specific, so very fine grained, a specific sort of label, like an email address.

202
00:09:52,340 --> 00:09:53,180
Or I can be very abstract.

203
00:09:53,180 --> 00:09:56,940
If I as a developer don't really know what I'm dealing with, it might be just, I know it's user data.

204
00:09:56,940 --> 00:09:58,660
I could just declare it as user data.

205
00:09:58,660 --> 00:10:00,300
So just very conceptually, it's very simple.

206
00:10:01,300 --> 00:10:03,500
Then we get into Fidesz primitives, right?

207
00:10:03,500 --> 00:10:07,540
So there are sort of four primitive resources that exist in Fidesz.

208
00:10:07,540 --> 00:10:09,940
There are organizations, they're pretty obvious, right?

209
00:10:09,940 --> 00:10:13,620
So it represents all or part or any of an organization, right?

210
00:10:13,620 --> 00:10:18,140
So it could be a company like Acme Inc. or it could be a department in a company.

211
00:10:18,140 --> 00:10:20,380
It establishes the root of the resource hierarchy.

212
00:10:20,380 --> 00:10:23,220
So what we mean by that is they've limited scope.

213
00:10:23,220 --> 00:10:28,180
So if you had two resources within one project or two organizations within one project, for example,

214
00:10:28,220 --> 00:10:30,380
one organization cannot seek out the scope of another, right?

215
00:10:30,380 --> 00:10:31,860
They are completely distinct.

216
00:10:31,860 --> 00:10:33,660
There's no way around that by design.

217
00:10:33,660 --> 00:10:35,420
Then you've got systems.

218
00:10:35,420 --> 00:10:41,940
So systems represent the privacy properties of a single project or a service or some code base or an application.

219
00:10:41,940 --> 00:10:44,540
So think of that as describing the behavior of my system.

220
00:10:44,540 --> 00:10:48,220
Again, think of that, that could be a single service, could be an entire monolithic application.

221
00:10:48,220 --> 00:10:53,380
It depends on how coarsely or how narrowly you want to describe a system's behaviors or properties.

222
00:10:53,420 --> 00:10:58,420
So we see a lot of use of this with larger enterprise customer users of the Open Source tools.

223
00:10:58,420 --> 00:11:00,300
And they're describing individual services, right?

224
00:11:00,300 --> 00:11:03,140
Like right down to a single microservice.

225
00:11:03,140 --> 00:11:07,860
And so it describes the category of data you're using and the purpose for which you're using it.

226
00:11:07,860 --> 00:11:09,260
And then you've got datasets.

227
00:11:09,260 --> 00:11:13,180
Now, in most cases, datasets, you can think of them as really describing a sort of data source,

228
00:11:13,180 --> 00:11:16,620
like a database or data warehouse or some other storage system.

229
00:11:16,620 --> 00:11:21,980
More practically, datasets in Fidesz are designed to represent or model anything that might contain data.

230
00:11:21,980 --> 00:11:24,260
So a list, a collection, an interface, et cetera.

231
00:11:26,100 --> 00:11:27,780
And finally, you've got policies.

232
00:11:27,780 --> 00:11:29,660
So think of policies exactly as they sound.

233
00:11:29,660 --> 00:11:33,740
The ability to describe a set of rules about your system in code.

234
00:11:33,740 --> 00:11:40,180
So it's like taking the privacy policy of a business and converting it into an enforceable rule on that system.

235
00:11:40,180 --> 00:11:41,700
And I'll show you an example of that in a minute.

236
00:11:41,700 --> 00:11:46,420
And we do that by evaluating policies against systems and organizational datasets.

237
00:11:46,420 --> 00:11:48,900
OK, so what can you do with Fidesz?

238
00:11:48,900 --> 00:11:51,380
You can do a whole bunch of things, but the ones that we're going to focus on today,

239
00:11:51,420 --> 00:11:55,380
you can effectively declare what type of data you're handling in your application.

240
00:11:55,380 --> 00:11:58,700
You can describe how your system is using that data.

241
00:11:58,700 --> 00:12:01,380
And then you can declare what kind of policies you want to enforce.

242
00:12:01,380 --> 00:12:04,620
So I don't want to be able to do these things or I permit these things, for example.

243
00:12:04,620 --> 00:12:06,980
There's a bunch of other nifty things.

244
00:12:06,980 --> 00:12:12,100
So again, just a reminder, Fidesz Control, which is the developer tools for shifting privacy left,

245
00:12:12,100 --> 00:12:14,900
so making it easier for developers to do this.

246
00:12:14,900 --> 00:12:19,740
And then there's Fidesz Ops, which is that runtime application that allows us to execute tasks on our systems.

247
00:12:19,740 --> 00:12:21,660
I'm going to try and show you an example of both.

248
00:12:21,660 --> 00:12:24,020
How are we doing for time?

249
00:12:24,020 --> 00:12:25,420
We're good? OK, awesome.

250
00:12:25,420 --> 00:12:27,420
So we've got two use cases.

251
00:12:27,420 --> 00:12:30,220
So the first is policy enforcement in development.

252
00:12:30,220 --> 00:12:32,700
OK, now here's a really frightening.

253
00:12:32,700 --> 00:12:34,300
Well, I'll get to that very frightening slide in a minute.

254
00:12:34,300 --> 00:12:37,660
Let's just explain Fidesz Control's configuration and components for a moment.

255
00:12:37,660 --> 00:12:40,500
It's very simple. You could deploy it in about 10 minutes.

256
00:12:40,500 --> 00:12:45,460
If you grab the Fidesz demo repo and just run make demo, it'll be up and running and you can play with it.

257
00:12:45,500 --> 00:12:51,260
But there's essentially a Fidesz Control server backed to, in this case, a Postgres database.

258
00:12:51,260 --> 00:12:53,220
And there are integrations to your CI pipelines.

259
00:12:53,220 --> 00:12:55,900
And there's also a CLI tool so you can query it directly.

260
00:12:55,900 --> 00:12:58,740
So at the moment, the web server is essentially headless.

261
00:12:58,740 --> 00:13:01,700
There's some basic UIs for reporting, which I'll show you in a moment.

262
00:13:01,700 --> 00:13:07,620
But the server essentially maintains the metadata that's checked in and out through the CI pipeline and evaluates policies.

263
00:13:07,620 --> 00:13:10,980
So it's a very simple architectural model.

264
00:13:10,980 --> 00:13:13,180
To get started, super simple.

265
00:13:13,220 --> 00:13:15,660
You can declare an Atom file or a bunch of environment variables.

266
00:13:15,660 --> 00:13:20,180
Straightforward. You can see here there's a URL for the database, server, etc.

267
00:13:20,180 --> 00:13:30,300
If you want to get it set up, as I mentioned, you can go to Fidesz FID.es for control and the configuration documents like Quickstart is just slash config.

268
00:13:30,300 --> 00:13:36,100
So there's a bunch of commands and we won't have time to get through them all here, but I just want to draw your attention to four that are particularly interesting.

269
00:13:36,100 --> 00:13:39,940
They're listed here alphabetically, but I'm going to reorganize them to explain them.

270
00:13:39,940 --> 00:13:52,820
So the first thing that you can do is actually scan data infrastructure so you can actually point FID.es control at, for example, an AWS environment and it will return a list of systems that may contain data and generate data set and system files for them.

271
00:13:52,820 --> 00:13:54,980
So you don't have to do that manually. You're not manually declaring everything.

272
00:13:54,980 --> 00:13:58,260
It will generate a scanned view of your systems.

273
00:13:58,260 --> 00:14:02,980
From there, you can generate a data set so you can point it directly at a data source.

274
00:14:02,980 --> 00:14:09,100
You can credential it against a Postgres or Redshift and it will generate a FID.es data set declaration file.

275
00:14:09,100 --> 00:14:13,260
You can then manually annotate it so you can describe the types of information that are in that data set.

276
00:14:13,260 --> 00:14:20,340
I'm going to show you these in a moment and then you can evaluate the risk so you can say, hey, check this against policies for the organization.

277
00:14:20,340 --> 00:14:24,260
If you want to see a full list of the commands, it's just FID.es slash command.

278
00:14:24,260 --> 00:14:27,660
Now, onto the scary thing. I mentioned we're going to do a policy evaluation.

279
00:14:27,660 --> 00:14:30,140
Some of you may be familiar with this because you're in larger organizations.

280
00:14:30,140 --> 00:14:33,660
Some of you less so. This is what privacy laws are all about.

281
00:14:33,660 --> 00:14:35,460
This is not ours. This is the Kniell.

282
00:14:35,460 --> 00:14:37,900
So the Kniell is the French regulator.

283
00:14:37,900 --> 00:14:45,220
So if your software is deployed and being used by consumers in France, the French regulator is looking at it to make sure that you're doing these types of things.

284
00:14:45,220 --> 00:14:50,020
This thing I'm referring to is a protection impact assessment or a privacy review.

285
00:14:50,020 --> 00:14:57,260
If you're not familiar with these particular regulations, privacy review basically seeks to create like a giant Rube Goldberg machine.

286
00:14:57,260 --> 00:15:00,540
Tell us what type of data you're handling. What are you doing with that data?

287
00:15:00,540 --> 00:15:04,500
For what purpose? With who? Like by who the user's data?

288
00:15:04,500 --> 00:15:06,100
What kind of risk does it create?

289
00:15:06,100 --> 00:15:09,340
Is there a risk of compromising the user's rights or a security risk?

290
00:15:09,340 --> 00:15:12,580
Evaluate that risk, mitigate it, and generate an audit trail.

291
00:15:12,580 --> 00:15:16,900
Like show us that you sort of thoughtfully considered the risks to your users.

292
00:15:16,900 --> 00:15:19,180
Sort of like what you would do in threat modeling for security.

293
00:15:19,180 --> 00:15:21,860
Like the same thing but for privacy. It's pretty complex.

294
00:15:21,860 --> 00:15:30,220
It usually involves legal specialists because they understand the privacy stuff and engineers because they understand the engineering stuff.

295
00:15:30,220 --> 00:15:33,260
So what we wanted to do was make this easy.

296
00:15:33,300 --> 00:15:39,700
We wanted to provide tools that allow us to describe or declare a policy like a thing we want to support in a business or a team.

297
00:15:39,700 --> 00:15:43,020
And then to enforce that so we don't have to check it constantly manually.

298
00:15:43,020 --> 00:15:48,100
So in this example, what we're going to do is we're going to declare the type of data we're processing and storing.

299
00:15:48,100 --> 00:15:50,620
We're going to talk about the type of system that we're doing that in.

300
00:15:50,620 --> 00:15:54,460
And we're going to check it against a fictional policy and enforce it.

301
00:15:54,460 --> 00:15:57,340
And then we're going to maintain an audit trail so we'll be able to see reports about that.

302
00:15:57,340 --> 00:16:01,300
So I'm just going to jump into this document for a second and I'm now going to attempt to do this live.

303
00:16:01,300 --> 00:16:03,820
And of course, it'll probably break, but we'll have a go.

304
00:16:03,820 --> 00:16:07,740
So the first thing we're going to do is just to show you an example of the scan command running.

305
00:16:07,740 --> 00:16:11,900
Basically, this is our demo environment. You can spin these tools up yourself very quickly.

306
00:16:11,900 --> 00:16:16,340
I've declared some environment variables just to hide URLs that were pointed out.

307
00:16:16,340 --> 00:16:20,540
And basically what I'm going to do here is I'm going to first of all, let's see.

308
00:16:25,180 --> 00:16:27,820
Run the scanner first.

309
00:16:27,860 --> 00:16:32,660
So the first thing we're going to do here is we're just going to scan AWS infrastructure that I've credentialed this to.

310
00:16:32,660 --> 00:16:38,180
So it's basically using the FIDA's control server to connect to an AWS environment and scan it for resources.

311
00:16:38,180 --> 00:16:40,540
By this we mean sort of anything that may contain data.

312
00:16:40,540 --> 00:16:43,660
And I put that to a YAML file.

313
00:16:43,660 --> 00:16:47,900
Ignore that. That's actually not an error. That's a known issue.

314
00:16:47,900 --> 00:16:51,020
Let me see if I can make this a little bit clearer.

315
00:16:51,020 --> 00:16:55,940
Okay, it's a bit information dense, but you'll see what it's returned here is a list of systems with keys, essentially.

316
00:16:55,940 --> 00:17:01,780
If I expand one of these out, you can see here it describes it with a little bit of metadata, what kind of resource it is,

317
00:17:01,780 --> 00:17:06,620
and then some of the, you know, arms and addresses for this particular Redshift cluster in AWS.

318
00:17:06,620 --> 00:17:10,340
So it's generated this metadata view, essentially metadata map at the very abstract level.

319
00:17:10,340 --> 00:17:13,060
Each of the systems in our distributed infrastructure.

320
00:17:13,060 --> 00:17:17,780
So that's the first step. We've just identified systems that need to be understood.

321
00:17:17,780 --> 00:17:23,020
The next thing we want to do is actually connect to one of those and generate a data set that we can actually label.

322
00:17:23,020 --> 00:17:27,500
So we want to say what kind of information we're handling in these systems.

323
00:17:27,500 --> 00:17:28,500
So, let me.

324
00:17:31,580 --> 00:17:33,140
So we'll just do generate.

325
00:17:33,140 --> 00:17:38,820
So here we've got you'll see at the top half, we've just got a YAML file that's empty that I'm just using as a destination for this, essentially.

326
00:17:38,820 --> 00:17:42,860
And here you'll see I'm basically using the generate data set command against a database.

327
00:17:42,860 --> 00:17:46,620
There's a database URL declared. I'm just writing out to this YAML file.

328
00:17:46,620 --> 00:17:51,860
This will take a few seconds to run. It's actually going to query the DDL and return a data model for it.

329
00:17:54,660 --> 00:17:55,660
Hopefully.

330
00:17:58,060 --> 00:17:59,340
Uncomfortable silence, I know.

331
00:18:05,700 --> 00:18:06,700
There we go. OK.

332
00:18:08,340 --> 00:18:12,700
So you'll see here there are four simple databases here.

333
00:18:12,780 --> 00:18:19,780
If I look at one of these, I've got a set of metadata that describes the database itself and then each of the tables that exist here.

334
00:18:19,780 --> 00:18:22,780
And if I expand those out, I can see there's a description about what it is.

335
00:18:22,780 --> 00:18:25,820
There's a data qualifier and this empty data categories.

336
00:18:25,820 --> 00:18:28,540
So I don't know what type of data is in here today, right? That's not declared.

337
00:18:28,540 --> 00:18:31,060
It's not modeled. I need to label this manually.

338
00:18:31,060 --> 00:18:36,660
So all we've done so far, just to be clear, is scanned for data systems, generated data sets.

339
00:18:36,660 --> 00:18:39,500
And the next thing we're going to do is talk about how we label these.

340
00:18:39,540 --> 00:18:44,540
Now, to make this a little bit more real, I'm running from that demo project that I mentioned, the Fides demo repo.

341
00:18:44,540 --> 00:18:46,940
I'm running this fictional e-commerce system that I'll show you very quickly.

342
00:18:48,540 --> 00:18:52,860
It's a thing of beauty. It has three products, a Postgres database, no images.

343
00:18:52,860 --> 00:18:56,420
But it's a very simple application. It's a Flask application.

344
00:18:56,420 --> 00:18:58,900
It allows us to test the sort of concepts of Fides.

345
00:18:58,900 --> 00:19:01,300
So in this case, we've got a couple of products, we've got some users.

346
00:19:01,300 --> 00:19:03,580
So it means that we have a very simple data model, right?

347
00:19:03,580 --> 00:19:09,140
If I go to the Postgres database for this system, you can see here it's got a products table, a purchases table,

348
00:19:09,140 --> 00:19:12,900
and a users table. And if I expand out the users table for a moment,

349
00:19:13,620 --> 00:19:17,060
you can see it essentially has six columns or six fields of information.

350
00:19:17,060 --> 00:19:19,980
A create.time stamp, email, first name, ID, last name, and password.

351
00:19:20,780 --> 00:19:23,660
Now we look at how these are annotated or declared in Fides.

352
00:19:24,140 --> 00:19:30,060
So if I look at the create.field, you can see it carries a data categories label, a systems operations data.

353
00:19:30,060 --> 00:19:34,260
It's the type of information that's in here. It's data related to system operations.

354
00:19:34,260 --> 00:19:38,700
If I compare this to email address, I've got user provided.

355
00:19:38,740 --> 00:19:41,380
So user provided identifiable contact email.

356
00:19:41,380 --> 00:19:47,260
Now as a developer, if I don't know the structure of this, I can simply declare it maybe as user data.

357
00:19:47,260 --> 00:19:49,020
I might just say it's user information.

358
00:19:49,020 --> 00:19:51,660
I could say it's user provided. I know that it's actually been provided by the user.

359
00:19:51,660 --> 00:19:53,940
It could have been derived about the user through observation.

360
00:19:55,020 --> 00:19:56,860
Is it then identifiable?

361
00:19:58,100 --> 00:19:59,980
Is it contact information? I could just leave it there.

362
00:19:59,980 --> 00:20:01,500
Or I could specify that it's email.

363
00:20:01,500 --> 00:20:03,420
Now these are coming from the Fides taxonomy, right?

364
00:20:03,420 --> 00:20:05,660
So these are specifically from this taxonomy.

365
00:20:05,860 --> 00:20:06,380
Okay.

366
00:20:07,260 --> 00:20:10,060
So I'm declaring the type of information I'm handling in my data set.

367
00:20:10,060 --> 00:20:11,860
Now the other side of this is the system.

368
00:20:11,860 --> 00:20:14,260
So the actual application is processing information.

369
00:20:14,260 --> 00:20:15,500
So let's take a quick look at that.

370
00:20:16,060 --> 00:20:19,060
So we've got our data set declaration, got our system declaration.

371
00:20:19,060 --> 00:20:20,860
And we can see here that it describes a little bit about it.

372
00:20:20,860 --> 00:20:22,820
It's a sample e-commerce application.

373
00:20:22,820 --> 00:20:25,980
It's used for the purpose of simulating an e-commerce system.

374
00:20:25,980 --> 00:20:31,700
Now if we expand our privacy declaration, we'll see that it says we provide e-commerce capabilities.

375
00:20:31,700 --> 00:20:33,940
And it declares the types of data we're handling.

376
00:20:33,980 --> 00:20:37,020
So in this case, we're handling user provided identifiable data,

377
00:20:37,020 --> 00:20:40,700
user derived identifiable data, and system operations data.

378
00:20:40,700 --> 00:20:43,620
And it's for the purpose that data uses e-commerce, right?

379
00:20:43,620 --> 00:20:46,340
We're not advertising, we're not selling the data,

380
00:20:46,340 --> 00:20:48,660
we're not doing any behavioral analysis, any machine learning classification.

381
00:20:48,660 --> 00:20:50,500
We're just providing e-commerce capability.

382
00:20:50,500 --> 00:20:54,180
And we've got a custom subject type or user type called a Flask customer.

383
00:20:54,180 --> 00:20:54,940
That's all it is.

384
00:20:54,940 --> 00:20:55,820
Okay.

385
00:20:55,820 --> 00:20:57,980
So this declares the purpose of the system.

386
00:20:57,980 --> 00:21:00,180
The other declares the type of data we're handling.

387
00:21:00,180 --> 00:21:01,780
Now let's run an evaluation.

388
00:21:01,860 --> 00:21:03,420
So I'm not running a CI pipeline here, so

389
00:21:03,420 --> 00:21:06,620
I'm basically forcing an evaluation with the Fidesk control server on my CLI.

390
00:21:06,620 --> 00:21:09,020
So we'll just run that very quickly.

391
00:21:09,020 --> 00:21:10,460
Fidesk control evaluate.

392
00:21:10,460 --> 00:21:13,260
Now if you look carefully here, this is maybe a little hard to see.

393
00:21:13,260 --> 00:21:16,260
And you'll see that what it's done is it's grabbed the system resources files,

394
00:21:16,260 --> 00:21:18,460
all of the data uses and declarations.

395
00:21:18,460 --> 00:21:20,980
And it basically checks it against a policy.

396
00:21:20,980 --> 00:21:22,060
And it says that it's passed.

397
00:21:22,060 --> 00:21:22,940
We get all greens.

398
00:21:22,940 --> 00:21:24,300
Everything's good to go, okay?

399
00:21:24,300 --> 00:21:26,180
So code's checked in.

400
00:21:26,180 --> 00:21:30,740
Now let's imagine for a moment that I, working with as a developer,

401
00:21:30,740 --> 00:21:34,060
on this e-commerce application, I actually decide actually what we're handling here

402
00:21:34,060 --> 00:21:38,380
isn't, is not gonna be an email address.

403
00:21:38,380 --> 00:21:43,260
It's actually users provided identifiable health and medical data, okay?

404
00:21:43,260 --> 00:21:45,900
So this email address is actually some kind of health and medical data.

405
00:21:45,900 --> 00:21:47,140
And now I attempt to check it in again.

406
00:21:50,140 --> 00:21:51,340
So it's checking against our policy.

407
00:21:51,340 --> 00:21:52,820
Big red failure message.

408
00:21:52,820 --> 00:21:53,900
If we look carefully here,

409
00:21:53,900 --> 00:21:57,940
the failure is caused by a rule about rejecting the use of sensitive data.

410
00:21:57,940 --> 00:22:00,660
So it's basically said, hey, we have a policy in this business that says,

411
00:22:00,660 --> 00:22:03,460
we reject the use of sensitive data for any of these purposes.

412
00:22:03,460 --> 00:22:04,460
And you're attempting to do that.

413
00:22:04,460 --> 00:22:05,380
That's a big risk for us.

414
00:22:05,380 --> 00:22:06,860
We can't do that.

415
00:22:06,860 --> 00:22:09,820
So how has that actually been generated?

416
00:22:09,820 --> 00:22:11,660
Well, let's look at the policy file.

417
00:22:11,660 --> 00:22:13,900
So this policy file written in feeders is very, very primitive.

418
00:22:13,900 --> 00:22:15,420
There's two rules.

419
00:22:15,420 --> 00:22:18,020
One is minimizing the use of identifiable data.

420
00:22:18,020 --> 00:22:20,300
And the other is rejecting the use of sensitive data.

421
00:22:20,300 --> 00:22:22,220
So let's look at the rejection one first.

422
00:22:22,220 --> 00:22:24,340
We can see here that it's a simple rejection policy, right?

423
00:22:24,340 --> 00:22:27,740
If any of these categories of data, any of these categories, so

424
00:22:27,780 --> 00:22:31,340
any of these types, biometric, children's, genetic, political, race, religion,

425
00:22:31,340 --> 00:22:34,900
is being used for any of these purposes, right?

426
00:22:34,900 --> 00:22:38,020
For personalizing our system, for advertising, for third party sharing,

427
00:22:38,020 --> 00:22:39,020
for training an AI system.

428
00:22:39,020 --> 00:22:39,620
We don't permit that.

429
00:22:39,620 --> 00:22:40,860
We reject that.

430
00:22:40,860 --> 00:22:43,460
That code cannot be committed in this organization.

431
00:22:43,460 --> 00:22:49,340
Now that sounds very harsh, but the effort here is to prevent us from failing,

432
00:22:49,340 --> 00:22:50,820
and then finding a risk in production.

433
00:22:50,820 --> 00:22:53,220
In addition to which, it prevents the Rube Goldberg machine I mentioned,

434
00:22:53,220 --> 00:22:55,580
errors of paperwork with legal teams to review risk.

435
00:22:55,580 --> 00:22:56,580
So we check it in.

436
00:22:56,580 --> 00:22:58,100
We find that there's a failure.

437
00:22:58,100 --> 00:22:59,420
We update something in our code base.

438
00:22:59,420 --> 00:23:01,420
We re-declare our YAML file, and we're good to go.

439
00:23:01,420 --> 00:23:02,420
It gets checked in.

440
00:23:02,420 --> 00:23:05,940
And each of these policies is evaluated into an audit trail, right?

441
00:23:05,940 --> 00:23:07,620
So if I look here, we can see the audit trail.

442
00:23:07,620 --> 00:23:10,540
I've been running some fails and passes throughout the day just to test it.

443
00:23:10,540 --> 00:23:13,300
The most recent one here was this violation that relates to the use of

444
00:23:13,300 --> 00:23:15,380
health and medical data in our e-commerce system.

445
00:23:15,380 --> 00:23:17,100
So we generate audit trail for these things.

446
00:23:17,100 --> 00:23:20,980
So let's go back to our document very quickly.

447
00:23:20,980 --> 00:23:24,620
So what we've done there is essentially declared the type of data we're handling,

448
00:23:24,660 --> 00:23:27,820
what our system is doing, and evaluate the risk related to policies we have in

449
00:23:27,820 --> 00:23:29,780
the business, and generate a report on it.

450
00:23:29,780 --> 00:23:31,060
Okay? Very, very quick.

451
00:23:31,060 --> 00:23:31,780
I'm running out of time, so

452
00:23:31,780 --> 00:23:34,500
I'm gonna go really, really fast through the last one, I promise.

453
00:23:34,500 --> 00:23:37,180
So the last use case is programmatic data rights.

454
00:23:37,180 --> 00:23:40,940
So sometimes called subject rights, DSRs, data subject rights requests, etc.

455
00:23:40,940 --> 00:23:43,020
This is the idea that we can say, hey Google,

456
00:23:43,020 --> 00:23:44,540
delete me from your distributed systems.

457
00:23:44,540 --> 00:23:47,980
And in Europe, they've got 30 days to completely nuke you from those systems,

458
00:23:47,980 --> 00:23:48,980
right? No soft deletes.

459
00:23:48,980 --> 00:23:50,540
You gotta be gone.

460
00:23:50,540 --> 00:23:52,620
We can all imagine the engineering difficulty in achieving this.

461
00:23:52,620 --> 00:23:55,100
The first issue is you don't know where the data is.

462
00:23:55,100 --> 00:23:58,340
So this is what data subject access requests look like in practice.

463
00:23:58,340 --> 00:24:01,500
They can be submitted by paper, email, a phone call.

464
00:24:01,500 --> 00:24:04,300
This is a real one that's been anonymized a little bit.

465
00:24:04,300 --> 00:24:06,740
So this is how they arrive in a company.

466
00:24:06,740 --> 00:24:09,540
And then usually what happens is they go into some kind of ticketing system.

467
00:24:09,540 --> 00:24:11,380
Someone in the legal team maybe reviews the risk, or

468
00:24:11,380 --> 00:24:12,980
customer support teams have been trained.

469
00:24:12,980 --> 00:24:14,660
Then they open up a ticket in JIRA.

470
00:24:14,660 --> 00:24:16,940
And then that ticket is distributed to a bunch of engineers to try and

471
00:24:16,940 --> 00:24:19,060
go spelunking to find a user's data.

472
00:24:19,060 --> 00:24:22,260
The average time here in the US to complete a data subject request

473
00:24:22,300 --> 00:24:25,220
is 19 days and it involves 10 people.

474
00:24:25,220 --> 00:24:27,980
So it's like a huge amount of effort, right?

475
00:24:27,980 --> 00:24:31,860
So what we've attempted to do with that metadata layer that you've just seen

476
00:24:31,860 --> 00:24:35,940
being generated by the control server is leverage that to build a map of

477
00:24:35,940 --> 00:24:39,780
where categories of information exist in the organization and then process them.

478
00:24:39,780 --> 00:24:41,900
So we can create a policy for enforcement.

479
00:24:41,900 --> 00:24:43,060
Then we can retrieve or

480
00:24:43,060 --> 00:24:46,060
erase data from those systems without having to keep updating it.

481
00:24:46,060 --> 00:24:50,260
So we have consistency between our CI pipeline and our production data model.

482
00:24:50,300 --> 00:24:53,100
So in this example what we're gonna do is we're gonna retrieve some fictional data

483
00:24:53,100 --> 00:24:55,700
about our fictional user in this e-commerce system.

484
00:24:55,700 --> 00:24:57,300
We're gonna look at what that looks like.

485
00:24:57,300 --> 00:24:59,780
And then we're gonna run it again when we've made some changes to the data model.

486
00:25:01,060 --> 00:25:04,180
So back to VS Code.

487
00:25:04,180 --> 00:25:06,580
Actually not back to VS Code, let's go to our browser for a minute.

488
00:25:06,580 --> 00:25:10,180
So if you recall, we had our fictional e-commerce system.

489
00:25:10,180 --> 00:25:14,500
We've got a test user in here called example user, I think it is.

490
00:25:14,500 --> 00:25:15,220
Let me double check.

491
00:25:18,140 --> 00:25:19,580
Yeah, example user, okay.

492
00:25:19,580 --> 00:25:21,180
So we've already got a test user in this system.

493
00:25:21,180 --> 00:25:23,220
So what I'm gonna do is go to the privacy center.

494
00:25:23,220 --> 00:25:26,060
These are all part of the open source tools.

495
00:25:26,060 --> 00:25:29,060
So there's a web application that sits on top of the feed as ops tools.

496
00:25:29,060 --> 00:25:30,700
That you can style whatever you want.

497
00:25:30,700 --> 00:25:33,820
And allows you to receive and process privacy requests.

498
00:25:33,820 --> 00:25:36,420
So in this one it's simply configured to take an email address.

499
00:25:38,300 --> 00:25:39,860
So I'm gonna submit my email address.

500
00:25:39,860 --> 00:25:41,220
There's no validations, it's very simple.

501
00:25:41,220 --> 00:25:42,980
You can modify it whatever way you want.

502
00:25:42,980 --> 00:25:45,500
And that's submitted to the feed as ops server.

503
00:25:45,500 --> 00:25:46,940
And there's a UI that sits on top of that for

504
00:25:46,940 --> 00:25:49,060
managing those requests so we don't have to do this manually.

505
00:25:49,060 --> 00:25:51,540
And it's all API driven, you could ignore the UIs if you wanted to.

506
00:25:51,540 --> 00:25:53,700
We built these to make it easier for non-technical users.

507
00:25:54,940 --> 00:25:57,140
So in this case we'll see that hopefully when it reloads,

508
00:25:57,140 --> 00:26:00,380
we've got a new privacy request has been submitted.

509
00:26:00,380 --> 00:26:03,060
When we reveal the PII we can see it's for our example user.

510
00:26:03,060 --> 00:26:05,380
And we're gonna approve this request.

511
00:26:06,620 --> 00:26:07,620
What's gonna happen is it's gonna go and

512
00:26:07,620 --> 00:26:10,980
retrieve from any of the connected systems that user's data based on the data

513
00:26:10,980 --> 00:26:12,060
categories in that policy.

514
00:26:12,060 --> 00:26:14,860
It's gonna write it to a JSON file and dump it in this directory.

515
00:26:14,860 --> 00:26:15,780
There it is.

516
00:26:15,780 --> 00:26:17,260
So what I'm gonna do is just open it very quickly.

517
00:26:17,260 --> 00:26:18,860
We'll just try and prettyify it a bit.

518
00:26:18,860 --> 00:26:19,980
Okay, there we go.

519
00:26:19,980 --> 00:26:23,820
So if you look carefully at what it's returned, just a blob of JSON,

520
00:26:23,820 --> 00:26:26,300
there's the purchase address used by the user.

521
00:26:26,300 --> 00:26:29,900
There's actually a MailChimp we found not only is it the user's email address,

522
00:26:29,900 --> 00:26:32,060
it's also storing the user's physical address in MailChimp.

523
00:26:32,060 --> 00:26:33,780
I'm not sure why, but it does.

524
00:26:33,780 --> 00:26:36,740
And some basic information about the user in that way.

525
00:26:36,740 --> 00:26:41,500
Now, from here, what would normally happen is we continue to Agile,

526
00:26:41,500 --> 00:26:43,380
through Agile development in our software development life cycle,

527
00:26:43,380 --> 00:26:45,540
we make changes, the data model adjusts, and

528
00:26:45,540 --> 00:26:47,100
then we've got to rewrite all of the scripts and

529
00:26:47,100 --> 00:26:49,340
tasks that are run to retrieve user data.

530
00:26:49,340 --> 00:26:52,340
Now, this is where Feda's control and Feda's ops play together.

531
00:26:52,340 --> 00:26:56,340
So for example, if I go back to that Postgres data set I had, and

532
00:26:56,340 --> 00:27:02,020
let's make this, well, we'll return this to contact email.

533
00:27:02,020 --> 00:27:05,100
And let's say that system's operations data, we'll say, well,

534
00:27:05,100 --> 00:27:06,900
actually this isn't system's operations data.

535
00:27:06,900 --> 00:27:08,380
We're gonna say it's user data.

536
00:27:08,380 --> 00:27:10,820
Now, by doing that, we're declaring this personal information.

537
00:27:10,820 --> 00:27:15,100
So if a privacy request is run, now we would return that timestamp, okay?

538
00:27:15,140 --> 00:27:17,620
So I've got a watch engine running in the background here that's

539
00:27:17,620 --> 00:27:21,300
monitoring these, it's a cheat so I can do this more quickly for you.

540
00:27:21,300 --> 00:27:23,580
And so I'm gonna resubmit that privacy request for this user.

541
00:27:34,220 --> 00:27:35,740
We should have a new one come up now.

542
00:27:35,740 --> 00:27:36,580
There we go, there's a new one.

543
00:27:36,580 --> 00:27:37,580
I'm just gonna approve this.

544
00:27:39,460 --> 00:27:41,860
We'll see another JSON object get generated.

545
00:27:41,860 --> 00:27:43,660
If I open this, we just compare it to our first one.

546
00:27:45,460 --> 00:27:50,460
What we'll see is now it's returning some additional data about the user.

547
00:27:50,460 --> 00:27:54,740
That is identified as the timestamp information because that was identified

548
00:27:54,740 --> 00:27:57,420
as user information as metadata in the data model.

549
00:27:57,420 --> 00:28:01,300
So effectively, changes in our CI pipeline are now reflected directly in

550
00:28:01,300 --> 00:28:03,500
the metadata layer that's enforceable for our policies.

551
00:28:03,500 --> 00:28:06,380
So engineers don't have to rewrite anything that's spelunking in Postgres or

552
00:28:06,380 --> 00:28:08,580
Redshift, etc., to retrieve data.

553
00:28:08,580 --> 00:28:11,140
So what we've done there is construct a very simple policy,

554
00:28:11,140 --> 00:28:13,540
retrieve the user data, change the data model, and

555
00:28:13,580 --> 00:28:16,380
then reflect that in that production system.

556
00:28:16,380 --> 00:28:20,140
Some of the other things that we're working on, I'll jump forward,

557
00:28:20,140 --> 00:28:22,300
are basically about simplifying privacy.

558
00:28:22,300 --> 00:28:24,460
So we'll be releasing our machine learning classifier.

559
00:28:24,460 --> 00:28:26,540
I don't have time, but I was gonna give you guys a sneak preview.

560
00:28:26,540 --> 00:28:28,340
We've been working on it for a year, and

561
00:28:28,340 --> 00:28:31,580
it automatically labels data sets with the feed as taxonomy.

562
00:28:31,580 --> 00:28:34,860
We're adding a UI for better visualization for non-technical users.

563
00:28:34,860 --> 00:28:37,540
And then a bunch of features that we're building over the next three years to help

564
00:28:37,540 --> 00:28:41,660
alleviate issues like access controls in production systems.

565
00:28:41,660 --> 00:28:43,300
And so that's it, right?

566
00:28:43,300 --> 00:28:44,140
What is privacy as code?

567
00:28:44,140 --> 00:28:44,620
We've covered that.

568
00:28:44,620 --> 00:28:47,340
It's an easy way to manage privacy in the development lifecycle.

569
00:28:47,340 --> 00:28:49,140
You can do this with feed as, which is free.

570
00:28:49,140 --> 00:28:52,380
I've shown you a couple of examples how you can check a policy in CI and

571
00:28:52,380 --> 00:28:54,700
automated data rights request.

572
00:28:54,700 --> 00:28:56,660
It's free and open source, all Python based.

573
00:28:56,660 --> 00:28:58,140
You can get up and running in 15 minutes.

574
00:28:58,140 --> 00:28:59,740
I'd urge you to go and grab the project.

575
00:28:59,740 --> 00:29:02,500
Or you can join the Slack community and learn more about it.

576
00:29:02,500 --> 00:29:03,740
Thank you very much for your time.

577
00:29:03,740 --> 00:29:04,820
I really, really appreciate it.

578
00:29:04,820 --> 00:29:09,300
pureteam.com

