1
00:00:00,000 --> 00:00:12,040
Hello, everyone. Welcome back. Let's welcome Jess to deliver a talk on productionized research-oriented

2
00:00:12,040 --> 00:00:16,480
code by Python. Over to you, Jess.

3
00:00:16,480 --> 00:00:28,200
Hello, everyone. I'm Jesse. I have been in education and technology industry for several

4
00:00:28,200 --> 00:00:35,320
years. In this industry, I have tackled the issues between data science and engineering

5
00:00:35,320 --> 00:00:43,520
in AI ML projects. So based on my experiences, I'm going to talk about the process to productionize

6
00:00:43,520 --> 00:00:48,440
research-oriented code by Python.

7
00:00:48,440 --> 00:00:56,840
Let me share background first. Python engineers assigned to AI ML project and the few countries

8
00:00:56,840 --> 00:01:09,600
that are involved in the process can help make AI ML projects work more smoothly. The

9
00:01:09,600 --> 00:01:17,120
target audience might be Python engineers who are involved with R&D, data scientists,

10
00:01:17,120 --> 00:01:22,680
product owners in AI ML projects, or data engineers.

11
00:01:22,680 --> 00:01:30,400
So three years ago, I made a five-minute talk about this topic in Python US 2019. This was

12
00:01:30,400 --> 00:01:36,160
my first talk on the stage in my life. So during the three years, I made the talks about

13
00:01:36,160 --> 00:01:42,760
Python data science topics around the world and published a book about them. Then I came

14
00:01:42,760 --> 00:01:46,400
back to Python US today.

15
00:01:46,400 --> 00:01:52,680
So this talk is aggregated based on my three years' experiences of the projects with data

16
00:01:52,680 --> 00:02:02,080
scientists, what I wrote in the book, and the past talks in Python.

17
00:02:02,080 --> 00:02:09,880
So I abstracted the four steps to transform research-oriented code into products. Understand

18
00:02:09,880 --> 00:02:16,680
the code, modularize the code, refactor them, and then make them product. That's it. It's

19
00:02:16,680 --> 00:02:23,960
very simple. I will explain each step one by one, much deeper.

20
00:02:23,960 --> 00:02:30,320
So the code samples in the four steps are written based on item response theory, also known

21
00:02:30,320 --> 00:02:36,240
as the two-parameter logistic regression, which is a classic statistical model mainly

22
00:02:36,600 --> 00:02:42,600
used in educational psychology. But this model is normally used for predicting how many students

23
00:02:42,600 --> 00:02:50,320
can get correct answers based on the features of questions. So this talk, I use the code

24
00:02:50,320 --> 00:02:57,160
based on this statistical model, but the model is not the main topic in this talk, so I will

25
00:02:57,160 --> 00:03:02,480
not talk about the model itself.

26
00:03:02,480 --> 00:03:08,160
So let's look at the first step. The first step is to understand the research-oriented

27
00:03:08,160 --> 00:03:17,440
code. So in general, a research-oriented code is implemented for figuring out something

28
00:03:17,440 --> 00:03:25,360
new and summarizing the results into paper. So research-oriented code, AI, project, is

29
00:03:25,360 --> 00:03:32,320
the code written mainly by data scientists or researchers for figuring out new knowledge.

30
00:03:32,320 --> 00:03:40,880
So finding something new is the highest priority when writing the code.

31
00:03:40,880 --> 00:03:46,000
This is a common life cycle to write paper with code. Only one person starts to collect

32
00:03:46,000 --> 00:03:52,040
data, pre-process it, train or calculate the data, and then see what the results are telling

33
00:03:52,040 --> 00:03:59,040
you. If the results present new knowledge, write paper with it and publish.

34
00:03:59,040 --> 00:04:07,120
So this code is pre-processing code written during this process. This code seems to be

35
00:04:07,120 --> 00:04:13,320
a long function and used for loop, but it tends to be visually traceable from the top

36
00:04:13,320 --> 00:04:19,880
to the bottom, also it can be easily and quickly written. It's not critical, but it's enough

37
00:04:19,880 --> 00:04:28,360
to quickly get results. So this is an example of calculation code. This code uses .pand

38
00:04:28,360 --> 00:04:34,800
and uses data frame. It easily handles the input data and it trace output data with data

39
00:04:34,800 --> 00:04:39,000
frame.

40
00:04:39,000 --> 00:04:49,000
So on the other hand, what is the code in production level?

41
00:04:49,000 --> 00:04:54,800
So according to this thesis about how we can evaluate ML production system based on the

42
00:04:54,800 --> 00:05:02,740
rubric, the thesis scores ML system from 0 to 12 plus. So 0 points seems to be more like

43
00:05:03,740 --> 00:05:10,900
before transformation. The ML system scoring from 7 to 12 plus is the product a few years

44
00:05:10,900 --> 00:05:17,460
after deployed on the production environment. So the target goal of the four steps I suggest

45
00:05:17,460 --> 00:05:25,860
in this talk is the product scoring from 5 to 6. That means reasonably tested, but it's

46
00:05:25,860 --> 00:05:32,860
possible that most of those tests and procedures may be automated.

47
00:05:34,980 --> 00:05:42,780
So the product quality improved through these kinds of development cycle. I have had experiences

48
00:05:42,780 --> 00:05:50,700
to be only one Python engineer in a company for a few years, but generally engineers develop

49
00:05:50,860 --> 00:05:57,860
applications with a team. So they make architecture, implement features, test and review, and release

50
00:05:58,540 --> 00:06:05,300
product. So they receive feedback all the time unless they develop very perfect product

51
00:06:05,300 --> 00:06:12,300
with no bugs. So they need to keep repeating, release and fix the product. So this means

52
00:06:13,020 --> 00:06:20,020
that production code needs high maintainability, scalability, and processing speed.

53
00:06:20,700 --> 00:06:27,700
This is an example of both previous code I refactored in a PySonic way. This code seems

54
00:06:31,180 --> 00:06:35,940
to be shorter than previous code, and there are less complications and set complications.

55
00:06:35,940 --> 00:06:42,060
There are two simple functions at the bottom. This code can be the model in a faster and

56
00:06:42,060 --> 00:06:49,060
simple way. So I identified three differences between

57
00:06:51,100 --> 00:06:56,700
research-oriented code and production code. There are different scripts, different characteristics

58
00:06:56,700 --> 00:07:03,700
for coding style, and different objectives coding style. So researchers focus more on

59
00:07:03,940 --> 00:07:10,940
writing pre-processing code and ML code, calculation code. On the other hand, engineers have responsibility

60
00:07:11,340 --> 00:07:15,500
to write whole part of the code in production level.

61
00:07:15,500 --> 00:07:21,380
The research-oriented code seems to be easily handled and visually traceable. On the other

62
00:07:21,380 --> 00:07:28,300
hand, production code needs to be concerned about high calculation speed, high readability,

63
00:07:28,300 --> 00:07:34,140
and should be testable and modular. This is because researchers focus more on finding

64
00:07:34,140 --> 00:07:40,500
the most efficient and suitable machine learning model or calculation code. On the other hand,

65
00:07:40,500 --> 00:07:47,500
engineers have responsibility to make the code work on the server correctly and reliably.

66
00:07:48,740 --> 00:07:53,860
So what are Python engineers supposed to do for research-oriented code at first? So what

67
00:07:53,860 --> 00:08:00,860
is the first thing to do? So deep-typing the code. So read the code before write it.

68
00:08:01,500 --> 00:08:08,500
The left picture is a Jupyter notebook with just long lines. I used to frequently receive

69
00:08:11,780 --> 00:08:18,780
the code. On the Jupyter notebook from the... Okay, I will continue. So, I will read the

70
00:08:31,540 --> 00:08:38,540
code. After you receive the research-oriented code, take notes for deep-typing the code

71
00:08:39,340 --> 00:08:46,340
as if you were scribbling. So there are three strategies to take notes. Prepare for modularization.

72
00:08:46,340 --> 00:08:53,340
Just quickly take notes while reading the research-oriented code. So first one is to

73
00:09:01,100 --> 00:09:08,100
write comments by using SHARP. Second is add to comments. These two things everyone already

74
00:09:08,100 --> 00:09:15,100
does. So third one, third is mobile documentation. So, the third one is to write comments. So,

75
00:09:38,100 --> 00:09:45,100
mobile documentation is very useful to efficiently understand the code with the team. So, if

76
00:09:50,020 --> 00:09:57,020
you're using a VS code, install a Live Share extension of VS code which enables to share

77
00:09:57,820 --> 00:10:04,820
code and edit together. So after mobile documentation, the team members who joined this mob can be

78
00:10:04,820 --> 00:10:11,820
on the same page and do not need to extra wiki and documents about this code. So, also

79
00:10:13,340 --> 00:10:20,340
if you do with junior when you hire, it can be good for onboarding. So, this is an example

80
00:10:20,340 --> 00:10:27,340
of part of the research-oriented code with comments. So, while reading the code, add

81
00:10:41,260 --> 00:10:47,420
what you understood from the code with comments and to do memo. So, this should be done for

82
00:10:47,420 --> 00:10:54,420
understanding more about the code, not only for remembering the code and to do. So, the

83
00:10:55,620 --> 00:11:01,220
conscious and attitude about understanding more about the code and getting more familiar

84
00:11:01,220 --> 00:11:08,220
with the code is going to make different in the way to take notes on the code and easily

85
00:11:08,580 --> 00:11:15,580
lead to the next step. So, next thing to do is to modularize the code by using SHARP.

86
00:11:17,420 --> 00:11:24,420
Using the levels. A research-oriented code has three types of scopes such as the code

87
00:11:28,220 --> 00:11:34,780
to prepare, pre-post-process, and calculate data. So, based on the comments you wrote

88
00:11:34,780 --> 00:11:41,780
in previous step, categorize the code into preparation code, pre-post-processing code,

89
00:11:42,260 --> 00:11:49,260
and calculation code. So, after grouping each code with levels, we can break them into functions

90
00:11:54,140 --> 00:12:01,140
and make them testable. So, while grouping each code, you would find duplicated codes

91
00:12:01,620 --> 00:12:08,620
and use lines or libraries or small bugs. You can fix them in this step. So, then we

92
00:12:08,620 --> 00:12:15,620
can get modularization outcome like the table. So, we could get modules for each from one

93
00:12:17,900 --> 00:12:23,900
page of research-oriented code. We could get preparation.py which has functions to access

94
00:12:23,900 --> 00:12:30,900
database, execute query, and load input data. We also could get pre-processing.py which

95
00:12:31,900 --> 00:12:38,900
has functions to replace categorical data with discrete numbers, filter input data,

96
00:12:39,580 --> 00:12:46,580
and rename columns. So, finally, we get prediction.py which has functions to calculate logistic

97
00:12:46,860 --> 00:12:53,860
regression and output results. So, now as a result of that, so the research-oriented

98
00:12:54,260 --> 00:13:01,260
code became loosely coupled. So, you can make directory structure mapping

99
00:13:02,260 --> 00:13:08,700
each module. This is one of the directory structure examples of API developed with Flask.

100
00:13:08,700 --> 00:13:15,700
I found that this kind of directory structure is very simple and easy to work with. The

101
00:13:16,100 --> 00:13:21,220
directory structure on the left box has API directory and module directory and main root

102
00:13:21,300 --> 00:13:28,300
directory. The danda-init.py module located in API directory has a routing list. Each

103
00:13:28,740 --> 00:13:35,740
logic for each endpoint is implemented in each module located in URL directory. So,

104
00:13:36,340 --> 00:13:42,440
if you are working in a small team from one to three members and implement new modules,

105
00:13:42,440 --> 00:13:49,440
you can just add it to a new endpoint in URL directory. If the number of team members increase,

106
00:13:49,440 --> 00:13:54,160
you can split each module into each version and work together staying loosely coupled

107
00:13:54,160 --> 00:14:01,160
between modules. So, I wrote the directory on the slide in the case of API with Flask

108
00:14:01,240 --> 00:14:07,320
and fast API also can have similar directory structure, but generally the way of mapping

109
00:14:07,320 --> 00:14:14,320
relies on the manner of each framework and what you want to develop.

110
00:14:15,060 --> 00:14:22,060
So, now we reach to the step to refactor preparation code and preprocessing code. So, before starting

111
00:14:25,860 --> 00:14:32,860
to refactor the code, we should write test code roughly at the doc thing such as restructure

112
00:14:33,660 --> 00:14:40,660
the ticket style, numpy style, and google style and execute code formatter such as black

113
00:14:41,400 --> 00:14:48,400
or alt-pep 8 and check if code correctly work by using py-test and check the consistency

114
00:14:48,820 --> 00:14:55,820
of coding style by using flake 8. And other than that, type annotation could also be one

115
00:14:58,180 --> 00:15:05,180
of the options as code documentation if all team members are familiar with the grammar

116
00:15:05,180 --> 00:15:12,180
of type hinting. You can narrow down more about requirements of each code through these

117
00:15:13,180 --> 00:15:20,180
three processes, then we can finally start to refactor the code.

118
00:15:22,400 --> 00:15:29,400
Now which part of the code should we refactor? Just focus on preparation code and preprocessing

119
00:15:29,400 --> 00:15:36,400
code. In general, ML library and mathematical library are used in prediction module, so

120
00:15:38,160 --> 00:15:43,980
the code does not have many lines to write and refactor. The optimization of CPU bounding

121
00:15:43,980 --> 00:15:48,680
processing can be done later after we could make sure if the code can work correctly on

122
00:15:48,680 --> 00:15:55,680
the server. Also, we have chances to figure out if the prediction is not valid yet through

123
00:15:55,680 --> 00:16:02,680
refactoring here. So we are trying to refactor I.O. Preparation.py has a group of codes to

124
00:16:07,660 --> 00:16:14,660
access to database or object storage by using query and current library. So let me share

125
00:16:14,880 --> 00:16:21,880
a few examples I frequently have faced and how I simplify them.

126
00:16:22,440 --> 00:16:29,440
In exploratory analysis phase or data wrangling, data scientists are not sure of which data

127
00:16:29,480 --> 00:16:35,840
is specifically useful. If you receive the first query with a star, so narrow down the

128
00:16:35,840 --> 00:16:42,840
data to extract from database and it could be much faster and lower cost like the second

129
00:16:43,240 --> 00:16:50,240
code. In accessing the object storage, sometimes you might make effort to write a code that

130
00:16:51,920 --> 00:16:58,920
write many lines just for loading CSP5 by using current libraries. So one of my suggestions

131
00:16:59,920 --> 00:17:06,800
is that you wrap the code and make it simpler, then it can be reusable and cost effective.

132
00:17:06,800 --> 00:17:13,200
That's because this kind of code is the same code even though anyone write. Everyone in

133
00:17:13,200 --> 00:17:18,200
AI and ML projects such as data engineers, Python engineers and data scientists write

134
00:17:18,200 --> 00:17:25,200
similar code. Not only your team members but also the members in other team might be writing

135
00:17:25,520 --> 00:17:32,520
the same code just for loading the data. So also it might be chances for Python engineers

136
00:17:32,680 --> 00:17:39,680
to make a big impact internally. Next, let's look at the preprocessing.py. The

137
00:17:40,200 --> 00:17:47,200
libraries for analysis use and development use are different. A group of the code to

138
00:17:49,320 --> 00:17:56,320
preprocess or post-process the data tend to include various kinds of libraries and data

139
00:17:57,120 --> 00:18:04,120
type is also mixed. As one of the common cases, there are three ways to preprocess the data

140
00:18:04,320 --> 00:18:11,320
like Python only and SQL query. The common operations are filtering, replace, duplicate,

141
00:18:14,520 --> 00:18:20,840
and there is the data for each three ways. So most of research needs Python SQL query

142
00:18:20,840 --> 00:18:27,840
or both. So these three ways have different features. The code with Python is iterative

143
00:18:28,840 --> 00:18:35,840
to fix and write the code. Python only code can be testable and SQL query has high processing

144
00:18:36,480 --> 00:18:41,200
performance and simple grammar. It depends on the architecture and the project scale

145
00:18:41,200 --> 00:18:48,200
but the first thing to think about is whether the SQL query is necessary or not and refactor

146
00:18:48,880 --> 00:18:55,000
them into Python only code for making them more testable. If the data using for calculation

147
00:18:55,000 --> 00:19:02,000
is already specific and extracted in preparation.py, we don't need to write many preprocessing

148
00:19:03,040 --> 00:19:10,040
codes by Python. So last step is to make them product which is API. So what products can

149
00:19:10,040 --> 00:19:17,040
be generated from research oriented code? So output is not paper, it's a product which

150
00:19:26,040 --> 00:19:33,040
means a code working on the server and users can access to it. But there are several kinds

151
00:19:33,280 --> 00:19:40,280
of product appearances. So quickly let me narrow down the scope about the products in

152
00:19:40,280 --> 00:19:47,280
this talk. So the products can be web application or web API. So this is drawing of the flow

153
00:19:49,440 --> 00:19:55,920
chart of transformation from research oriented code into products. So in this slide, POC

154
00:19:55,920 --> 00:20:02,920
is more like products and analysis scripts are mainly written for business reports. So

155
00:20:03,160 --> 00:20:08,600
when having a look at general overview of data oriented projects, both type of the code

156
00:20:08,600 --> 00:20:15,600
can also exist in this flow but this talk focuses on research oriented code. So the

157
00:20:20,960 --> 00:20:27,160
four transformation steps could be mapped on the flow chart like this slide. There are

158
00:20:27,360 --> 00:20:34,360
four patterns of transformation from research oriented code into products. One is the model

159
00:20:34,440 --> 00:20:39,480
or the calculation code extracted from the research oriented code is directly integrated

160
00:20:39,480 --> 00:20:46,480
into web application. The second is that we can implement API based on the research oriented

161
00:20:47,520 --> 00:20:54,520
code. Third is that when we use ready-made API or third party API, we normally integrate

162
00:20:54,920 --> 00:21:01,480
them into web application for using them. Last one is that when the model or the calculation

163
00:21:01,480 --> 00:21:07,800
code is already integrated into web application, we extracted this part of the code and make

164
00:21:07,800 --> 00:21:14,800
it API for public use. So this talk focuses on developing web API. So in order to transform

165
00:21:14,800 --> 00:21:21,800
our research oriented code into web API, we have to implement how to route request

166
00:21:24,720 --> 00:21:31,720
and check request, check error and the routing points which are endpoints. So let me share

167
00:21:34,560 --> 00:21:41,560
the tips to implement request routing and URL. There is a table of input data on the

168
00:21:44,800 --> 00:21:50,120
left side on this slide which is used for two parameters logistic regression. On the

169
00:21:50,120 --> 00:21:57,120
right side, there is a table of output data calculated by the model. The input data is

170
00:21:57,120 --> 00:22:02,880
about whether students answer each question correctly or not. Their item name which is

171
00:22:02,880 --> 00:22:08,640
the name of question, item ID, how difficult each question is, subject name, exam name

172
00:22:08,640 --> 00:22:14,280
and correction which is binary data. On the other hand, output data is about probabilities

173
00:22:14,320 --> 00:22:21,320
to answer questions correctly. So the role of this API is to get probabilities so we

174
00:22:21,800 --> 00:22:28,800
can directly level probabilities as endpoint name. So based on best practices, function

175
00:22:29,840 --> 00:22:35,480
name should be verb or verb plus now. So the role of this function is to calculate the

176
00:22:35,480 --> 00:22:42,480
results or get probabilities so we can level, calculate the result or get props as a function

177
00:22:42,640 --> 00:22:49,640
name. So understanding what data is input and output which means understanding what

178
00:22:49,680 --> 00:22:56,680
data the code calculates and makes is considerably important step to make API. Next is implementation

179
00:23:03,640 --> 00:23:10,640
of request parameter check. My suggestion is to implement request parameter check. So

180
00:23:12,480 --> 00:23:18,560
request parameter check with JSON schema. This request call command has student name

181
00:23:18,560 --> 00:23:25,560
and student grade as parameters. This JSON file is sample JSON schema. In this file,

182
00:23:26,000 --> 00:23:33,000
you can define what data should be checked and how the data should be checked. So this

183
00:23:34,400 --> 00:23:41,240
make name grade of JSON file is defining the data type of student name as string and it

184
00:23:41,240 --> 00:23:47,800
must be included as request parameters and the data type of student grade is also defined

185
00:23:47,800 --> 00:23:53,560
as string and it must be included as true and maximum length of character is limited

186
00:23:53,560 --> 00:24:00,560
to 120 and minimum length is one character. So in this example, request parameters are

187
00:24:00,680 --> 00:24:07,680
not allowed to be empty. So based on JSON schema, the JSON validator.py is defined as

188
00:24:11,240 --> 00:24:18,240
validator request parameters. The first validator JSON function checks whether the JSON schema

189
00:24:18,320 --> 00:24:24,560
exists or not. The second validator schema function checks whether the request parameters

190
00:24:24,560 --> 00:24:31,560
are correct or not based on JSON schema defined in previous slides. In order to execute these

191
00:24:33,640 --> 00:24:40,640
functions, write function name as syntax sugar and as endpoint. The code with JSON schema

192
00:24:40,640 --> 00:24:47,640
seems to be complicated, but most of the code are like this on this slide. So you can

193
00:24:47,960 --> 00:24:54,560
refer to this code. I will upload this slide on my Twitter as well. So last implementation

194
00:24:54,560 --> 00:25:01,560
is error check. So before implementing the code, we need to think if processing should

195
00:25:02,760 --> 00:25:09,600
be stopped or continue for each code. So whether the processing is stopped or not depends on

196
00:25:09,600 --> 00:25:15,120
service specification. For example, if you implement recommendation system, sometimes

197
00:25:15,120 --> 00:25:22,120
you do not want to stop services and continue recommendation even when the preparation

198
00:25:22,200 --> 00:25:29,200
code failed to load certain data. So this is an example of error handler with Flask.

199
00:25:29,880 --> 00:25:35,400
If you stop processing, you can use a board function and detect error. If you do not want

200
00:25:35,400 --> 00:25:42,400
to stop processing, you can just return the results with JSONify. It's very easy to implement

201
00:25:43,640 --> 00:25:48,160
it, but it's more difficult to decide which part of the code processing should be stopped

202
00:25:48,160 --> 00:25:53,960
or not. So I suggest to talk with product manager or QA team about the service specification

203
00:25:53,960 --> 00:26:00,960
before starting to implement error check. Okay. So let me quickly summarize four-step transformation

204
00:26:01,960 --> 00:26:08,960
from research-oriented code into products. First thing is to understand the characteristics

205
00:26:09,120 --> 00:26:15,520
of the code and figure out how it is working by taking notes. They can modularize the code

206
00:26:15,520 --> 00:26:21,520
based on the code documentation by labeling the code as preparation, preprocessing, and

207
00:26:21,520 --> 00:26:28,520
calculation. Thirdly, refactor the preparation code by simplifying I.O. and the preprocessing

208
00:26:29,240 --> 00:26:36,240
code by changing the coding style. So lastly, make them product, which is an API composed

209
00:26:36,240 --> 00:26:43,240
of request routing, request parameter check, and error check. So after deploying the product,

210
00:26:44,720 --> 00:26:50,920
we need to optimize the speed and the stability by doing load test. So there are useful load

211
00:26:50,920 --> 00:26:56,480
test tools such as Locust and Vegeta. Locust is a Python-based load test tool which can

212
00:26:56,480 --> 00:27:02,320
make it possible to manage load test scenario by Python code and monitor the loading of

213
00:27:02,320 --> 00:27:09,320
the server by GUI. Vegeta is a Go-RAM based HTTP load test tool. It can be used both as

214
00:27:09,480 --> 00:27:15,280
command line, utility, and library. If you are looking for the light load test tool,

215
00:27:15,280 --> 00:27:22,280
Vegeta could be one of the options. And you can do Vegeta attack on your code. So by using

216
00:27:23,160 --> 00:27:28,200
those kind of load test tools, if we figure out the performance is not good enough, we

217
00:27:28,200 --> 00:27:35,200
do parameter tuning of web server and application server. And think about asynchronous, asynchronous.

218
00:27:36,400 --> 00:27:41,800
Even if it is not improved, we think about architecture, infrastructure, or refactoring

219
00:27:41,800 --> 00:27:48,800
the code with different languages. So just in case again, four-step transformation from

220
00:27:49,000 --> 00:27:56,000
research-oriented code into product is very simple. Understand the code, modularize the

221
00:27:56,000 --> 00:28:03,000
code, refactor them, and then make them product after you deploy the code, check the performance.

222
00:28:03,000 --> 00:28:05,000
Thank you.

