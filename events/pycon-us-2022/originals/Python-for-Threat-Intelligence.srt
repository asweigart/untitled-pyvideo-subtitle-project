1
00:00:00,000 --> 00:00:12,680
Our next talk is Python for Threat Intelligence.

2
00:00:12,680 --> 00:00:16,800
Everyone please welcome Aaron Stevens.

3
00:00:16,800 --> 00:00:21,280
Hi, everyone.

4
00:00:21,280 --> 00:00:25,520
Thanks for coming.

5
00:00:25,520 --> 00:00:30,200
My name is Aaron and this is Python for Threat Intelligence.

6
00:00:30,200 --> 00:00:31,480
A little bit of background.

7
00:00:31,480 --> 00:00:38,460
I work at a company called Mandiant and we provide cybersecurity services for other businesses.

8
00:00:38,460 --> 00:00:40,040
You may have heard of us before.

9
00:00:40,040 --> 00:00:45,240
We investigate some of the most high profile hacks around the world, whether that's criminals

10
00:00:45,680 --> 00:00:56,520
get rich or nation states conducting espionage or other intelligence operations.

11
00:00:56,520 --> 00:01:01,000
As you may have guessed, my team focuses on threat intelligence.

12
00:01:01,000 --> 00:01:02,560
If you're already lost, don't worry.

13
00:01:02,560 --> 00:01:11,000
I'm going to go over what that means and how we do it in the next few slides.

14
00:01:11,000 --> 00:01:14,080
I had to summarize what my specific role is.

15
00:01:14,080 --> 00:01:18,880
It's to provide capability and automation for my team.

16
00:01:18,880 --> 00:01:23,720
So basically enabling our analysts to do more of the work that they do and help them do

17
00:01:23,720 --> 00:01:27,680
it faster.

18
00:01:27,680 --> 00:01:32,320
And last, a little disclaimer, I'm not a software engineer and I never have been.

19
00:01:32,320 --> 00:01:34,560
I'm an analyst myself.

20
00:01:34,560 --> 00:01:40,520
So I'm not here to tell you how to write code or to show off some new cutting edge

21
00:01:40,520 --> 00:01:41,800
project.

22
00:01:41,800 --> 00:01:46,520
My goal here today is to show you a little bit about what threat intelligence is and

23
00:01:46,520 --> 00:01:49,880
how we use Python to help us do it.

24
00:01:49,880 --> 00:01:54,560
And by sharing some of the things that we've had success with on my team, my hope is that

25
00:01:54,560 --> 00:02:00,480
it might inspire some of you to do something similar in your own work.

26
00:02:00,480 --> 00:02:03,360
So what is threat intelligence?

27
00:02:03,360 --> 00:02:09,720
Threat intelligence is knowing about adversaries, what they do, and how they do it.

28
00:02:09,720 --> 00:02:14,520
Mandiant conducts hundreds of investigations every single year and it's my team's job to

29
00:02:14,520 --> 00:02:18,460
look at all of those investigations and keep track of who's doing what and how they're

30
00:02:18,460 --> 00:02:21,120
doing it.

31
00:02:21,120 --> 00:02:22,320
Why do we do this?

32
00:02:22,320 --> 00:02:25,020
To help defenders make informed decisions.

33
00:02:25,020 --> 00:02:29,360
This may sound like common sense, but it helps as a defender to know what you're defending

34
00:02:29,360 --> 00:02:31,400
against, right?

35
00:02:31,400 --> 00:02:37,760
So a customer may ask, what are the most common threats targeting my industry and what tools

36
00:02:37,760 --> 00:02:42,040
and techniques do they use?

37
00:02:42,040 --> 00:02:47,520
And last but not least, we do this with data, expertise, and technology.

38
00:02:47,520 --> 00:02:51,440
I know that sounds super vague, but I think it's actually really important to understand

39
00:02:51,440 --> 00:02:57,980
that we need all three of these things working together in order to be successful.

40
00:02:57,980 --> 00:03:02,100
To illustrate this, let me walk you through an example.

41
00:03:02,100 --> 00:03:04,220
Say we have an investigation.

42
00:03:04,220 --> 00:03:07,420
One of the first things that we want to do is collect data.

43
00:03:07,420 --> 00:03:14,420
Now when I say data, I'm talking about things like files, event logs, network telemetry,

44
00:03:14,420 --> 00:03:20,880
pretty much anything that we can use as forensic evidence during our analysis.

45
00:03:20,880 --> 00:03:24,220
At the same time, we're documenting what happened.

46
00:03:24,260 --> 00:03:30,220
I know this sounds similar to collecting data, but I think that the key distinction is organization.

47
00:03:30,220 --> 00:03:36,080
So if the data that we collect are puzzle pieces, then documenting what happened is

48
00:03:36,080 --> 00:03:41,540
putting those pieces together so that we can begin to see the bigger picture.

49
00:03:41,540 --> 00:03:46,200
That usually means building a timeline of events and doing something that we call modeling,

50
00:03:46,200 --> 00:03:49,740
which I'll talk about in the next slide.

51
00:03:49,740 --> 00:03:54,800
And with this bigger picture beginning to take shape, we start applying context to the data.

52
00:03:54,800 --> 00:04:00,060
This means taking a deeper look at the information that we have, expanding that information with

53
00:04:00,060 --> 00:04:07,900
additional sources, and beginning to derive meaning from it.

54
00:04:07,900 --> 00:04:12,860
For example, our collected data might show a program is being installed or executed,

55
00:04:12,860 --> 00:04:15,060
but we still want to know what that program does.

56
00:04:15,060 --> 00:04:20,080
By reverse engineering the file and understanding what its capabilities are, this can help us

57
00:04:20,080 --> 00:04:25,780
determine whether it's relevant to our investigation.

58
00:04:25,780 --> 00:04:30,900
And when we do this for every investigation, it helps us identify patterns across them.

59
00:04:30,900 --> 00:04:35,300
These patterns become the backbone of our analysis, which ultimately allows us to make

60
00:04:35,300 --> 00:04:42,620
assessments about the adversary, their tools, and their objectives.

61
00:04:42,620 --> 00:04:47,020
Looking more now on expertise in technology, I don't think it's news to anyone here that

62
00:04:47,020 --> 00:04:52,660
computers are better at some things than humans, and vice versa.

63
00:04:52,660 --> 00:04:55,060
I think this is especially true here.

64
00:04:55,060 --> 00:04:58,300
And the more that I thought about it, the more that I realized there's kind of this

65
00:04:58,300 --> 00:05:02,700
transition between the two as we work through this process.

66
00:05:02,700 --> 00:05:06,240
Computers are really good at collecting and organizing data, and they can help us apply

67
00:05:06,240 --> 00:05:11,140
context and identify patterns if we tell them where to look and what to look for.

68
00:05:11,140 --> 00:05:15,140
While ultimately in the end, we still need humans with the right expertise to analyze

69
00:05:15,140 --> 00:05:19,020
the data and turn it into knowledge.

70
00:05:19,020 --> 00:05:23,660
The last point I want to make here is that this process is a cycle and when done well,

71
00:05:23,660 --> 00:05:25,140
feeds itself.

72
00:05:25,140 --> 00:05:30,340
The assessments that we make and the patterns that we identify allow us to apply more context

73
00:05:30,340 --> 00:05:33,860
and better informs our collection.

74
00:05:33,860 --> 00:05:38,300
Now I want to quickly cover data modeling, which I'll define as a framework for documenting

75
00:05:38,300 --> 00:05:41,500
both our data and analysis.

76
00:05:41,500 --> 00:05:47,220
Throughout the course of every investigation and research project, our analysts are carefully

77
00:05:47,220 --> 00:05:52,800
modeling key information, enshrining the work that they've done into an ever-growing graph.

78
00:05:52,800 --> 00:05:56,580
This graph serves as the authoritative knowledge base for all of Mandiant.

79
00:05:56,580 --> 00:06:04,020
So everything that we know about adversaries, what they do and how they do it lives here.

80
00:06:04,020 --> 00:06:06,640
Again I think it's easiest to talk about via example.

81
00:06:06,640 --> 00:06:11,520
So let's say that we start with a Word document that maybe we found attached in an email at

82
00:06:11,520 --> 00:06:14,520
a customer.

83
00:06:14,520 --> 00:06:18,280
It might seem harmless, but on closer analysis, we determine that the document contains a

84
00:06:18,280 --> 00:06:24,040
macro that when executed, installs a malicious file of some sort.

85
00:06:24,040 --> 00:06:29,880
So we model that by connecting these two files or nodes with an edge.

86
00:06:29,880 --> 00:06:33,440
And we're already beginning to document what we have and apply context.

87
00:06:33,440 --> 00:06:36,560
So let's keep going.

88
00:06:36,560 --> 00:06:42,080
After our reverse engineer analyzes the file, they report that it communicates via HTTPS

89
00:06:42,080 --> 00:06:44,180
to a domain.

90
00:06:44,180 --> 00:06:48,480
And continuing to expand what we know, we may also notice a code signature for the file

91
00:06:48,480 --> 00:06:54,640
and a resolution for the domain thanks to our passive DNS source.

92
00:06:54,640 --> 00:06:59,280
Eventually we have enough information to assess that this file belongs to the malware family

93
00:06:59,280 --> 00:07:01,720
bad apple.

94
00:07:01,720 --> 00:07:07,280
Using malware families is often the first big step toward attribution as adversaries

95
00:07:07,280 --> 00:07:11,160
tend to use the same malware families across multiple intrusions.

96
00:07:11,160 --> 00:07:13,200
Which makes sense when you think about it.

97
00:07:13,200 --> 00:07:17,600
Bad guys are people too and they like to stick to what they know.

98
00:07:17,600 --> 00:07:20,440
Imagine having to learn a new piece of software every time you came into work.

99
00:07:20,440 --> 00:07:23,680
It just doesn't really make sense.

100
00:07:23,680 --> 00:07:28,400
So we've applied our malware label to the file along with all the data linked to it,

101
00:07:28,400 --> 00:07:34,160
signifying that each node is a component of the bad apple ecosystem.

102
00:07:34,160 --> 00:07:36,840
Let's keep digging.

103
00:07:36,840 --> 00:07:43,720
We know our malware communicates over HTTPS, which means a TLS handshake and an X509 certificate

104
00:07:43,720 --> 00:07:45,060
for the domain.

105
00:07:45,060 --> 00:07:48,920
And here's where things get interesting.

106
00:07:48,920 --> 00:07:53,480
We can't find any more direct relationships to our certificate, but we're able to identify

107
00:07:53,480 --> 00:07:59,980
a pattern softly linking it to another certificate with a similar organization name.

108
00:07:59,980 --> 00:08:03,260
Maybe we saw it recently and we just happened to remember.

109
00:08:03,260 --> 00:08:08,420
Or maybe one of our teammates codified that pattern into a detection which made the link

110
00:08:08,420 --> 00:08:10,080
for us.

111
00:08:10,080 --> 00:08:13,580
Either way, we've just uncovered a big clue because this certificate has already been

112
00:08:13,580 --> 00:08:16,980
attributed to the adversary, Dr. Evil.

113
00:08:16,980 --> 00:08:19,640
Now we have to be careful when we make jumps like this.

114
00:08:19,640 --> 00:08:25,160
Just because two things are similar doesn't necessarily mean that they're related.

115
00:08:25,160 --> 00:08:29,820
But as we continue to traverse the graph, we find that this certificate is tied to another

116
00:08:29,820 --> 00:08:35,760
bad apple sample, including a document much like our own.

117
00:08:35,760 --> 00:08:40,700
And I'm simplifying here, but with all of these identified overlaps, we're eventually

118
00:08:40,700 --> 00:08:45,600
able to attribute our activity to Dr. Evil, expanding our knowledge of this particular

119
00:08:45,600 --> 00:08:46,600
adversary.

120
00:08:47,360 --> 00:08:51,640
And because we modeled all of it, our team can now use that intelligence in their own

121
00:08:51,640 --> 00:08:54,360
analysis and investigations.

122
00:08:54,360 --> 00:08:55,800
Okay.

123
00:08:55,800 --> 00:09:00,000
Now let's talk about Python.

124
00:09:00,000 --> 00:09:05,740
Thinking about what analysts need to do, how can we enhance or automate parts of that process?

125
00:09:05,740 --> 00:09:08,840
As I mentioned before, I think collecting data is the easiest.

126
00:09:08,840 --> 00:09:13,520
Our analysts work with so much data, they shouldn't have to wrangle it all manually.

127
00:09:13,520 --> 00:09:18,340
So we can automate most of this with some good I.O. functionality.

128
00:09:18,340 --> 00:09:22,720
And once we have that data, we can also organize it according to our data model and publish

129
00:09:22,720 --> 00:09:24,860
it directly to the graph.

130
00:09:24,860 --> 00:09:29,040
This way, analysts don't have to create every single node and edge by hand.

131
00:09:29,040 --> 00:09:33,400
They can jump straight to analysis and focus on the really hard questions that require

132
00:09:33,400 --> 00:09:37,360
their expertise.

133
00:09:37,360 --> 00:09:41,420
Applying context and identifying patterns gets a little bit harder, but we can still

134
00:09:41,500 --> 00:09:46,860
help by giving analysts easy access to data sources, extract and model the key information

135
00:09:46,860 --> 00:09:52,860
we get from those sources, and generate detections.

136
00:09:52,860 --> 00:09:57,700
And finally, our mission is to enable analysts, but we can't replace them.

137
00:09:57,700 --> 00:10:01,100
They still need to be in the driver's seat telling us what they need so that we can go

138
00:10:01,100 --> 00:10:02,420
and get it.

139
00:10:02,420 --> 00:10:06,200
In other words, we need a UI.

140
00:10:06,200 --> 00:10:10,840
So over the next several slides, I'll be covering the Python packages that we use, the features

141
00:10:10,840 --> 00:10:16,400
we like, and some examples of how we use them.

142
00:10:16,400 --> 00:10:18,800
The first one I want to talk about is argparse.

143
00:10:18,800 --> 00:10:22,120
We use argparse for all of our command line input.

144
00:10:22,120 --> 00:10:24,780
We like it because it's part of the standard library.

145
00:10:24,780 --> 00:10:27,720
It's really easy to use and it's well documented.

146
00:10:27,720 --> 00:10:32,200
It comes with a ton of default functionality without having to write a lot of code, but

147
00:10:32,200 --> 00:10:35,480
it's still flexible enough for our various use cases.

148
00:10:35,480 --> 00:10:41,280
You can see here that with five lines of code, we get a ton of functionality telling our

149
00:10:41,280 --> 00:10:47,720
analysts about the tool and how to use it.

150
00:10:47,720 --> 00:10:54,720
Another thing about argparse that we really like is that it allows us to define arguments

151
00:10:54,880 --> 00:10:58,720
that we can then use across multiple argument parsers.

152
00:10:58,720 --> 00:11:04,800
So for example, say we want an argument that allows us to define the log level that we

153
00:11:04,800 --> 00:11:06,400
want to output.

154
00:11:06,400 --> 00:11:13,480
We can store that in, say, like a core args module, and then in another module, we can

155
00:11:13,480 --> 00:11:18,520
just import it and stuff it into an argument parser, and as you can see, it's right there

156
00:11:18,520 --> 00:11:22,960
showing us exactly how to use it without any extra code.

157
00:11:22,960 --> 00:11:28,080
This makes it really easy to have a good base of arguments that we can then use across our

158
00:11:28,080 --> 00:11:31,400
project.

159
00:11:31,400 --> 00:11:36,640
Speaking of logging, we try and use a logging package really heavily throughout our project.

160
00:11:36,640 --> 00:11:42,600
It not only helps us test functionality and triage bugs, but it also gives our analysts

161
00:11:42,600 --> 00:11:45,920
more control over the output that they get.

162
00:11:45,920 --> 00:11:56,440
Specifically, we like to use this for file I.O., HTTP requests, and error handling.

163
00:11:56,440 --> 00:12:01,140
The exception method there on the bottom right, I think, is particularly cool because it not

164
00:12:01,140 --> 00:12:05,540
only tells us that we got an error, but it also logs the entire trace back for us as

165
00:12:05,540 --> 00:12:09,820
well.

166
00:12:09,820 --> 00:12:15,500
For console output, we really love the package rich, and I'm sure a lot of you do too.

167
00:12:15,500 --> 00:12:22,500
Rich gives us automatic highlighting, colors, styling, and even emojis.

168
00:12:22,500 --> 00:12:24,180
And that's really just the surface of it.

169
00:12:24,180 --> 00:12:29,700
It also provides things like tables, syntax highlighting, markdown, better trace backs,

170
00:12:29,940 --> 00:12:31,780
progress bars, and so, so, so much more.

171
00:12:31,780 --> 00:12:39,140
I highly recommend checking it out if you haven't already.

172
00:12:39,140 --> 00:12:44,980
And for the specific things that we need highlighted that might not be covered by the default Regex

173
00:12:44,980 --> 00:12:48,700
highlighter that's included, we can make our own.

174
00:12:48,700 --> 00:12:53,780
Pretty much anything that you can specify with a Regex, you can easily make a highlighter

175
00:12:53,780 --> 00:12:58,080
for, and then that just becomes automatic when you print things out.

176
00:12:58,080 --> 00:13:02,320
So you can see here that we've made one for MD5 hashes, something that's really common

177
00:13:02,320 --> 00:13:11,200
for us in threat intelligence.

178
00:13:11,200 --> 00:13:16,760
Another feature of rich that we really like is the logging handler.

179
00:13:16,760 --> 00:13:27,120
The rich handler, by default, adds a ton of extra output to logging, things like timestamps,

180
00:13:27,120 --> 00:13:32,320
a better formatting, all of the same highlighting that we get with the print statements, and

181
00:13:32,320 --> 00:13:42,720
the location in the file where that log message was generated from.

182
00:13:42,720 --> 00:13:46,720
Because we interact with so many different data sources, we need to be able to make lots

183
00:13:46,720 --> 00:13:53,000
of HTTP requests to various APIs, both internal and external.

184
00:13:53,000 --> 00:13:56,880
We really like to use HTTPX.

185
00:13:56,880 --> 00:14:00,960
One reason is because it's both synchronous and asynchronous, so depending on our use

186
00:14:00,960 --> 00:14:04,600
case, we don't need multiple packages to do one or the other.

187
00:14:04,600 --> 00:14:09,560
We can just import HTTPX, and that's it.

188
00:14:09,560 --> 00:14:13,400
Also it's really similar to requests, so if it's something that you're interested in and

189
00:14:13,400 --> 00:14:22,220
you're already familiar with requests, it makes it a lot easier to port over.

190
00:14:22,220 --> 00:14:28,540
One particular feature of HTTPX that I think is really cool is the event hooks.

191
00:14:28,540 --> 00:14:34,780
For any request or response, if we want to do something to that request or response before

192
00:14:34,780 --> 00:14:41,380
it goes out or comes in respectively, then we can create an event hook to make that happen.

193
00:14:41,380 --> 00:14:48,380
For example, if we have, say, an API that we need specific authorization for, then we

194
00:14:48,380 --> 00:14:55,140
can create an event hook that will automatically add a token in when it sees that we're making

195
00:14:55,140 --> 00:14:58,460
a request to that specific host.

196
00:14:58,460 --> 00:15:04,500
Then we can just import that client into another module, and we don't have to worry about authorization

197
00:15:04,500 --> 00:15:11,940
in our other code.

198
00:15:11,940 --> 00:15:17,800
In thinking back to data modeling, we need a way to represent data as nodes and edges.

199
00:15:17,800 --> 00:15:20,480
We do this with simple classes.

200
00:15:20,480 --> 00:15:26,480
For example, we have a class node that it might take a type and maybe some properties,

201
00:15:26,480 --> 00:15:33,640
and then we have a method so that we can add tags to it, aka apply context.

202
00:15:33,640 --> 00:15:38,960
Building on top of that, we can also create an edge class that takes nodes and relates

203
00:15:38,960 --> 00:15:43,840
them together.

204
00:15:43,880 --> 00:15:50,520
Then we can use those to build a data model, which specifies the exact types of nodes and

205
00:15:50,520 --> 00:15:54,100
edges that we want to be able to create.

206
00:15:54,100 --> 00:16:00,120
At the same time, we can also build in some validation here to make sure that we're not

207
00:16:00,120 --> 00:16:10,080
modeling anything that doesn't make sense.

208
00:16:10,080 --> 00:16:14,960
In the same class, but I couldn't fit it here, so we had to just pretend it's further down,

209
00:16:14,960 --> 00:16:20,120
we also can create a payload which transforms those nodes and edges into whatever it is

210
00:16:20,120 --> 00:16:26,780
that our API accepts, say it's JSON, and we can use our auth client that we just talked

211
00:16:26,780 --> 00:16:37,120
about from HTTPX and submit that automatically to our graph.

212
00:16:37,160 --> 00:16:43,920
For an example, let's say that we want to model network scan data.

213
00:16:43,920 --> 00:16:49,280
We have a function here, model scans, and say it takes an instance of our data model

214
00:16:49,280 --> 00:16:53,300
and a bunch of scans that maybe we got from an API.

215
00:16:53,300 --> 00:17:00,280
For each scan, we pull out the IP and the port, and we can model that as a node.

216
00:17:00,280 --> 00:17:07,040
If we're particularly interested in TLS data, we can then parse out the certificate, model

217
00:17:07,040 --> 00:17:13,920
that as a file, apply some context for say if it's self-issued, and then we can link

218
00:17:13,920 --> 00:17:18,120
those two nodes together with an edge.

219
00:17:18,120 --> 00:17:22,300
Then we can display all of that information that we've modeled, and by the way, it looks

220
00:17:22,300 --> 00:17:29,840
super awesome because of rich, and then we can submit that to the graph if the user so

221
00:17:29,840 --> 00:17:36,760
chooses.

222
00:17:36,760 --> 00:17:42,320
As far as modeling data from sources, we also often need to inspect files that we found.

223
00:17:42,320 --> 00:17:51,620
The most common type of file that we deal with are PEs, so executables on Windows.

224
00:17:51,620 --> 00:17:56,760
One of the packages that we use a lot is called PE file.

225
00:17:56,760 --> 00:18:00,680
This package is really awesome for parsing pretty much anything and everything that you

226
00:18:00,680 --> 00:18:03,160
could ever want out of a PE.

227
00:18:03,160 --> 00:18:09,000
For example here, we pass it some bytes, and then we can pull out the things we want like

228
00:18:09,000 --> 00:18:19,040
the compile timestamp, the rich hash, the import hash, so on and so forth.

229
00:18:19,040 --> 00:18:24,440
Then going a little bit further, we can also parse out the code signature.

230
00:18:24,440 --> 00:18:28,920
It's not as easy to do, but I've got an example here of how to get that done.

231
00:18:28,960 --> 00:18:33,800
Once we have the bytes, we can hand it off to another package, ASN1 Crypto, which will

232
00:18:33,800 --> 00:18:42,440
then automatically parse all of the certificates for us.

233
00:18:42,440 --> 00:18:47,740
Much in the same way, we can then pull out the specific data points from those certificates

234
00:18:47,740 --> 00:18:54,760
that we're interested in and do whatever we want with them, display them, use them for

235
00:18:54,760 --> 00:19:02,560
detections, all that good stuff.

236
00:19:02,560 --> 00:19:06,780
When I talk about generating detections, I think probably the most common one, not only

237
00:19:06,780 --> 00:19:10,560
for us but across the industry, are Yara rules.

238
00:19:10,560 --> 00:19:15,760
I've got an example here and a link if you want to check it out.

239
00:19:15,760 --> 00:19:19,720
Yara rules, like I said, are pretty much industry standard.

240
00:19:19,720 --> 00:19:21,360
It's all open source.

241
00:19:21,360 --> 00:19:27,620
It's extensible and there are a bunch of default modules for things like PEs, for things like

242
00:19:27,620 --> 00:19:35,840
generating hashes, and it has a Python package, Yara Python, which lets you run all of your

243
00:19:35,840 --> 00:19:42,120
rules against files and all that other good stuff.

244
00:19:42,120 --> 00:19:45,640
At the end of the day, Yara rules are really just strings.

245
00:19:45,640 --> 00:19:51,100
Once we have the data from these files that we want, it makes it really easy to then generate

246
00:19:51,100 --> 00:19:54,340
these boilerplate detections.

247
00:19:54,340 --> 00:19:59,140
For example, if we have a serial number and a common name for a certificate that we're

248
00:19:59,140 --> 00:20:06,040
interested in, we can create a Yara rule and then we can look for any PE file that might

249
00:20:06,040 --> 00:20:09,560
contain that certificate.

250
00:20:09,560 --> 00:20:17,460
Or with a rich hash from a PE, we can use the hash module and PE modules to look for

251
00:20:17,460 --> 00:20:20,680
that exact same hash in another file.

252
00:20:20,680 --> 00:20:26,860
These are both really good examples of ways that we look for similar malware across our

253
00:20:26,860 --> 00:20:34,280
data set.

254
00:20:34,280 --> 00:20:39,340
And then finally, I want to talk a little bit about engineering without engineers.

255
00:20:39,340 --> 00:20:44,600
As I explained before, we don't have any engineers on my team.

256
00:20:44,600 --> 00:20:48,620
We're all just analysts trying to make do with the Python that we know.

257
00:20:48,620 --> 00:20:55,100
And so my advice for others who maybe are in a similar situation is to start small.

258
00:20:55,100 --> 00:21:00,540
Think about the things that you do every day that take a lot of time, that are tedious,

259
00:21:00,540 --> 00:21:03,460
but that don't require a lot of brain power.

260
00:21:03,460 --> 00:21:06,500
The boring stuff.

261
00:21:06,500 --> 00:21:10,140
The more that you can automate these things, the more that you can focus on the stuff that

262
00:21:10,140 --> 00:21:13,840
really matters, the stuff that really needs your attention, and honestly, the stuff that's

263
00:21:13,840 --> 00:21:16,140
more fun anyways.

264
00:21:16,940 --> 00:21:21,260
Next, I would say invest in your environment.

265
00:21:21,260 --> 00:21:24,060
Get yourself an IDE that really works for you.

266
00:21:24,060 --> 00:21:25,460
I love VS code.

267
00:21:25,460 --> 00:21:28,340
PyCharm is great, too.

268
00:21:28,340 --> 00:21:33,560
There's a lot of built-in extensions specifically for Python, and some of the folks that work

269
00:21:33,560 --> 00:21:38,300
on that are here at the conference.

270
00:21:38,300 --> 00:21:44,260
Then I would say look at things like automatic formatting and linting.

271
00:21:44,260 --> 00:21:47,100
Things like black, like pylint.

272
00:21:47,100 --> 00:21:52,460
These can really help you catch bugs and make your code more readable, more understandable,

273
00:21:52,460 --> 00:21:58,380
not only for you, but for others.

274
00:21:58,380 --> 00:22:04,500
And then remember that productivity is better than perfection.

275
00:22:04,500 --> 00:22:08,380
We're not software engineers, but we don't have to be.

276
00:22:08,380 --> 00:22:13,980
We're not trying to ship a product that needs to be absolutely perfect, that has no bugs.

277
00:22:14,580 --> 00:22:17,940
We don't want bugs, but we're not afraid of them either.

278
00:22:17,940 --> 00:22:21,940
Any little improvement that we can make in our process is a win.

279
00:22:21,940 --> 00:22:29,460
Any time that we can save, it allows us, again, to focus on those things that need our attention more.

280
00:22:33,900 --> 00:22:35,900
And that's pretty much it.

281
00:22:35,900 --> 00:22:36,900
Thank you.

