1
00:00:00,000 --> 00:00:07,000
Hola, buenos días. Segundo día de las Python charlas. Estamos muy felices, se recuerdan

2
00:00:11,920 --> 00:00:15,280
de años anteriores, solamente ha habido un solo día. Este año tenemos dos días, así

3
00:00:15,280 --> 00:00:20,240
que el segundo se viene mucho más recargado. Y, sin más preámbulo, vamos a darle la bienvenida

4
00:00:20,240 --> 00:00:25,400
a la primera speaker de esta nueva sesión, Laura Gutiérrez-Funderburg. Y el título de

5
00:00:25,400 --> 00:00:29,400
su charla es Reduciendo el prejuicio de la inteligencia artificial, Introducción a Fer

6
00:00:29,400 --> 00:00:36,400
Learn. Le damos un aplauso a Laura. Muchas gracias, Cristian, y bienvenidos a todos.

7
00:00:40,640 --> 00:00:44,480
Espero que estén teniendo una experiencia increíble aquí en la PyCon. Estoy bastante

8
00:00:44,480 --> 00:00:51,120
emocionada de estar aquí. Fue toda una aventura llegar a PyCon. Primero, yo soy originalmente

9
00:00:51,120 --> 00:00:55,440
de México y he estado viviendo en Canadá los últimos diez años. Primero dije, bueno,

10
00:00:55,440 --> 00:00:58,720
ya voy a aplicar una charla y me aceptaron mi charla. Dije, bueno, voy a aplicar a que

11
00:00:58,720 --> 00:01:03,720
me den mi visa y me dan mi cita el año que entra. Y dije, no, cómo lo voy a hacer, no

12
00:01:03,720 --> 00:01:09,000
voy a poder llegar a la PyCon. Pero, por suerte, hace como dos años empecé una aplicación

13
00:01:09,000 --> 00:01:14,120
para convertirme ciudadana de Canadá y a finales de marzo tuve mi ceremonia y todo

14
00:01:14,120 --> 00:01:18,440
muy bien. Dije, bueno, ya puedo aplicar por mi pasaporte. Pero con lo que no estaba contando

15
00:01:18,440 --> 00:01:23,600
es que las oficinas de Canadá están echadas una locura. Entonces, toda la semana anterior

16
00:01:23,600 --> 00:01:28,040
de venir aquí a la PyCon estuve yendo a filas y había gente formándose desde las doce

17
00:01:28,040 --> 00:01:35,320
de la noche del día anterior. Y llegó martes, miércoles, jueves, viernes y yo sin pasaporte

18
00:01:35,320 --> 00:01:39,800
pensando, solo tengo hasta lunes para que me den mi documento. Y el lunes dije, no,

19
00:01:39,800 --> 00:01:43,520
hoy es el día, tengo que hacerlo funcionar. Así es que me fui al edificio a las tres

20
00:01:44,520 --> 00:01:48,240
de la mañana y había como ya otras como diez, quince personas, pero fui de las primeras

21
00:01:48,240 --> 00:01:54,680
cinco personas. Llegué y hice mis fan cartas para organizar a la gente y lo logré. Fue

22
00:01:54,680 --> 00:02:02,080
toda una aventura, pero me dio mucho gusto que pueda estar aquí con ustedes. Entonces,

23
00:02:02,080 --> 00:02:08,920
esta charla está inspirada en un paquete de Python llamado Fairlearn. Ellos están

24
00:02:08,920 --> 00:02:15,920
trabajando bastante cercanamente con los desarrolladores de Scikit-learn y originalmente

25
00:02:15,920 --> 00:02:21,160
ellos comenzaron como un proyecto por parte de Microsoft, pero después empezaron, ahora

26
00:02:21,160 --> 00:02:28,160
son gobernados como un open source project. Y el enfoque de Fairlearn es mitigar la injusticia

27
00:02:29,720 --> 00:02:34,920
en la inteligencia artificial, sobre todo cuando estamos utilizando algoritmos de aprendizaje

28
00:02:34,920 --> 00:02:40,360
automático. Es importante que cuando utilizamos esos algoritmos para tomar decisiones que

29
00:02:40,360 --> 00:02:45,920
impactan vidas humanas, que tengamos en consideración cómo es que nuestros algoritmos tomen ese

30
00:02:45,920 --> 00:02:52,920
impacto. Para los que están uniéndose en vivo, aquí tengo un link si quieren seguir

31
00:02:53,400 --> 00:03:00,400
las diapositivas, sería tinyurl.com, Python, charlas ea. Vamos a empezar. Entonces, un

32
00:03:01,400 --> 00:03:08,480
poquito de mí. Yo ahorita estoy basada en Canadá y estoy trabajando como científico

33
00:03:08,480 --> 00:03:13,960
de datos en C-Span Ship Management. Lo que hago ahí es que utilizo Python para ayudar

34
00:03:13,960 --> 00:03:20,960
a mi equipo a manejar a los botes. En particular, los botes son de esos barcos de contenedores.

35
00:03:22,340 --> 00:03:28,960
Son unos barcos gigantes que cargan toneladas de productos. Y lo que queremos hacer juntos

36
00:03:28,960 --> 00:03:35,960
es que queremos reducir las emisiones de CO2 en los barcos. Entonces, a veces cosas

37
00:03:37,440 --> 00:03:40,360
de las que tenemos que hacer también es que tenemos que evaluar cómo son que los capitanes

38
00:03:40,360 --> 00:03:44,940
están manejando sus botes y si alguien tiene un problema en el bote o si alguien está

39
00:03:44,940 --> 00:03:49,200
manejando el barco mal, queremos utilizar ciencia de datos para detectar esos problemas.

40
00:03:49,200 --> 00:03:53,160
Pero por supuesto un problema es, ¿cómo nos aseguramos de que no seamos injustos al

41
00:03:53,160 --> 00:03:59,940
evaluar a las personas? Entonces, un poquito de motivación. En los últimos años estamos

42
00:03:59,940 --> 00:04:04,760
viendo un incremento en la luz de la inteligencia artificial en nuestra vida cotidiana. Por

43
00:04:04,760 --> 00:04:10,840
ejemplo, de las cosas más simples, los algoritmos de YouTube recomendándonos contenido. Pero

44
00:04:10,840 --> 00:04:15,600
cosas un poquito más serias, por ejemplo, inteligencia artificial utilizada para evaluar

45
00:04:15,600 --> 00:04:20,860
las ofertas de trabajo, inteligencia artificial para evaluar pacientes y quién necesita más

46
00:04:20,860 --> 00:04:25,500
atención e incluso inteligencia artificial para evaluar quién tiene riesgo de cometer

47
00:04:25,500 --> 00:04:29,860
un crimen. Por supuesto, estas cosas empiezan a ponerse muy serias cuando empezamos a pensar

48
00:04:29,860 --> 00:04:36,260
cuáles son las consecuencias de evaluar a alguien de manera equivocada.

49
00:04:36,260 --> 00:04:40,540
La primera pregunta para nosotros mismos es, ¿cuál es nuestra responsabilidad como científicos

50
00:04:40,540 --> 00:04:46,220
de datos, como ingenieros, programadores y como colectores de datos? A final de cuentas,

51
00:04:46,220 --> 00:04:50,620
la responsabilidad es no solo en el científico de datos que procesa los datos, pero empiece

52
00:04:50,620 --> 00:04:56,100
en el momento en el que empezamos a colectar información. Y la pregunta es, ¿qué podemos

53
00:04:56,100 --> 00:05:02,380
hacer para identificar áreas de prejuicio en los modelos y minimizar el daño?

54
00:05:02,380 --> 00:05:06,700
En esta charla voy a motivar la importancia de la ética en la inteligencia artificial

55
00:05:06,700 --> 00:05:11,460
y voy a enfocarme en un ejemplo en el sector de salud en el que vamos a aprender con un

56
00:05:11,460 --> 00:05:16,380
poquito de código sobre un nuevo paquete de Python llamado Fairlearn.

57
00:05:17,220 --> 00:05:22,900
El plan para la charla va a motivar la importancia de la ética. Voy a hablar un poquito de los

58
00:05:22,900 --> 00:05:27,780
tipos de daños y el papel de la abstracción en la inteligencia artificial y voy a dar

59
00:05:27,780 --> 00:05:32,060
una introducción a Fairlearn. En particular, voy a hablar un poquito de la precisión y

60
00:05:32,060 --> 00:05:37,620
la equidad de un modelo y voy a enfocarme en dos submódulos del paquete de Fairlearn.

61
00:05:37,620 --> 00:05:43,540
Uno va a ser metric frame y el otro va a ser threshold optimizer.

62
00:05:43,540 --> 00:05:51,580
Enfin, tenemos la siguiente situación. Tenemos una colección de datos de pacientes con diabetes

63
00:05:51,580 --> 00:05:55,940
y nuestro goal es escribir un programa de Aprendizaje Automático que ayude a los médicos

64
00:05:55,940 --> 00:06:00,260
a determinar qué pacientes requieren de mayor atención. El tipo de información que vamos

65
00:06:00,260 --> 00:06:06,620
a tener incluye cuándo fue que el paciente visitó el hospital por primera vez, cuándo

66
00:06:06,620 --> 00:06:11,380
fue que fue readmitido y si fue una situación de emergencia o si fue nada más para dar

67
00:06:11,380 --> 00:06:20,940
un chequeo de seguido. Los datos son públicos. Para los que están siguiendo la diapositiva

68
00:06:20,940 --> 00:06:24,780
hay una liga ahí para investigar un poquito los datos. Van a tener información sobre

69
00:06:24,780 --> 00:06:30,420
los pacientes. En este caso vamos a tener un punto representando la admisión hospitalaria

70
00:06:30,420 --> 00:06:36,340
para un paciente entre 11 y 14 días y los datos incluyen información como la demografía,

71
00:06:36,340 --> 00:06:44,020
cómo su edad, el género y su raza, los medicamentos que están tomando, cuántas visitas tomaron

72
00:06:44,020 --> 00:06:50,140
y si el paciente regresó en 30 días o menos. La tarea es vamos a desarrollar un modelo

73
00:06:50,140 --> 00:06:54,580
de clasificación que determine qué pacientes deben ser tratados por medio del programa

74
00:06:54,580 --> 00:06:59,700
de cuidado para pacientes de algo riesgo. Básicamente el algoritmo debería predecir

75
00:06:59,700 --> 00:07:07,220
si un paciente tiene mayor probabilidad de ser de alto riesgo. Ahora no puedo seguir

76
00:07:07,220 --> 00:07:12,780
sin el taller sin dar un poquito de gracias y agradecimientos a las dos inspiraciones

77
00:07:12,780 --> 00:07:21,260
de esta charla. Una es sobre un artículo llamado equidad de abstracciones de sistemas

78
00:07:22,260 --> 00:07:29,260
y el otro es un tutorial desarrollado por los mantenedores de Fairlearn para SciPy.

79
00:07:31,260 --> 00:07:35,940
Tomé como solo tengo 30 minutos, tomé básicamente las cosas más importantes de este tutorial

80
00:07:35,940 --> 00:07:40,100
pero la idea está concentrada. Si tienen ganas de visitar ese tutorialero, ahí está

81
00:07:40,100 --> 00:07:47,100
la cita. Vamos a empezar. ¿Por qué es que los algoritmos de inteligencia artificial

82
00:07:47,100 --> 00:07:52,100
se comportan de manera injusta? Por una variedad de razones. La primera es porque a veces

83
00:07:52,100 --> 00:07:59,100
cuando nosotros programamos, los sesgos sociales pueden reflejarse en cómo es que estamos

84
00:07:59,100 --> 00:08:05,100
representando la información. Esto también puede ocurrir cuando nosotros tomamos decisiones

85
00:08:05,100 --> 00:08:12,100
para abstraer información y a veces esto puede pasar porque las características a veces

86
00:08:13,100 --> 00:08:19,100
pasan por cómo programamos las cosas pero sino cómo es que estamos representando información

87
00:08:19,100 --> 00:08:24,100
en los datos. Normalmente estas dos situaciones no son mutuamente exclusivas. Hay veces en

88
00:08:24,100 --> 00:08:29,100
que el programador o los que están desarrollando tienen conocimiento limitado por una situación

89
00:08:29,100 --> 00:08:33,100
entonces cuando toman decisiones en el modelo, esas decisiones se van reflejadas en el algoritmo

90
00:08:33,100 --> 00:08:38,100
pero a veces esto ocurre cuando estamos colectando los datos y a veces ocurren al mismo tiempo.

91
00:08:38,100 --> 00:08:45,100
¿Cómo podemos determinar si una inteligencia artificial se comporta de manera injusta?

92
00:08:45,100 --> 00:08:52,100
Obviamente esta es una pregunta bastante complicada pero si queremos ponernos en términos de

93
00:08:52,100 --> 00:08:57,100
cuantificar, podemos estudiar el impacto en la inteligencia artificial a nivel de daños

94
00:08:57,100 --> 00:09:02,100
y ganancias. Y en ciencia de datos podemos cuantificarlo mediante el número de falsos

95
00:09:02,100 --> 00:09:09,100
positivos o falsos negativos. ¿Cómo es que incurrimos en un daño? Básicamente en el

96
00:09:12,100 --> 00:09:18,100
corazón de esta idea de dañar está la noción de abstracción. ¿Cómo es que tomamos una

97
00:09:18,100 --> 00:09:24,100
situación en la vida real y cómo es que decidimos representar esa situación en el uso de datos?

98
00:09:24,100 --> 00:09:29,100
Normalmente podemos utilizar abstracción en ciencias de computación, en matemáticas

99
00:09:29,100 --> 00:09:33,100
y obviamente en el lenguaje de aprendizaje automático. Las situaciones de la vida real

100
00:09:33,100 --> 00:09:38,100
normalmente son muy complejas y no podemos representar toda una situación en un modelo

101
00:09:38,100 --> 00:09:44,100
así que normalmente tenemos que tomar decisiones sobre qué cosas vamos a implementar, qué

102
00:09:44,100 --> 00:09:50,100
cosas vamos a asumir y es en este proceso de decidir qué incorporamos y qué dejamos

103
00:09:50,100 --> 00:09:57,100
afuera que podemos introducir los daños. Por ejemplo, la abstracción normalmente conduce

104
00:09:58,100 --> 00:10:04,100
a la eliminación de atributos o propiedades que dependen de un contexto social. Por ejemplo,

105
00:10:04,100 --> 00:10:11,100
en el escenario de salud un aspecto muy importante que es muy difícil cuantificar es la interacción

106
00:10:13,100 --> 00:10:17,100
entre un doctor y un paciente. Normalmente el doctor va a estar familiarizado con la historia

107
00:10:17,100 --> 00:10:22,100
de un paciente, a lo mejor el doctor tiene información adicional que no está encapsulada

108
00:10:22,100 --> 00:10:27,100
en una tabla de datos. Entonces cuando el científico de datos obtiene esa tabla de

109
00:10:27,100 --> 00:10:35,100
datos hay muchísima información y contexto que se perdió. Una pregunta muy importante

110
00:10:35,100 --> 00:10:40,100
que hay que hacernos es cuál es la compensación que hacemos al desechar una propiedad y somos

111
00:10:40,100 --> 00:10:49,100
conscientes de las consecuencias de esa compensación. Por ejemplo, si tenemos un paciente que tiene

112
00:10:49,100 --> 00:10:56,100
forma parte de una población de minoría y decidimos no incorporar esa parte del paciente,

113
00:11:00,100 --> 00:11:05,100
¿cuál es la consecuencia? Tenemos entendimiento de qué es lo que va a pasar cuando decidimos

114
00:11:05,100 --> 00:11:12,100
remover esa propiedad. Y aquí es cuando Fairlearn viene. Este es un paquete de Python centralizado

115
00:11:12,100 --> 00:11:16,100
en disminuir la injusticia en los modelos de aprendizaje automático. Y cómo lo hace

116
00:11:16,100 --> 00:11:22,100
Fairlearn es estudiando esta noción de los daños o las ganancias por medio de cuantificar

117
00:11:22,100 --> 00:11:27,100
el número de falsos positivos y falsos negativos y evaluar cuál es la consecuencia de esos

118
00:11:27,100 --> 00:11:34,100
falsos positivos y negativos. Como mencioné al principio, este es un proyecto de código

119
00:11:34,100 --> 00:11:38,100
abierto impulsado por la comunidad para ayudar a los científicos de datos a mejorar equidad

120
00:11:38,100 --> 00:11:44,100
en sistemas de inteligencia artificial. Incluye el paquete de Python, recursos educativos

121
00:11:44,100 --> 00:11:48,100
que la comunidad ha puesto juntos como Jupyter Notebooks. Y como mencioné, ellos empezaron

122
00:11:48,100 --> 00:11:54,100
en 2018 en Microsoft Research. En el 2021, ellos adoptaron una estructura de gobernanza

123
00:11:54,100 --> 00:11:59,100
neutral y desde entonces es completamente impulsado por la comunidad. Tienen su servidor

124
00:11:59,100 --> 00:12:06,100
de Discord y hace relativamente poco yo empecé a traducir la documentación. Si hay alguien

125
00:12:06,100 --> 00:12:11,100
que está interesado en ayudarme, por favor, contáctenme. Yo estoy muy feliz de trabajar

126
00:12:11,100 --> 00:12:15,100
con ellos y de que me guste traducir este paquete.

127
00:12:15,100 --> 00:12:22,100
Vamos a saltar un poquitito al ejemplo de data. En este caso escribí un script de Python

128
00:12:22,100 --> 00:12:28,100
que lo que hace es que va a leer o va a incorporar esos datos y va a procesarlos usando Pandas

129
00:12:28,100 --> 00:12:33,100
y Scikit Learn. Y vamos a usar el paquete de Fairlearn para mejorar cómo es que está

130
00:12:33,100 --> 00:12:38,100
evaluando el algoritmo a los pacientes. En este caso, los datos tienen información como

131
00:12:38,100 --> 00:12:45,100
su raza, su género, la edad, cuánto tiempo estuvieron en el hospital, el medicamento

132
00:12:45,100 --> 00:12:51,100
que estaban usando, etc. Entonces, una de las cosas que podemos empezar a estudiar es,

133
00:12:51,100 --> 00:12:56,100
por ejemplo, podemos estudiar cómo es que los pacientes se diferenciaban en términos

134
00:12:56,100 --> 00:13:03,100
de su raza. En este caso, la mayoría de los pacientes eran Caucasian. Tenemos un, el siguiente

135
00:13:03,100 --> 00:13:10,100
grupo era pacientes africanamericanos. Había raza que era no conocida, hispánicos, otro,

136
00:13:12,100 --> 00:13:19,100
y pacientes de origen asiático. Si estudiamos, por ejemplo, el número de visitas que no

137
00:13:21,100 --> 00:13:27,100
eran de emergencia versus el número de visitas que sí fueron de emergencia, si estudiamos

138
00:13:27,100 --> 00:13:34,100
eso en contra de, o si lo ponemos en correlación con si fueron redmitidos al hospital en 30

139
00:13:35,100 --> 00:13:42,100
días o menos, cero significa no en este caso y uno significa sí. Podemos ver, por ejemplo,

140
00:13:42,100 --> 00:13:49,100
en el primer plot hacia la izquierda, donde está uno 0.5, vemos que aquellos pacientes

141
00:13:50,100 --> 00:13:57,100
que fueron redmitidos en 30 días o menos solieron tener más visitas que no eran de emergencia

142
00:13:57,100 --> 00:14:02,100
y podemos ver ese mismo fenómeno para las visitas que sí fueron de emergencia. Ahora,

143
00:14:02,100 --> 00:14:09,100
esos son para todos los pacientes, pero si lo partimos en términos de cuál era la raza

144
00:14:09,100 --> 00:14:15,100
de los pacientes, utilizando barras de error, podemos observar un fenómeno bastante interesante.

145
00:14:15,100 --> 00:14:22,100
Por ejemplo, los pacientes cuya raza era no conocida u otra, tienen barras de error

146
00:14:23,100 --> 00:14:28,100
muchísimo más largas que los pacientes, por ejemplo, que eran Caucasian y lo que eso

147
00:14:28,100 --> 00:14:34,100
nos está diciendo es que hubo pacientes que fueron redmitidos al hospital dentro de 30

148
00:14:34,100 --> 00:14:40,100
días o menos, pero que no necesariamente fueron redmitidos al hospital. Entonces, lo

149
00:14:40,100 --> 00:14:45,100
que esto nos está diciendo en una emergencia, lo que esto nos está diciendo es que hay

150
00:14:45,100 --> 00:14:52,100
razón para sospechar que estos pacientes no están recibiendo los servicios que necesitan.

151
00:14:53,100 --> 00:14:59,100
Entonces, lo que esto nos dice desde el principio es que tenemos un problema en nuestros datos,

152
00:14:59,100 --> 00:15:05,100
quiere decir, en nuestra información tenemos un prejuicio reflejado sobre cómo pacientes

153
00:15:05,100 --> 00:15:09,100
de ciertas razas están siendo tratados en el hospital. Ahora, lo que podemos hacer como

154
00:15:09,100 --> 00:15:13,100
científicos de datos es que si estamos comunicándonos con el hospital decimos, oye, ¿sabes qué?

155
00:15:13,100 --> 00:15:18,100
Estamos viendo que hay un problema sobre cómo estás tratando a estos pacientes y aquí están

156
00:15:18,100 --> 00:15:22,100
los datos, creo que el hospital puede hacer algo para mejorar. Pero la pregunta más importante

157
00:15:22,100 --> 00:15:30,100
es cuando empecemos a utilizar programas de aprendizaje automático en estos datos, es

158
00:15:30,100 --> 00:15:36,100
muy probable que nuestro algoritmo va a enforzar este prejuicio. Tenemos que ser muy, muy

159
00:15:36,100 --> 00:15:43,100
cuidadosos cuando hagamos esto. Ok, vamos a estudiar tantito el imbalance de las clasificaciones.

160
00:15:43,100 --> 00:15:50,100
Este plot aquí nos está mostrando si el paciente fue redmitido dentro de 30 días o menos,

161
00:15:50,100 --> 00:15:58,100
partido por su raza. Entonces, aquí el punto es como familiarizarse más con los datos

162
00:15:58,100 --> 00:16:02,100
y observar si hay algún problema. En este caso, por ejemplo, podemos ver que para los pacientes

163
00:16:02,100 --> 00:16:10,100
cuya raza era otro o no conocida, ellos solían no ser redmitidos entre 30 días o menos.

164
00:16:10,100 --> 00:16:17,100
Ahora, aquí es donde podemos utilizar Scikit Learn para hacer un poquitito de aprendizaje automático.

165
00:16:17,100 --> 00:16:25,100
En este caso, lo que dice es que puse una función, si tienen curiosidad en el repositorio,

166
00:16:25,100 --> 00:16:30,100
tengo el código, pero lo que estoy haciendo es que estoy entrenando un modelo y voy a separar mis

167
00:16:30,100 --> 00:16:38,100
datos para entrenar, para probarlo y después para evaluarlo. Entonces, en este caso, lo que queremos

168
00:16:38,100 --> 00:16:45,100
hacer es evaluar si los pacientes están siendo clasificados de manera equilibrada. Como tenemos

169
00:16:45,100 --> 00:16:50,100
problemas en cuanto al número de pacientes, eso nos va a introducir problemas cuando estemos

170
00:16:50,100 --> 00:16:57,100
evaluando el modelo. Así es que voy a introducir modelo de entrenamiento balanceado.

171
00:16:57,100 --> 00:17:06,100
En este caso, una cosita que puedo hacer es que pueda estudiar cómo es que mis datos de entrenamiento

172
00:17:06,100 --> 00:17:13,100
y de prueba están siendo clasificados. Entonces, por ejemplo, si quiero estudiar cuáles son las

173
00:17:13,100 --> 00:17:21,100
propiedades sensibles para el dataset de entrenamiento y para el dataset de prueba, puedo ver que,

174
00:17:21,100 --> 00:17:28,100
por ejemplo, para el dataset de prueba es bastante sensible cuando empiezo a ver acerca de la raza

175
00:17:28,100 --> 00:17:33,100
del paciente y también si fue readmitido dentro de 30 días o menos. Otra cosa con la que tengo que

176
00:17:33,100 --> 00:17:40,100
ser bastante cuidadosa. Vamos a entrenar el modelo. En este caso, lo que estoy haciendo es que primero,

177
00:17:40,100 --> 00:17:46,100
como tengo problemas en términos del tamaño de la población, primero voy a aplicar un standard

178
00:17:46,100 --> 00:17:51,100
scaler y después de eso, como estoy clasificando los pacientes en términos de si deberían ser,

179
00:17:51,100 --> 00:17:57,100
si son readmitidos o no, voy a utilizar a regresión logística. Este es nuestro pipeline.

180
00:17:57,100 --> 00:18:03,100
Lo que voy a hacer ahora es que voy a mostrarles los resultados del algoritmo antes de mitigar y

181
00:18:03,100 --> 00:18:09,100
después de mitigar. Vamos a observar cómo es que esto impacta a cómo son los pacientes clasificados.

182
00:18:10,100 --> 00:18:19,100
En este caso, después de entrenar al algoritmo, puedo empezar a predecir y a decir que ya que el

183
00:18:19,100 --> 00:18:27,100
algoritmo entrenó, puedo darle los datos de prueba y puedo estudiar cómo es que esto está impactando

184
00:18:27,100 --> 00:18:35,100
los resultados. En este caso, ya que entendí los datos, voy a estudiar qué tan efectivo el algoritmo

185
00:18:35,100 --> 00:18:39,100
es. En este caso, estamos viendo un ROC curve que nos está mostrando que el algoritmo no es

186
00:18:39,100 --> 00:18:48,100
super efectivo en clasificar. Si estudio, por ejemplo, cuál es su balance accuracy score,

187
00:18:48,100 --> 00:19:04,100
estoy viendo un puntaje de 0.59 a 0.6. Básicamente, si estoy lanzando una moneda,

188
00:19:04,100 --> 00:19:10,100
es básicamente lo que también el algoritmo es. ¿Por qué está pasando esto? Uno, porque los datos

189
00:19:10,100 --> 00:19:17,100
no están distribuidos de manera uniforme. Otro, porque, por ejemplo, cuando vimos en un plot anterior,

190
00:19:17,100 --> 00:19:23,100
el número de pacientes que era readmitido y no, la diferencia era bastante grande. La mayoría de los

191
00:19:23,100 --> 00:19:28,100
pacientes no estaban siendo readmitidos, era solo un grupo muy pequeño que estaba regresando al hospital

192
00:19:28,100 --> 00:19:41,100
otra vez. En este caso, es como lanzar una moneda. En este caso, como científicos de datos,

193
00:19:41,100 --> 00:19:47,100
una de las cosas que podemos hacer es que nunca queremos tomar la responsabilidad sola de tomar una

194
00:19:47,100 --> 00:19:53,100
decisión. De manera ideal, porque esos problemas son bastante complejos, queremos trabajar con alguien

195
00:19:53,100 --> 00:19:58,100
que sea experto. En este caso, vamos a trabajar con doctores, vamos a trabajar con los hospitales

196
00:19:58,100 --> 00:20:04,100
y decir, sabes que los datos están bastante incompletos, necesitamos más información o necesitamos

197
00:20:04,100 --> 00:20:11,100
entender por qué es que los datos están distribuidos de manera tan pobre. Antes de que les des un

198
00:20:11,100 --> 00:20:17,100
programa, que ellos van a tomar y decir, ok, no, somos el algoritmo y creemos de manera ciega lo que

199
00:20:17,100 --> 00:20:23,100
el algoritmo nos diga. Entonces, es muy, muy importante que involucremos tanto como podemos aquellos

200
00:20:23,100 --> 00:20:32,100
que tienen experiencia con el dominio. Ok, ahora sí podemos inspeccionar cómo es que están haciendo

201
00:20:32,100 --> 00:20:38,100
los coeficientes de modelo entrenado. En este caso, podemos ver si estudio, por ejemplo, cuáles son

202
00:20:38,100 --> 00:20:45,100
los coeficientes que el modelo está utilizando. En este caso, el modelo está utilizando como uno

203
00:20:45,100 --> 00:20:50,100
de los coeficientes más pesados si el paciente estuvo admitido en el hospital en algún momento,

204
00:20:50,100 --> 00:20:56,100
seguido por si tuvo alguna emergencia, cuánto tiempo estuvo en el hospital, el número de diagnósticos,

205
00:20:56,100 --> 00:21:02,100
si el paciente era 60 años o mayor y cuál medicamento estaba usando. Y conforme voy viendo

206
00:21:02,100 --> 00:21:08,100
los coeficientes, puedo ver que, por ejemplo, el aspecto de la data o de los datos que menos utilizó

207
00:21:08,100 --> 00:21:14,100
fue si su diagnóstico tenía que ver con problemas respiratorios. En este caso, tiene sentido porque

208
00:21:14,100 --> 00:21:19,100
estamos estudiando pacientes con diabetes, entonces en este caso no tendría mucho sentido que el algoritmo

209
00:21:19,100 --> 00:21:26,100
utilicera eso como uno de los parámetros. Ok, pero aquí es cuando entra Fairlearn y aquí podemos estudiar.

210
00:21:26,100 --> 00:21:31,100
Ok, bueno, ya vimos que el algoritmo si solo utilizó PsychicLearn y una regresión logística,

211
00:21:31,100 --> 00:21:39,100
veremos que el algoritmo solo está diciendo 50% con certidumbre que pacientes recomendar,

212
00:21:39,100 --> 00:21:44,100
pero todavía no hemos estudiado cómo está clasificando a los pacientes de acuerdo a su raza,

213
00:21:44,100 --> 00:21:51,100
es lo que vamos a estudiar enseguida. Entonces con MetricFrame podemos evaluar si hay prejuicio

214
00:21:51,100 --> 00:21:59,100
en el algoritmo entrenado. Entonces, si recuerdan al principio de esta charla, pensé que como podemos

215
00:21:59,100 --> 00:22:05,100
evaluar los daños es por medio de gananzas y pérdidas y contando el número de falsos positivos

216
00:22:05,100 --> 00:22:11,100
y falsos negativos. En este caso estamos clasificando a pacientes que deben ser tomados como pacientes

217
00:22:11,100 --> 00:22:18,100
de algo riesgo, entonces en este caso una ganancia es que un paciente que de verdad necesita

218
00:22:18,100 --> 00:22:23,100
tratamiento se ha recomendado al programa, pero una pérdida por ejemplo es que alguien que

219
00:22:23,100 --> 00:22:28,100
necesite el tratamiento no se ha elegido para el programa, esa es la pérdida.

220
00:22:28,100 --> 00:22:33,100
Y en este caso tenemos que pensar cuál es el falso positivo y cuál es el falso negativo,

221
00:22:33,100 --> 00:22:38,100
un falso positivo por ejemplo es alguien que necesita la ayuda y que no lo está recibiendo,

222
00:22:38,100 --> 00:22:43,100
un falso positivo es alguien que es recomendado para el programa pero que no necesita la ayuda.

223
00:22:43,100 --> 00:22:49,100
Entonces aquí tenemos que pensar cuáles son las consecuencias y quién es el que recibe el mayor

224
00:22:49,100 --> 00:22:56,100
impacto de ser clasificado de manera errónea. Si pensamos por ejemplo que si alguien que no necesita

225
00:22:56,100 --> 00:23:02,100
la ayuda recibe la ayuda, cuál es el costo, el costo es a lo mejor para el hospital porque están

226
00:23:02,100 --> 00:23:07,100
dando tratamiento a alguien que no lo necesita, pero cuál es la consecuencia de alguien que

227
00:23:07,100 --> 00:23:12,100
necesita ayuda y no la está recibiendo, creo que esa es una consecuencia muchísimo más grande,

228
00:23:12,100 --> 00:23:16,100
porque quiere decir que esa persona necesita que la traten y no está recibiendo la ayuda,

229
00:23:16,100 --> 00:23:20,100
a lo mejor tiene complicaciones y no está recibiendo ese tratamiento.

230
00:23:20,100 --> 00:23:25,100
Entonces lo que vamos a hacer con metric frame es que vamos a evaluar los falsos negativos

231
00:23:25,100 --> 00:23:31,100
y vamos a evaluar también qué también están siendo elegidos y lo que tenemos aquí por ejemplo

232
00:23:31,100 --> 00:23:38,100
si les muestro el código, este ya es directamente de Fairlearn, este metric frame es un submódulo

233
00:23:38,100 --> 00:23:44,100
dentro de Fairlearn que puede decir, bueno quiero que me digas qué tan bien elegidos están siendo

234
00:23:44,100 --> 00:23:49,100
los pacientes y si quiero cambiar por ejemplo falso positivo, vamos a poner falso positivo.

235
00:23:49,100 --> 00:24:03,100
Podemos elegir falso positivo o falso negativo y lo que Fairlearn nos va a dar es una tablita

236
00:24:03,100 --> 00:24:08,100
que nos dice más o menos, ya está, más o menos cuántos están siendo evaluados de

237
00:24:08,100 --> 00:24:20,100
manera correcta o incorrecta. En este caso yo elegí la raza como la característica sensible,

238
00:24:20,100 --> 00:24:24,100
en este caso quiero asegurarme de que como sea que el algoritmo está haciendo las cosas,

239
00:24:24,100 --> 00:24:31,100
que yo pueda evaluar si lo está haciendo de manera justa o injusta en cuanto a su raza.

240
00:24:31,100 --> 00:24:35,100
Se deja nada más que han visto tantito y me muevo a la siguiente positiva.

241
00:24:39,100 --> 00:24:44,100
Entonces con metric frame podemos evaluar más o menos cómo es que cada uno de los grupos

242
00:24:44,100 --> 00:24:51,100
está siendo evaluado, por ejemplo para los falsos negativos, estamos viendo que los grupos other

243
00:24:51,100 --> 00:24:55,100
y unknown que fueron los grupos que nos llamaron la atención en términos de cómo están siendo

244
00:24:55,100 --> 00:25:03,100
tratados en el hospital, son elegidos normalmente con un porcentaje menor, quiere decir que el algoritmo

245
00:25:03,100 --> 00:25:09,100
está ahora enforzando el mismo prejuicio, cuando en el hospital estamos viendo que normalmente llegan

246
00:25:09,100 --> 00:25:13,100
al hospital pero no los reciben, ahora el algoritmo no los está seleccionando de la misma manera,

247
00:25:13,100 --> 00:25:19,100
entonces esa es una bandera roja, quiere decir que el algoritmo está trayendo los mismos problemas.

248
00:25:19,100 --> 00:25:27,100
Si vemos los falsos negativos, por ejemplo, vemos que la mayoría tienen números 0.4, 0.5

249
00:25:27,100 --> 00:25:34,100
y los grupos otro o no conocido tienen números 0.6, quiere decir que estos dos grupos tienen

250
00:25:34,100 --> 00:25:37,100
una mayor cantidad de falsos negativos.

251
00:25:41,100 --> 00:25:47,100
Ahora de aquí, por ejemplo la pregunta es, ya sabemos que el algoritmo está enforzando los prejuicios,

252
00:25:47,100 --> 00:25:53,100
pero bueno, ¿qué hacemos ahora? Ya que sabemos esa cuestión, es entonces cuando podemos utilizar

253
00:25:53,100 --> 00:26:01,100
Threshold Optimizer y lo que es muy bonito de este paquete es que está bien conectado con Scikit Learn,

254
00:26:01,100 --> 00:26:06,100
lo que quiere decir que por ejemplo, si yo quiero, lo que tengo que hacer es que si yo quiero decir,

255
00:26:06,100 --> 00:26:12,100
bueno, ya sé que mi pipeline tiene un problema, ¿cómo es que lo arreglo con Threshold Optimizer?

256
00:26:12,100 --> 00:26:18,100
Bueno, Threshold Optimizer puede decir que dame el pipeline que tiene ese problema y vamos a arreglar,

257
00:26:18,100 --> 00:26:24,100
por ejemplo, cómo es que está el, cómo vamos a mejorar ese número de falsos negativos.

258
00:26:24,100 --> 00:26:28,100
Y puedo cambiar aquí también, por ejemplo, falso positivo, dependiendo en cuál es la situación.

259
00:26:28,100 --> 00:26:34,100
Y después de eso, va a pasar lo mismo, puedo volver a reentrenar el algoritmo,

260
00:26:34,100 --> 00:26:39,100
pero con Threshold Optimizer para mejorar el número de falsos positivos o falsos negativos.

261
00:26:39,100 --> 00:26:48,100
Entonces, en este caso, puedo hacer un reentrenamiento de los datos, pero con Threshold.

262
00:26:48,100 --> 00:26:53,100
Y como vimos anterior, utilizamos Standard Scaler y Regresión Logística,

263
00:26:53,100 --> 00:27:00,100
pero esta vez estamos reentrenando en el pipeline original, pero mejorando la cantidad de falsos positivos

264
00:27:00,100 --> 00:27:03,100
o falsos negativos, dependiendo de cómo queremos enfocarnos.

265
00:27:03,100 --> 00:27:07,100
Entonces, al final, podemos otra vez estudiar cómo es que les está yendo.

266
00:27:07,100 --> 00:27:12,100
Y podemos comparar, por ejemplo, cuál es la selección, número de selección,

267
00:27:12,100 --> 00:27:19,100
número de falsos positivos, número de aquellos que son balanceados.

268
00:27:19,100 --> 00:27:24,100
Y podemos comparar antes y después, por ejemplo, cuando una vez que aplicamos Threshold Optimizer,

269
00:27:24,100 --> 00:27:34,100
vemos un incremento en la selección, por ejemplo, del grupo otro, el grupo no conocido.

270
00:27:34,100 --> 00:27:40,100
Si estudio, por ejemplo, el número de aquellos que fueron evaluados como falsos negativos,

271
00:27:40,100 --> 00:27:47,100
antes de aplicar Threshold Optimizer, estoy observando que el número de falsos negativos era 0.65

272
00:27:47,100 --> 00:27:49,100
y disminuyó a 0.44.

273
00:27:49,100 --> 00:27:54,100
Entonces, lo que esto nos da básicamente, nos da herramientas como científicos de datos,

274
00:27:54,100 --> 00:28:00,100
primero para evaluar dónde hay problemas, nos da herramientas para evaluar si nuestro algoritmo

275
00:28:00,100 --> 00:28:05,100
está reenforzando un prejuicio, pero más importante, nos da herramientas para mejorar

276
00:28:05,100 --> 00:28:08,100
esos prejuicios en nuestro algoritmo.

277
00:28:08,100 --> 00:28:13,100
Y creo que ya me queda poquitito tiempo, así que vamos a saltar al final.

278
00:28:13,100 --> 00:28:15,100
Entonces, las observaciones finales.

279
00:28:15,100 --> 00:28:20,100
Primero que nada, motivamos la importancia de la ética en la inteligencia artificial,

280
00:28:20,100 --> 00:28:24,100
sobre todo cuando estamos tratando con problemas que impactan vidas humanas.

281
00:28:24,100 --> 00:28:31,100
Estudiamos que podemos cuantificar esos prejuicios por medio de ganancias o pérdidas

282
00:28:31,100 --> 00:28:37,100
y podemos cuantificar esas ganancias o pérdidas por medio de falsos positivos y falsos negativos.

283
00:28:37,100 --> 00:28:42,100
El último paso que utilizamos, normalmente en estas charlas siempre es como mucha charla teórica,

284
00:28:42,100 --> 00:28:45,100
pero luego la pregunta es cómo lo hacemos en la vida real.

285
00:28:45,100 --> 00:28:50,100
Y lo que les estoy presentando es una herramienta que es, primero que nada, compatible con Scikit-Learn

286
00:28:50,100 --> 00:28:55,100
y que les da herramientas a los científicos de datos, primero para evaluar si un algoritmo

287
00:28:55,100 --> 00:28:58,100
reenforza un prejuicio y segundo para mejorarlo.

288
00:28:58,100 --> 00:28:59,100
OK.

289
00:28:59,100 --> 00:29:05,100
Y antes de que termine, lo último con lo que les voy a dejar es la documentación de Fairlearn.

290
00:29:05,100 --> 00:29:10,100
Si quieren aprender más, Fairlearn está activo.

291
00:29:10,100 --> 00:29:16,100
Normalmente tienen llamadas en Discord una vez a la semana si tienen curiosidad de aprender más.

292
00:29:16,100 --> 00:29:22,100
Yo empecé, yo aprendí de ellos en un Mentored Sprint el año pasado y puedo decir que son súper amigables

293
00:29:22,100 --> 00:29:25,100
y siempre quieren involucrar a la gente.

294
00:29:25,100 --> 00:29:30,100
Empecé hace como dos, tres meses a traducir la documentación porque creo que hay muchísimas aplicaciones

295
00:29:30,100 --> 00:29:34,100
con este problema en países latinoamericanos, hispanohablantes.

296
00:29:34,100 --> 00:29:38,100
Así es que si hay alguien que le gustaría colaborar conmigo para traducir,

297
00:29:38,100 --> 00:29:40,100
estoy muy, muy feliz de colaborar.

298
00:29:40,100 --> 00:29:42,100
Y eso es todo para mí.

299
00:29:42,100 --> 00:29:43,100
Gracias.

300
00:29:43,100 --> 00:29:44,100
Gracias.

