1
00:00:00,000 --> 00:00:12,160
Hi, everyone. We'll start off with our next talks. We have Jigya Sir Grover and Rishabh

2
00:00:12,160 --> 00:00:17,000
Mishra, who will be talking about sculpting data for machine learning. We do not have

3
00:00:17,000 --> 00:00:22,240
Q&As during or after the session, but if you would like to ask questions to the speakers,

4
00:00:22,240 --> 00:00:29,080
feel free to meet them in the hallway. Yeah, that's pretty much it. Over to you guys.

5
00:00:29,080 --> 00:00:37,800
Hey, everyone. I'm Jigya Sir Grover, and I work as a machine learning engineer in the

6
00:00:37,800 --> 00:00:42,680
performance ads ranking team at Twitter. And my co-presenter, Rishabh Mishra, he works in

7
00:00:42,680 --> 00:00:48,200
the content quality team at Twitter. Today, we'll be talking about how we can sculpt the

8
00:00:48,200 --> 00:00:54,600
right kind of data for our machine learning models. The last couple of years have seen an

9
00:00:54,600 --> 00:01:00,000
immense growth of machine learning in multiple domains, from influencing our shopping carts to

10
00:01:00,000 --> 00:01:05,120
cars self-driving themselves around the town. Unquestionably, machine learning is the most

11
00:01:05,120 --> 00:01:11,920
used and the most abused and the most used subdomain of artificial intelligence presently.

12
00:01:11,920 --> 00:01:18,120
It is being wielded in improving healthcare and advancing warfare, scrutinizing your resume to

13
00:01:18,120 --> 00:01:23,160
determining your credit worthiness, in creating music and meaningful lyrics, to synthesizing

14
00:01:23,160 --> 00:01:28,680
pictures and videos of people non-existent on this planet. Regardless of our fascination or

15
00:01:28,680 --> 00:01:34,000
loathe for it, it is influencing our decision making power heavily and is dominating our lives

16
00:01:34,000 --> 00:01:42,320
every millisecond. So dominant leaps are leaving the scientists, investors, policymakers, business

17
00:01:42,320 --> 00:01:48,000
leaders and audience bedazzled that human-like intelligence in machines might be just around

18
00:01:48,000 --> 00:01:53,720
the corner. Nonetheless, progress in machine learning has been very impressive, but there's

19
00:01:53,720 --> 00:02:00,160
a lot of pending explanation and examination which keeps this research going on. So for the

20
00:02:00,160 --> 00:02:05,880
state-of-the-art machine learning algorithms to work their magic, it's important to focus on three

21
00:02:05,880 --> 00:02:17,360
key dimensions, well calibrated data, sophisticated algorithms and efficient computation. Since we

22
00:02:17,360 --> 00:02:23,720
have progressed from the rule-based approach to a more data-driven approach, it goes without

23
00:02:23,720 --> 00:02:29,320
saying that machine learning algorithms are trained to capture implicit information from the data

24
00:02:29,320 --> 00:02:35,840
provided. It is true that these approaches are very data hungry and without going into a lot of

25
00:02:35,840 --> 00:02:41,320
details, for example deep learning algorithms, they have a lot of parameters that need to be

26
00:02:41,320 --> 00:02:46,400
tuned and therefore need a lot of data in order to come up with a more generalizable model. So in

27
00:02:46,400 --> 00:02:51,680
that sense, having a lot of data is the key to coming up with good training sets for those

28
00:02:51,680 --> 00:03:02,680
approaches. So look at this pyramid which is the AI hierarchy of needs. It is drawn parallel to

29
00:03:02,680 --> 00:03:07,960
Maslow's hierarchy of human needs by Monica Rogati, who's a renowned data scientist and AI

30
00:03:07,960 --> 00:03:14,400
advisor. She puts forward that wordy data collection forms the foundation of this pyramid of the AI

31
00:03:14,480 --> 00:03:20,640
hierarchy of needs. Data literacy, data collection and data flow. These forms the basic needs which

32
00:03:20,640 --> 00:03:26,920
must be satisfied in order for us to achieve self-actualization or Nirvana, which in our case

33
00:03:26,920 --> 00:03:37,160
would be the attainment of artificial intelligence. So nonetheless, the quantity of data we also have

34
00:03:37,160 --> 00:03:41,960
to take care about the type of data that we feed into the algorithms, which has a profound effect

35
00:03:42,000 --> 00:03:49,040
on the success of the algorithm. As we say garbage in, garbage out. So machine learning models are

36
00:03:49,040 --> 00:03:54,520
essentially as good or as bad as the data that you have. Peter Norvig has put it in very clearly

37
00:03:54,520 --> 00:04:03,040
that more data beats clever algorithms, but better data beats more data. So in that case,

38
00:04:03,040 --> 00:04:10,000
we should talk about data curation. And first of all, let's talk about academia. Core machine

39
00:04:10,040 --> 00:04:16,360
learning and data science research groups in academia are oriented more towards novelty and

40
00:04:16,360 --> 00:04:22,760
advancing the fields, many a times weighing it much more than the money making logic or performance

41
00:04:22,760 --> 00:04:28,680
scaling. In this attempt to work on new problems, finding collaborators from pertinent domains and

42
00:04:28,680 --> 00:04:34,160
seeking funding for research groups are not the only obstacles they face. They also have to look

43
00:04:34,160 --> 00:04:40,840
for relevant data sources in the absence of in-house data, contrary to corporate giants like Facebook,

44
00:04:40,840 --> 00:04:46,960
Google, who have access to tons of data. So these established organizations on the other hands,

45
00:04:46,960 --> 00:04:52,880
seldom have obstacles in obtaining computation power, hiring folks with expertise in corresponding

46
00:04:52,880 --> 00:04:58,920
domains or accessing relevant data. The challenge, however, comes in scaling up the solutions to

47
00:04:58,920 --> 00:05:05,680
massive user base. Any organization that would have a lot of unstructured data available on its

48
00:05:05,680 --> 00:05:13,080
hand in the form of raw logs, however, developing data processing pipeline for them remains a task.

49
00:05:13,080 --> 00:05:19,760
And for creating such resilient pipelines, apart from relevant technology, we also require the

50
00:05:19,760 --> 00:05:27,280
skills to identify and curate meaningful data sets, unbiased data sets, from the sea of unstructured

51
00:05:27,280 --> 00:05:33,720
data that we have. And that's where we go for search of data sources. And you should be able to take it.

52
00:05:33,720 --> 00:05:48,000
So, yeah, so there can be like different approaches to building like a good quality data set. And like

53
00:05:48,000 --> 00:05:54,720
we'll be like giving you guys some overview on like how we can approach that, basically some

54
00:05:54,720 --> 00:06:00,920
pointers regarding that. But before we can do that, like, let me introduce you to three of our data

55
00:06:00,920 --> 00:06:07,240
sets that we have collected ourselves. And these are available on Kaggle. So feel free to explore them.

56
00:06:07,240 --> 00:06:17,920
So the first data set is clothing fed data set. So this data set was collected from our e-commerce

57
00:06:17,920 --> 00:06:30,040
website called ModCloth. So there people kind of report their transactions that they have done,

58
00:06:30,040 --> 00:06:38,920
and what all like they have liked about the product, and specifically whether the product was fitting

59
00:06:38,920 --> 00:06:49,560
to them or not. So the data which we have here is user ID, product ID, feedback, size purchased,

60
00:06:49,560 --> 00:06:55,680
ratings and reviews of the product that has been purchased, and some sort of like customer

61
00:06:55,680 --> 00:07:06,200
measurements. And second data set we have is sarcasm detection data set. So these we have collected

62
00:07:06,200 --> 00:07:17,960
from two websites, so the Onion and HuffPost. So past studies in sarcasm detection were mostly

63
00:07:17,960 --> 00:07:24,040
done from like extracting data set from Twitter, but they tend to be like noisy in terms of like

64
00:07:24,040 --> 00:07:30,560
the labels they have and the language they contain. So people use like lots of abbreviations, they're

65
00:07:30,560 --> 00:07:37,880
like spelling mistakes. So we collected this high quality data set from two news websites. So the

66
00:07:37,880 --> 00:07:45,200
Onion contains like, if you know about it, like sarcastic versions of like the current events going

67
00:07:45,200 --> 00:07:54,560
on. And the HuffPost contains like real news reports. So we can treat them as like non-sarcastic. So we

68
00:07:54,560 --> 00:08:04,760
kind of combined them and made this like one good data set. And lastly, like we also have like news

69
00:08:04,760 --> 00:08:12,240
category data set. So this was extracted just from HuffPost. So these contains like real news

70
00:08:12,240 --> 00:08:20,600
articles. And we have details like headline, preview text, tags, authors, and date published.

71
00:08:20,640 --> 00:08:30,160
So essentially, if we have like news from different categories that could potentially help us in

72
00:08:30,160 --> 00:08:46,800
extracting like exciting, sorry, I'm just kind of blanking out, exciting, yeah, exciting findings.

73
00:08:47,000 --> 00:08:54,320
Sorry. So yeah, I'll be talking about those like later on, like how that is kind of relevant in our

74
00:08:54,320 --> 00:09:03,400
case. So let's talk about searching for the data sources to build good quality data sets. So there

75
00:09:03,400 --> 00:09:10,360
are like basically two types of scenarios that we'll come across. So first would be that we have like

76
00:09:10,360 --> 00:09:17,600
a problem statement in mind that we want to solve using machine learning. So for that case, like we'll

77
00:09:17,600 --> 00:09:25,040
be using a guided search approach. And second case is when let's say we don't have any specific

78
00:09:25,040 --> 00:09:31,080
problem in mind, but we would like to like contribute something like worthwhile to the

79
00:09:31,080 --> 00:09:39,400
community. So in that way, like we'll be using like unguided search approach. So I'll be talking

80
00:09:39,600 --> 00:09:45,160
about like the guided search approach first. So this is where like we have like specific

81
00:09:45,160 --> 00:09:56,520
problem in mind. And so the main challenge for this type of data search approach is that when

82
00:09:56,520 --> 00:10:02,640
we know what problem we have to solve, we have to kind of formally state it out and understand

83
00:10:02,640 --> 00:10:11,640
the problem thoroughly so that we can identify what all data we actually need to collect so

84
00:10:11,640 --> 00:10:16,840
that like we can address the problem. So it could be daunting, but like we'll be sharing some pointers

85
00:10:16,840 --> 00:10:25,840
so that it's like a little bit easier. So first step for that would be defining the problem formally.

86
00:10:25,840 --> 00:10:35,840
So what I mean by that is let's say we are trying to solve this size recommendation problem where

87
00:10:35,840 --> 00:10:41,440
we have like a customer and we have to recommend like a clothing product to them so that the size

88
00:10:41,440 --> 00:10:51,120
for that fits kind of perfectly to the user. So defining the problem formally means like identifying

89
00:10:51,120 --> 00:10:57,480
all the aspects of it and what are like all the variables involved. So for size recommendation

90
00:10:57,480 --> 00:11:05,840
problem, like we can say that given a user and a product with different catalog sizes, we have to

91
00:11:05,840 --> 00:11:13,800
recommend a product size that fits the user's best. So this essentially kind of lays out all

92
00:11:13,800 --> 00:11:24,120
the variables that are kind of involved in solving this kind of problem. So after we have

93
00:11:24,120 --> 00:11:31,160
done that, like we have to determine essential data signals that are involved in like solving

94
00:11:31,160 --> 00:11:41,080
this kind of problem. So in this case, like as you can see in the formal definition, so we'd see

95
00:11:41,320 --> 00:11:51,720
probably user ID, product ID, size of the product purchased and the feedback from the user are kind

96
00:11:51,720 --> 00:12:01,360
of essential things that would help us in like addressing this kind of problem. So basically

97
00:12:01,360 --> 00:12:10,360
these will then contain, these will constitute all the essential signals that we need to like

98
00:12:10,360 --> 00:12:15,600
address this problem and we could have like other types of data as well like product category or

99
00:12:15,600 --> 00:12:24,160
price, but they could help in solving the problem in a better way, but they are not like absolutely

100
00:12:24,160 --> 00:12:33,200
essential. Then next is like data volume consideration. So whenever we are looking for

101
00:12:33,200 --> 00:12:39,600
the data source, we have to see that we are able to build like a sufficiently large data set. So

102
00:12:39,600 --> 00:12:47,120
for example, if we go to some websites that don't have like enough reviews on the product or let's

103
00:12:47,120 --> 00:12:55,680
say if you are looking at like news website, they don't have enough historical articles that

104
00:12:55,680 --> 00:13:03,920
they have published. So those might not be like very good kind of data sources for us to construct

105
00:13:03,920 --> 00:13:11,480
our data sets and the reason is like if we don't have like a good enough data set where each let's

106
00:13:11,480 --> 00:13:17,640
say entity has sufficient amount of information available, our machine learning models might not

107
00:13:17,640 --> 00:13:26,600
be able to like learn good parameters from such like underpowered data set. So yeah, this was it

108
00:13:26,600 --> 00:13:34,240
for guided search. Some pointers that can help if you have like some problem in mind, so like you

109
00:13:34,240 --> 00:13:41,240
can follow maybe these pointers to see like how you can collect like a good quality data set.

110
00:13:41,240 --> 00:13:49,880
So now the next scenario is like unguided search. This is let's say for like creative souls who,

111
00:13:49,880 --> 00:13:56,280
so this could be like similar to let's say if you want to do research and you're looking for the

112
00:13:56,280 --> 00:14:02,260
problems to solve, so this could be like drawn parallel to that. So if you want are looking to

113
00:14:02,260 --> 00:14:08,620
make like data set contributions and don't have like specific problem in mind, like you can try

114
00:14:08,620 --> 00:14:16,940
to follow this approach and see if like it provides any help and we'll be referring to the news category

115
00:14:16,940 --> 00:14:26,540
data set for this because that was like specifically collected without having any problems in mind and

116
00:14:26,540 --> 00:14:36,380
then kind of backtracking to see how it can be like useful. So yeah, so the biggest challenge for

117
00:14:36,380 --> 00:14:43,140
unguided search is that it has like lots of uncertainty. So we don't know like what problem

118
00:14:43,140 --> 00:14:49,740
we are going to solve, so there's no like proper structure in the way like we could approach our

119
00:14:49,740 --> 00:14:55,180
search operation. Let's say when I say search operations, it's just like Googling on the web.

120
00:14:55,180 --> 00:15:03,020
And so although it makes things a little difficult, but the pointers that I'll be just talking about

121
00:15:03,020 --> 00:15:12,780
will provide some structure to that. So first like we have to see whether the data that we are trying

122
00:15:12,780 --> 00:15:19,340
to collect, whether it can address some practical problem or not and whether it has the ability to

123
00:15:19,340 --> 00:15:29,060
lead us to some fascinating insights or not. So yeah, to take an example like for the news category

124
00:15:29,060 --> 00:15:37,780
data set, so we can train a classifier on this data set and it can help us identify some writing

125
00:15:37,780 --> 00:15:44,140
style of the news articles, whether some writing style belongs to like a political category or humorous

126
00:15:44,140 --> 00:15:53,500
tone or it's based on like fashion trends or something like that. And it can also help us to tag

127
00:15:53,500 --> 00:16:02,940
and track news articles and understand how basically the writing styles kind of differ and so on.

128
00:16:02,940 --> 00:16:10,900
Yeah. So then the next thing after we have determined, let's say that something is worth

129
00:16:10,900 --> 00:16:20,940
estimating, we have to see that we have sufficient metadata available around that. So having enough

130
00:16:20,940 --> 00:16:28,380
metadata will help your machine learning models to draw associations between like different

131
00:16:28,380 --> 00:16:35,340
attributes and will help in ultimately like predicting the target. For example, let's say the

132
00:16:35,340 --> 00:16:42,900
news category. So another example would be like, let's say if you are trying to predict like a

133
00:16:42,900 --> 00:16:51,020
product price from just like user ID, product ID, so that might not go very well and we need like

134
00:16:51,020 --> 00:16:59,540
sufficient data around the product, let's say brand of it and the fabric of it for us to able to like

135
00:16:59,540 --> 00:17:10,500
give like a meaningful predictions. Now data volume requirement applies here as well, but this has like

136
00:17:10,500 --> 00:17:16,540
a relaxed constraint because we are not looking to solve like any specific problem we have in mind.

137
00:17:16,540 --> 00:17:25,500
So we can always like adjust the problem definition according to what data we have. So for example,

138
00:17:25,500 --> 00:17:32,180
in the news category data set, like we had many instances where let's say there was some news category

139
00:17:32,180 --> 00:17:39,100
which had less than 100 articles. So if you have some underpowered category, our machine learning

140
00:17:39,100 --> 00:17:46,620
model might not be able to learn well, but that didn't stop us from making it a high quality data set

141
00:17:46,620 --> 00:17:53,620
because our purpose was not specifically to predict some category, just that like we have like different

142
00:17:53,620 --> 00:18:00,820
categories and if those have like sufficient data, then only like we'll be able to train good models

143
00:18:00,820 --> 00:18:13,060
on it and make the model useful. So yeah, so based on the data availability, we can like always adjust

144
00:18:13,060 --> 00:18:21,860
the problem statement. And finally, like this is just like, it's like a critical point, like we should

145
00:18:21,860 --> 00:18:28,420
always, because there is some uncertainty involved and we don't know what all is out there. So if we

146
00:18:28,420 --> 00:18:34,660
have some ideas, always try to check if something of that sort is already not on the web. So we don't

147
00:18:34,660 --> 00:18:41,700
want to like duplicate the efforts and if something is already available, we just want to see, let's say

148
00:18:41,700 --> 00:18:50,460
what are the shortcomings of those and try to kind of improve on that. Now, yeah, so that was some

149
00:18:50,860 --> 00:18:58,620
pointers on like how to structure your search operation and now after, let's say we have collected

150
00:18:58,620 --> 00:19:06,220
the data, like we have found out the data sources, extracted the data out of it, that is just the

151
00:19:06,220 --> 00:19:13,100
beginning. So that at that time, we have like a raw data available to us, but still we have to like

152
00:19:13,100 --> 00:19:21,500
kind of groom it for it to be like very useful to our machine learning models. So we'll be talking

153
00:19:21,500 --> 00:19:29,660
about some of the techniques that can be used to kind of groom your data set. So first technique is

154
00:19:29,660 --> 00:19:40,340
data trimming. So data trimming, so I'll take an example from the clothing fit data set. So there

155
00:19:40,380 --> 00:19:46,540
could be like some reviews, so it's not always the case that all the essential signals are available

156
00:19:46,540 --> 00:19:55,420
in each transaction. Like the data could be very noisy and if we don't have like a transaction with

157
00:19:55,420 --> 00:20:04,180
fit feedback available or size purchase, those are not really useful. So and then our job becomes to

158
00:20:04,180 --> 00:20:10,500
kind of let's say sanitize our data set, try to trim all the records which won't be ultimately

159
00:20:10,500 --> 00:20:18,060
helpful in predicting what we want to predict. Basically, let's say I recommend whether a

160
00:20:18,060 --> 00:20:25,220
particular product fits a customer or not. So we have to kind of remove those and one thing to keep

161
00:20:25,220 --> 00:20:33,580
in mind is after we have done data trimming, like we still have to ensure that the volume is kind of

162
00:20:33,580 --> 00:20:44,460
good enough. Let's say we are not like drastically reducing the data volume. So yeah, for the next

163
00:20:44,460 --> 00:20:53,260
few set of data preparation techniques, Digasa will be talking more about that. So we've already

164
00:20:53,260 --> 00:20:59,780
talked about how we can collect a data set and build a data set from different sources and in

165
00:20:59,900 --> 00:21:05,140
an ideal scenario, we would assume that we are using a single web source to create that data set.

166
00:21:05,140 --> 00:21:10,300
For example, for the clothing fit data set, we use mod-cloth. But there would be many circumstances

167
00:21:10,300 --> 00:21:15,980
where we would have to gather the different attributes from different kind of sources to be

168
00:21:15,980 --> 00:21:22,700
it for a more inclusive, meaningful and a valuable data set. So coalescing these data from separate

169
00:21:22,700 --> 00:21:28,580
sets, separate sources into one unified view is what is called data integration. For example,

170
00:21:28,740 --> 00:21:34,580
let's talk about vertical integration. Remember the data set that we had for sarcasm detection.

171
00:21:34,580 --> 00:21:40,820
There was data collected from the onion which was non-sarcastic and then we had data collected from

172
00:21:40,820 --> 00:21:45,820
half-posts which was non-sarcastic. So in that case, the attributes collected from both these

173
00:21:45,820 --> 00:21:50,620
sources were exactly the same and all we needed was to integrate them in a vertical sense because

174
00:21:50,620 --> 00:21:55,820
the attributes were the same. So we created a final data set with the label, say, one sarcastic

175
00:21:55,820 --> 00:22:02,900
and non-sarcastic by combining the data from these two sources. This is called vertical because

176
00:22:02,900 --> 00:22:08,700
the attributes that belong, for example, the headline, the author and is it sarcastic or not

177
00:22:08,700 --> 00:22:18,620
sarcastic were kind of same across both the sources. The other could be horizontal integration,

178
00:22:18,620 --> 00:22:24,000
which is when, for example, to create one single record of the data set, you have to combine

179
00:22:24,000 --> 00:22:29,400
sources. For example, let's just assume we are building a data set for different cities and their

180
00:22:29,400 --> 00:22:36,360
attributes. So we would assume fields like state, country, area, population come from one kind of

181
00:22:36,360 --> 00:22:41,680
source and the others like administrative, religion, language festivals come from the other. So in that

182
00:22:41,680 --> 00:22:47,880
case, you would be identifying one city ID and integrating them horizontally. In such case, this

183
00:22:47,880 --> 00:22:53,240
is how you would approach the data integration and it is very important to try and make your data set

184
00:22:53,240 --> 00:22:58,040
as exhaustive as possible to have as many records and attributes. Further down the line, when you do

185
00:22:58,040 --> 00:23:02,960
feature engineering, you can always trim that, but it's always necessary to have a good quality

186
00:23:02,960 --> 00:23:08,840
robust data set. And for the other thing, you also need to do data transformation to make it more

187
00:23:08,840 --> 00:23:14,680
robust, as I said, because there's so much content available on the web these days that it's impossible

188
00:23:14,680 --> 00:23:19,800
to check the correctness of all. For example, in the reviews or the content posted by the publisher,

189
00:23:19,800 --> 00:23:24,320
like the product name, the product sizing, they have to be moderated to some extent. However,

190
00:23:24,320 --> 00:23:31,360
people might use small, large, medium as sizes or someone would use like 12, 13, 14. So it totally

191
00:23:31,360 --> 00:23:36,680
depends. Some people use different, just two letters for identifying the state. Someone writes

192
00:23:36,680 --> 00:23:42,160
completely the state name or someone would just use like month, date, year or someone. So depending

193
00:23:42,160 --> 00:23:48,080
for the date formats. So it's very important to sanitize the data, to transform it so that it's

194
00:23:48,080 --> 00:23:54,080
all in one form for it to be a robust data set. And so to say, the data transformations,

195
00:23:54,080 --> 00:23:59,520
a primary focus is to scrub the data sets raw values to remove redundancies and correct the

196
00:23:59,520 --> 00:24:06,080
language and the logical semantics that you have. So the some, like some techniques for transforming

197
00:24:06,080 --> 00:24:11,280
your data set could be handling special characters, handling grammar, spellings error, encoding the

198
00:24:11,280 --> 00:24:17,400
sizes in numerical order, changing the format of date, etc. So these are just a few cues on how

199
00:24:17,400 --> 00:24:24,640
you can scrub your data set. So as I mentioned that once you have your big data set, with all the

200
00:24:24,640 --> 00:24:29,720
records and with all the attributes, it's necessary to proceed to feature engineering before you input

201
00:24:29,720 --> 00:24:35,680
this data set as is to your machine learning model. So if you consider data to be the crude oil,

202
00:24:35,680 --> 00:24:41,680
features are the fuel that we need. So feature engineering is nothing but applied machine learning.

203
00:24:41,880 --> 00:24:47,920
It is also one of the key techniques and the processes that does not get the due limelight in

204
00:24:47,920 --> 00:24:55,480
the machine learning workflow. Much of the success of machine learning algorithms is attributed to

205
00:24:55,480 --> 00:25:01,040
the success in engineering features from the data so that the learner can understand. This is where

206
00:25:01,040 --> 00:25:07,280
your intuition comes into play and you know, black art and creativity and domain knowledge. So this is

207
00:25:07,280 --> 00:25:12,080
the step that should be considered much more important than the model itself or hyper parameter

208
00:25:12,080 --> 00:25:18,320
tuning and whatever shenanigans people do and because machine learning algorithm as you said,

209
00:25:18,320 --> 00:25:25,760
is as powerful as the data you supply. So you have to take care what your intuition says, domain

210
00:25:25,760 --> 00:25:32,240
knowledge, trial and error. So that is what feature engineering is all about and choosing the right

211
00:25:32,240 --> 00:25:38,480
features in right format can by far boost the performance using even using simpler machine

212
00:25:38,480 --> 00:25:43,680
learning models. So it increases the transparency. You can better explain the model if you have fewer

213
00:25:43,680 --> 00:25:48,180
features, but they are much more powerful. It reduces the need for heavy ensemble learning

214
00:25:48,180 --> 00:25:53,140
techniques and also further reduces the need for hyper parameter tuning and model optimization.

215
00:25:53,140 --> 00:26:00,320
So the bottom line for this entire talk would be that typical machine learning workflow looks

216
00:26:00,320 --> 00:26:05,520
like this. You go from defining a problem statement to developing an intuition of how machine learning

217
00:26:05,520 --> 00:26:11,320
can tackle your problem, get the data, process the data and defining the appropriate success

218
00:26:11,320 --> 00:26:16,880
metrics, choosing the model that you feel would fit your problem the best, deploying the model

219
00:26:16,880 --> 00:26:22,640
and then monitor it. The problem comes when you focus more on defining the success metrics and

220
00:26:22,640 --> 00:26:28,520
choosing appropriate model and just get stuck in this loop. However, the focus should be on how you

221
00:26:28,520 --> 00:26:32,920
source your data, how you create your data set and how you do feature engineering because these

222
00:26:32,920 --> 00:26:38,640
form the part of applied machine learning and can boost your machine learning algorithm by 100x.

223
00:26:38,640 --> 00:26:46,080
And that is what our experience has taught us. Even Andrew Ng, who is a machine learning expert,

224
00:26:46,080 --> 00:26:53,080
has started a campaign for data-centric AI and he says that machine learning model is nothing

225
00:26:53,080 --> 00:26:58,600
but code, like AI is equal to code plus data. And for so many years, we focused on code,

226
00:26:58,600 --> 00:27:04,480
which is like how to create a more complex architecture, how to include more layers by

227
00:27:04,480 --> 00:27:09,320
keeping the data stagnant. But now is the time that we have state of the art algorithms and the

228
00:27:09,320 --> 00:27:13,880
focus should be on data. How can we enrich our data? How can we make more creative features and

229
00:27:13,880 --> 00:27:19,000
how we can have more signals into our data to basically boost the performance of a machine

230
00:27:19,000 --> 00:27:23,960
learning algorithm. So that being on the same lines, we've co-authored a book called

231
00:27:23,960 --> 00:27:29,480
Sculpting Data for Machine Learning. And this is a QR code if anyone wants to check it out on

232
00:27:29,480 --> 00:27:37,680
Amazon. It's also available on Kindle. And it talks about how to identify signals from web sources to

233
00:27:37,680 --> 00:27:45,080
creating your own data set using Python, Selenium and all the other open source tools. And it also

234
00:27:45,240 --> 00:27:50,400
does a synopsis of data preprocessing and feature engineering, introducing you to machine learning

235
00:27:50,400 --> 00:27:56,920
algorithms from a data perspective. And we are glad that is endorsed by leading ML experts in

236
00:27:56,920 --> 00:28:01,160
the industry. And you can read forwards by Lawrence Moroney, Julian McCauley and Mengting

237
00:28:01,160 --> 00:28:06,800
Wong. And that's it. Thank you so much. If you have any questions, please feel free to reach us.

238
00:28:06,800 --> 00:28:07,560
Thank you.

