1
00:00:00,000 --> 00:00:08,040
Good afternoon, everyone.

2
00:00:08,040 --> 00:00:12,500
The next talk is about speeding up data access with PyArrow.

3
00:00:12,500 --> 00:00:14,860
The speaker is Deepak Gupta.

4
00:00:14,860 --> 00:00:18,040
As a reminder, we're not taking questions at the end of the talk.

5
00:00:18,040 --> 00:00:20,200
You can reach out to the speaker after the talk.

6
00:00:20,200 --> 00:00:21,200
Thank you.

7
00:00:21,200 --> 00:00:22,200
Thank you.

8
00:00:22,200 --> 00:00:23,200
123ABC.

9
00:00:23,200 --> 00:00:24,200
Okay.

10
00:00:24,200 --> 00:00:25,200
Hello, everyone.

11
00:00:25,200 --> 00:00:26,200
Good afternoon.

12
00:00:26,200 --> 00:00:27,200
Thank you for joining us.

13
00:00:28,200 --> 00:00:31,200
Thank you for joining this talk.

14
00:00:31,200 --> 00:00:36,200
We are going to talk about speeding up data access with Apache Arrow, the Python library,

15
00:00:36,200 --> 00:00:37,200
PyArrow.

16
00:00:37,200 --> 00:00:41,200
And what is the meaning of data is the new API.

17
00:00:41,200 --> 00:00:46,200
Can you please take this?

18
00:00:46,200 --> 00:00:48,200
So the golden question.

19
00:00:48,200 --> 00:00:51,200
All of you can see my screen.

20
00:00:51,200 --> 00:00:53,200
Right?

21
00:00:53,200 --> 00:00:55,200
Okay.

22
00:00:55,200 --> 00:00:59,200
So this is the third time I'm presenting in Python US.

23
00:00:59,200 --> 00:01:03,200
I thank Python staff and members for this opportunity.

24
00:01:03,200 --> 00:01:05,200
Before I go ahead, some introduction about me.

25
00:01:05,200 --> 00:01:07,200
My name is Deepak Gupta.

26
00:01:07,200 --> 00:01:09,200
I'm also known by the name of Daksha.

27
00:01:09,200 --> 00:01:15,200
I'm a tech consultant and CTO to a company called Hotel Hub, which looks after business,

28
00:01:15,200 --> 00:01:19,200
travel logistics, and based out of India, UK, and France.

29
00:01:19,200 --> 00:01:25,200
Since we are not going to take any question, I have come up with a trick.

30
00:01:25,200 --> 00:01:28,200
My Twitter DM will be open for today.

31
00:01:28,200 --> 00:01:34,200
If you have any question, you can just, you know, write the question without following.

32
00:01:34,200 --> 00:01:35,200
Okay.

33
00:01:35,200 --> 00:01:36,200
So let's start.

34
00:01:36,200 --> 00:01:41,200
I believe all of you have heard about this term called data is new oil.

35
00:01:41,200 --> 00:01:43,200
Right?

36
00:01:43,200 --> 00:01:44,200
Yes, it is.

37
00:01:44,200 --> 00:01:46,200
Data is the new oil.

38
00:01:46,200 --> 00:01:53,200
If you cannot access the data when it needed, you cannot extract the information from the data.

39
00:01:53,200 --> 00:01:58,200
This oil will not be as valuable as it is there.

40
00:01:58,200 --> 00:01:59,200
Okay.

41
00:01:59,200 --> 00:02:06,200
So since we are talking about data, we need to talk about some of the very basic aspects of software development,

42
00:02:06,200 --> 00:02:10,200
which is what is a software program made of?

43
00:02:10,200 --> 00:02:17,200
Irrespective of whether you are writing code in any programming language,

44
00:02:17,200 --> 00:02:22,200
whether you are creating front-end or back-end, any program you write,

45
00:02:22,200 --> 00:02:26,200
each and every software program consists of only two things.

46
00:02:26,200 --> 00:02:32,200
These two things are data and functions which acts on that data.

47
00:02:32,200 --> 00:02:35,200
There is nothing else in a software program.

48
00:02:35,200 --> 00:02:40,200
Unfortunately, in the last four to five decades, whenever we are learning programming,

49
00:02:40,200 --> 00:02:44,200
we are focusing more on functions, not data.

50
00:02:44,200 --> 00:02:47,200
Remember, main, print, hello world.

51
00:02:47,200 --> 00:02:49,200
This is a function.

52
00:02:49,200 --> 00:02:55,200
And whenever we are talking about data, we just create one class, a few variables,

53
00:02:55,200 --> 00:03:03,200
and we learn complete programming language without understanding how to take care of large amount of data within the program.

54
00:03:03,200 --> 00:03:05,200
Remember, within the program.

55
00:03:05,200 --> 00:03:09,200
And this is what we do in our software today.

56
00:03:09,200 --> 00:03:14,200
Fortunately, things have changed and we are focusing more on data today.

57
00:03:14,200 --> 00:03:18,200
And to all the programmers out there and the next generation programmers out there,

58
00:03:18,200 --> 00:03:24,200
think about data as much as you are thinking about functions.

59
00:03:24,200 --> 00:03:27,200
With this, I'm going to talk about Apache Arrow.

60
00:03:27,200 --> 00:03:30,200
Apache Arrow is not a framework or library.

61
00:03:30,200 --> 00:03:32,200
It's a specification.

62
00:03:32,200 --> 00:03:40,200
It's a specification for representing data in memory in a particular format which is called columnar format.

63
00:03:40,200 --> 00:03:43,200
And it is language independent.

64
00:03:43,200 --> 00:03:49,200
But since it is a specification, it doesn't mean that we need to implement that specification.

65
00:03:49,200 --> 00:03:55,200
It has libraries in 12 languages, either direct implementation or binding,

66
00:03:55,200 --> 00:03:59,200
which we can use to use this particular Apache Arrow specification.

67
00:03:59,200 --> 00:04:04,200
So, anyway, we are talking about data and all of you must have seen these kind of pictures

68
00:04:04,200 --> 00:04:09,200
where you are querying things with REST APIs or even with GraphQL.

69
00:04:09,200 --> 00:04:16,200
So when you say that, you know, REST API give me employee information or PyCon attendees information,

70
00:04:16,200 --> 00:04:22,200
how do you know that the information which is coming is actually coming from the API?

71
00:04:22,200 --> 00:04:28,200
It's actually the, you know, attendee information or employee information what you are asking.

72
00:04:28,200 --> 00:04:34,200
There is no way to know, but we trust that since I have called this particular API,

73
00:04:34,200 --> 00:04:38,200
this API will give me what I have asked for.

74
00:04:38,200 --> 00:04:39,200
Okay?

75
00:04:39,200 --> 00:04:42,200
So this is the way data communication happens.

76
00:04:42,200 --> 00:04:46,200
But the problem is that when you are asking for data,

77
00:04:46,200 --> 00:04:47,200
Okay?

78
00:04:47,200 --> 00:04:51,200
So this is the way data communication happens.

79
00:04:51,200 --> 00:04:58,200
But the problem is that if there are two different entities talking to each other at the different ends,

80
00:04:58,200 --> 00:05:01,200
you are seeing a couple of code snippets over here.

81
00:05:01,200 --> 00:05:04,200
One is in Java, one is in Python.

82
00:05:04,200 --> 00:05:06,200
Both are binary compatible.

83
00:05:06,200 --> 00:05:09,200
But still, if they need to talk to each other,

84
00:05:09,200 --> 00:05:15,200
they need to go ahead with something called serialization and deserialization.

85
00:05:15,200 --> 00:05:16,200
Okay?

86
00:05:16,200 --> 00:05:23,200
So that is one cost that needs to be paid even if the data is, you know, binary compatible.

87
00:05:23,200 --> 00:05:29,200
And this problem is not only between two different programming language systems.

88
00:05:29,200 --> 00:05:35,200
This is also between your program and databases.

89
00:05:35,200 --> 00:05:39,200
When you fetch data from a database or you send data to a database,

90
00:05:39,200 --> 00:05:45,200
this problem of serialization and deserialization is there and it's real.

91
00:05:45,200 --> 00:05:46,200
Okay?

92
00:05:46,200 --> 00:05:54,200
So I stumbled across one of a very nice paper online written by Marx and Haines.

93
00:05:54,200 --> 00:05:58,200
It says that don't hold my data hostage.

94
00:05:58,200 --> 00:06:02,200
And this is the second picture in that particular paper.

95
00:06:02,200 --> 00:06:05,200
You can just Google it online.

96
00:06:05,200 --> 00:06:09,200
I do not believe that anyone here can dispute this picture.

97
00:06:09,200 --> 00:06:13,200
There might be some disagreement with, you know, what is the exact time that it will take,

98
00:06:13,200 --> 00:06:17,200
you know, serialization will be less or deserialization will be more.

99
00:06:17,200 --> 00:06:24,200
But there can't be any dispute on the output of this particular picture.

100
00:06:24,200 --> 00:06:27,200
So how to solve this problem?

101
00:06:28,200 --> 00:06:36,200
One way of solving this problem is to do what we call it as data is the new API.

102
00:06:36,200 --> 00:06:39,200
So what is the meaning of data is the new API?

103
00:06:39,200 --> 00:06:44,200
The meaning of data is the new API is that, you know, when I am getting some data,

104
00:06:44,200 --> 00:06:50,200
if it is possible for data for itself to tell me that, okay, what am I?

105
00:06:50,200 --> 00:06:55,200
So I am not sure I am expecting a data, but I am not sure which data it is coming,

106
00:06:55,200 --> 00:06:58,200
whether it is employee data, attendee data, or some other data.

107
00:06:58,200 --> 00:07:04,200
But if data can itself tell me that, okay, I am an employee data and these are the places where I am

108
00:07:04,200 --> 00:07:09,200
and you can directly use me, that is the idea behind data is the new API.

109
00:07:09,200 --> 00:07:18,200
And this is one of the ways to remove what we call it as the cost associated with serialization and deserialization.

110
00:07:19,200 --> 00:07:27,200
And this is where the concept of the in-memory columnar representation of Apache Arrow comes into picture.

111
00:07:27,200 --> 00:07:34,200
So I have created this picture to make you understand that if things are speaking same language,

112
00:07:34,200 --> 00:07:39,200
it is very similar to, you know, I am speaking in English, you are listening in English,

113
00:07:39,200 --> 00:07:43,200
but if this is not the case, there will be some interpreter in between.

114
00:07:43,200 --> 00:07:48,200
And the communication will slow because of that interpreter, okay.

115
00:07:48,200 --> 00:07:52,200
The same thing happens with serialization and deserialization.

116
00:07:52,200 --> 00:07:57,200
So if you are using Apache Arrow compliant entities on both sides,

117
00:07:57,200 --> 00:08:04,200
they don't need to serialize and deserialize because both understand the format of memory data that is coming.

118
00:08:04,200 --> 00:08:10,200
Now, if you are interested in Apache Arrow or PyArrow and if you want to try it out,

119
00:08:10,200 --> 00:08:15,200
this is a homework for all of you. If you want to try it out, this is the first thing you should do.

120
00:08:15,200 --> 00:08:21,200
Take Apache Spark and Pandas. Right now, without Apache Arrow,

121
00:08:21,200 --> 00:08:25,200
if you want to take a Spark data frame to a Pandas data frame,

122
00:08:25,200 --> 00:08:33,200
you need to go through complete conversion of a Spark data frame to Pandas data frame from JVM to Python memory.

123
00:08:34,200 --> 00:08:41,200
You can enable Apache Arrow in Spark with simple parameter and just try it out.

124
00:08:41,200 --> 00:08:47,200
This will give you good enough indication of what Apache Arrow or PyArrow can do for you

125
00:08:47,200 --> 00:08:51,200
or whether it is worth for you to invest some time in it.

126
00:08:51,200 --> 00:08:58,200
You can get some, you know, open source code for this, but this is something I would like you to try it out first if you are interested in this.

127
00:08:58,200 --> 00:09:01,200
That's why I have shown Apache Spark and Pandas.

128
00:09:01,200 --> 00:09:05,200
So, in general, what Apache Arrow is all about?

129
00:09:05,200 --> 00:09:12,200
So, it says that, you know, no matter whether your data is in some blob storage or in some database,

130
00:09:12,200 --> 00:09:22,200
when you want to access that data, you put that data in memory using the in-memory columnar format of Apache Arrow,

131
00:09:22,200 --> 00:09:27,200
using PyArrow or for other programming language using their respective libraries.

132
00:09:27,200 --> 00:09:35,200
And you can do lots of things what you could do with Pandas or R in Apache Arrow in memory itself.

133
00:09:35,200 --> 00:09:45,200
But still, if you cannot do that, you can take the Apache Arrow in memory directly to Pandas, NumPy, R

134
00:09:45,200 --> 00:09:49,200
or in case of other languages in their respective data structures.

135
00:09:49,200 --> 00:09:51,200
You can take this directly.

136
00:09:51,200 --> 00:10:01,200
And the biggest thing is that it is possible to have a zero copy access to the memory that is there with Apache Arrow to Pandas, NumPy or R.

137
00:10:01,200 --> 00:10:07,200
I have written it limited because there are some limitations that it can happen,

138
00:10:07,200 --> 00:10:12,200
zero copy happen only for purely numeric values and the values which are not null.

139
00:10:12,200 --> 00:10:20,200
But it is possible to do a zero copy so you can have a Pandas data frame without occupying even a byte in memory.

140
00:10:20,200 --> 00:10:27,200
As well as you can create memory mapped files for the format that is there in the Apache Arrow

141
00:10:27,200 --> 00:10:32,200
and you can save that file and use it across processes in a memory mapped way.

142
00:10:32,200 --> 00:10:34,200
So this is the complete idea.

143
00:10:34,200 --> 00:10:39,200
The idea is that we should get rid of serialization and deserialization as much as possible.

144
00:10:39,200 --> 00:10:46,200
And to do that, we need to make sure that data itself is capable of telling that what this data is all about.

145
00:10:46,200 --> 00:10:53,200
And Apache Arrow is a specification, PyArrow is a Python library and these are the things you can do with it.

146
00:10:53,200 --> 00:10:58,200
There is a similar picture that is there in Apache Arrow website

147
00:10:58,200 --> 00:11:07,200
which talks about things in a similar way that you don't do copy and convert because it's a very costly operation.

148
00:11:07,200 --> 00:11:12,200
And in case if you are worried about the dark blue color and light blue color,

149
00:11:12,200 --> 00:11:17,200
it is all about whatever is in the disk and whatever you want to do in memory.

150
00:11:17,200 --> 00:11:25,200
So it is all about whenever you are taking your data in memory to do something, to do some computation, to get some data.

151
00:11:25,200 --> 00:11:30,200
If you can do it in an arrow format, you don't need to copy and convert.

152
00:11:30,200 --> 00:11:36,200
That's one part. Let's talk about this Apache Arrow columnar in memory format.

153
00:11:37,200 --> 00:11:44,200
So in a table we have rows of data. Even for PyCon attendees, we will have rows of data.

154
00:11:44,200 --> 00:11:50,200
Traditionally, this is being stored as row wise, like one row is stored in memory,

155
00:11:50,200 --> 00:11:55,200
the next row is stored in memory and third and fourth row is stored in memory.

156
00:11:55,200 --> 00:12:04,200
But the Apache Arrow columnar representation said that we have to store data in a column format.

157
00:12:04,200 --> 00:12:10,200
Why? Because during in-memory computations, vectorization can happen easily.

158
00:12:10,200 --> 00:12:19,200
The SIMD of the CPU architecture can help fetching lots of data in the easiest possible way and faster way.

159
00:12:19,200 --> 00:12:23,200
That's one of the reasons why this is fast.

160
00:12:23,200 --> 00:12:28,200
So this is the columnar format, columnar way of representing data in memory.

161
00:12:28,200 --> 00:12:32,200
Now you might be having a question stating that, you know what,

162
00:12:32,200 --> 00:12:38,200
I understand it, but it looks very similar to, you know, optimized row column or parquet files.

163
00:12:38,200 --> 00:12:44,200
Yes, you are right, but the difference is Apache Arrow is in-memory and directly computable,

164
00:12:44,200 --> 00:12:49,200
which means that you can compute on Apache Arrow in-memory data directly,

165
00:12:49,200 --> 00:12:53,200
which you cannot do in parquet file ORC or for that matter any other database.

166
00:12:53,200 --> 00:12:56,200
You need to extract data to compute something.

167
00:12:56,200 --> 00:13:03,200
And when we are talking about computation, I would like to show you one example of how the, you know,

168
00:13:03,200 --> 00:13:08,200
Pi Arrow computation looks like, but before that let me tell you the kind of computation functions

169
00:13:08,200 --> 00:13:12,200
that are available with Apache Arrow, which you can use with Pi Arrow.

170
00:13:12,200 --> 00:13:17,200
There are vector functions where you can take lots of data and do some calculation.

171
00:13:17,200 --> 00:13:21,200
There are scalar function where you can, you know, take a single value and do some calculation

172
00:13:21,200 --> 00:13:27,200
as well as aggregate functions which are similar to reduce where you can take lots of vectorized data

173
00:13:27,200 --> 00:13:31,200
and come up with a result, for example, sum of elements.

174
00:13:31,200 --> 00:13:35,200
So here is one example which I want to show you about using compute.

175
00:13:35,200 --> 00:13:39,200
In line number one, I have created a NumPy array.

176
00:13:39,200 --> 00:13:44,200
In the line number two, I have created a Pi Arrow array from the NumPy array

177
00:13:44,200 --> 00:13:50,200
and I am using Pi Arrow compute function to get min and max of the array.

178
00:13:50,200 --> 00:13:55,200
In the line number two, since it is purely numeric data, it is zero copy.

179
00:13:55,200 --> 00:14:03,200
It can be zero copy in the reverse way also, just like I have shown in the pictures a few minutes back.

180
00:14:03,200 --> 00:14:07,200
If you are having Apache Arrow, you want to take a NumPy, it could be zero copy.

181
00:14:07,200 --> 00:14:12,200
You want to take a Pandas, it could be zero copy provided some conditions are met.

182
00:14:14,200 --> 00:14:19,200
So let's talk about the main aspect of this talk, which is speeding up data access,

183
00:14:19,200 --> 00:14:24,200
because I said data is new oil, but unless and until you can access the data when you need it,

184
00:14:24,200 --> 00:14:28,200
the oil is not as valuable as it should be.

185
00:14:28,200 --> 00:14:35,200
So I am going to show you three examples and how you can speed up data access and how Pi Arrow comes into picture.

186
00:14:35,200 --> 00:14:39,200
The first example is trivial of reading a CSV file.

187
00:14:39,200 --> 00:14:44,200
So the file which I am reading seems to be a very famous file.

188
00:14:44,200 --> 00:14:53,200
It is a yellow taxi trip data file, January 2021 file, which is approximately 126 MB of size.

189
00:14:53,200 --> 00:14:57,200
So in line number one, I am reading the CSV file using Pandas.

190
00:14:57,200 --> 00:15:02,200
Line number two, I am reading the same CSV file using Pi Arrow.

191
00:15:02,200 --> 00:15:06,200
Line number three, I am converting Pi Arrow to Pandas.

192
00:15:06,200 --> 00:15:08,200
Remember that picture? We can do that.

193
00:15:08,200 --> 00:15:15,200
Line number four is all about making sure that both data frames are same.

194
00:15:15,200 --> 00:15:17,200
Look at the wall timings.

195
00:15:17,200 --> 00:15:27,200
Of course, these timings are from my system, but there is no reason for these timings not to proportionate in any system you run.

196
00:15:27,200 --> 00:15:36,200
So in line number one, you get read CSV, which is wall time 1.7 seconds. Huge.

197
00:15:36,200 --> 00:15:50,200
The same CSV file you can read using Pi Arrow in 129 millisecond, and you can convert that Pi Arrow to Pandas in 65 milliseconds.

198
00:15:50,200 --> 00:16:02,200
So if you sum up second and third, it is around 200 milliseconds, a few numbers here and there, which is still faster than 1.7 seconds it takes.

199
00:16:02,200 --> 00:16:12,200
But again, I want to remind you that you can convert arrows to Pandas, but you still have to make sure that there are numerical values,

200
00:16:12,200 --> 00:16:18,200
and there are some values like date and time, which is having a different way of representing in arrow tables.

201
00:16:18,200 --> 00:16:22,200
So this is what you can do in approximately 200 seconds.

202
00:16:22,200 --> 00:16:28,200
And again, the output of both is true. Both data frames are same.

203
00:16:28,200 --> 00:16:37,200
So this is about reading a CSV file. You can read CSV file fast, even if you consider converting it to data frame at a later point of time.

204
00:16:37,200 --> 00:16:42,200
Second thing is reading a parquet file.

205
00:16:42,200 --> 00:16:52,200
So for parquet files and this Apache Arrow, this is kind of a match made in heaven. Both are columnar representation.

206
00:16:52,200 --> 00:17:02,200
But there are some compression thing that happens with parquet file, and when Arrow loads it, it needs to decompress that.

207
00:17:02,200 --> 00:17:09,200
So here is the wall time between these two. So if you read it through Pandas, it is 600 milliseconds approximately,

208
00:17:09,200 --> 00:17:16,200
and if you read it through Pi Arrow, it is approximately 100 milliseconds.

209
00:17:17,200 --> 00:17:23,200
And the most important example which I am going to show you now is using memory mapped files.

210
00:17:23,200 --> 00:17:35,200
So in Pandas, what happens is that if you have 16 gigs of RAM, you will make sure that your Pandas memory doesn't go beyond 8 or something, 10.

211
00:17:35,200 --> 00:17:38,200
Good will be 5.

212
00:17:38,200 --> 00:17:48,200
But with this particular memory mapped file which you can create for Apache Arrow, you can read 50 gigs of data frame using Pandas.

213
00:17:48,200 --> 00:17:52,200
Now there are some limitations with memory mapped file.

214
00:17:52,200 --> 00:18:05,200
What I would like to do to get the best possible performance is that I write a memory mapped file or Apache Arrow file directly onto the disk, and then I do a memory mapping.

215
00:18:05,200 --> 00:18:15,200
So I have converted the same yellow taxi trip data into an Arrow file. So I have taken that into memory and just written that raw file.

216
00:18:15,200 --> 00:18:21,200
So line number one, I have created a memory mapped file, which means no memory is allocated.

217
00:18:21,200 --> 00:18:31,200
Line number two, I am reading the complete 50 gigs of data, if it is 50 gigs. Read all columns, no memory is allocated.

218
00:18:31,200 --> 00:18:40,200
Line number three, I am reading just one column of this 50 gig data. It may be 5 gig data, so no memory is allocated.

219
00:18:40,200 --> 00:18:43,200
And I can run some computations on that.

220
00:18:43,200 --> 00:18:47,200
The interesting thing is line number four.

221
00:18:47,200 --> 00:18:58,200
We can convert table to Pandas on 50 gigs if it is compatible with Arrow format, which means it is numerical data, there is no null data.

222
00:18:58,200 --> 00:19:05,200
We can create a Pandas data frame without allocating any memory for 50 gigs of data.

223
00:19:05,200 --> 00:19:18,200
But if that is not the case and you want to do some calculations on some of the columns of the data set, you can take only those columns and put that into the data frame.

224
00:19:18,200 --> 00:19:27,200
What will happen is that even if the memory is allocated with Pandas data frame, the memory will be limited in nature.

225
00:19:27,200 --> 00:19:37,200
So for a 50 gig file, if you want to calculate only for column trip distance and that column is just 5 gigs of size,

226
00:19:37,200 --> 00:19:45,200
then even if the Pandas data frame allocates memory, it will be 5 gig. It will never overrun your RAM.

227
00:19:45,200 --> 00:19:52,200
So this is the benefit of reading memory mapped file using PyArrow.

228
00:19:52,200 --> 00:20:03,200
Now this is all about reading data within your program, within your process, but what about reading data across network?

229
00:20:03,200 --> 00:20:13,200
So when Apache Arrow came into picture, this was not the case, but later something got added which is called Apache Arrow Flight.

230
00:20:13,200 --> 00:20:23,200
This flight is mainly for transferring PyArrowBuffer or Apache Arrow Buffers over gRPC.

231
00:20:23,200 --> 00:20:32,200
Given you can transfer anything over gRPC, but in this particular case, if you want to take care of the performance,

232
00:20:32,200 --> 00:20:41,200
what Apache Arrow has to offer, you can transmit only Arrow Buffers over gRPC in case of Arrow Flight.

233
00:20:41,200 --> 00:20:45,200
So it's the on the wire representation of the Arrow data.

234
00:20:45,200 --> 00:20:53,200
Now in case of inter-process communication, some kind of serialization and deserialization is mandatory.

235
00:20:53,200 --> 00:20:58,200
We can't avoid that, some kind of serialization and deserialization.

236
00:20:58,200 --> 00:21:09,200
So what Apache Arrow does is that it's not deserialization, they call it rehydration of data, which is much cheaper than deserialization,

237
00:21:09,200 --> 00:21:16,200
which means that rehydration means that I know which memory buffer corresponds to which data and I can just do a memcpy on that.

238
00:21:16,200 --> 00:21:26,200
As far as serialization goes, there is no data serialization, but there is a metadata that is created using Google FlatBuffer and that is serialized,

239
00:21:26,200 --> 00:21:38,200
but that is a small part of the data. So that is the additional cost you give if you use Apache Arrow Flight using PyArrow or any other respective language libraries.

240
00:21:38,200 --> 00:21:48,200
So this is a purely client server architecture where you can create a flight server and everything is done by client.

241
00:21:48,200 --> 00:21:55,200
Client gets the flight info, it's very similar to your flight, whatever airport flight info you get.

242
00:21:55,200 --> 00:22:02,200
It puts the data, it gets back the data and this can also be distributed.

243
00:22:02,200 --> 00:22:17,200
People are tending too much to use it as an in-memory cache, but the creators of Apache Arrow said that this is not meant for that, but something works for you, works for you.

244
00:22:17,200 --> 00:22:27,200
So let's talk about some of the statistics of flight. Again, I am reading the same New York trip data, 126 MB file, 1.83 seconds this time, fine.

245
00:22:27,200 --> 00:22:34,200
Now with Apache Arrow Flight, we can only transfer arrow buffers, not any other buffers.

246
00:22:34,200 --> 00:22:44,200
So I worry about wall time, so I have converted that data frame into arrow table and it took me approximately 194-200 milliseconds.

247
00:22:44,200 --> 00:22:55,200
And when I transfer that particular 126 MB data to a server, it took me approximately again within the range of 200 milliseconds.

248
00:22:55,200 --> 00:23:04,200
That's a huge, I mean, that's fast considering that much data transfer over network.

249
00:23:04,200 --> 00:23:19,200
Of course these were, you know, fastest LAN possible and in the ideal condition, but again, if this is going to be proportionate with all the other loads, your other things is also going to be proportionate.

250
00:23:19,200 --> 00:23:33,200
And when we talk about fetching the data back, same 126 MB odd data fetching back, again it's approximately 158 or in that range only within 200 milliseconds.

251
00:23:33,200 --> 00:23:47,200
Now why I am talking about this particular thing, you know, we have managed to transfer 126 MB data and get back 126 MB data within 400 odd milliseconds.

252
00:23:47,200 --> 00:23:59,200
That's fast. In contrast, if you are trying to store a data frame somewhere and you want to serialize that data frame, one of the most often used technique is pickling.

253
00:23:59,200 --> 00:24:07,200
Pickling takes time. So does unpickling. That will also take time. Here you can use the data as soon as you get it.

254
00:24:07,200 --> 00:24:17,200
So the table which you are getting it over here in the top of the screen, you can use that table then and there itself. You don't need to unpickle it.

255
00:24:17,200 --> 00:24:28,200
So Aeroflite, in my opinion, has interesting possibilities. It is very fast and for the purpose you want to use it, it's up to you.

256
00:24:28,200 --> 00:24:42,200
So the opportunities with this particular new format, new way of representing columnar in-memory data is limitless, but always evaluate the cost.

257
00:24:42,200 --> 00:24:52,200
Even if something is very fast, performant, there is a cost associated with it. So nothing is free in this world.

258
00:24:52,200 --> 00:25:05,200
And I guess I forget to give a disclaimer in the beginning that I am not related to Apache Aero. I am just a user and maybe a potential future contributor, but at this moment of time I am a user.

259
00:25:05,200 --> 00:25:18,200
So with this I complete my talk. I thank all of you. I hope the time you spent here was worth it. Thank you very much.

