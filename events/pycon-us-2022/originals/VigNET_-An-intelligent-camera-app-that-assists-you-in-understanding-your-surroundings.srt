1
00:00:00,000 --> 00:00:08,920
Good evening, everyone.

2
00:00:08,920 --> 00:00:16,160
We have a next talk that is on Viznet, an intelligent camera app that assists you in

3
00:00:16,160 --> 00:00:19,480
understanding your surroundings by Padmaja Bhagwat and Manishahar.

4
00:00:19,480 --> 00:00:21,440
Over to you, Padmaja.

5
00:00:21,440 --> 00:00:23,920
Thank you so much.

6
00:00:23,920 --> 00:00:29,840
Helen Keller once said that there is no better way to thank God for your sight than by giving

7
00:00:29,840 --> 00:00:32,160
a helping hand to someone in the dark.

8
00:00:32,160 --> 00:00:33,800
A very pleasant evening to everyone.

9
00:00:33,800 --> 00:00:40,560
I'm Padmaja Bhagwat, and I'm super excited to present Project Viznet to you all.

10
00:00:40,560 --> 00:00:44,080
Before we get into the talk, here's a small introduction about us.

11
00:00:44,080 --> 00:00:47,760
I'm Padmaja Bhagwat, and this is my dear friend, Manishah.

12
00:00:47,760 --> 00:00:51,440
We both work as data scientists at Glance India.

13
00:00:51,440 --> 00:00:56,280
While I work on monetizing the content for Glance, Manishahar works on personalizing

14
00:00:56,280 --> 00:01:01,680
the Glance user experience, which, by the way, has 150 million daily active users.

15
00:01:01,680 --> 00:01:07,320
Originally, this talk was supposed to be given by both Manishah and I, but unfortunately,

16
00:01:07,320 --> 00:01:09,280
she couldn't join us here today.

17
00:01:09,280 --> 00:01:13,720
You see, I now have double the responsibility to make this talk as interesting and fun as

18
00:01:13,720 --> 00:01:14,720
possible.

19
00:01:14,720 --> 00:01:21,200
Trust me, it's not an easy job to fit into her shoes, but I'll try my best.

20
00:01:21,200 --> 00:01:25,200
Let's get into the main part of the talk.

21
00:01:25,200 --> 00:01:27,640
First things first, what are we going to learn today?

22
00:01:27,640 --> 00:01:33,040
We are going to see how to build an end-to-end deep learning-based application.

23
00:01:33,040 --> 00:01:38,640
Particularly, we'll be looking into a visual question-answering-based application.

24
00:01:38,640 --> 00:01:42,040
This is how the outline of today's talk is going to look like.

25
00:01:42,040 --> 00:01:45,800
First, we'll look into the data and model that enable this application.

26
00:01:45,800 --> 00:01:51,320
Then, we'll look into how to quickly prototype such deep learning-based application.

27
00:01:51,320 --> 00:01:56,900
The final part would cover how can we productionize these applications by building services on

28
00:01:56,900 --> 00:01:57,900
top of your model.

29
00:01:57,900 --> 00:02:03,720
Particularly, we'll be looking into front-end service and an API service.

30
00:02:03,720 --> 00:02:08,680
With that said, let's dive into the talk.

31
00:02:08,680 --> 00:02:13,280
Some of you might be aware that more than 253 million of words' population suffers from

32
00:02:13,280 --> 00:02:17,480
visual impairment, which is roughly 1 in 30.

33
00:02:17,480 --> 00:02:21,720
For them, simple everyday tasks like locating a matching pair of socks or telling what dollar

34
00:02:21,720 --> 00:02:24,880
bill it is can become extremely difficult.

35
00:02:24,880 --> 00:02:31,160
Hence, we decided to build a visual question-answering-based application which can truly transform your

36
00:02:31,160 --> 00:02:34,080
visual world into an audible experience.

37
00:02:34,080 --> 00:02:40,440
Now, you might be wondering what visual question-answering is.

38
00:02:40,440 --> 00:02:45,360
Simply put, it's an intelligent camera app which takes image and a question related to

39
00:02:46,240 --> 00:02:47,240
and gives an answer out.

40
00:02:47,240 --> 00:02:53,240
For instance, let's say you're on your everyday evening walk where you come across this really

41
00:02:53,240 --> 00:02:54,240
cute puppy.

42
00:02:54,240 --> 00:02:59,280
You take its picture and ask a question like, what is the puppy eating?

43
00:02:59,280 --> 00:03:04,440
Now, both these inputs then go into our VQA model which gives out an answer saying it's

44
00:03:04,440 --> 00:03:09,280
eating stick or rather playing with stick.

45
00:03:09,280 --> 00:03:14,440
So, that's the goal of today's application that we're going to build.

46
00:03:14,520 --> 00:03:18,880
The model which can take both image and a question and give out an answer in the form

47
00:03:18,880 --> 00:03:19,880
of audio.

48
00:03:19,880 --> 00:03:24,960
So, with that goal in mind, let's see how our data set looks like.

49
00:03:24,960 --> 00:03:29,220
We used VQA V2 data set in order to train our model.

50
00:03:29,220 --> 00:03:31,520
This is openly available on visualqa.org.

51
00:03:31,520 --> 00:03:39,640
So, what this data set contains is it has 120,000 plus images and 600,000 plus questions.

52
00:03:39,840 --> 00:03:44,800
Each image has at least three questions associated with it and every question has 10 ground truth

53
00:03:44,800 --> 00:03:45,800
answers.

54
00:03:45,800 --> 00:03:49,920
Of course, there can just be one correct answer depending on the context in which the question

55
00:03:49,920 --> 00:03:51,400
is asked.

56
00:03:51,400 --> 00:03:57,920
You can see on the right that the data set contains a variety of images including the

57
00:03:57,920 --> 00:04:01,120
image of a woman whose mustache is made up of bananas.

58
00:04:01,120 --> 00:04:06,000
So, it does have a very interesting question like, what is the mustache made up of or what

59
00:04:06,000 --> 00:04:08,080
is the color of her eyes?

60
00:04:08,520 --> 00:04:11,360
You see, these are not extremely straightforward questions.

61
00:04:11,360 --> 00:04:17,480
We want our model to understand and be able to answer in such unusual and ambiguous scenarios

62
00:04:17,480 --> 00:04:19,640
as well.

63
00:04:19,640 --> 00:04:24,560
And one more thing to note over here is that our data set contains both image as well as

64
00:04:24,560 --> 00:04:26,360
question in the form of text.

65
00:04:26,360 --> 00:04:32,560
Hence, we are trying to look for a model which can understand both these modalities, which

66
00:04:32,560 --> 00:04:36,000
can understand both vision and language.

67
00:04:36,000 --> 00:04:40,600
And yes, that's the mighty vision and his famous dialogue for all the Marvel fans out

68
00:04:40,600 --> 00:04:42,460
there.

69
00:04:42,460 --> 00:04:48,000
On that note, I think we are good to look into the model which can understand both vision

70
00:04:48,000 --> 00:04:49,000
and language.

71
00:04:49,000 --> 00:04:54,000
I present to you the Vision Language Transformer, also known as VILT.

72
00:04:54,000 --> 00:04:59,520
Well, I understand that this entire architecture might look quite intimidating to look at in

73
00:04:59,520 --> 00:05:01,200
the beginning, but do not worry.

74
00:05:01,200 --> 00:05:05,320
I'll be breaking it down into smaller components and we'll go over it in detail.

75
00:05:05,880 --> 00:05:10,360
So what you need to know over here is that our model is taking both text, which is our

76
00:05:10,360 --> 00:05:13,480
question, as well as image as an input.

77
00:05:13,480 --> 00:05:20,480
Now, first let's see how the model is processing the input text.

78
00:05:20,480 --> 00:05:25,680
So the concept is similar to that of BERT, and some of you might already know that BERT

79
00:05:25,680 --> 00:05:29,080
stands for Bidirectional Encoder Representation from Transformers.

80
00:05:29,320 --> 00:05:35,480
Well, even if you do not know, that's okay, because we'll see how BERT operates.

81
00:05:35,480 --> 00:05:37,560
Let's take the very same question that we saw earlier.

82
00:05:37,560 --> 00:05:40,640
What is the puppy eating?

83
00:05:40,640 --> 00:05:46,840
So what BERT does is it breaks this input sentence into smaller pieces, which we call

84
00:05:46,840 --> 00:05:49,360
as tokens.

85
00:05:49,360 --> 00:05:54,000
So even when you're teaching a child, you break down complex sentence into smaller ones

86
00:05:54,000 --> 00:05:55,800
in order for it to learn, right?

87
00:05:55,800 --> 00:06:01,480
Similarly, you break down the complex sentence into smaller tokens.

88
00:06:01,480 --> 00:06:07,320
So once you do that, the next step, you will be masking a few words in the input sentence.

89
00:06:07,320 --> 00:06:10,600
And then you will be adding some special tokens to it.

90
00:06:10,600 --> 00:06:15,920
Here you can see that we have added CLS, which stands for classification, and SEP, which

91
00:06:15,920 --> 00:06:20,560
stands for separator, indicating that it's the end of the sentence.

92
00:06:20,560 --> 00:06:25,160
So now that we have broken down our input sentence into smaller tokens, we'll then have

93
00:06:25,160 --> 00:06:27,400
to convert it into vectors.

94
00:06:27,400 --> 00:06:29,160
Why do we have to do that?

95
00:06:29,160 --> 00:06:32,960
It's because ultimately we are teaching a machine and they do not understand anything

96
00:06:32,960 --> 00:06:35,880
other than numbers.

97
00:06:35,880 --> 00:06:41,080
So once we have vectors out of your tokens, you need to add something called as positional

98
00:06:41,080 --> 00:06:43,280
encoding.

99
00:06:43,280 --> 00:06:48,280
This positional encoding gives the information about the exact position where the token lies

100
00:06:48,280 --> 00:06:50,520
in your input sentence.

101
00:06:50,520 --> 00:06:54,880
It's an extremely crucial information which your BERT requires.

102
00:06:54,880 --> 00:07:01,280
So now that we have input which captures both content as well as positional information,

103
00:07:01,280 --> 00:07:05,200
we then feed it to the transformer encoder.

104
00:07:05,200 --> 00:07:08,640
And this is then trained to identify what the masked word is.

105
00:07:08,640 --> 00:07:11,960
In our case, the masked word is puppy.

106
00:07:11,960 --> 00:07:14,560
Let's see how it is done exactly.

107
00:07:14,560 --> 00:07:18,400
So this is how the transformer encoder architecture looks like.

108
00:07:18,400 --> 00:07:21,840
It is taking the input text and the positional encoding.

109
00:07:22,680 --> 00:07:28,480
This is passed through a series of self-attention layers which tries to extract as much information

110
00:07:28,480 --> 00:07:31,840
as possible from your input data.

111
00:07:31,840 --> 00:07:37,340
So with that, it is then sent to the feedforward layer which learns to classify what the masked

112
00:07:37,340 --> 00:07:39,060
word is.

113
00:07:39,060 --> 00:07:44,800
So you see the model is trying to derive context from the neighboring words.

114
00:07:44,800 --> 00:07:50,720
So we know that this kind of BERT-based model is extremely fast and computationally efficient

115
00:07:50,800 --> 00:07:53,600
in terms of language models.

116
00:07:53,600 --> 00:07:58,640
So why can't we use the very same concept for images?

117
00:07:58,640 --> 00:08:04,240
So yeah, control C, control V. One thing that we are great at doing.

118
00:08:04,240 --> 00:08:09,520
So yeah, let's use the very same concept of transformer for our images.

119
00:08:09,520 --> 00:08:12,640
And that's how we get vision transformers.

120
00:08:12,640 --> 00:08:16,960
So what it does is it takes your input image, breaks it down into smaller pieces which we

121
00:08:16,960 --> 00:08:18,840
call it as patches.

122
00:08:18,840 --> 00:08:24,000
These patches then get converted into vectors and we add positional encoding to it similar

123
00:08:24,000 --> 00:08:26,920
to that of earlier.

124
00:08:26,920 --> 00:08:31,600
We send it to the transformer encoder which is then trained to identify what the input

125
00:08:31,600 --> 00:08:33,400
image is.

126
00:08:33,400 --> 00:08:38,160
You can see that the transformer encoder looks exactly same as that of before except that

127
00:08:38,160 --> 00:08:43,880
instead of taking the text embedding, it is now taking the patch embedding which is the

128
00:08:43,880 --> 00:08:50,200
vectorized form of your patches combined with the positional encoding.

129
00:08:50,200 --> 00:08:57,760
So this idea was introduced in the paper called An Image is Worth 16 X 16 Words by Google.

130
00:08:57,760 --> 00:09:02,480
I highly recommend you to go over that paper because this concept of vision transformer

131
00:09:02,480 --> 00:09:06,840
is now revolutionizing the field of computer vision.

132
00:09:06,840 --> 00:09:10,400
So we saw how best we can process our input text.

133
00:09:10,400 --> 00:09:14,480
We also saw how best we can process our input image.

134
00:09:14,480 --> 00:09:16,680
Now let's bring it all together.

135
00:09:16,680 --> 00:09:18,960
And that's how we get VILT basically.

136
00:09:18,960 --> 00:09:24,400
So we took the input question, broke it down into smaller pieces or tokens.

137
00:09:24,400 --> 00:09:27,400
We get the word embedding out of it.

138
00:09:27,400 --> 00:09:31,280
We take the supporting image, break it down into smaller patches, get the patch embedding

139
00:09:31,280 --> 00:09:32,340
out of it.

140
00:09:32,340 --> 00:09:37,800
Now both this word embedding and patch embedding is sent to the vision, the transformer encoder

141
00:09:37,960 --> 00:09:41,640
which is then trained to identify what the masked word is.

142
00:09:41,640 --> 00:09:47,120
Here you can see that the model is making use of both text as well as image.

143
00:09:47,120 --> 00:09:52,200
It is trying to context using both this information and is trying to identify what the masked

144
00:09:52,200 --> 00:09:53,200
word is.

145
00:09:53,200 --> 00:09:59,000
Hence, our transformer encoder is kind of acting like a modality interaction layer.

146
00:09:59,000 --> 00:10:05,280
Modality it is combining both the modalities, vision and the language.

147
00:10:05,280 --> 00:10:10,680
Even now, our entire goal is not to predict the masked word.

148
00:10:10,680 --> 00:10:16,280
What we want to do is we want a model which can answer the question given an image.

149
00:10:16,280 --> 00:10:22,280
So you take this pre-trained VILT model and you fine tune it further using the VQA V2

150
00:10:22,280 --> 00:10:24,240
dataset that we saw earlier.

151
00:10:24,240 --> 00:10:29,240
So when you fine tune a model, which means you use the same set of weights and you fine

152
00:10:29,240 --> 00:10:32,740
tune it further using the new dataset.

153
00:10:32,740 --> 00:10:37,580
So when you fine tune it further you get a model which can answer the question given

154
00:10:37,580 --> 00:10:39,820
an image.

155
00:10:39,820 --> 00:10:44,220
And that is how we build the core part of our application.

156
00:10:44,220 --> 00:10:49,180
If it still sounds complicated then I do have a good news for you.

157
00:10:49,180 --> 00:10:53,460
Python provides all these amazing libraries out of the box for you.

158
00:10:53,460 --> 00:10:58,500
So if you are onto the latest version of PyTorch all you need to do is from transformers you

159
00:10:58,500 --> 00:11:02,500
need to import VILT processor and VILT for question answering.

160
00:11:02,500 --> 00:11:08,020
The best part is VILT for question answering is already fine tuned onto your VQA V2 dataset

161
00:11:08,020 --> 00:11:10,800
so you don't have to do anything extra.

162
00:11:10,800 --> 00:11:13,860
You just have to plug it into your code and it is good to be used.

163
00:11:13,860 --> 00:11:15,100
Let's see how it's done.

164
00:11:15,100 --> 00:11:21,100
So first you need to initialize your processor and then you need to initialize your model.

165
00:11:21,100 --> 00:11:26,740
You send in your raw input image and question to the processor which converts it into vectors,

166
00:11:26,740 --> 00:11:29,140
also known as encoding.

167
00:11:29,140 --> 00:11:35,780
This encoding we then send it through our model which gives out the probability of answer.

168
00:11:35,780 --> 00:11:37,780
Easy isn't it?

169
00:11:37,780 --> 00:11:38,780
That's all there is.

170
00:11:38,780 --> 00:11:43,740
I mean with just these five lines of code your core part of the application is ready

171
00:11:43,740 --> 00:11:48,380
which can be plugged into your application.

172
00:11:48,380 --> 00:11:53,540
So one thing that you need to ensure is you are on the latest version of the Torch and

173
00:11:53,540 --> 00:11:58,580
transformers in order to be able to import these libraries.

174
00:11:58,580 --> 00:12:03,820
So now the crux of our application is ready.

175
00:12:03,820 --> 00:12:09,180
So the next part is how do we quickly prototype it?

176
00:12:09,180 --> 00:12:14,920
But before that the advantage of using such transformer based model is that it's computationally

177
00:12:14,920 --> 00:12:18,240
fast and efficient and it has high expressive power.

178
00:12:18,240 --> 00:12:23,460
You will soon get to know it when I'll be showing the demo but yeah the core part of

179
00:12:23,460 --> 00:12:28,540
the application is built using the transformer based model.

180
00:12:28,540 --> 00:12:31,300
So the next part is how can we quickly prototype it?

181
00:12:31,300 --> 00:12:38,180
How can we quickly build a web application out of your model?

182
00:12:38,180 --> 00:12:42,740
So you might be wondering why is prototyping so necessary?

183
00:12:42,740 --> 00:12:46,020
Why do we have to build a web application?

184
00:12:46,020 --> 00:12:50,940
Trust me it's the most interesting part of building the entire application.

185
00:12:50,940 --> 00:12:55,620
So the reason for building a web application is that we as a data scientist are shielded

186
00:12:55,620 --> 00:13:00,860
from a lot of real world requirements and in order to be able to better communicate

187
00:13:00,860 --> 00:13:08,780
with your stakeholders having a web application helps a lot instead of showing the code files.

188
00:13:08,780 --> 00:13:12,860
So you might again question me saying that building a web application is not an easy

189
00:13:12,860 --> 00:13:19,660
task and if you're someone like me for whom dealing with HTML and CSS code is like a nightmare

190
00:13:19,660 --> 00:13:25,220
then Python has a solution for that as well.

191
00:13:25,220 --> 00:13:28,980
It provides an extremely elegant library called a streamlet which I absolutely love

192
00:13:28,980 --> 00:13:34,620
it which helps converting your data script into shareable web application in minutes.

193
00:13:34,620 --> 00:13:37,540
I mean literally in minutes.

194
00:13:37,540 --> 00:13:39,140
Let's see how it's done.

195
00:13:39,140 --> 00:13:45,700
So let's take a step back and first see what is it that we need out of our web application.

196
00:13:45,700 --> 00:13:49,100
How do you want your UI to look like?

197
00:13:49,100 --> 00:13:55,200
So for visual question answering based application we need an application which takes input images

198
00:13:56,180 --> 00:14:03,080
so that all you need to do is import streamlet as st and all you need to do is type st.streamlet

199
00:14:03,080 --> 00:14:09,440
and mention the kind of file you want your application to accept.

200
00:14:09,440 --> 00:14:13,000
After that you need a prompt which can take your question in.

201
00:14:13,000 --> 00:14:18,820
So that can just be done using st.textinput and there you go.

202
00:14:18,820 --> 00:14:24,640
You also need a button which can submit both your input image and question and for that

203
00:14:24,800 --> 00:14:30,760
it's just st.button and finally a prompt to display the answer that your model has given

204
00:14:30,760 --> 00:14:34,500
and that can be done using st.write.

205
00:14:34,500 --> 00:14:39,960
So seriously with just these five lines of code you have an entire web application up

206
00:14:39,960 --> 00:14:40,960
and running.

207
00:14:40,960 --> 00:14:49,300
I'll also show you how easy it is to run such kind of streamlet based application.

208
00:14:49,300 --> 00:14:52,540
So let me go into the code.

209
00:14:52,540 --> 00:14:59,980
Let me stop this.

210
00:14:59,980 --> 00:15:08,180
So all you need to do is type streamlet run and provide the path of the file where your

211
00:15:08,180 --> 00:15:09,660
code is running.

212
00:15:09,660 --> 00:15:14,580
In our case it's running onto streamletservice.py and you hit enter.

213
00:15:14,580 --> 00:15:20,540
You get an amazing application up and running onto your local system and this is how your

214
00:15:20,540 --> 00:15:24,100
prototype looks like.

215
00:15:24,100 --> 00:15:28,820
Now if Manisha was here she would have been like Padmaja what if you're showing us the

216
00:15:28,820 --> 00:15:31,900
static page does it actually work?

217
00:15:31,900 --> 00:15:35,020
Well let's see if it does.

218
00:15:35,020 --> 00:15:43,620
So let's take the very same image that we saw earlier and let's type in a question saying

219
00:15:43,620 --> 00:15:49,820
what is the puppy playing with?

220
00:15:49,820 --> 00:15:53,700
And hit submit.

221
00:15:53,700 --> 00:15:59,100
Well it does take some time to give an answer but hey it's meant for prototyping right?

222
00:15:59,100 --> 00:16:01,740
So yeah it did give an answer saying it's tick.

223
00:16:01,740 --> 00:16:06,700
I was so excited when I first built it so I showed it to Manisha who by the way is not

224
00:16:06,700 --> 00:16:10,500
at all an easy person to please.

225
00:16:10,500 --> 00:16:16,700
So she was happy to look at it but she did point out a bunch of valid concerns.

226
00:16:16,700 --> 00:16:21,980
She was like Padmaja what if I want to take this from prototyping stage to my actual

227
00:16:21,980 --> 00:16:22,980
customers.

228
00:16:22,980 --> 00:16:28,420
I cannot show this because firstly it's not taking an audio input and audio output.

229
00:16:28,420 --> 00:16:33,520
I mean I'm ultimately designing this for visually impaired right?

230
00:16:33,520 --> 00:16:40,720
And the output, the font of the output is not big enough and it does take a while to

231
00:16:40,720 --> 00:16:44,560
load the entire web application and give an answer.

232
00:16:44,560 --> 00:16:48,160
You cannot afford to have that kind of delay.

233
00:16:48,160 --> 00:16:52,800
So with all these limitations what she does is she goes and she builds an entire frontend

234
00:16:52,800 --> 00:16:56,760
service by herself using React.

235
00:16:56,760 --> 00:17:04,160
Now this React is a JavaScript library built and maintained by Facebook.

236
00:17:04,160 --> 00:17:08,560
This React gives extremely rich user interfaces I must tell you.

237
00:17:08,560 --> 00:17:09,920
It's extremely appealing to look at.

238
00:17:09,920 --> 00:17:14,640
I'll show you how it looks very soon.

239
00:17:14,640 --> 00:17:18,700
And also you can build the components which can be reused across your web application.

240
00:17:18,700 --> 00:17:21,260
You need not write the same code again and again.

241
00:17:21,260 --> 00:17:22,520
And also there is no delay at all.

242
00:17:22,520 --> 00:17:26,680
It's extremely fast when it comes to rendering pages.

243
00:17:27,600 --> 00:17:30,520
I'll soon show you how the final UI looks like.

244
00:17:30,520 --> 00:17:34,240
Before that I have a question for you.

245
00:17:34,240 --> 00:17:35,960
Can you guess who this is?

246
00:17:35,960 --> 00:17:39,560
I'll give you some time.

247
00:17:39,560 --> 00:17:45,560
Well are you interested to know what our model thinks of this image?

248
00:17:45,560 --> 00:17:47,040
I mean at least I am.

249
00:17:47,040 --> 00:17:51,240
But you know what we'll have to wait till the end of this talk in order to see what

250
00:17:51,240 --> 00:17:55,120
our model thinks of this image.

251
00:17:55,560 --> 00:18:01,160
What we covered so far is that we saw how to build the core part of our application

252
00:18:01,160 --> 00:18:03,320
which is our model using VLT.

253
00:18:03,320 --> 00:18:07,960
We also saw how to build the frontend service using React.

254
00:18:07,960 --> 00:18:10,760
Now these two components are separate in itself.

255
00:18:10,760 --> 00:18:13,800
How do we combine these two?

256
00:18:13,800 --> 00:18:17,280
So the bridge between these two would be our API service.

257
00:18:17,920 --> 00:18:25,040
API service helps exposing the amazing work that you have done on model to the external world.

258
00:18:25,040 --> 00:18:28,600
And it also helps in integrating your work with other applications.

259
00:18:28,600 --> 00:18:32,600
In our case we want to integrate with the frontend service.

260
00:18:32,600 --> 00:18:39,240
And having an API service is always good because deploying it to the production is extremely easy with that.

261
00:18:39,240 --> 00:18:42,560
Again we built it using fast API.

262
00:18:42,560 --> 00:18:46,320
Let's actually see how it works.

263
00:18:46,360 --> 00:18:47,760
It's very simple.

264
00:18:47,760 --> 00:18:53,480
You need to import fast API and kind of files that you will be dealing with.

265
00:18:53,480 --> 00:18:57,160
After that you need to initialize the fast API.

266
00:18:57,160 --> 00:18:58,560
And this is the function.

267
00:18:58,560 --> 00:19:02,400
This is the predict function which we want to expose to the external world.

268
00:19:02,400 --> 00:19:07,560
We want the logic of this function to be exposed to our UI.

269
00:19:07,560 --> 00:19:13,400
So that can be easily done just by adding this decorator like this.

270
00:19:13,400 --> 00:19:21,600
All you need to do is add the rate app.post and provide the relative URL where you want the logic to be exposed to.

271
00:19:21,600 --> 00:19:23,360
And that's how it's done.

272
00:19:23,360 --> 00:19:29,680
Now if you want to test out your API, if you want to make sure whether the logic is working as expected,

273
00:19:29,680 --> 00:19:34,440
then fast API provides this amazing swagger documentation out of the box for you.

274
00:19:34,440 --> 00:19:36,360
You don't have to do anything extra.

275
00:19:36,360 --> 00:19:40,440
All you need to do is go to the URL where your application is running.

276
00:19:40,480 --> 00:19:47,880
So in order to test it out, it usually runs on localhost 9000 when you are trying it on to your local machine.

277
00:19:47,880 --> 00:19:53,760
All you need to do is type in frontslash docs and it gives this amazing swagger UI for you.

278
00:19:53,760 --> 00:19:59,160
So here what we are interested to know is we are interested to test our predict API.

279
00:19:59,160 --> 00:20:04,520
So you can see that it is able to infer the kind of input that your API requires.

280
00:20:04,520 --> 00:20:07,560
It is question and an image file.

281
00:20:07,560 --> 00:20:09,880
It is able to infer the data type as well.

282
00:20:09,880 --> 00:20:13,200
So all you need to do is hit on try it out and submit it.

283
00:20:13,200 --> 00:20:16,200
So you will be getting an answer in the form of string.

284
00:20:16,200 --> 00:20:25,600
And if your model runs into any error, you can also get a proper error message displayed as a JSON format.

285
00:20:25,600 --> 00:20:33,040
So you can see that it's extremely easy to test out an entire application or your API specifically using such kind of UI.

286
00:20:33,040 --> 00:20:38,800
And I did not write any extra lines of code in order to bring up this UI.

287
00:20:38,840 --> 00:20:48,640
So that is how you can build an API which connects both your model logic to your front end service.

288
00:20:48,640 --> 00:20:55,120
So what we covered so far is we saw how to build a crux of our application using VLT.

289
00:20:55,120 --> 00:20:58,000
We also saw how to build a front end service using React.

290
00:20:58,000 --> 00:21:02,760
And we saw that API service acts like a bridge between those two.

291
00:21:02,760 --> 00:21:07,600
So that is how you build an end to end deep learning based application.

292
00:21:07,640 --> 00:21:12,360
Now let's see this entire application in action through a demo.

293
00:21:12,360 --> 00:21:14,400
And I'll let Manisha to demo it for you.

294
00:21:18,880 --> 00:21:24,320
Manisha just gave you a complete warm-up of how we went about building the VLT application.

295
00:21:24,320 --> 00:21:26,760
Now let us see it in action.

296
00:21:26,760 --> 00:21:29,840
This is the React based application that we have built.

297
00:21:29,840 --> 00:21:32,640
I'll give a demo on how to use this.

298
00:21:32,640 --> 00:21:35,920
So this is where we take a picture or upload an image.

299
00:21:35,920 --> 00:21:40,640
If you are using your mobile phones, you can capture an image, ask a question related to that image,

300
00:21:40,640 --> 00:21:43,480
and you'll get an answer out of this.

301
00:21:43,480 --> 00:21:44,760
Let me upload an image.

302
00:21:47,840 --> 00:21:50,840
Let me start off with a very simple question.

303
00:21:50,840 --> 00:21:52,320
So this is where we ask the question.

304
00:21:52,320 --> 00:21:57,040
We need to click on this microphone button in order to ask a question.

305
00:21:57,040 --> 00:22:02,520
And we need to click on it back so that it stops listening to us.

306
00:22:02,520 --> 00:22:04,280
How is the weather looking like?

307
00:22:06,360 --> 00:22:10,360
When I click on get answer, we get an answer both in audio and text format.

308
00:22:13,200 --> 00:22:14,520
Cloudy.

309
00:22:14,520 --> 00:22:17,480
It says cloudy, which is the correct answer.

310
00:22:17,480 --> 00:22:21,120
Now let us try out a bit more complicated image and a set of questions.

311
00:22:25,880 --> 00:22:29,840
Again, let me start off with a very simple question here.

312
00:22:29,840 --> 00:22:30,640
What is this?

313
00:22:34,240 --> 00:22:35,760
Street.

314
00:22:35,760 --> 00:22:42,000
It says street, but I'm not sure if it is actually able to identify all the things that are there in this image.

315
00:22:42,000 --> 00:22:43,560
So let me ask one more question.

316
00:22:46,000 --> 00:22:47,680
What do I see in front of me?

317
00:22:51,960 --> 00:22:53,240
Sign.

318
00:22:53,240 --> 00:22:54,440
Interesting.

319
00:22:54,440 --> 00:23:00,920
It says sign, but I'm not sure if it is actually able to identify the type of the signal that we are seeing.

320
00:23:00,920 --> 00:23:04,360
So let me ask one more question.

321
00:23:04,400 --> 00:23:05,760
What is the sign indicating?

322
00:23:09,640 --> 00:23:11,640
pedestrian crossing.

323
00:23:11,640 --> 00:23:12,760
Amazing, right?

324
00:23:12,760 --> 00:23:17,440
It is actually able to identify the type of the signal as well in this image.

325
00:23:17,440 --> 00:23:20,000
OK, let me try out one last image.

326
00:23:24,160 --> 00:23:32,080
By the way, this application has accessibility features and the accessibility score is 92, according to the Google Lighthouse 2.

327
00:23:33,080 --> 00:23:36,080
Now coming back to this, let me ask a question.

328
00:23:37,280 --> 00:23:38,680
Can I walk now?

329
00:23:40,880 --> 00:23:41,680
Well.

330
00:23:42,880 --> 00:23:43,680
No.

331
00:23:44,480 --> 00:23:49,080
It says no, but I'm not sure if it is because of the signal or something else.

332
00:23:49,080 --> 00:23:50,880
So let me ask one more question.

333
00:23:50,880 --> 00:23:52,680
I told you she is not easy to please.

334
00:23:52,680 --> 00:23:54,280
What is the color of the signal?

335
00:23:57,880 --> 00:23:58,680
Red.

336
00:23:59,280 --> 00:24:00,280
It says red.

337
00:24:00,880 --> 00:24:02,880
OK, let me ask one more question.

338
00:24:04,880 --> 00:24:06,280
What is the sign indicating?

339
00:24:09,680 --> 00:24:10,280
Stop.

340
00:24:11,280 --> 00:24:11,880
Awesome.

341
00:24:11,880 --> 00:24:15,080
So it is actually able to identify the type of the signal as well.

342
00:24:16,480 --> 00:24:17,680
That is it from the demo.

343
00:24:18,480 --> 00:24:20,080
I'll hand it over to Padmeja.

344
00:24:20,080 --> 00:24:21,880
She has more in store for you folks.

345
00:24:22,680 --> 00:24:26,880
So yeah, looks like our model does get a lot of answers correctly.

346
00:24:26,880 --> 00:24:31,880
But let's see if it is able to recognize the image that we saw earlier.

347
00:24:35,080 --> 00:24:37,080
So yeah, here's the application.

348
00:24:38,280 --> 00:24:40,080
We upload the image.

349
00:24:40,280 --> 00:24:42,880
Yeah, like some of you might have already guessed.

350
00:24:42,880 --> 00:24:45,280
It's Kim Kardashian from last year's Met Gala.

351
00:24:45,680 --> 00:24:49,080
I mean, come on, I'm a Gen Z standing in front of you giving this serious talk.

352
00:24:49,280 --> 00:24:50,480
I had to bring in this image.

353
00:24:52,280 --> 00:24:53,680
So let me ask a question.

354
00:24:54,280 --> 00:24:55,080
What is this?

355
00:24:57,880 --> 00:24:58,480
Uh-oh.

356
00:24:58,480 --> 00:24:59,280
Statue.

357
00:24:59,280 --> 00:25:02,080
Our model thinks that it's a statue.

358
00:25:02,480 --> 00:25:06,280
So it turns out that our model indeed cannot keep up with the Kardashians, you know.

359
00:25:07,680 --> 00:25:09,880
But you know, it works when it has to.

360
00:25:10,680 --> 00:25:14,080
So that's how our entire visual question answering application works.

361
00:25:14,480 --> 00:25:20,080
So how we plan on taking it forward is by integrating it with the amazing application called as Glance.

362
00:25:20,880 --> 00:25:23,680
So Glance is the world's largest screen zero platform.

363
00:25:23,680 --> 00:25:28,480
What I mean by that is you can browse all the amazing content right on the lock screen of your phone.

364
00:25:29,280 --> 00:25:34,080
So like I mentioned before, Glance has more than 150 million daily active users,

365
00:25:34,080 --> 00:25:38,280
out of which more than 50% of them belong to smaller cities and towns.

366
00:25:39,480 --> 00:25:46,680
In order to be able to address such kind of audience where the knowledge of computers or smart devices is relatively low,

367
00:25:46,680 --> 00:25:49,080
you need to make your application extremely easy to use.

368
00:25:49,080 --> 00:25:53,680
And what can be more easier than having your question answered right on the lock screen of your phone?

369
00:25:54,680 --> 00:25:56,480
So this is how we plan on doing it.

370
00:25:56,480 --> 00:25:59,080
On the lock screen, you do have the access to the camera.

371
00:25:59,080 --> 00:26:04,080
You just need to swipe right, take the image of the object, and you need to hit on Ask Glance,

372
00:26:04,080 --> 00:26:07,280
which will then get redirected to the Arvinette application.

373
00:26:08,280 --> 00:26:12,680
So this is how we plan on maximizing our reach, maximizing our user reach,

374
00:26:12,680 --> 00:26:17,280
with the hope of providing an extra pair of eyes to everyone who is watching.

375
00:26:17,280 --> 00:26:21,080
So, yeah, even if you did not understand what I just spoke for the last 20 minutes,

376
00:26:21,080 --> 00:26:24,680
here at the TLDR version, we used a VQA V2 dataset.

377
00:26:24,680 --> 00:26:27,280
We used a model using VIRT.

378
00:26:27,280 --> 00:26:31,280
We built a prototype using Streamlit and frontend using React.

379
00:26:31,280 --> 00:26:34,280
Our entire API service was built using FastAPI.

380
00:26:34,280 --> 00:26:37,680
You can go ahead and clone this repo in order to try it out.

381
00:26:37,680 --> 00:26:42,280
Plus, on this repo, you can also see how to deploy it on cloud platform.

382
00:26:42,280 --> 00:26:45,280
We have shown how to deploy it on the cloud platform.

383
00:26:46,280 --> 00:26:50,280
We have shown how to deploy it on GCP.

384
00:26:51,280 --> 00:26:56,280
So, yeah, try this out yourself, because don't go by the recorded demo.

385
00:26:56,280 --> 00:26:57,280
You need to try it out.

386
00:26:57,280 --> 00:27:01,280
Just a few things is that it works the best in Google Chrome and Microsoft Edge.

387
00:27:01,280 --> 00:27:05,280
Safari and Firefox has a bit of audio issue, which we'll be fixing it soon.

388
00:27:05,280 --> 00:27:08,280
So that's it for today, folks. Thank you so much.

389
00:27:08,280 --> 00:27:16,280
And this is the team that helped us build it,

390
00:27:16,280 --> 00:27:18,280
Raghav, Sivas, and Pavlos,

391
00:27:18,280 --> 00:27:22,280
because without whom this entire application wouldn't have been possible.

392
00:27:22,280 --> 00:27:27,280
So a big thanks to them and a huge thanks to you all for being such an amazing audience today.

393
00:27:27,280 --> 00:27:29,280
Thank you so much. I hope you enjoyed.

