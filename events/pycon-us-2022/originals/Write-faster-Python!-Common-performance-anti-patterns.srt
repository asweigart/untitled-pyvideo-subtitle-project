1
00:00:00,000 --> 00:00:10,540
Okay, we're ready for the last talk of the day.

2
00:00:10,540 --> 00:00:17,920
Please welcome Antony Shaw with his talk, Write Faster Python with Common Performance

3
00:00:17,920 --> 00:00:19,720
Anti-Patterns.

4
00:00:19,720 --> 00:00:22,720
Big applause, please.

5
00:00:22,720 --> 00:00:26,080
Hi, everybody.

6
00:00:26,080 --> 00:00:30,560
Yeah, so my name's Antony Shaw.

7
00:00:30,560 --> 00:00:34,800
Welcome to the trampoline and tumbling championships, regions one and two.

8
00:00:34,800 --> 00:00:40,320
I hope everyone has got rid of their stretches and everything and it's all going well for

9
00:00:40,320 --> 00:00:41,320
you.

10
00:00:41,320 --> 00:00:43,480
So I'm probably not going to talk about that today.

11
00:00:43,480 --> 00:00:46,760
I will talk about writing faster Python.

12
00:00:46,760 --> 00:00:53,200
There are parts of this talk which might cause some reactions from people in terms of the

13
00:00:53,200 --> 00:00:55,600
Python or un-Python code which is shown.

14
00:00:55,600 --> 00:01:00,840
So please save your questions for the end as there are no questions.

15
00:01:00,840 --> 00:01:05,760
So if you haven't seen this talk, I really recommend you watch it.

16
00:01:05,760 --> 00:01:08,120
This is a talk by James Murphy.

17
00:01:08,120 --> 00:01:11,280
He gave at PyCon last year.

18
00:01:11,280 --> 00:01:14,320
He wrote a NES emulator in pure Python.

19
00:01:14,320 --> 00:01:17,360
And in the talk, he basically presents his code.

20
00:01:17,360 --> 00:01:20,120
He shows the game not working.

21
00:01:20,120 --> 00:01:26,680
It gets to three frames a second playing not that game because that game is licensed, but

22
00:01:26,680 --> 00:01:29,500
playing a game similar to that.

23
00:01:29,500 --> 00:01:33,940
And basically, he argued that in order to get it from three frames a second to something

24
00:01:33,940 --> 00:01:38,920
acceptable and playable, you would need to synthesize the code base.

25
00:01:38,920 --> 00:01:43,200
So I was really curious about this after watching the talk.

26
00:01:43,360 --> 00:01:47,840
I've been working on a compiler project called Pigeon for the last couple of years.

27
00:01:47,840 --> 00:01:52,200
I was curious to see if I gave it to Pigeon, would Pigeon make it run any faster?

28
00:01:52,200 --> 00:01:54,600
It didn't make much difference.

29
00:01:54,600 --> 00:01:59,360
Then I was curious as to why Python was running this so slowly.

30
00:01:59,360 --> 00:02:04,720
When he synthesized things, why did it make such a drastic difference?

31
00:02:04,720 --> 00:02:10,200
Basically what I did was go through and refactored a number of things in the application whilst

32
00:02:10,200 --> 00:02:13,880
keeping it Pythonic but making some very small changes.

33
00:02:13,880 --> 00:02:18,440
And it sped up by four times, which is a big difference but still not enough.

34
00:02:18,440 --> 00:02:23,560
And I thought, okay, that was a reasonably successful project.

35
00:02:23,560 --> 00:02:27,040
An interesting experiment, but maybe I can learn from what I've just done and see if

36
00:02:27,040 --> 00:02:32,480
I can apply those same principles to something else, which doesn't need 30 times performance

37
00:02:32,480 --> 00:02:33,480
improvement.

38
00:02:33,480 --> 00:02:35,160
It just needed two or four.

39
00:02:35,160 --> 00:02:41,200
So when you think about trying to improve the performance of your applications, I like

40
00:02:41,200 --> 00:02:43,000
to think of it in terms of two axes.

41
00:02:43,000 --> 00:02:46,800
So on the left-hand side, we've got the amount of impact that it has.

42
00:02:46,800 --> 00:02:50,680
And across the bottom, you've got the code changes that you would need to make.

43
00:02:50,680 --> 00:02:58,240
So I guess the most drastic one you could do would be to rewrite, synthesize your code

44
00:02:58,240 --> 00:03:00,720
or write a pure C extension module.

45
00:03:00,720 --> 00:03:05,120
So in terms of changes, throwing Cython at it wouldn't make an immediate difference.

46
00:03:05,120 --> 00:03:06,640
You'd have to type it as well.

47
00:03:06,640 --> 00:03:12,680
So doing type Cython would give you both a dramatic improvement in performance but also

48
00:03:12,680 --> 00:03:15,600
require you to make a number of changes.

49
00:03:15,600 --> 00:03:20,640
And then on the other side of the graph, we've got the sort of hardware engineers view of

50
00:03:20,640 --> 00:03:24,480
things which is let's just throw more CPUs at this and more RAM at this.

51
00:03:24,480 --> 00:03:29,560
So that doesn't require any code changes and can make a dramatic difference.

52
00:03:29,600 --> 00:03:34,640
We then got other approaches which are mixed.

53
00:03:34,640 --> 00:03:38,600
Introducing caching tiers is always something that I'd recommend, especially in web applications.

54
00:03:38,600 --> 00:03:43,600
And I say tiers like from the front end from the view, and then you have a caching beneath

55
00:03:43,600 --> 00:03:49,560
that, a caching about the database, and maybe even a caching level on the function.

56
00:03:49,560 --> 00:03:54,640
Optimizing your database calls and any IAO operations, definitely a classic one.

57
00:03:54,640 --> 00:03:59,240
And also maybe implementing things in PyPy if it's required.

58
00:03:59,240 --> 00:04:01,600
We then got minor version Python upgrades.

59
00:04:01,600 --> 00:04:06,080
So moving from 3.8 to 3.9 or 3.9 to 3.10.

60
00:04:06,080 --> 00:04:10,720
Each of those normally comes with a 5 to 10% performance increase.

61
00:04:10,720 --> 00:04:16,680
3.11 is looking like a 25% performance gain on 3.10 which is awesome.

62
00:04:16,680 --> 00:04:20,240
And then what I want to talk to you about is this last one which is optimize existing

63
00:04:20,240 --> 00:04:21,240
code.

64
00:04:21,240 --> 00:04:26,000
And we're going to talk about micro-optimizations to Python code.

65
00:04:26,600 --> 00:04:31,720
I guess flexible as to where that box goes because you can make small changes which is

66
00:04:31,720 --> 00:04:33,720
what we're going to talk about.

67
00:04:33,720 --> 00:04:38,280
Or you can go nuts and you can just refactor your entire code base.

68
00:04:38,280 --> 00:04:41,200
But before you jump in there's some things you definitely want to do if you're going

69
00:04:41,200 --> 00:04:44,160
to think about micro-optimizing.

70
00:04:44,160 --> 00:04:47,160
Definitely create a benchmark before you start.

71
00:04:47,160 --> 00:04:48,600
So what is the baseline?

72
00:04:48,600 --> 00:04:51,640
What is the performance of the application like today?

73
00:04:51,640 --> 00:04:55,120
Don't go in there and start making changes unless you know whether it's making things

74
00:04:55,120 --> 00:04:57,480
better or worse.

75
00:04:57,480 --> 00:05:03,800
When you do run a benchmark, a baseline, think about the inputs you're testing it on.

76
00:05:03,800 --> 00:05:05,800
Don't just give it dummy data.

77
00:05:05,800 --> 00:05:10,680
Try and make it as realistic as possible with what your production app looks like.

78
00:05:10,680 --> 00:05:13,920
When you do make changes try and keep them small and atomic.

79
00:05:13,920 --> 00:05:18,280
So don't do a big refactor all at once because then if you see things slow down in one area

80
00:05:18,280 --> 00:05:22,720
and speed up in another it's hard to figure out what caused that.

81
00:05:22,760 --> 00:05:27,600
And then when you are reproducing improvements in speed you need to do that thousands of

82
00:05:27,600 --> 00:05:28,600
times.

83
00:05:28,600 --> 00:05:31,680
So just running a one-off benchmark is not normally enough because CPUs have a lot of

84
00:05:31,680 --> 00:05:32,680
noise on them.

85
00:05:32,680 --> 00:05:38,160
So things can run slower for absolutely no reason or something outside of your control.

86
00:05:38,160 --> 00:05:42,440
If you don't believe me look on the Python speed site and sometimes you see there was

87
00:05:42,440 --> 00:05:46,400
a 2% performance improvement in this commit and the commit was a change to a markdown

88
00:05:46,400 --> 00:05:47,400
file.

89
00:05:47,400 --> 00:05:51,280
So there's noise and you can ignore something like that.

90
00:05:51,280 --> 00:05:55,360
Also don't assume that the impact that you're measuring will be the same against minor Python

91
00:05:55,360 --> 00:05:56,720
versions.

92
00:05:56,720 --> 00:06:01,800
So the stuff I'm showing you today is against Python 3.10 and we've been having fun this

93
00:06:01,800 --> 00:06:05,800
week testing it against 3.11 alpha 7 as well.

94
00:06:05,800 --> 00:06:10,320
So all of the stuff I'm showing you today is relevant to 3.11 as well.

95
00:06:10,320 --> 00:06:17,960
And I say also if you see a performance gain of under 10% generally don't bother.

96
00:06:17,960 --> 00:06:22,680
So a lot of that is noise so you can give or take a couple of percent but a 10% less

97
00:06:22,680 --> 00:06:27,240
than 10% performance improvement is really not particularly helpful.

98
00:06:27,240 --> 00:06:33,400
And most of the stuff I'll show you is 30, 60, 80% gains.

99
00:06:33,400 --> 00:06:38,120
So when I said to benchmark your code first to do that you need to use a profiler.

100
00:06:38,120 --> 00:06:43,440
There are two types of profilers, tracing profilers and sampling profilers.

101
00:06:43,440 --> 00:06:45,440
So the first one is a tracing profiler.

102
00:06:45,440 --> 00:06:50,400
So the way that works is there is a tracing function which is a custom function that you

103
00:06:50,400 --> 00:06:57,480
can write or you can use one of the built-in profiling modules or there's a PyPI as well.

104
00:06:57,480 --> 00:06:58,720
That will run the trace.

105
00:06:58,720 --> 00:07:02,240
It will then execute the function and then it will run the trace.

106
00:07:02,240 --> 00:07:05,480
The pros of this is that it's pretty accurate in terms of measuring how long it took to

107
00:07:05,480 --> 00:07:07,720
run the function.

108
00:07:07,720 --> 00:07:09,920
The biggest con is that there's quite a big overhead.

109
00:07:09,920 --> 00:07:14,520
So it's pausing while it runs the profile before it runs your code.

110
00:07:14,600 --> 00:07:16,720
A sampling profiler is a bit different.

111
00:07:16,720 --> 00:07:24,240
So your code runs and then something in another process will periodically sample the Python

112
00:07:24,240 --> 00:07:28,920
executable to basically see what is running and what is happening at that time.

113
00:07:28,920 --> 00:07:33,040
Some advantages to that are you can be a lot more specific in terms of granularity and

114
00:07:33,040 --> 00:07:36,120
I'll show you some profilers in a second.

115
00:07:36,120 --> 00:07:38,520
The biggest difference is that the overhead is less.

116
00:07:38,520 --> 00:07:45,640
So if we look, I put together a list of my favorite profilers and I've highlighted my

117
00:07:45,640 --> 00:07:50,960
recommended flavors at the top which is Austin and Scalene.

118
00:07:50,960 --> 00:07:52,440
These are both samplers.

119
00:07:52,440 --> 00:07:58,280
They have a very small overhead so when you're profiling an application it should behave

120
00:07:58,280 --> 00:08:01,640
pretty much as it would normally without a profiler.

121
00:08:01,760 --> 00:08:09,240
Whereas if you run C profile sometimes, your code runs two times slower than it would do

122
00:08:09,240 --> 00:08:13,600
normally because it's a tracing profiler.

123
00:08:13,600 --> 00:08:18,760
The built-in one is C profile which is brilliant if you're testing alpha versions like if you're

124
00:08:18,760 --> 00:08:22,060
testing 3.11 for example because it's built into the standard library.

125
00:08:22,060 --> 00:08:23,060
It always works.

126
00:08:23,060 --> 00:08:28,720
Pyinstrument, PySpy and Yappi are pure Python ones.

127
00:08:28,800 --> 00:08:30,320
PySpy are great.

128
00:08:30,320 --> 00:08:35,040
The difference between why I recommended these two at the top is because they can do it at

129
00:08:35,040 --> 00:08:36,480
a line level.

130
00:08:36,480 --> 00:08:41,560
If you're doing micro-optimizations, often you're going to get a performance report that

131
00:08:41,560 --> 00:08:45,560
says most of the time is spent in this function and you're like, okay, great, but what do

132
00:08:45,560 --> 00:08:46,560
I do with that function?

133
00:08:46,560 --> 00:08:48,200
I can't just throw it away.

134
00:08:48,200 --> 00:08:50,640
I could split it up.

135
00:08:50,640 --> 00:08:55,080
But with something like Scalene, what you can actually get is this awesome output.

136
00:08:55,080 --> 00:08:56,760
Austin does this as well.

137
00:08:56,800 --> 00:09:03,720
It's basically a side-by-side copy of your code and it will highlight specific lines

138
00:09:03,720 --> 00:09:10,280
that took up a lot of memory, a lot of CPU time, and it also splits the time between

139
00:09:10,280 --> 00:09:13,080
native Python time and C extension time.

140
00:09:13,080 --> 00:09:15,440
So you can get a lot of detail.

141
00:09:15,440 --> 00:09:20,760
I like using this because in the case of the NES emulator, it's basically able to drill

142
00:09:20,840 --> 00:09:28,240
down to a set of ten lines in the entire code base that were causing a lot of the CPU time.

143
00:09:28,240 --> 00:09:34,960
So once you've done this and you've identified some lines or some loops, then we're going

144
00:09:34,960 --> 00:09:38,320
to talk about, I guess, what you can do about that.

145
00:09:38,320 --> 00:09:44,160
So I've been working on this repository that has the theories that I've got in this talk.

146
00:09:44,160 --> 00:09:46,960
So if you don't believe me about any of these and you want to download them and run them

147
00:09:47,000 --> 00:09:48,720
yourself, you can do.

148
00:09:48,720 --> 00:09:51,720
So it's a GitHub Tony Belloni antipatterns.

149
00:09:51,720 --> 00:09:57,200
And basically what I'll do is have two functions, A and a B. They're functionally equivalent.

150
00:09:57,200 --> 00:10:01,880
They do exactly the same thing, but they've been implemented in slightly different ways.

151
00:10:01,880 --> 00:10:04,920
And the B function is the one which I think is more efficient.

152
00:10:04,920 --> 00:10:10,920
And then on the table, you see the mean and in brackets the percentage difference.

153
00:10:11,760 --> 00:10:17,800
So by applying this, basically by undoing this antipattern, you get 65% performance

154
00:10:17,800 --> 00:10:20,480
improvement on tiny functions.

155
00:10:20,480 --> 00:10:24,880
And I'll go through some of the most important ones.

156
00:10:24,880 --> 00:10:30,800
I've also been working on taking these learnings and trying to build them into a linter.

157
00:10:30,800 --> 00:10:33,560
This is very much a work in progress.

158
00:10:33,560 --> 00:10:36,600
It will raise a ton of false positives on your code.

159
00:10:36,600 --> 00:10:40,480
So don't put it in the way of CI, CD.

160
00:10:40,480 --> 00:10:42,400
Give it a go and see what it comes up with.

161
00:10:42,400 --> 00:10:46,960
I also need more people to have a go at this with their own code bases and tell me what

162
00:10:46,960 --> 00:10:49,920
kind of false positives it's bringing up.

163
00:10:49,920 --> 00:10:53,200
But I'll show you some examples of what it's doing in a minute.

164
00:10:53,200 --> 00:10:57,760
It is a pilot and extension, but you can also run it as a standalone linter by just giving

165
00:10:57,760 --> 00:11:00,280
it a directory.

166
00:11:00,280 --> 00:11:01,280
Okay.

167
00:11:01,280 --> 00:11:04,480
So the first concept we're going to cover is a big one.

168
00:11:04,480 --> 00:11:09,680
If anyone's ever done compiler design or anything like that, you might be familiar with this

169
00:11:10,160 --> 00:11:12,520
But most people are not.

170
00:11:12,520 --> 00:11:20,120
Loop invariance means that there is an expression or some instructions within a loop which are

171
00:11:20,120 --> 00:11:24,120
invariant, which means that the result of that expression does not change with each

172
00:11:24,120 --> 00:11:25,920
iteration of the loop.

173
00:11:25,920 --> 00:11:31,400
So the example I've got here is a function before.

174
00:11:31,400 --> 00:11:33,360
We've got a tuple x.

175
00:11:33,360 --> 00:11:36,720
We've got a variable i, which is set to 6.

176
00:11:36,720 --> 00:11:43,120
And the expression len x multiplied by i is always the same because x never changes and

177
00:11:43,120 --> 00:11:44,920
i never changes.

178
00:11:44,920 --> 00:11:49,480
So what perf lint is doing is basically identifying that expression and saying this expression

179
00:11:49,480 --> 00:11:51,920
is invariant.

180
00:11:51,920 --> 00:11:59,360
What some compilers would do, such as LLVM, GCC, the go compiler, is they'd identify that

181
00:11:59,360 --> 00:12:04,320
statement automatically when you compile the code and they'd actually move the expression

182
00:12:04,320 --> 00:12:05,840
outside of the loop.

183
00:12:05,840 --> 00:12:10,120
This is called loop invariant code motion or hoisting.

184
00:12:10,120 --> 00:12:15,920
The Python compiler doesn't do this, so you need to do it manually if it's worth it.

185
00:12:15,920 --> 00:12:17,760
And I'll touch on that in a second.

186
00:12:17,760 --> 00:12:22,080
So in the after function, this is what you would do to refactor that, which is to create

187
00:12:22,080 --> 00:12:27,720
a new variable called x i, or probably something slightly better named, and then to use that

188
00:12:27,720 --> 00:12:30,480
inside the loop.

189
00:12:30,480 --> 00:12:36,800
That probably looks like a really trivial thing, and it's 55% faster.

190
00:12:36,800 --> 00:12:40,600
So we're essentially doing the same thing, but we're not running the len x multiplied

191
00:12:40,600 --> 00:12:44,000
by i expression for every iteration of the loop.

192
00:12:44,000 --> 00:12:47,080
But it breaks two rules.

193
00:12:47,080 --> 00:12:49,280
First of all, it's more code.

194
00:12:49,280 --> 00:12:51,780
Secondly, it's less readable.

195
00:12:51,780 --> 00:12:56,040
So it was kind of more obvious before what len x multiplied by i was when you're reading

196
00:12:56,040 --> 00:12:57,040
that statement.

197
00:12:57,040 --> 00:13:01,760
Let's say this loop is massive and you're like 50 lines down, so it might not be that

198
00:13:01,760 --> 00:13:03,440
obvious.

199
00:13:03,440 --> 00:13:08,640
However, if you've run your profiler and you discover there's a particularly hot loop in

200
00:13:08,640 --> 00:13:14,920
your code, then you can apply these kind of methods, and you can see this kind of impact.

201
00:13:14,920 --> 00:13:18,680
Perflint will actually identify more than just tiny expressions like this.

202
00:13:18,680 --> 00:13:22,440
It'll actually pull out entire branches and say this branch is actually invariant, as

203
00:13:22,440 --> 00:13:25,440
in it's exactly the same each time.

204
00:13:25,440 --> 00:13:29,600
We've been running some benchmarks on the types of expressions that it makes sense to

205
00:13:29,600 --> 00:13:31,240
hoist out of your loop.

206
00:13:31,240 --> 00:13:37,720
They include things like dictionary lookups, method calls, function calls, a whole bunch

207
00:13:37,720 --> 00:13:41,360
of different things like that, which you would think as being cheap things that you can do

208
00:13:41,360 --> 00:13:42,360
in Python.

209
00:13:42,360 --> 00:13:46,480
But if you're running a hot loop like this, it can make a pretty dramatic difference.

210
00:13:46,480 --> 00:13:50,240
So that's number one.

211
00:13:50,240 --> 00:13:55,840
Number two, and I'm going to introduce my new favorite font, which is Comic Sans Mono.

212
00:13:55,840 --> 00:14:06,040
So this one, we've got our functions A and B. Hopefully this one's familiar to people.

213
00:14:06,040 --> 00:14:10,240
If you're fairly new to Python or you're still getting into the intermediate space, then

214
00:14:10,240 --> 00:14:12,200
I'll introduce this one to you.

215
00:14:12,200 --> 00:14:15,880
List comprehensions are faster than for loops in this kind of pattern.

216
00:14:15,880 --> 00:14:20,080
So where you're taking a list of something and then you're creating another list and

217
00:14:20,080 --> 00:14:27,200
filtering the items by some sort of expression, it is faster to do that in a list comprehension.

218
00:14:27,200 --> 00:14:28,200
It is also less code.

219
00:14:28,200 --> 00:14:31,420
It's also more readable.

220
00:14:31,420 --> 00:14:36,520
If you're familiar with list comprehensions, you might not be familiar with the siblings

221
00:14:36,520 --> 00:14:41,520
such as dictionary comprehensions and set comprehensions.

222
00:14:41,520 --> 00:14:45,600
Perflint, so the linter that I've been working on, will now automatically detect code where

223
00:14:46,320 --> 00:14:50,680
used dictionary or a list comprehension.

224
00:14:50,680 --> 00:14:51,680
And this is a quick win.

225
00:14:51,680 --> 00:14:56,240
So this is 23% performance gain for less code.

226
00:14:56,240 --> 00:14:58,200
So that's number two.

227
00:14:58,200 --> 00:15:00,880
So let's step this up a bit.

228
00:15:00,880 --> 00:15:05,920
The third one I see quite a lot is that people are using the wrong types for things.

229
00:15:05,920 --> 00:15:11,080
So there are lots of data structure types in Python that are built in.

230
00:15:11,080 --> 00:15:15,780
And depending on what you want to use them for, you should pick the appropriate type.

231
00:15:15,780 --> 00:15:20,840
If you use a type that was not designed for the purpose that you're trying to use it for,

232
00:15:20,840 --> 00:15:23,280
it will very likely be inefficient.

233
00:15:23,280 --> 00:15:29,000
So for example, if you have a collection of values which is constant and then you loop

234
00:15:29,000 --> 00:15:35,520
over them and you subscript that and use a list, that is actually more efficient to use

235
00:15:35,520 --> 00:15:36,520
a tuple.

236
00:15:36,520 --> 00:15:37,520
Tuple lookups are quicker.

237
00:15:37,520 --> 00:15:39,600
Tuple creation is quicker than lists.

238
00:15:40,520 --> 00:15:42,240
So I put this simple tree together.

239
00:15:42,240 --> 00:15:44,040
And I'm sure we could expand on this.

240
00:15:44,040 --> 00:15:47,480
But really the question was mutability.

241
00:15:47,480 --> 00:15:52,240
So do I need to change the information inside this collection?

242
00:15:52,240 --> 00:15:55,040
Does it have unique items?

243
00:15:55,040 --> 00:16:01,140
And then you could choose between a set a list, a byte array.

244
00:16:01,140 --> 00:16:05,000
And then if it's immutable, then what are the contents?

245
00:16:05,000 --> 00:16:07,320
So they byte string any.

246
00:16:07,320 --> 00:16:10,520
And then if it's any, do we need unique items or not?

247
00:16:10,520 --> 00:16:15,920
There's also memory view under bytes.

248
00:16:15,920 --> 00:16:17,720
But that's a really niche one.

249
00:16:17,720 --> 00:16:22,400
So yeah, this is my simple structure.

250
00:16:22,400 --> 00:16:24,920
But that was the simple types.

251
00:16:24,920 --> 00:16:28,720
You've then got things like classes, data classes, dictionaries, or named tuples.

252
00:16:28,720 --> 00:16:32,020
I wasn't aware of this when I started these benchmarks.

253
00:16:32,020 --> 00:16:36,240
But there's a massive difference in performance between all of these.

254
00:16:36,240 --> 00:16:40,200
So when you're considering which of these to use, first of all, think about how you're

255
00:16:40,200 --> 00:16:41,200
going to use it.

256
00:16:41,200 --> 00:16:46,600
So how much mapping are you doing between the source data and your representation in

257
00:16:46,600 --> 00:16:47,600
Python?

258
00:16:47,600 --> 00:16:54,640
So if you're reading from JSON or you're reading from JSON data from an API, you're then converting

259
00:16:54,640 --> 00:16:58,400
that into a list of classes with attributes.

260
00:16:58,400 --> 00:17:00,680
So you're doing a mapping function.

261
00:17:00,680 --> 00:17:05,040
And then you're unmapping all of that back out into JSON for your API endpoint.

262
00:17:05,040 --> 00:17:07,480
That's super inefficient.

263
00:17:07,480 --> 00:17:14,320
So if you look at a lot of the ODMs and ORMs, in terms of their efficiency gain, they try

264
00:17:14,320 --> 00:17:16,080
and reduce mapping.

265
00:17:16,080 --> 00:17:19,240
So how much mapping is there between your source and target data?

266
00:17:19,240 --> 00:17:22,560
What operations are you mainly doing with these types?

267
00:17:22,560 --> 00:17:25,160
So are you looping over them?

268
00:17:25,160 --> 00:17:26,360
Are you mapping them?

269
00:17:26,360 --> 00:17:27,360
Are you sorting them?

270
00:17:27,360 --> 00:17:28,360
Are you searching them?

271
00:17:28,360 --> 00:17:31,520
And it's very likely there's an appropriate type for each of those.

272
00:17:32,480 --> 00:17:36,760
Even if you do love data classes, sometimes it's better or more efficient to use something

273
00:17:36,760 --> 00:17:37,760
else.

274
00:17:37,760 --> 00:17:41,440
Even if you love dictionaries, sometimes it's better or more efficient to use something

275
00:17:41,440 --> 00:17:42,440
else.

276
00:17:42,440 --> 00:17:45,520
It really depends on what you're trying to represent.

277
00:17:45,520 --> 00:17:49,160
And then lastly, is the API important?

278
00:17:49,160 --> 00:17:54,000
So is what you present back to the consumer of your function or your code important?

279
00:17:54,000 --> 00:17:58,280
So dictionaries are brilliant, but if your functions all just return a dictionary and

280
00:17:58,320 --> 00:18:04,160
you don't document what keys it has, it's not particularly helpful to your users, whereas

281
00:18:04,160 --> 00:18:08,000
classes obviously can be a lot more strongly typed.

282
00:18:08,000 --> 00:18:09,760
So let's jump into an example.

283
00:18:09,760 --> 00:18:12,960
So we've got three implementations of the same thing.

284
00:18:12,960 --> 00:18:15,680
We've got a data class.

285
00:18:15,680 --> 00:18:17,680
We've got a name tuple.

286
00:18:17,680 --> 00:18:24,040
I still can't ever decide which pronunciation it's supposed to be.

287
00:18:24,040 --> 00:18:29,560
And then also the lesser known one is you can create a class which inherits from typing.nameTuple,

288
00:18:29,560 --> 00:18:33,280
which basically does the same thing as the middle one.

289
00:18:33,280 --> 00:18:37,400
So those three implementations all do the same thing.

290
00:18:37,400 --> 00:18:42,200
OK, let's try and split the room.

291
00:18:42,200 --> 00:18:45,880
Would you think data classes are faster or slower than name tuples?

292
00:18:45,880 --> 00:18:48,680
Put up your hand if you think data classes are faster.

293
00:18:48,680 --> 00:18:50,800
OK, there's a couple.

294
00:18:50,800 --> 00:18:51,800
Yeah.

295
00:18:51,800 --> 00:18:52,800
And name tuples are faster?

296
00:18:52,800 --> 00:18:56,600
OK, good.

297
00:18:56,600 --> 00:19:01,840
This is a class that we can write which basically does the same thing.

298
00:19:01,840 --> 00:19:03,960
I've just explicitly defined everything.

299
00:19:03,960 --> 00:19:07,640
So this basically is the hard way of doing data classes, which is just to write them

300
00:19:07,640 --> 00:19:10,040
by hand.

301
00:19:10,040 --> 00:19:14,200
Would you think that the concrete class is faster than the name tuple?

302
00:19:14,200 --> 00:19:16,200
Put up your hand.

303
00:19:16,200 --> 00:19:21,520
OK, that's interesting.

304
00:19:21,520 --> 00:19:24,760
The concrete class is 51% faster than the data class.

305
00:19:24,760 --> 00:19:25,760
Probably no surprise.

306
00:19:25,760 --> 00:19:29,560
A lot of the reason for that is actually the overhead of the magic methods in data classes.

307
00:19:29,560 --> 00:19:34,400
It's 28% faster than the name tuple.

308
00:19:34,400 --> 00:19:40,360
So that might come as a surprise to 99% of you who didn't put your hand up.

309
00:19:40,360 --> 00:19:41,860
It breaks two rules, though.

310
00:19:41,860 --> 00:19:42,860
It's more code.

311
00:19:42,860 --> 00:19:44,720
And it's probably less readable as well.

312
00:19:44,720 --> 00:19:48,240
So this is an interesting one.

313
00:19:48,240 --> 00:19:51,880
So I want to push this a bit further and say, OK, hopefully this is readable.

314
00:19:51,880 --> 00:19:52,880
I'll read them out.

315
00:19:52,880 --> 00:19:55,880
Python 3.9, tested as in 3.10.

316
00:19:55,880 --> 00:19:58,880
Doesn't make much difference.

317
00:19:58,880 --> 00:20:01,040
3.9 is in green.

318
00:20:01,040 --> 00:20:07,640
Pypy 3.9 is in grayish black.

319
00:20:07,640 --> 00:20:11,920
Dictionaries are slower than custom classes with slots.

320
00:20:11,920 --> 00:20:14,640
That's the weird thing I discovered, number one.

321
00:20:14,680 --> 00:20:21,360
Secondly, name tuple is around the same as typing.name tuple, which was not that surprising.

322
00:20:21,360 --> 00:20:25,560
Custom classes and then custom classes with slots.

323
00:20:25,560 --> 00:20:28,760
There's something wrong there.

324
00:20:28,760 --> 00:20:30,640
That's wrong.

325
00:20:30,640 --> 00:20:32,240
OK, sorry.

326
00:20:32,240 --> 00:20:35,360
Custom classes should be further down.

327
00:20:35,360 --> 00:20:39,360
And that's because I tested on 3.10.

328
00:20:39,360 --> 00:20:41,080
Custom classes are much faster in 3.10.

329
00:20:41,080 --> 00:20:42,800
This is a 3.9 graph.

330
00:20:42,800 --> 00:20:44,600
OK, my apologies.

331
00:20:44,640 --> 00:20:48,000
So in 3.10, custom classes will be further down.

332
00:20:48,000 --> 00:20:51,400
So they would be the fastest implementation.

333
00:20:51,400 --> 00:20:53,920
Data classes are significantly slower.

334
00:20:53,920 --> 00:20:58,120
I mentioned the magic methods as one of the reasons.

335
00:20:58,120 --> 00:21:05,560
However, if you've got only a couple of instances of the data class, then don't bother rewriting

336
00:21:05,560 --> 00:21:11,840
it as a concrete class, because 51% faster of next to nothing is still next to nothing.

337
00:21:11,840 --> 00:21:15,880
Even if you're creating hundreds of instances, then it's probably worth considering your

338
00:21:15,880 --> 00:21:18,880
structured types.

339
00:21:18,880 --> 00:21:21,560
So that's the next pattern.

340
00:21:21,560 --> 00:21:28,960
So this one I see quite a bit, which is people not really understanding how iterables work

341
00:21:28,960 --> 00:21:29,960
in Python.

342
00:21:29,960 --> 00:21:36,560
So here we're defining a tuple of items, and then in the loop we're saying, OK, let's turn

343
00:21:36,560 --> 00:21:39,280
it into a list so we can loop over it.

344
00:21:40,080 --> 00:21:46,440
The tuple iterator is built into tuple, so that's completely unnecessary.

345
00:21:46,440 --> 00:21:50,480
And also it would eagerly iterate it and then create another iterator and iterate it a second

346
00:21:50,480 --> 00:21:51,480
time.

347
00:21:51,480 --> 00:21:54,880
So it basically does twice as much work to do the same thing.

348
00:21:54,880 --> 00:21:59,360
Perflint has underlined it because it's saying what you're doing is wrong.

349
00:21:59,360 --> 00:22:07,880
The next tiny one is that if you have an unmutated collection, so you've created a collection

350
00:22:07,920 --> 00:22:14,080
of something and you're not changing it, don't use the list, use the tuple, unless you're

351
00:22:14,080 --> 00:22:17,280
leaking that variable out to other functions.

352
00:22:17,280 --> 00:22:24,040
20% faster to create and index, and it's a fairly small change.

353
00:22:24,040 --> 00:22:26,080
So that's a tiny one.

354
00:22:26,080 --> 00:22:32,640
So we're going to get onto the big one now, which is calling too many functions.

355
00:22:32,640 --> 00:22:37,800
So if you're familiar with the acronym DRY, don't repeat yourself.

356
00:22:37,800 --> 00:22:41,640
This is the concept that in your code you should not write the same code over and over

357
00:22:41,640 --> 00:22:42,640
again.

358
00:22:42,640 --> 00:22:47,440
And you probably learn this when you learn to program.

359
00:22:47,440 --> 00:22:51,000
In some languages that makes 100% sense.

360
00:22:51,000 --> 00:22:54,160
In Python there's caveats to that.

361
00:22:54,160 --> 00:23:00,600
So calling a function, especially a pure Python function, has an overhead in Python, and I'll

362
00:23:00,600 --> 00:23:02,920
talk about that in a second.

363
00:23:02,920 --> 00:23:07,240
If you're calling a C function like a built-in or something that's a C library, there is

364
00:23:07,240 --> 00:23:11,840
still an overhead and it's still significant, but it's less.

365
00:23:11,840 --> 00:23:16,040
Okay, so let's give an example.

366
00:23:16,040 --> 00:23:21,240
A and B, and this is the smallest example I could think of.

367
00:23:21,240 --> 00:23:25,640
We've made a function called add, which in reality you probably never do, but I wanted

368
00:23:25,640 --> 00:23:28,480
to just extract the difference as well as possible.

369
00:23:28,480 --> 00:23:36,440
So A and B, we're going to loop 100,000 times between over N and then add X, which is a

370
00:23:36,480 --> 00:23:43,480
constant of one to N. So B is 56% faster than A. So the difference between the two is that

371
00:23:45,280 --> 00:23:46,720
we're not calling a function.

372
00:23:46,720 --> 00:23:51,840
So that basically isolates what's the overhead of calling this function.

373
00:23:51,840 --> 00:23:58,840
If you call the function twice, 65%, if you go even further, then basically it goes up

374
00:24:00,160 --> 00:24:02,640
to a point where it hits the line.

375
00:24:02,680 --> 00:24:06,880
So basically the overhead is a function of the number of times the function is called

376
00:24:06,880 --> 00:24:09,440
or the cost of the function.

377
00:24:09,440 --> 00:24:13,440
The reason I bring this up is because I see this quite a lot in code where people create

378
00:24:13,440 --> 00:24:17,600
a small utility function and they call that in a hot loop.

379
00:24:17,600 --> 00:24:22,080
So if you have a hot loop and you've identified what they are when you've been profiling your

380
00:24:22,080 --> 00:24:28,400
code, start to think about how you can possibly inline some of those instructions.

381
00:24:28,640 --> 00:24:33,600
So this is horrible and it creates more code and it's very unpythonic.

382
00:24:33,600 --> 00:24:38,400
But if you need to be brutal in some hot areas of your code, then this is a sledgehammer

383
00:24:38,400 --> 00:24:41,320
that you can use.

384
00:24:41,320 --> 00:24:48,320
I've been experimenting with a utility library, which I'd love for people to have a go with

385
00:24:49,360 --> 00:24:52,400
and to give me some feedback on.

386
00:24:52,400 --> 00:24:59,200
The concept is basically that you could put a decorator on a function and whenever you

387
00:24:59,200 --> 00:25:05,000
call that function, the module will basically inline the instructions into the callee, so

388
00:25:05,000 --> 00:25:07,880
wherever it was called.

389
00:25:07,880 --> 00:25:11,520
This works by doing a, it basically manipulates the source code.

390
00:25:11,520 --> 00:25:15,460
So you have to actually give the source file to the script and it will give you the equivalent

391
00:25:15,460 --> 00:25:17,200
after the inlining.

392
00:25:17,200 --> 00:25:22,800
I don't think there's any plans to have inlining in CPython or in the language.

393
00:25:22,800 --> 00:25:27,280
I know that the team are actively working on reducing function call overhead.

394
00:25:27,280 --> 00:25:32,200
This week we've been doing a lot of testing on 3.11 versus 3.10.

395
00:25:32,200 --> 00:25:36,520
So my point about this is still relevant in 3.11.

396
00:25:36,520 --> 00:25:41,340
Hopefully in 3.12 that will become less the case.

397
00:25:41,340 --> 00:25:44,080
So that was point number four.

398
00:25:44,080 --> 00:25:51,080
The bonus one I want to bring up, which only got discovered this morning, is the match

399
00:25:55,520 --> 00:25:56,520
statement.

400
00:25:56,520 --> 00:26:00,040
I watched the talk on the match statement this week, which is brilliant.

401
00:26:00,040 --> 00:26:06,320
We were running some benchmarks on 3.11 to compare the match statement with the equivalent

402
00:26:06,320 --> 00:26:07,480
of Python code.

403
00:26:07,480 --> 00:26:10,840
For sequences, it's 80% faster.

404
00:26:10,840 --> 00:26:13,920
The match statement is 80% faster for sequences.

405
00:26:13,920 --> 00:26:16,720
For mappings, it wasn't.

406
00:26:16,720 --> 00:26:22,240
But then I nerd sniped Brandt Butcher and he was up until 3 a.m. working on the code

407
00:26:22,240 --> 00:26:25,120
base and he has fixed that.

408
00:26:25,120 --> 00:26:29,600
Once it's merged, it will be 80% faster.

409
00:26:29,600 --> 00:26:35,600
If that can get merged into 3.11 before the beta date, then there's a new rule, which

410
00:26:35,600 --> 00:26:42,560
is use match statements because they're significantly faster for both sequences and mappings.

411
00:26:42,560 --> 00:26:46,920
So let's wrap this up.

412
00:26:46,920 --> 00:26:50,600
We're looking for these four anti-patterns that I've raised in the talk.

413
00:26:50,600 --> 00:26:55,320
These are not the only ones, but it's really the biggest ones I've seen, the ones with

414
00:26:55,320 --> 00:26:57,440
the biggest impact.

415
00:26:57,440 --> 00:27:04,160
Loop invariance as a concept is really important if you can understand that as a developer.

416
00:27:04,160 --> 00:27:09,240
Doing a subscript on a dictionary, so doing a dictionary key lookup, is a statement that

417
00:27:10,000 --> 00:27:13,920
a lot of cases if the dictionary hasn't changed.

418
00:27:13,920 --> 00:27:16,720
So look for these in your code.

419
00:27:16,720 --> 00:27:20,480
Utilize comprehension when and where you can.

420
00:27:20,480 --> 00:27:25,400
Very importantly, make sure you're selecting the right data type and also consider the

421
00:27:25,400 --> 00:27:31,680
overheads of data types but also the APIs of the data types that you're using.

422
00:27:31,680 --> 00:27:38,080
And avoid tiny functions in caveat in hot code.

423
00:27:38,120 --> 00:27:41,240
Tiny functions are great and they create clean code, utility functions.

424
00:27:41,240 --> 00:27:46,040
However, if you're using it in a loop, which is getting slammed millions of times, then

425
00:27:46,040 --> 00:27:50,040
you want to micro-optimize that, then I'd recommend looking at maybe inlining some of

426
00:27:50,040 --> 00:27:51,040
those.

427
00:27:51,040 --> 00:27:54,960
The methods I showed you is hoisting, where you basically just assign that expression

428
00:27:54,960 --> 00:28:01,040
to a variable, a local variable, and then just refer to that variable in the loop.

429
00:28:01,040 --> 00:28:04,320
But before you do any of this, make sure you've set up some sampling.

430
00:28:04,320 --> 00:28:08,600
You've sampled your code correctly and you understand how your application performed

431
00:28:08,600 --> 00:28:12,800
and you know which are the areas of your code, which are the areas of your program which

432
00:28:12,800 --> 00:28:15,200
are causing most of the execution time.

433
00:28:15,200 --> 00:28:18,880
So don't just go and apply this principle blindly to all of your code base.

434
00:28:18,880 --> 00:28:22,920
Perflint will give you input across everything.

435
00:28:22,920 --> 00:28:27,400
It will tell you that you should optimize your test functions and it will tell you all

436
00:28:27,400 --> 00:28:31,640
sorts of stuff, but you need to apply common sense to that as a developer and combine that

437
00:28:31,640 --> 00:28:32,640
with profiling.

438
00:28:32,640 --> 00:28:37,120
Let's say, okay, let's focus on this part of our code because we think we can make this

439
00:28:37,120 --> 00:28:44,160
50% faster, just this function, but if that uses up 80% of our execution time, then potentially

440
00:28:44,160 --> 00:28:47,680
you've just quadrupled your execution performance.

441
00:28:47,680 --> 00:28:51,420
So focus on the areas that matter.

442
00:28:51,420 --> 00:28:52,760
Track regressions if you can.

443
00:28:52,760 --> 00:28:57,840
So if you can introduce a tool to measure performance between commits, that also makes

444
00:28:57,840 --> 00:28:58,840
a difference.

445
00:28:59,840 --> 00:29:03,120
for doing benchmarking, which is great.

446
00:29:03,120 --> 00:29:07,320
And also talk to your team about some of these ideas and some of these principles so if you

447
00:29:07,320 --> 00:29:14,800
can catch it in code review, you can avoid performance regressions in the future.

448
00:29:14,800 --> 00:29:18,520
And then lastly, to wrap it up, that's my website.

449
00:29:18,520 --> 00:29:19,520
That's my Twitter handle.

450
00:29:19,520 --> 00:29:26,120
I wrote a book on the CPython compiler called CPython internals.

451
00:29:26,120 --> 00:29:27,920
And so make sure you check that out.

452
00:29:28,000 --> 00:29:31,760
I've also got loads of socks, which is probably the most important thing.

453
00:29:31,760 --> 00:29:34,600
So if you want a pair of socks, we've got some really nice socks.

454
00:29:34,600 --> 00:29:35,600
They're not these ones, though.

455
00:29:35,600 --> 00:29:39,480
These are from PyCon APAC and they're awesome.

456
00:29:39,480 --> 00:29:41,400
So I have loads of socks at the front.

457
00:29:41,400 --> 00:29:42,760
Thank you for coming to my talk.

458
00:29:42,760 --> 00:29:46,480
If you have any comments or any feedback or anything and you want to try out Perflint,

459
00:29:46,480 --> 00:29:49,680
please catch me outside after the talk and I'd be happy to chat.

460
00:29:49,680 --> 00:29:50,680
Thanks, everybody.

