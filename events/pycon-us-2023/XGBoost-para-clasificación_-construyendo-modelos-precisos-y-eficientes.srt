1
00:00:00,000 --> 00:00:02,280
Hola, OK.

2
00:00:02,280 --> 00:00:03,400
Buenas tardes.

3
00:00:03,400 --> 00:00:06,960
Vamos a iniciar ya con nuestra segunda parte del día,

4
00:00:06,960 --> 00:00:09,480
el último día de las PyCon charlas.

5
00:00:09,480 --> 00:00:12,880
Les dejamos con Laura, que nos va a estar platicando un poco

6
00:00:12,880 --> 00:00:17,920
del uso de XGBOOST para problemas de clasificación.

7
00:00:17,920 --> 00:00:18,760
Un aplauso para ella.

8
00:00:23,360 --> 00:00:24,360
Muchas gracias.

9
00:00:24,360 --> 00:00:27,160
Y si puedo pedir el micrófono, si el micrófono está prendido.

10
00:00:30,080 --> 00:00:30,800
Si me pueden escuchar.

11
00:00:30,800 --> 00:00:31,960
OK, perfecto.

12
00:00:31,960 --> 00:00:35,280
OK, muchísimas gracias por llegar y espero que hayan tenido

13
00:00:35,280 --> 00:00:36,280
una buena comida.

14
00:00:36,280 --> 00:00:37,720
Que tengan provechito.

15
00:00:37,720 --> 00:00:38,920
Y vamos a empezar.

16
00:00:38,920 --> 00:00:43,920
Creo que antes de empezar quiero sentir un poquito más el

17
00:00:43,920 --> 00:00:44,640
público.

18
00:00:44,640 --> 00:00:47,400
¿Me pueden decir si han tenido la oportunidad de trabajar con

19
00:00:47,400 --> 00:00:50,600
problemas de clasificación antes?

20
00:00:50,600 --> 00:00:51,520
OK.

21
00:00:51,520 --> 00:00:52,600
OK.

22
00:00:52,600 --> 00:00:54,360
Para los que dejaron la mano abajo,

23
00:00:54,360 --> 00:00:55,360
es la primera.

24
00:00:55,360 --> 00:00:58,400
¿Han tenido oportunidad de trabajar con manejo de datos,

25
00:00:58,400 --> 00:01:01,000
de limpieza de datos?

26
00:01:01,000 --> 00:01:01,720
OK.

27
00:01:01,720 --> 00:01:02,240
OK.

28
00:01:02,240 --> 00:01:03,200
Entonces vamos a empezar.

29
00:01:03,200 --> 00:01:08,560
Si quieren, las dispositivas están en línea y hay un

30
00:01:08,560 --> 00:01:09,320
componente en línea.

31
00:01:09,320 --> 00:01:12,280
Entonces, si quieren revisarlo, pueden visitar a tinyurl.com

32
00:01:12,280 --> 00:01:15,160
XGBOOST charla.

33
00:01:15,160 --> 00:01:15,600
OK.

34
00:01:15,600 --> 00:01:18,840
Entonces, lo que voy a compartir con ustedes es el uso de una

35
00:01:18,840 --> 00:01:21,040
biblioteca de Python llamada XGBOOST,

36
00:01:21,040 --> 00:01:23,240
que puede ser utilizada no solo para clasificación,

37
00:01:23,240 --> 00:01:25,320
sino también para varios problemas de aprendizaje

38
00:01:25,320 --> 00:01:25,840
automático.

39
00:01:25,840 --> 00:01:28,480
Estos incluyen problemas como la regresión,

40
00:01:28,480 --> 00:01:29,960
clasificación, entre otros.

41
00:01:29,960 --> 00:01:31,880
Entonces, vamos a empezar.

42
00:01:31,880 --> 00:01:33,600
Les voy a decir un poquito sobre mí.

43
00:01:33,600 --> 00:01:37,040
Yo trabajo como developer advocate en un startup que se

44
00:01:37,040 --> 00:01:38,400
llama Plumber.

45
00:01:38,400 --> 00:01:41,560
En Plumber lo que hacemos es que desarrollamos herramientas para

46
00:01:41,560 --> 00:01:46,400
científicos de datos para que sea más fácil llamar bases de

47
00:01:46,400 --> 00:01:51,200
datos y para desarrollar y lanzar modelos de aprendizaje

48
00:01:51,200 --> 00:01:53,240
automático a nivel producción.

49
00:01:53,240 --> 00:01:54,960
Yo nací en México, en Durango.

50
00:01:55,000 --> 00:01:58,920
Viviendo en Durango como 15 años.

51
00:01:58,920 --> 00:02:02,160
Me mudé a Lusito Federal, donde empecé a estudiar en el poli.

52
00:02:02,160 --> 00:02:04,600
Y después de estar en el poli, me mudé a Canadá con mi familia.

53
00:02:04,600 --> 00:02:09,440
Entonces, básicamente empecé de ceros y estudié matemáticas

54
00:02:09,440 --> 00:02:10,080
allá en Canadá.

55
00:02:10,080 --> 00:02:12,200
Ahorita sigo viviendo en Canadá.

56
00:02:12,200 --> 00:02:15,440
Y hace poquito, relativamente poquito empecé a practicar el

57
00:02:15,440 --> 00:02:16,360
yujitsu brasileño.

58
00:02:16,360 --> 00:02:19,000
Es como un arte marcial que me gusta muchísimo.

59
00:02:19,000 --> 00:02:20,000
Es como grappling.

60
00:02:20,000 --> 00:02:23,080
Entonces, es muy intenso, pero me ha gustado bastante.

61
00:02:25,560 --> 00:02:29,200
OK. Y si quieren conectar conmigo, yo estoy disponible para hablar

62
00:02:29,200 --> 00:02:30,160
después de la charla.

63
00:02:30,160 --> 00:02:33,520
O si quieren conectarme, si tienen más preguntas, mi sitio web es

64
00:02:33,520 --> 00:02:37,960
básicamente la primera letra de mi nombre y mi segundo nombre.

65
00:02:37,960 --> 00:02:40,840
Me pueden encontrar en GitHub, en Twitter, en LinkedIn y en más

66
00:02:40,840 --> 00:02:43,040
todo también.

67
00:02:43,040 --> 00:02:45,160
Eso es que bueno, vamos a lanzarnos.

68
00:02:45,160 --> 00:02:48,200
Cuando estamos pensando en un problema de clasificación o

69
00:02:48,200 --> 00:02:52,040
cuando pensamos en clasificación como desde el punto de vista de

70
00:02:52,080 --> 00:02:56,600
ciencia de datos, pensamos, bueno, tenemos un problema y nuestro

71
00:02:56,600 --> 00:02:59,320
goal es decidir si algo va a pasar o no.

72
00:02:59,320 --> 00:03:01,920
Entonces, por ejemplo, un ejemplo de clasificación es vamos a

73
00:03:01,920 --> 00:03:04,920
decidir si un email es spam o no.

74
00:03:04,920 --> 00:03:06,480
Ustedes cuando les llega un email,

75
00:03:06,480 --> 00:03:08,320
¿cómo determinan qué es spam?

76
00:03:14,000 --> 00:03:15,680
¿Cómo?

77
00:03:15,680 --> 00:03:16,960
¿El contenido del asunto?

78
00:03:16,960 --> 00:03:18,680
¿Cuál otro?

79
00:03:18,680 --> 00:03:21,720
OK.

80
00:03:21,760 --> 00:03:24,440
Si tiene demasiados links o si los links se ven medio sospechosos,

81
00:03:24,440 --> 00:03:27,400
si te están pidiendo dinero o si te dicen, hay una urgencia,

82
00:03:27,400 --> 00:03:29,880
tu familia lo secuestraron, necesitamos que nos des los

83
00:03:29,880 --> 00:03:32,480
llames de tu casa, o sea, algo así.

84
00:03:32,480 --> 00:03:34,960
Entonces, cuando pensamos en clasificación,

85
00:03:34,960 --> 00:03:37,880
piensa que vas a poner tu problema,

86
00:03:37,880 --> 00:03:39,720
vas a plantear tu problema de tal manera que pienses,

87
00:03:39,720 --> 00:03:43,080
bueno, si yo fuera a plantear la situación de que voy a escribir

88
00:03:43,080 --> 00:03:47,440
OK, los resultados como sí o no, si es binario,

89
00:03:47,440 --> 00:03:49,480
o si hay más de una categoría.

90
00:03:49,520 --> 00:03:52,880
Entonces, por ejemplo, cuando pensamos en clasificación,

91
00:03:52,880 --> 00:03:55,000
estamos pensando que queremos predecir,

92
00:03:55,000 --> 00:03:57,680
básicamente, resultados discretos.

93
00:03:57,680 --> 00:04:00,160
Uno, dos, tres, cuatro, cinco, discretos.

94
00:04:00,160 --> 00:04:01,640
Mientras que cuando estamos haciendo regresión,

95
00:04:01,640 --> 00:04:04,720
estamos pensando en predecir cosas que son más continuas,

96
00:04:04,720 --> 00:04:08,320
por ejemplo, cuál va a ser el precio del bienes raíces dentro

97
00:04:08,320 --> 00:04:12,320
de 20 años o cuántos años le queda a una persona que tiene,

98
00:04:12,320 --> 00:04:15,200
no sé, alguna enfermedad, ese tipo de cosas.

99
00:04:15,200 --> 00:04:18,600
Y lo que voy a introducir aquí es el uso de XGBoost

100
00:04:18,600 --> 00:04:20,120
para un problema de clasificación,

101
00:04:20,120 --> 00:04:24,720
pero XGBoost puede ser utilizado también para problemas de regresión.

102
00:04:24,720 --> 00:04:26,880
OK, entonces con eso les dije un poquitito de sobre

103
00:04:26,880 --> 00:04:28,160
qué es la clasificación.

104
00:04:28,160 --> 00:04:30,680
En el problema que vamos a usar aquí, voy a decir OK,

105
00:04:30,680 --> 00:04:33,800
digamos que, por ejemplo, quiero preguntarles a ustedes,

106
00:04:33,800 --> 00:04:38,120
vamos a predecir si un vino es de buena calidad o no.

107
00:04:38,120 --> 00:04:40,440
Entonces, el problema como está planteado es,

108
00:04:40,440 --> 00:04:44,760
el vino es de buena calidad, sí o no, y los datos que vamos a usar va a ser,

109
00:04:44,760 --> 00:04:47,000
básicamente, bueno, características del vino,

110
00:04:47,000 --> 00:04:50,280
de cuál es la composición química,

111
00:04:50,280 --> 00:04:56,400
si tiene algún ingrediente especial, cuántos años estuvo añejando, etcétera.

112
00:04:56,400 --> 00:04:58,680
OK, entonces ya pasamos por eso.

113
00:04:58,680 --> 00:05:02,720
Y XGBoost está desarrollado como una biblioteca que está basada

114
00:05:02,720 --> 00:05:04,200
en el algoritmo de gradient boosting.

115
00:05:04,200 --> 00:05:08,440
Este es un algoritmo que está diseñado para minimizar el,

116
00:05:08,440 --> 00:05:10,520
creo que en inglés se llama loss function,

117
00:05:10,520 --> 00:05:12,320
creo que es la función de pérdida.

118
00:05:12,320 --> 00:05:14,600
Básicamente, cuando estamos desarrollando un modelo

119
00:05:14,680 --> 00:05:18,760
de aprendizaje automático, nuestro goal es disminuir lo más que podamos

120
00:05:18,760 --> 00:05:23,120
la distancia entre la realidad y lo que nuestro modelo está prediciendo.

121
00:05:23,120 --> 00:05:27,040
Entonces, XGBoost está desarrollado de tal manera que su primer goal es

122
00:05:27,040 --> 00:05:29,880
estar basado en minimizar esa distancia.

123
00:05:29,880 --> 00:05:33,800
Si pensamos, por ejemplo, bueno, yo estoy prediciendo mañana,

124
00:05:33,800 --> 00:05:37,360
mañana va a estar soleado y el día de mañana no está soleado,

125
00:05:37,360 --> 00:05:39,200
la distancia es grandísima.

126
00:05:39,200 --> 00:05:40,640
Pero si yo predico mañana va a ser soleado,

127
00:05:40,640 --> 00:05:43,080
mañana está soleado, la distancia es básicamente cero.

128
00:05:43,080 --> 00:05:48,520
Entonces, XGBoost está enfocado a minimizar esa distancia.

129
00:05:48,520 --> 00:05:52,480
En términos de utilizarlo, es muy, muy parecido a usar Scikit Learn.

130
00:05:52,480 --> 00:05:56,920
Es el mismo workflow o el mismo número de pasos.

131
00:05:56,920 --> 00:06:01,120
Y antes de que siga, ¿cuántos de ustedes han trabajado con Scikit Learn?

132
00:06:01,120 --> 00:06:04,480
OK. Entonces, usar XGBoost es básicamente la misma idea.

133
00:06:04,480 --> 00:06:06,640
Vamos a inicializar un modelo.

134
00:06:06,640 --> 00:06:09,640
Bueno, primero vamos a separar nuestros datos, inicializamos el modelo,

135
00:06:09,640 --> 00:06:12,480
utilizamos el modelo para ajustarlo y luego lo utilizamos

136
00:06:12,520 --> 00:06:14,120
para predecir.

137
00:06:14,120 --> 00:06:18,280
Pero tiene otras herramientas que Scikit Learn no tiene.

138
00:06:18,280 --> 00:06:22,880
Dos de las herramientas que podemos utilizar es el uso de hiperparámetros,

139
00:06:22,880 --> 00:06:27,440
que básicamente si nosotros queremos decir, bueno, vamos a atuniar

140
00:06:27,440 --> 00:06:30,840
o vamos a mejorar el performance del modelo,

141
00:06:30,840 --> 00:06:33,360
XGBoost nos da esas herramientas.

142
00:06:33,360 --> 00:06:36,600
OK. Entonces, vamos a empezar.

143
00:06:36,600 --> 00:06:39,880
Para instalar XGBoost es compatible con Python,

144
00:06:39,920 --> 00:06:43,480
entonces podemos utilizar pip install XGBoost para descargarlo

145
00:06:43,480 --> 00:06:46,320
en nuestro ambiente local.

146
00:06:46,320 --> 00:06:49,480
Y para usarlo, en este caso es bastante fácil,

147
00:06:49,480 --> 00:06:52,400
es bastante similar a cómo utilizaríamos otros modelos.

148
00:06:52,400 --> 00:06:56,920
Entonces, por ejemplo, para visualizar, puedo usar mapply pipeline as PLT.

149
00:06:56,920 --> 00:07:01,360
Aquí tenemos XGBoost y voy a utilizar el alias XGB

150
00:07:01,360 --> 00:07:04,080
para básicamente decir quiero utilizar todos los modelos de ahí

151
00:07:04,080 --> 00:07:06,400
para XGBoost.

152
00:07:06,440 --> 00:07:11,640
Cosas que voy a hacer aquí incluye Scikit Learn para seleccionar datos,

153
00:07:11,640 --> 00:07:15,440
voy a seleccionar train test split, que es una función que utilizo

154
00:07:15,440 --> 00:07:18,400
para separar mis datos en testing y training.

155
00:07:18,400 --> 00:07:23,320
Básicamente datos para probar, datos para que el modelo entrene

156
00:07:23,320 --> 00:07:31,000
y datos para evaluar cómo es que el modelo está resultando.

157
00:07:31,000 --> 00:07:33,240
Entonces, vamos a empezar.

158
00:07:33,240 --> 00:07:35,800
Entonces, como les dije hace al principio de la charla,

159
00:07:35,840 --> 00:07:39,280
en este caso vamos a utilizar un problema de clasificación

160
00:07:39,280 --> 00:07:42,720
donde nuestro goal es clasificar la calidad de un vino.

161
00:07:42,720 --> 00:07:46,800
El vino es de buena calidad o el vino no es de buena calidad.

162
00:07:46,800 --> 00:07:49,960
El problema en este caso podemos pensar que es así

163
00:07:49,960 --> 00:07:51,480
como clasificación binaria.

164
00:07:51,480 --> 00:07:56,000
No vamos a pensar que está medio bien o medio mal, vamos a pensar binario.

165
00:07:56,000 --> 00:08:00,600
Y nuestro goal va a ser utilizar los datos, en este caso,

166
00:08:00,600 --> 00:08:04,920
en el repositorio de Aprendizaje Automático de UCI.

167
00:08:04,960 --> 00:08:07,200
Entonces, el procedimiento es exactamente el mismo

168
00:08:07,200 --> 00:08:09,840
a que si estuvieras utilizando Scikit Learn, primero que nada,

169
00:08:09,840 --> 00:08:11,680
vamos a leer nuestros datos.

170
00:08:11,680 --> 00:08:15,040
Normalmente pasaríamos bastante tiempo limpiando los datos,

171
00:08:15,040 --> 00:08:17,600
haciendo cosas como feature engineering,

172
00:08:17,600 --> 00:08:21,360
que es el proceso de elegir cuáles son las características

173
00:08:21,360 --> 00:08:23,800
del conjunto de datos que nosotros nos interesan.

174
00:08:23,800 --> 00:08:26,400
En este caso XGBoost tiene una herramienta muy,

175
00:08:26,400 --> 00:08:30,360
muy padre que se llama, en inglés se llama feature importances,

176
00:08:30,360 --> 00:08:33,080
creo que en español se diría cómo cuál es la importancia

177
00:08:33,080 --> 00:08:35,160
de cierta característica.

178
00:08:35,160 --> 00:08:39,640
Y eso, lo que quiere decir es que podemos ir ciegos y podemos decir,

179
00:08:39,640 --> 00:08:42,600
sabes que entrena con todos los datos y luego quiero que me digas

180
00:08:42,600 --> 00:08:45,320
de todas las características que mis datos tienen,

181
00:08:45,320 --> 00:08:49,040
cuáles son las características que yo debería estar enfocándome.

182
00:08:49,040 --> 00:08:52,600
Vamos a separar nuestros datos, datos para entrenamiento y datos

183
00:08:52,600 --> 00:08:55,760
para evaluación, vamos a inicializar el modelo,

184
00:08:55,760 --> 00:08:57,640
ajustarlo, utilizarlo y evaluarlo.

185
00:08:57,640 --> 00:09:00,480
Entonces, si han usado Scikit Learn es exactamente lo mismo

186
00:09:00,480 --> 00:09:01,800
que con XGBoost.

187
00:09:02,560 --> 00:09:08,680
OK. Entonces, en este caso voy a empezar leyendo los datos

188
00:09:08,680 --> 00:09:12,200
en un Pandas DataFrame.

189
00:09:12,200 --> 00:09:14,960
Y en este caso los datos son un poquito interesantes

190
00:09:14,960 --> 00:09:17,360
porque los datos no tienen las características,

191
00:09:17,360 --> 00:09:18,760
es bueno vino o no es bueno vino.

192
00:09:18,760 --> 00:09:22,240
Entonces, nosotros tenemos que construir esa característica de

193
00:09:22,240 --> 00:09:23,920
manera artificial.

194
00:09:23,920 --> 00:09:31,320
En este caso lo que tengo aquí es, puedo elegir, vaya como un límite,

195
00:09:31,400 --> 00:09:35,600
donde los datos tienen una variable que se llama calidad

196
00:09:35,600 --> 00:09:38,720
y la calidad es una variable continua.

197
00:09:38,720 --> 00:09:42,920
Entonces, yo lo que puedo hacer para transformar un conjunto de datos

198
00:09:42,920 --> 00:09:46,240
que solo tiene datos continuos a datos discretos es que yo puedo decir,

199
00:09:46,240 --> 00:09:51,000
bueno sabes, ¿qué es lo que yo, de qué manera puedo elegir si un vino es

200
00:09:51,000 --> 00:09:52,320
de buena calidad o no?

201
00:09:52,320 --> 00:09:56,440
Bueno, vamos a decir que números de 6 a más alto, si la escala es de 1

202
00:09:56,440 --> 00:09:59,920
a 10, de 6 a más alto voy a decir este es un vino de buena calidad

203
00:09:59,960 --> 00:10:02,480
y algo más bajo es no de buena calidad.

204
00:10:02,480 --> 00:10:06,960
Entonces, en este sentido puedo definir mis características o mis dos

205
00:10:06,960 --> 00:10:12,760
grupos discretos o binarios para determinar qué vino o cuál es la

206
00:10:12,760 --> 00:10:14,440
calidad de los vinos.

207
00:10:14,440 --> 00:10:19,120
El siguiente paso es como normalmente decidir cuál es la variable

208
00:10:19,120 --> 00:10:23,720
que yo voy a predecir y cuáles son las variables que yo voy a utilizar

209
00:10:23,720 --> 00:10:25,000
para predecir.

210
00:10:25,000 --> 00:10:28,720
Ahora, normalmente cuando estamos usándolo como Second Learn queremos

211
00:10:28,760 --> 00:10:31,920
primero pasar bastante tiempo eligiendo cuáles son los datos que voy a elegir,

212
00:10:31,920 --> 00:10:34,640
cuáles son las variables que me importan.

213
00:10:34,640 --> 00:10:37,840
Obviamente desde un punto de vista de conocimiento siempre queremos pasar

214
00:10:37,840 --> 00:10:40,520
tiempo pensando en el problema desde el punto de vista real.

215
00:10:40,520 --> 00:10:43,760
Tiene sentido que yo utilice todas estas variables o haya lo que me pueda

216
00:10:43,760 --> 00:10:44,280
decidir.

217
00:10:44,280 --> 00:10:48,920
Ahora, si no tienen ese punto de vista, la ventaja de XGBoost es que pueden

218
00:10:48,920 --> 00:10:50,960
ponerlo todo y después pedir, ¿sabes qué?

219
00:10:50,960 --> 00:10:53,520
Dime cuáles son las variables en las que tengo que enfocarme y XGBoost

220
00:10:53,520 --> 00:10:56,480
te va a decir eso.

221
00:10:56,560 --> 00:10:57,240
OK.

222
00:10:57,240 --> 00:11:00,600
Entonces, empezamos con el entrenamiento del modelo.

223
00:11:00,600 --> 00:11:03,560
Si han utilizado Second Learn es exactamente lo mismo.

224
00:11:03,560 --> 00:11:07,800
En vez de utilizarnos el Second Learn Model Selection Linear Regression

225
00:11:07,800 --> 00:11:11,400
dicen XGB punto XGB Classifier, para en este caso porque es la

226
00:11:11,400 --> 00:11:12,360
edificación.

227
00:11:12,360 --> 00:11:16,840
Si quisieran utilizar el regresor podrían decir XGB punto XGB creo que

228
00:11:16,840 --> 00:11:19,640
es regresor para un programa de manera continua.

229
00:11:19,640 --> 00:11:22,520
Entonces, es la misma idea que cuando están utilizando Second Learn es

230
00:11:22,520 --> 00:11:25,440
exactamente el mismo procedimiento.

231
00:11:25,480 --> 00:11:28,760
Después de que hago eso voy a ajustar utilizando los datos de

232
00:11:28,760 --> 00:11:33,800
entrenamiento y después puedo utilizar el módulo para predecir.

233
00:11:33,800 --> 00:11:39,040
Básicamente quiero evaluar, OK, tú me dijiste que el modelo entrenó con

234
00:11:39,040 --> 00:11:43,480
los datos Xtrain y Ytrain y después digo, bueno, vamos a darle al modelo

235
00:11:43,480 --> 00:11:45,920
los datos llamados Xtest.

236
00:11:45,920 --> 00:11:50,680
Básicamente es como esos datos en los que el modelo nunca ha visto

237
00:11:50,680 --> 00:11:51,160
antes.

238
00:11:51,160 --> 00:11:55,160
Quiero saber cómo es que el modelo va a comportarse cuando vea ese nuevo

239
00:11:55,160 --> 00:11:58,440
informe y voy a poner esos datos.

240
00:11:58,440 --> 00:12:01,880
Básicamente la predicción en una variable que normalmente la llamamos

241
00:12:01,880 --> 00:12:04,880
y ypred o ypredicción.

242
00:12:06,720 --> 00:12:09,120
Y aquí es cuando podemos empezar a evaluar el modelo.

243
00:12:09,120 --> 00:12:17,120
Entonces, en este caso lo que yo utilicé es un montón de módulos o

244
00:12:17,120 --> 00:12:21,360
métodos que están asociados al objeto de XGBoost.

245
00:12:21,400 --> 00:12:24,520
Entonces, antes de que vaya a esa, ¿cuántos de ustedes han trabajado con

246
00:12:24,520 --> 00:12:26,680
clases o programación de objetos?

247
00:12:26,680 --> 00:12:28,080
Aquí hay una persona.

248
00:12:28,080 --> 00:12:31,800
OK, entonces XGBoost está construido como una clase.

249
00:12:31,800 --> 00:12:34,480
La clase va a tener varias funciones o varios métodos asociados.

250
00:12:34,480 --> 00:12:38,640
Entonces, en este sentido, XGBoost tiene métricas de evaluación muy,

251
00:12:38,640 --> 00:12:40,600
muy similares a Scikit Learn.

252
00:12:40,600 --> 00:12:44,480
Entonces, puedo decir, OK, bueno, vamos a utilizar, vamos a evaluar cómo

253
00:12:44,480 --> 00:12:50,120
fue que el modelo se evaluó o se comportó en cuanto a los datos de

254
00:12:50,160 --> 00:12:55,440
entrenamiento, en cuanto a los datos para hacer un test.

255
00:12:55,440 --> 00:12:59,320
Y tenemos otros tipos de test que podamos utilizar para evaluar más o

256
00:12:59,320 --> 00:13:00,400
menos cómo se comportó.

257
00:13:00,400 --> 00:13:05,440
En este caso estoy utilizando una,

258
00:13:05,440 --> 00:13:07,440
se me van las palabras a veces.

259
00:13:07,440 --> 00:13:11,720
Quiero utilizar Balance Stack Your C-Score que me va a ayudar a evaluar.

260
00:13:11,720 --> 00:13:17,080
Voy a comparar la variable independiente, en este caso la calidad de vino real,

261
00:13:17,120 --> 00:13:20,960
en cuanto a la calidad de vino que el modelo está prediciendo.

262
00:13:20,960 --> 00:13:23,600
Y las últimas dos cosas que voy a hacer es que voy a decir, bueno,

263
00:13:23,600 --> 00:13:27,280
después de eso quiero que me des un reporte de predicción.

264
00:13:27,280 --> 00:13:29,720
Quiero saber qué tal está bien el modelo.

265
00:13:29,720 --> 00:13:34,720
Y después de eso podemos utilizar cosas como la matriz de confusión.

266
00:13:34,720 --> 00:13:38,600
En este caso nos está diciendo el número de veces que el modelo predijo

267
00:13:38,600 --> 00:13:42,000
de manera exitosa, el número de veces que el modelo predijo de manera

268
00:13:42,000 --> 00:13:43,080
incorrecta.

269
00:13:43,120 --> 00:13:51,520
Entonces voy a agregar aquí nada más a imprimir report.

270
00:13:51,520 --> 00:13:56,840
Ay, y esto siempre pasa.

271
00:13:56,840 --> 00:13:59,480
Ok, déjame nada más coger el cuaderno otra vez.

272
00:13:59,480 --> 00:14:07,040
Ok, eso se ve mejor.

273
00:14:07,280 --> 00:14:14,640
Ok, entonces en este caso, esta es una función que yo construí porque la quiero

274
00:14:14,640 --> 00:14:17,200
volver a utilizar después cuando les estoy diciendo cosas como los

275
00:14:17,200 --> 00:14:21,640
hiperparámetros y cuando les, cuando empecemos a explorar,

276
00:14:21,640 --> 00:14:24,960
cuando empecemos a tunar el modelo.

277
00:14:24,960 --> 00:14:29,480
Entonces en este caso nos podemos ver que el modelo se comporta bastante bien

278
00:14:29,480 --> 00:14:33,440
con los datos de entrenamiento, pero no le va tan bien cuando empezamos

279
00:14:33,520 --> 00:14:36,960
a ver los datos que no ha visto antes.

280
00:14:36,960 --> 00:14:43,120
Punto 8 quiere decir que está prediciendo con 80%,

281
00:14:43,120 --> 00:14:48,400
está siendo exitoso de aproximadamente 80%, que no está tan mal,

282
00:14:48,400 --> 00:14:50,840
pero como le está yendo mejor con los datos que ya vio,

283
00:14:50,840 --> 00:14:54,720
quiere decir que podamos mejorar las cosas con el resto de los datos.

284
00:14:54,720 --> 00:14:59,520
Las últimas dos, perdón, déjame hacerlo otra vez por aquí.

285
00:14:59,560 --> 00:15:03,720
Las últimas dos tablas nos dicen básicamente qué tan exitoso es el

286
00:15:03,720 --> 00:15:05,400
modelo prediciendo las categorías.

287
00:15:05,400 --> 00:15:09,640
En este caso la categoría 0 representa el vino es de buena calidad y la

288
00:15:09,640 --> 00:15:13,360
categoría 1 representa el modelo, el vino no es de buena calidad.

289
00:15:13,360 --> 00:15:20,560
Entonces aquí podemos ver por ejemplo que de 141 entradas o de 141 puntos,

290
00:15:20,560 --> 00:15:26,080
110 los predijo de manera exitosa, 31 lo predijo de manera incorrecta.

291
00:15:26,120 --> 00:15:33,000
Y de la misma manera aquí de 179, 179 puntos que correspondían al vino que

292
00:15:33,000 --> 00:15:37,480
no tenía buena calidad, 146 predijo de manera correcta y 33 de manera

293
00:15:37,480 --> 00:15:38,000
incorrecta.

294
00:15:38,000 --> 00:15:41,120
Entonces lo que vamos a explorar ahorita con Nexty Boost es, bueno,

295
00:15:41,120 --> 00:15:47,400
vamos a ver primero, vamos a explorar cosas como los hiperparámetros.

296
00:15:47,400 --> 00:15:50,840
Y lo otro que podamos explorar con Nexty Boost es el uso de feature

297
00:15:50,840 --> 00:15:51,280
importance.

298
00:15:51,320 --> 00:15:56,360
Entonces básicamente lo que me dice es que el modelo no fue tan exitoso,

299
00:15:56,360 --> 00:15:59,160
no le fue tan mal pero me gustaría que fuera más exitoso.

300
00:15:59,160 --> 00:16:03,600
Entonces vamos a ver si yo reduzco el número de variables en vez de darle

301
00:16:03,600 --> 00:16:09,680
todo menos calidad, si digamos que puedo identificar variables que no impactan

302
00:16:09,680 --> 00:16:12,880
el resultado, entonces podemos ver cómo eso impacta a los resultados.

303
00:16:12,880 --> 00:16:16,960
Entonces lo primero que vamos a explorar es el uso de los hiperparámetros.

304
00:16:16,960 --> 00:16:20,840
Y en este caso esto me tomó a mí un poquito de tiempo acostumbrarme a la

305
00:16:20,880 --> 00:16:27,320
idea de los hiperparámetros porque cada una de estas cosas como que imagina

306
00:16:27,320 --> 00:16:32,320
que estás creando como una caja de arena para el modelo y le dices al modelo,

307
00:16:32,320 --> 00:16:36,560
bueno, yo quiero que entrenes bajo estas circunstancias y quiero que

308
00:16:36,560 --> 00:16:38,840
asumas lo siguiente.

309
00:16:38,840 --> 00:16:43,160
Entonces estos números pueden variar muchísimo y básicamente lo que

310
00:16:43,160 --> 00:16:47,200
Nexty Boost dice, bueno, si yo cambio las condiciones de la caja de arena,

311
00:16:47,240 --> 00:16:52,200
podemos evaluar y podemos básicamente decir, bueno, cómo es que se comporta

312
00:16:52,200 --> 00:16:58,520
el modelo y obviamente nuestro goal es maximizar los resultados y la eficacia

313
00:16:58,520 --> 00:17:02,640
del modelo, minimizando los gastos computacionales.

314
00:17:03,560 --> 00:17:07,560
Entonces lo padre de esto es que podemos utilizar, por ejemplo,

315
00:17:08,880 --> 00:17:12,040
déjame, no, no, es algo de aquí para ver un poquito.

316
00:17:12,040 --> 00:17:16,320
En este caso, por ejemplo, puedo decir, bueno, quiero que entren en Nexty Boost,

317
00:17:16,320 --> 00:17:20,560
va a ser lo mismo como antes, XGB punto XP Classifier y puedo pasar mis

318
00:17:20,560 --> 00:17:24,520
hiperparámetros y en este caso va a ser lo mismo que antes.

319
00:17:24,520 --> 00:17:26,480
Ajusto los datos y puedo evaluar.

320
00:17:28,520 --> 00:17:33,040
Ok, en este caso, cuando pasé estos parámetros,

321
00:17:33,040 --> 00:17:35,160
el modelo se comportó de manera peor.

322
00:17:35,160 --> 00:17:41,640
Ahora dice bueno, para los datos de entrenamiento fue de 1 a 0.85.

323
00:17:41,680 --> 00:17:48,200
Para los datos de evaluación fue de 0.8 a 0.77.

324
00:17:48,200 --> 00:17:51,000
Entonces la siguiente pregunta naturalmente es, bueno,

325
00:17:51,000 --> 00:17:54,640
cómo elijo esos hiperparámetros, cómo puedo,

326
00:17:54,640 --> 00:17:57,280
imagínate que yo voy a estar aquí como fuerza bruta,

327
00:17:57,280 --> 00:18:02,040
vamos a intentar 0.2, vamos a intentar 5, vamos a intentar 200.

328
00:18:02,040 --> 00:18:05,920
Entonces eso nos va a tomar muchísimo tiempo y no tendríamos como que un buen

329
00:18:05,920 --> 00:18:06,880
sistema.

330
00:18:06,920 --> 00:18:12,280
Ahora, lo que sigue es podemos utilizar hiperparámetros en vez de un valor,

331
00:18:12,280 --> 00:18:15,040
puedo pasar una lista de valores y pues si le sabes que XGBoost,

332
00:18:15,040 --> 00:18:20,600
quiero que explores todos estos valores y que me digas cuáles son los mejores.

333
00:18:20,600 --> 00:18:27,080
Entonces, básicamente paso los hiperparámetros como una lista de valores y

334
00:18:27,080 --> 00:18:27,880
digo, ¿sabes qué?

335
00:18:27,880 --> 00:18:34,400
Ok, quiero que ahora explores todos esos parámetros y me digas cuáles son los

336
00:18:34,400 --> 00:18:35,440
mejores.

337
00:18:35,440 --> 00:18:38,800
Pero si, por ejemplo, empezamos a pensar en gastos computacionales,

338
00:18:38,800 --> 00:18:44,400
imagínate que, por ejemplo, si fuera hacerlo de manera bruta o a fuerza

339
00:18:44,400 --> 00:18:50,640
bruta, tendría que hacer 0.1 con 3, con 50, con 0.5, con 0.5,

340
00:18:50,640 --> 00:18:55,800
luego 0.1 con 3, con 50, con 0.5, con 0.6, etcétera.

341
00:18:55,800 --> 00:19:01,600
Entonces, XGBoost tiene tres funcionalidades diferentes para explorar

342
00:19:01,600 --> 00:19:03,440
diferentes combinaciones.

343
00:19:03,560 --> 00:19:07,880
Está la búsqueda de cuadrícula que básicamente puedes pensar como en una

344
00:19:07,880 --> 00:19:12,640
cuadrícula o en un grid donde va a intentar las combinaciones en pares o en

345
00:19:12,640 --> 00:19:13,760
el mismo orden.

346
00:19:13,760 --> 00:19:18,240
Está la búsqueda aleatoria donde en vez de estar buscando cada posible

347
00:19:18,240 --> 00:19:22,280
combinación, va a empezar a utilizar una búsqueda aleatoria donde esté

348
00:19:22,280 --> 00:19:23,920
mejorando los datos.

349
00:19:23,920 --> 00:19:26,600
Básicamente, si utiliza un hiperparámetro que le fue mal,

350
00:19:26,600 --> 00:19:29,160
no lo va a volver a intentar, se va a mover a otro.

351
00:19:29,160 --> 00:19:30,920
Y la variación cruzada.

352
00:19:30,960 --> 00:19:34,560
Entonces, en este caso es bastante fácil utilizar esos módulos,

353
00:19:34,560 --> 00:19:38,360
solo tengo que decir, ok, vamos a hacerlo con la aleatoria.

354
00:19:38,360 --> 00:19:40,160
Sería por randomize search.

355
00:19:40,160 --> 00:19:41,320
Y puedo correr modelo.

356
00:19:41,320 --> 00:19:46,640
Si quisiera utilizar para grid, solo digo grid search CV,

357
00:19:46,640 --> 00:19:48,920
puedo cambiar esto aquí, grid search CV.

358
00:19:48,920 --> 00:19:52,080
Y creo que las únicas cosas que cambian son los parámetros.

359
00:19:52,080 --> 00:19:53,360
Hay cosas que no les gustan.

360
00:19:53,360 --> 00:19:55,640
Por ejemplo, no les gustó la palabra parámetros.

361
00:19:55,640 --> 00:20:06,560
Sí, no me acuerdo de las palabras clave para cada modelo,

362
00:20:06,560 --> 00:20:11,240
pero básicamente si quieren elegir aleatoria o grid o cualquier otro,

363
00:20:11,240 --> 00:20:14,280
lo pueden importar como un módulo.

364
00:20:14,280 --> 00:20:19,200
El módulo también se asegura de hacer cross validation,

365
00:20:19,200 --> 00:20:20,720
que básicamente quiere decir, bueno,

366
00:20:20,720 --> 00:20:23,520
cuando yo intenté entrenar el módulo la primera vez,

367
00:20:23,560 --> 00:20:24,800
vieron que se pare los datos.

368
00:20:24,800 --> 00:20:28,280
Datos para entrenar y datos para evaluar.

369
00:20:28,280 --> 00:20:30,640
Pero eso me da como que una partición de los datos.

370
00:20:30,640 --> 00:20:34,600
Yo quisiera saber si yo parto los datos de diferentes maneras,

371
00:20:34,600 --> 00:20:37,360
puedo asegurarme de que mi modelo siga siendo tan efectivo.

372
00:20:37,360 --> 00:20:40,640
Entonces, este parámetro de CV, o cross validation,

373
00:20:40,640 --> 00:20:41,920
es lo que se encarga.

374
00:20:41,920 --> 00:20:44,960
Dice, bueno, vamos a hacer cinco particiones diferentes

375
00:20:44,960 --> 00:20:50,280
y vamos a calcular su resultado.

376
00:20:50,320 --> 00:20:56,320
Y vamos a tomar un, como se dice la palabra en español, average.

377
00:20:56,320 --> 00:20:57,880
¿Cuál es la palabra para average?

378
00:20:57,880 --> 00:21:00,760
Promedio. Vamos a tomar el promedio de la evaluación.

379
00:21:00,760 --> 00:21:04,360
Y si mi promedio es alto, quiere decir que el modelo se está

380
00:21:04,360 --> 00:21:09,560
comportando bien con diferentes conjuntos de datos de entrenamiento.

381
00:21:09,560 --> 00:21:11,560
Entonces, en este caso va a seguir corriendo ahorita

382
00:21:11,560 --> 00:21:13,240
porque se tarda un poquito.

383
00:21:13,240 --> 00:21:15,440
Pero ya cuando termina me va a decir, ¿sabes qué?

384
00:21:15,520 --> 00:21:21,720
De los parámetros que mediste, subsample 0.9 con mil estimadores,

385
00:21:21,720 --> 00:21:26,040
max step 8, learning rate 0.01 es el mejor modelo,

386
00:21:26,040 --> 00:21:29,800
básicamente basado en lo que le dimos al modelo.

387
00:21:29,800 --> 00:21:32,400
Entonces, después de esto solo toca pasar los mejores

388
00:21:32,400 --> 00:21:34,120
ciproparámetros.

389
00:21:34,120 --> 00:21:38,160
Y en este sentido lo que puede hacer es bueno, regresó a 1,

390
00:21:38,160 --> 00:21:40,520
sigue bajo aquí en 0.80.

391
00:21:40,560 --> 00:21:45,520
Como que no mejoró mucho en términos de la,

392
00:21:45,520 --> 00:21:49,360
no mejoró mucho en términos de cómo se está comportando.

393
00:21:49,360 --> 00:21:52,040
Lo que quiere decir es que bueno, a lo mejor lo que está yendo mal es

394
00:21:52,040 --> 00:21:54,160
que estoy dando demasiados parámetros.

395
00:21:54,160 --> 00:21:56,360
A lo mejor le estoy dando parámetros al modelo que no importa

396
00:21:56,360 --> 00:22:02,200
y eso está impactando cómo es que funciona la clasificación.

397
00:22:02,200 --> 00:22:05,480
Entonces, lo que sigue como la siguiente estrategia

398
00:22:05,480 --> 00:22:08,160
que nosotros utilizaríamos es evaluar, bueno,

399
00:22:08,200 --> 00:22:10,400
usé todas mis características,

400
00:22:10,400 --> 00:22:12,720
pero como que eso no me dio los mejores resultados,

401
00:22:12,720 --> 00:22:16,360
entonces vamos a elegir cuáles son las características que más

402
00:22:16,360 --> 00:22:17,200
importan.

403
00:22:17,200 --> 00:22:20,120
Y de nuevo, podemos hacer esto de manera como fuerza bruta.

404
00:22:20,120 --> 00:22:23,960
Vamos a intentar todos los pares posibles de características

405
00:22:23,960 --> 00:22:25,640
hasta que nos den algo mejor.

406
00:22:25,640 --> 00:22:26,520
Podría ser eso.

407
00:22:26,520 --> 00:22:30,600
Oh, podría utilizar visualización de importancia

408
00:22:30,600 --> 00:22:31,880
de características.

409
00:22:31,880 --> 00:22:35,360
En este caso, XGBoost tiene una función que se llama

410
00:22:35,360 --> 00:22:36,360
blood importance.

411
00:22:36,400 --> 00:22:38,880
Entonces, lo que yo hago aquí es que le digo, bueno,

412
00:22:38,880 --> 00:22:43,600
XGBoost.plotimportance y pasas el nombre del modelo.

413
00:22:43,600 --> 00:22:46,600
Por ejemplo, si digo modelo XGBin, que es el original.

414
00:22:46,600 --> 00:22:51,400
Ay, como ya lo corrí, te voy a volver a correr otra vez.

415
00:22:51,400 --> 00:22:55,320
Es lo único malo de trabajar con cuadernos que luego cambia las

416
00:22:55,320 --> 00:22:57,200
cosas y ya no le gusta.

417
00:22:57,200 --> 00:23:01,520
OK, entonces ya le di fit.

418
00:23:01,520 --> 00:23:02,720
OK, vamos a hacer esto aquí.

419
00:23:02,720 --> 00:23:04,680
Entonces, por ejemplo, para el primero modelo,

420
00:23:04,760 --> 00:23:08,200
para el primer modelo, me dijo que los parámetros más

421
00:23:08,200 --> 00:23:12,000
importantes incluye clorides o clorides,

422
00:23:12,000 --> 00:23:14,040
no sé cuál es la palabra en español,

423
00:23:14,040 --> 00:23:19,200
el dióxido sulfúrico total, los sulfitos, el pH, etcétera.

424
00:23:19,200 --> 00:23:22,000
Entonces, me está diciendo de todas las variables y vamos a ver

425
00:23:22,000 --> 00:23:23,400
aquí las datos otra vez.

426
00:23:23,400 --> 00:23:27,480
Si digo df.columns.

427
00:23:27,480 --> 00:23:30,440
OK, entonces, estos son todos mis parámetros.

428
00:23:30,440 --> 00:23:33,080
En este caso, creo que no removió nada.

429
00:23:33,320 --> 00:23:33,960
Bueno, ¿sabes qué?

430
00:23:33,960 --> 00:23:38,880
Creo que todos los parámetros sí están siendo impactados.

431
00:23:38,880 --> 00:23:40,880
Sí, creo que en este caso no removió nada.

432
00:23:40,880 --> 00:23:43,560
Pero básicamente, imagínate que tuvieras, por ejemplo,

433
00:23:43,560 --> 00:23:48,800
200 parámetros o 200 características en los datos.

434
00:23:48,800 --> 00:23:51,120
Elegir cuáles son los más importantes de manera manual

435
00:23:51,120 --> 00:23:51,800
sería difícil.

436
00:23:51,800 --> 00:23:57,480
Si tienes conocimiento de campo relacionado al problema,

437
00:23:57,480 --> 00:24:00,400
puedes elegir los valores.

438
00:24:00,400 --> 00:24:02,440
En este caso, nos está diciendo, ¿sabes qué?

439
00:24:02,440 --> 00:24:05,360
Las importancias nos dicen, sí, está bien que utilices todos los

440
00:24:05,360 --> 00:24:06,840
datos que estás utilizando.

441
00:24:06,840 --> 00:24:13,720
Entonces, vamos a ver, por ejemplo, si utilizo el primer

442
00:24:13,720 --> 00:24:15,400
modelo.

443
00:24:15,400 --> 00:24:18,160
Entonces, si utilizo el primer modelo,

444
00:24:18,160 --> 00:24:21,080
voy a agregar una célula más.

445
00:24:21,080 --> 00:24:25,920
Y digo xgb, winterplot importances,

446
00:24:25,920 --> 00:24:28,440
y paso el segundo modelo.

447
00:24:28,480 --> 00:24:32,040
Ahora, en el segundo modelo me dijo algo un poquitito diferente.

448
00:24:32,040 --> 00:24:35,200
En el primer modelo, el modelo que no tenía ningún parámetro,

449
00:24:35,200 --> 00:24:38,640
me dijo, OK, primero que nada al modelo le va a importar los

450
00:24:38,640 --> 00:24:42,240
cloridos como la característica más importante.

451
00:24:42,240 --> 00:24:44,600
Pero cuando, por ejemplo, pasé los primeros parámetros,

452
00:24:44,600 --> 00:24:52,960
que fue el modelo que bajó de 1 a 8 y de 0.8 a 0.7,

453
00:24:52,960 --> 00:24:57,040
ese modelo dijo los cloridos son el factor más importante.

454
00:24:57,040 --> 00:25:01,960
Pero cuando pasé el primer conjunto de hiperparámetros,

455
00:25:01,960 --> 00:25:04,640
el que yo elegí de manera aleatoria, me dijo,

456
00:25:04,640 --> 00:25:08,640
no, sabes qué, el factor más importante para determinar la

457
00:25:08,640 --> 00:25:11,200
calidad del vino es el nivel de alcohol.

458
00:25:11,200 --> 00:25:13,560
Ahora, si hacemos, por ejemplo, el último,

459
00:25:13,560 --> 00:25:19,000
que fue el modelo que básicamente trató diferentes

460
00:25:19,000 --> 00:25:23,440
combinaciones de diferentes parámetros.

461
00:25:23,640 --> 00:25:25,640
Deja que intento eso otra vez.

462
00:25:25,640 --> 00:25:30,440
OK, entonces intento la última vez.

463
00:25:30,440 --> 00:25:40,040
Va a ser xgb.plotimportance y va a ser, le dije, modelo xgb.

464
00:25:40,040 --> 00:25:44,720
Digamos, modelo xgbhb.

465
00:25:44,720 --> 00:25:47,160
Entonces, en este modelo, regresó otra vez y dijo,

466
00:25:47,160 --> 00:25:52,520
sabes qué, no, los cloridos y el sulfur dióxido total siguen

467
00:25:52,560 --> 00:25:54,480
siendo los factores más importantes.

468
00:25:54,480 --> 00:25:58,040
Lo que eso nos está diciendo es que nosotros,

469
00:25:58,040 --> 00:26:00,360
si nosotros elegimos esos parámetros,

470
00:26:00,360 --> 00:26:05,880
como así de manera aleatoria, como yo lo hice al principio,

471
00:26:05,880 --> 00:26:09,000
este tipo de información puede cambiar bastante.

472
00:26:09,000 --> 00:26:11,800
Entonces, ¿cuál es la recomendación de aquí?

473
00:26:11,800 --> 00:26:16,240
A mí me gusta utilizar grids o varias combinaciones de valores

474
00:26:16,240 --> 00:26:18,400
para obtener un mejor resultado.

475
00:26:18,400 --> 00:26:22,120
Y después de eso evaluar cuáles son las características

476
00:26:22,160 --> 00:26:25,120
que nos importan más o que no nos importan más.

477
00:26:25,120 --> 00:26:27,800
Entonces, vamos a regresar aquí.

478
00:26:27,800 --> 00:26:31,080
Ahora, en términos de por qué querría usarlo como xgboost

479
00:26:31,080 --> 00:26:34,240
y no scikit-learn, por qué querría utilizar algo así,

480
00:26:34,240 --> 00:26:37,640
depende del problema y cómo lo quieren utilizar.

481
00:26:37,640 --> 00:26:39,560
Hay veces en que, por ejemplo, tienen un problema de

482
00:26:39,560 --> 00:26:42,920
clasificación donde los datos se comportan de manera extraña o

483
00:26:42,920 --> 00:26:47,520
donde hay ciertas asumciones por parte del problema que

484
00:26:47,520 --> 00:26:49,960
ocasionan que el modelo no se esté comportando bien.

485
00:26:50,000 --> 00:26:51,720
En clasificación esto no pasa tanto,

486
00:26:51,720 --> 00:26:55,520
donde sí pasa bastante más, es en problemas de regresión,

487
00:26:55,520 --> 00:26:58,440
donde queremos predecir una variable continua y los

488
00:26:58,440 --> 00:27:01,120
problemas de regresión tienen ciertas asumciones que son muy

489
00:27:01,120 --> 00:27:03,760
fuertes, por ejemplo, la distribución de nuestros datos.

490
00:27:03,760 --> 00:27:05,360
Si esa asumción no está ahí,

491
00:27:05,360 --> 00:27:07,160
los resultados del modelo no son válidos.

492
00:27:07,160 --> 00:27:10,000
Entonces, una ventaja de xgboost en ese caso es que podemos

493
00:27:10,000 --> 00:27:15,000
asegurarnos de que seguimos minimizando el loss function

494
00:27:15,000 --> 00:27:17,720
sin perder la calidad de los resultados.

495
00:27:17,760 --> 00:27:19,520
Otra cosa que es bastante importante,

496
00:27:19,520 --> 00:27:22,000
nosotros no tuvimos oportunidad de ver esto en la charla porque

497
00:27:22,000 --> 00:27:23,960
mis datos estaban completitos,

498
00:27:23,960 --> 00:27:27,520
pero si tienen una base de datos o datos donde hay valores que no

499
00:27:27,520 --> 00:27:31,200
se encuentran ahí, xgboost puede manejar ese tipo de situaciones.

500
00:27:31,200 --> 00:27:32,400
Scikit-learn, por ejemplo, te diría,

501
00:27:32,400 --> 00:27:37,320
hay valores que tienen NAN, tienes que ir a lidiar con ellos.

502
00:27:37,320 --> 00:27:39,360
Normalmente lidiar con ellos significa,

503
00:27:39,360 --> 00:27:43,160
a lo mejor utilizamos impute, básicamente queremos utilizar

504
00:27:43,160 --> 00:27:46,680
valores, observaciones que sí tienen esa observación para

505
00:27:46,680 --> 00:27:49,680
cómo completar el valor que no se encuentra ahí.

506
00:27:49,680 --> 00:27:52,360
Hay veces en que decimos, sabes que si tiene sentido en tu

507
00:27:52,360 --> 00:27:56,200
problema, pues sencillamente removerlos.

508
00:27:56,200 --> 00:27:58,880
No es la mejor idea, lo mejor es que te sientes con esos

509
00:27:58,880 --> 00:28:00,160
problemas valores y piensas, bueno,

510
00:28:00,160 --> 00:28:04,240
qué voy a hacer con esas entradas que no tienen un valor.

511
00:28:04,240 --> 00:28:06,960
Lo padre de xgboost es que puedes pasar un modelo que tiene

512
00:28:06,960 --> 00:28:12,800
esa situación donde tienes datos faltantes y puede manejarlo.

513
00:28:12,800 --> 00:28:16,280
Otros casos, otras ventajas de xgboost es que puede manejar

514
00:28:16,280 --> 00:28:17,400
paralelización.

515
00:28:17,400 --> 00:28:19,400
En este caso, los datos fueron bastante chiquitos,

516
00:28:19,400 --> 00:28:21,880
pero digamos que tienes conjuntos de datos con millones

517
00:28:21,880 --> 00:28:23,640
y millones de entradas.

518
00:28:23,640 --> 00:28:28,040
En ese caso, xgboost tiene funcionalidad para manejar,

519
00:28:28,040 --> 00:28:30,520
paralizar tu modelo.

520
00:28:30,520 --> 00:28:32,520
La regularización y otra es la flexibilidad,

521
00:28:32,520 --> 00:28:35,720
vaya si quieres jugar con varios parámetros,

522
00:28:35,720 --> 00:28:39,200
tienes la oportunidad de hacer eso con esta biblioteca.

523
00:28:39,200 --> 00:28:43,880
OK, entonces, para cerrar, en este caso utilizamos xgboost

524
00:28:43,880 --> 00:28:45,680
para un problema de clasificación.

525
00:28:45,720 --> 00:28:50,240
El problema era evaluar la calidad de vino.

526
00:28:50,240 --> 00:28:54,480
Nos dieron datos continuos y nosotros tuvimos que definirlo

527
00:28:54,480 --> 00:28:56,680
con problema de clasificación diciendo, OK,

528
00:28:56,680 --> 00:28:59,960
bueno, ¿cómo decidimos que un vino es de buena calidad o no?

529
00:28:59,960 --> 00:29:03,280
Mediante el uso de un threshold de calidad.

530
00:29:03,280 --> 00:29:07,040
En este caso, modelamos el problema de manera binaria,

531
00:29:07,040 --> 00:29:10,520
predecir sí o no como 0 o 1.

532
00:29:10,520 --> 00:29:11,760
Utilizamos xgboost.

533
00:29:11,760 --> 00:29:13,840
Una de las cosas que vimos es que es súper,

534
00:29:13,840 --> 00:29:16,560
súper similar a entrenar un modelo con scikit-learn.

535
00:29:16,560 --> 00:29:18,200
Es exactamente la misma idea.

536
00:29:18,200 --> 00:29:21,800
Empezamos inicializando el modelo y antes de empezar a

537
00:29:21,800 --> 00:29:24,000
entrenar las cosas, separamos nuestros datos en datos de

538
00:29:24,000 --> 00:29:26,640
entrenamiento, datos de testeo.

539
00:29:26,640 --> 00:29:29,040
Entonces, utilizamos el modelo para ajustar,

540
00:29:29,040 --> 00:29:32,800
o vaya, decimos modelo.fit en nuestros datos de entrenamiento,

541
00:29:32,800 --> 00:29:35,400
sí, en los datos de entrenamiento y podemos utilizar

542
00:29:35,400 --> 00:29:37,440
un montón de métricas para evaluar cómo es que el modelo

543
00:29:37,440 --> 00:29:39,520
se está comportando.

544
00:29:39,520 --> 00:29:43,680
Una diferencia de scikit-learn es que podemos evaluar cosas como

545
00:29:44,640 --> 00:29:47,880
la importancia de las características y podemos utilizar

546
00:29:47,880 --> 00:29:49,920
cosas como el grid search o podemos,

547
00:29:49,920 --> 00:29:53,520
cómo se llaman los parámetros, donde básicamente podemos ajustar

548
00:29:53,520 --> 00:29:56,520
cómo es que el modelo se comporta para,

549
00:29:56,520 --> 00:30:00,520
básicamente para maximizar los resultados.

550
00:30:00,520 --> 00:30:02,240
OK, y esto sería todo.

551
00:30:02,240 --> 00:30:04,760
Entonces, creo que todavía me quedan como 5 minutos para

552
00:30:04,760 --> 00:30:06,680
preguntas.

553
00:30:06,680 --> 00:30:10,000
Sí. OK, entonces, si quieren charlar conmigo,

554
00:30:10,000 --> 00:30:12,720
yo estoy disponible después de la charla.

555
00:30:12,800 --> 00:30:14,200
Estoy disponible en Ine también.

556
00:30:14,200 --> 00:30:16,880
Entonces, me pueden seguir en GitHub o en Twitter.

557
00:30:16,880 --> 00:30:18,200
También estoy en Mastodon, entonces,

558
00:30:18,200 --> 00:30:20,040
feliz de platicar con ustedes.

559
00:30:20,040 --> 00:30:21,880
Y ahorita puedo tomar preguntas.

560
00:30:21,880 --> 00:30:22,840
Muchas gracias, Laura.

561
00:30:22,840 --> 00:30:24,360
¿Alguien tiene pregunta?

562
00:30:28,760 --> 00:30:31,400
Laura, primero, gracias por tu presentación.

563
00:30:31,400 --> 00:30:35,280
Y segundo, yo estuve utilizando un modelo de gradient boosting

564
00:30:35,280 --> 00:30:37,920
para hacer un asunto de clasificación.

565
00:30:37,920 --> 00:30:40,400
Trae un problema muy serio de desbalance en el objetivo,

566
00:30:40,400 --> 00:30:42,320
en el target.

567
00:30:42,320 --> 00:30:49,840
Bateñé mucho para arreglar el problema y yo no sé, digo,

568
00:30:49,840 --> 00:30:52,000
es algo que pasa frecuentemente, ¿no?

569
00:30:52,000 --> 00:30:56,520
Que tienes un data set con el objetivo bien desbalanceado.

570
00:30:56,520 --> 00:30:58,840
Entonces, ¿cuáles serían las recomendaciones?

571
00:30:58,840 --> 00:31:00,880
Porque yo hice de todo, ¿no?

572
00:31:00,880 --> 00:31:04,360
Hice feature selection, hice regularización,

573
00:31:04,360 --> 00:31:07,960
hice, utilicé otros modelos para sacar la importancia de

574
00:31:07,960 --> 00:31:08,520
variables.

575
00:31:08,520 --> 00:31:10,320
O sea, hice muchas cosas.

576
00:31:10,320 --> 00:31:13,240
De repente dije, al final, ya después de todo, no,

577
00:31:13,240 --> 00:31:19,600
no pasé de un umbral en el F1 de las métricas del 60,

578
00:31:19,600 --> 00:31:22,920
70% sobre la variable de relevancia,

579
00:31:22,920 --> 00:31:25,920
que era la que tenía menos presencia, ¿no?

580
00:31:25,920 --> 00:31:29,960
Entonces, digo, no sé si hay así como recomendaciones

581
00:31:29,960 --> 00:31:32,680
generales o cómo atacarlo.

582
00:31:32,680 --> 00:31:35,240
A mí al final me quedó la idea que tenía que buscar más datos,

583
00:31:35,240 --> 00:31:35,560
¿no?

584
00:31:35,560 --> 00:31:37,680
Que a lo mejor el dato, o sea, la información que tenía no era

585
00:31:37,680 --> 00:31:39,480
suficiente para explicar lo que quería, ¿no?

586
00:31:40,440 --> 00:31:42,400
Pues, de repente sentí que tuve que comparar, ¿no?

587
00:31:42,400 --> 00:31:45,280
Ya, ya no seguí, ya no pude seguir.

588
00:31:45,280 --> 00:31:47,080
Esa es una pregunta excelente.

589
00:31:47,080 --> 00:31:50,400
Lo que puedes hacer en ese caso es que puedes construir un

590
00:31:50,400 --> 00:31:54,720
pipeline dentro de tu desarrollo.

591
00:31:54,720 --> 00:31:57,200
Tengo un ejemplo que puedo compartir contigo, de hecho.

592
00:31:57,200 --> 00:31:59,680
Pero básicamente lo que quieres hacer es que cuando construyes

593
00:31:59,680 --> 00:32:02,640
tu pipeline, a lo mejor tienes regularización,

594
00:32:02,640 --> 00:32:05,400
donde si la data, los datos no están normalizados,

595
00:32:05,400 --> 00:32:07,760
puedes normalizar los datos como el primer paso.

596
00:32:07,800 --> 00:32:10,800
Si tienes datos categóricos, puedes usar One Hot Encoding para

597
00:32:10,800 --> 00:32:12,680
marcarlos como números.

598
00:32:12,680 --> 00:32:15,320
Y el siguiente paso que puedes utilizar es que puedes usar,

599
00:32:15,320 --> 00:32:16,200
hay varias técnicas.

600
00:32:16,200 --> 00:32:19,880
Hay una técnica que se llama SMOTE y son,

601
00:32:19,880 --> 00:32:21,160
hay dos tipos de técnicas.

602
00:32:21,160 --> 00:32:26,840
La primera técnica es que puedes reducir el número de datos de

603
00:32:26,840 --> 00:32:30,360
la clase que está sobre representada o el otro,

604
00:32:30,360 --> 00:32:35,400
que puedes utilizar SMOTE básicamente para crear más entradas

605
00:32:35,440 --> 00:32:39,880
de datos de la clase que está no también representada.

606
00:32:39,880 --> 00:32:42,840
Y después de que te haces SMOTE o oversampling,

607
00:32:42,840 --> 00:32:45,480
entonces puedes utilizar el modelo.

608
00:32:45,480 --> 00:32:48,800
Entonces, ¿has trabajado con pipelines de ese tipo antes?

609
00:32:48,800 --> 00:32:54,560
Sí, pero de hecho es una parte que quiero,

610
00:32:54,560 --> 00:32:59,120
ya cuando tienes un problema, pues ya tienes un chorro de pedazos

611
00:32:59,120 --> 00:33:00,000
de código bien grandes.

612
00:33:00,000 --> 00:33:01,760
Entonces, sí como que tienes que pegarlos y meterlos en

613
00:33:01,760 --> 00:33:03,400
funciones y hacerlo así, ¿no?

614
00:33:03,480 --> 00:33:05,800
Como pipelines ya más automatizados.

615
00:33:05,800 --> 00:33:07,840
De hecho, eso es una cosa que sí quiero,

616
00:33:07,840 --> 00:33:13,440
o sea, el pipeline de lo que es el EDA completito,

617
00:33:13,440 --> 00:33:16,120
el pipeline completo del desarrollo del modelo y el

618
00:33:16,120 --> 00:33:19,760
pipeline del monitoreo y tener como tres pipelines grandes.

619
00:33:19,760 --> 00:33:20,680
Déjate muestro.

620
00:33:20,680 --> 00:33:23,680
Tengo un código donde utilizo un pipeline.

621
00:33:23,680 --> 00:33:25,480
De hecho, te lo puedo compartir.

622
00:33:25,480 --> 00:33:27,880
Si quieres hablamos y te puedo compartir en el video donde

623
00:33:27,880 --> 00:33:28,840
construyo los pipelines.

624
00:33:28,840 --> 00:33:32,840
Y de hecho, lo padre es que no tienes que definir tus propias

625
00:33:32,840 --> 00:33:36,400
funciones y funcionalidad para que solo pases los pasitos.

626
00:33:36,400 --> 00:33:38,600
Entonces, puedo compartir eso para que se te haga un poquito

627
00:33:38,600 --> 00:33:39,080
más fácil.

628
00:33:39,080 --> 00:33:39,600
Sí, sí.

629
00:33:43,320 --> 00:33:48,480
Tengo tiempo para una pregunta más, si alguien quiere.

630
00:33:55,200 --> 00:33:57,200
Hola, me gustó bastante la charla.

631
00:33:57,200 --> 00:33:59,040
Solo una pregunta rápida.

632
00:33:59,080 --> 00:34:04,600
Exiboost cuenta con algún tipo de funcionalidad para hacer

633
00:34:04,600 --> 00:34:06,520
transformación de los datos de acuerdo a la varianza,

634
00:34:06,520 --> 00:34:09,600
por ejemplo, un PCA o algo por ese estilo.

635
00:34:09,600 --> 00:34:15,440
Y si también, es esa parte, pero me interesa la funcionalidad.

636
00:34:15,440 --> 00:34:19,440
Si tiene algo de seleccionar automáticamente,

637
00:34:19,440 --> 00:34:22,520
como cuál sería el modelo dentro de este framework de

638
00:34:22,520 --> 00:34:26,000
gradient boosting, como algo de auto ML,

639
00:34:26,000 --> 00:34:27,800
si tiene alguna funcionalidad de eso.

640
00:34:27,800 --> 00:34:29,520
Para la primera pregunta, creo que no.

641
00:34:29,520 --> 00:34:32,720
Creo que Exiboost no tiene cosas como reducción de dimensión,

642
00:34:32,720 --> 00:34:33,520
que sería lo del PCA.

643
00:34:33,520 --> 00:34:35,000
Entonces, para reducción de dimensión,

644
00:34:35,000 --> 00:34:36,920
creo que Scikit Learn tiene un montón de cosas.

645
00:34:36,920 --> 00:34:41,640
Para aprendizaje automático, hasta donde yo sé ahorita, no.

646
00:34:41,640 --> 00:34:44,240
No tiene una implementación de como auto ML.

647
00:34:48,680 --> 00:34:50,080
Muchas gracias.

648
00:34:50,080 --> 00:34:51,080
Muchas gracias, Laura.

649
00:34:51,080 --> 00:34:52,600
Un aplauso para ella.

650
00:34:52,600 --> 00:34:55,640
Y confiamos en la siguiente charla de los buenos minutos.

651
00:34:55,640 --> 00:34:57,920
Muchas gracias por unirse en este sábado.

652
00:34:57,920 --> 00:35:00,880
Y espero que disfruten de esto la conferencia.

