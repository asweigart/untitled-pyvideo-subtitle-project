1
00:00:00,000 --> 00:00:08,000
All right. We'll have to wake you up more. How's everybody doing?

2
00:00:08,000 --> 00:00:15,000
Awesome. Well, welcome to PyCon 2023. It's my first chance to say that this time. So

3
00:00:15,000 --> 00:00:19,160
if it's your first time, welcome for your first time. If you've been here before, nice

4
00:00:19,160 --> 00:00:27,760
to meet you. So we're excited to talk to you today about best practices using Python for

5
00:00:27,760 --> 00:00:32,720
serverless applications. So if you're here, you probably have some inkling of what serverless

6
00:00:32,720 --> 00:00:39,720
is. So we won't touch too deeply on that, but we'll go through some good overview stuff.

7
00:00:39,720 --> 00:00:42,560
So first off, you probably want to know who the heck we are and why you should listen

8
00:00:42,560 --> 00:00:47,560
to us about this. I'll let Brian start, actually.

9
00:00:47,560 --> 00:00:54,680
Hey, everybody. My name is Brian McNamara. I'm a distinguished engineer at Capital One.

10
00:00:54,680 --> 00:01:01,680
I work in our retail bank where my attention is primarily on serverless applications. Prior

11
00:01:01,680 --> 00:01:07,680
to my time at Capital One, I spent several years at AWS working as a serverless specialist.

12
00:01:07,680 --> 00:01:13,040
I also had a chance to work at a startup that was basically using serverless services under

13
00:01:13,040 --> 00:01:16,040
the covers. With that.

14
00:01:16,040 --> 00:01:22,160
Awesome. And I'm Dan Furman. My social media is up here. You're welcome to connect and

15
00:01:22,160 --> 00:01:27,080
follow both of us. We talk about this stuff relatively routinely on there. I'm also a

16
00:01:27,080 --> 00:01:30,920
distinguished engineer at Capital One. I work in our card line of business in the card tech

17
00:01:30,920 --> 00:01:37,600
space and my job is to transform the way we do data. Trying to take different approaches,

18
00:01:37,600 --> 00:01:42,480
find better ways to make better decisions for the end customers with real-time information.

19
00:01:42,480 --> 00:01:48,720
And I tend to be a serverless advocate within card.

20
00:01:48,720 --> 00:01:53,800
So let's go ahead and jump in and kind of talk about, well, why should we use Python

21
00:01:53,800 --> 00:01:58,360
for serverless apps? When we're talking about serverless applications, we mean compute instances

22
00:01:58,360 --> 00:02:03,200
that you don't need to manage. If you need to actually run something in a code space

23
00:02:03,200 --> 00:02:09,920
or in a technology space, you may be used to managing a container, provisioning a server,

24
00:02:09,920 --> 00:02:13,920
going in and doing that. What we're talking about is things that allow you to execute

25
00:02:14,600 --> 00:02:20,300
without any of those. And they scale very nicely. There's a lot of fun stuff there.

26
00:02:20,300 --> 00:02:25,160
But why Python specifically? It comes down to a couple things. One is a theme that you'll

27
00:02:25,160 --> 00:02:30,280
see from us over and over. Simple is better than complex. It's just not fun to deal with

28
00:02:30,280 --> 00:02:35,040
a bunch of complicated things. Can you lock down the server? Can you build the container?

29
00:02:35,040 --> 00:02:41,640
Can you do all that stuff? Of course you can. You know, your Python engineers and developers,

30
00:02:41,640 --> 00:02:44,720
you can definitely do this. You do it on your laptop every time you run the Python

31
00:02:44,720 --> 00:02:50,240
or Apple console. But do you really want to do that in production? If your application

32
00:02:50,240 --> 00:02:55,340
is wildly successful and it goes from 10 TPS to 4,000 TPS, is that something you really

33
00:02:55,340 --> 00:03:02,340
want to go in and re-provision and re-set up for whatever's there? It's hard to manage.

34
00:03:02,380 --> 00:03:06,720
But serverless meshes very well with Python because it has the same kind of development

35
00:03:06,720 --> 00:03:11,060
cycle. So just like we're used to running a test suite every time we write a couple

36
00:03:11,060 --> 00:03:14,900
lines of code in the Python world, we can do the same thing inside of the serverless

37
00:03:14,900 --> 00:03:18,780
world because we can constantly deploy our application, we can constantly keep it up

38
00:03:18,780 --> 00:03:23,660
to date, we have the ability to run through really as quick as we want to, and we can

39
00:03:23,660 --> 00:03:28,900
run as many releases as we want of that code out there. It's fantastic for scaling. I mentioned

40
00:03:28,900 --> 00:03:33,300
kind of some of my baseline numbers there of it doesn't matter whether you're invoking

41
00:03:33,300 --> 00:03:39,620
a batch job once a day or if you're running something that's at 4,000 TPS, you can do

42
00:03:39,620 --> 00:03:44,860
that in the serverless space at a really nice low cost. And the reason for that is

43
00:03:44,860 --> 00:03:48,740
that you're only paying for what you consume. And this is kind of the core tenet of what

44
00:03:48,740 --> 00:03:53,300
I'm calling serverless services. I know there's a few operational definitions out there. But

45
00:03:53,300 --> 00:03:58,460
if your cost is based on usage and then you factor in total cost of ownership, which is

46
00:03:58,460 --> 00:04:03,060
your time as a developer, an engineer, or a data scientist to keep maintaining that

47
00:04:04,060 --> 00:04:10,600
the more we keep that time for you focused on where you're adding value, the more profitable

48
00:04:10,600 --> 00:04:14,180
you are for the company because you're getting more work done and you're not having to deal

49
00:04:14,180 --> 00:04:19,300
with that keep the lights on kind of work. So total cost of ownership is extraordinarily

50
00:04:19,300 --> 00:04:25,740
small. And because Python scales so well in the serverless world, it gives us a way to

51
00:04:25,740 --> 00:04:31,300
do extraordinarily rapid scaling and rapid upkeep.

52
00:04:31,300 --> 00:04:35,860
There's a few kinds of serverless applications. We'll be demoing only one of these today,

53
00:04:35,860 --> 00:04:40,060
but it's important to know that this pattern supports way more than what we're going to

54
00:04:40,060 --> 00:04:44,420
have time to talk to you about in this hour. We're going to be going through an API provider

55
00:04:44,420 --> 00:04:49,460
example because while we tend to think of API clients as long running things that are

56
00:04:49,460 --> 00:04:54,940
listening on port 443 and they're processing all of that information, in reality, every

57
00:04:54,940 --> 00:04:59,980
call that comes in, you can think of as an event. And if you start processing individual

58
00:04:59,980 --> 00:05:05,420
calls as individual events, you now have the ability to scale out and use load balancers

59
00:05:05,420 --> 00:05:12,940
in front of that and imagine not just an army of containers, but an army of army of containers.

60
00:05:12,940 --> 00:05:18,220
You've got a huge amount of horizontal scaling that you can do because everything runs that

61
00:05:18,220 --> 00:05:22,980
way. We'll also touch on streaming data processors. So if you're going to process data that's

62
00:05:22,980 --> 00:05:27,740
coming in real time, could be video, could be audio, could be data events that are coming

63
00:05:27,740 --> 00:05:32,140
through, could be you taking your favorite social media and piping it through a data

64
00:05:32,140 --> 00:05:37,060
stream, we can process what that looks like. And then the other one that I'm excited to

65
00:05:37,060 --> 00:05:41,800
get a chance to kind of talk to you about is the machine learning inference. Because

66
00:05:41,800 --> 00:05:47,160
when we're doing machine learning, we tend to run our training system in enormous clusters

67
00:05:47,160 --> 00:05:51,460
that are accelerated by GPUs and have all of these requirements with them. But when you

68
00:05:51,460 --> 00:05:56,340
get to actually running that inference code, it kind of is a different story. When your

69
00:05:56,340 --> 00:06:00,780
production environment is not the traditional environment that everyone's used to, and because

70
00:06:00,780 --> 00:06:06,460
all of these are events that are coming in, your code is really gluing things together.

71
00:06:06,460 --> 00:06:12,100
So how do you make sure that the intent is satisfied for what's there without writing

72
00:06:12,100 --> 00:06:16,740
more code than you need to, which, again, why do we do Python in the first place? Because

73
00:06:16,740 --> 00:06:21,140
simple is better than complex. We don't need to put a whole bunch of additional engineering

74
00:06:21,140 --> 00:06:25,860
into that. One of the other things that's really nice with serverless applications is

75
00:06:25,980 --> 00:06:31,940
we get to logically organize our code. If, for example, posting a new payment into your

76
00:06:31,940 --> 00:06:35,900
account is something that's pretty highly sensitive and that's something that you want

77
00:06:35,900 --> 00:06:40,900
certain level of security on, but getting the current account balance, well, that's

78
00:06:40,900 --> 00:06:45,820
probably okay as long as you know who it is. We can segregate those code bases so there's

79
00:06:45,820 --> 00:06:50,540
no chance of something invoking a function you don't intend it to invoke, and it allows

80
00:06:50,700 --> 00:06:56,340
you to keep that logical organization there. So let's look at some architecture because

81
00:06:56,340 --> 00:07:00,220
it wouldn't be that or it wouldn't be a presentation if we didn't at least draw some pictures for

82
00:07:00,220 --> 00:07:06,140
you. But in an API application, we can use, we'll use AWS as the cloud provider platform

83
00:07:06,140 --> 00:07:11,260
as an example here. You can use the Amazon API gateway to directly invoke your Lambda

84
00:07:11,260 --> 00:07:16,820
functions, and Lambda is the function as a service offering from AWS. That function can

85
00:07:16,860 --> 00:07:22,620
then connect to databases, it can handle queues, you can work on Pub-Sub data, you can even

86
00:07:22,620 --> 00:07:27,340
send those to an event bus and have other events work with it. Stateless functions work

87
00:07:27,340 --> 00:07:32,620
the best in this use case, and you want to interact with other services where needed.

88
00:07:32,620 --> 00:07:36,940
This could be a cloud managed service or this could be something that you're hosting yourself.

89
00:07:36,940 --> 00:07:41,820
If you have background with AWS, you may recognize some of these symbols as pertaining to specific

90
00:07:41,820 --> 00:07:45,900
services, but it doesn't matter if you're running RabbitMQ yourself or you're using

91
00:07:45,940 --> 00:07:50,900
Amazon SQS or you're using any other version of a queue, it all works. The same way with

92
00:07:50,900 --> 00:07:54,900
Pub-Sub and everything else, you know how to connect to it, you've already got the libraries

93
00:07:54,900 --> 00:07:59,860
for it, that's one of the cool things about the Python community is I can go to pypi.org

94
00:07:59,860 --> 00:08:04,540
and pull pretty much whatever I need to. And we can come through with that, which is really

95
00:08:04,540 --> 00:08:10,800
nice. A streaming data application looks a little bit different. We start with something

96
00:08:10,800 --> 00:08:15,620
like Amazon Kinesis or Kafka or whatever your favorite streaming provider is on the

97
00:08:15,620 --> 00:08:20,780
front end. We can invoke and then work on those elements that are coming in. We could

98
00:08:20,780 --> 00:08:25,920
do that in Lambda, we could do it in AWS Glue. And the difference there is what are you doing?

99
00:08:25,920 --> 00:08:31,380
If you're working in the Lambda environment, it's very short-lived events, not doing aggregation,

100
00:08:31,380 --> 00:08:36,140
pulling individual data elements together, transforming or enriching in some way, and

101
00:08:36,140 --> 00:08:40,300
then exhausting out to either another stream, a data lake, a database, whatever it needs

102
00:08:40,300 --> 00:08:45,260
to be. In the world of AWS Glue, you're talking about something that's either a ray or a

103
00:08:45,260 --> 00:08:50,300
PySpark job where you may be aggregating data across the stream, you may be running

104
00:08:50,300 --> 00:08:55,940
higher-level analytics and metrics on that. And as you process that data through, you

105
00:08:55,940 --> 00:09:00,060
want to make sure that you have a managed PySpark instance. And yes, I said managed

106
00:09:00,060 --> 00:09:05,740
PySpark instance. Isn't that cool? I got very excited when I saw this the first time of

107
00:09:05,740 --> 00:09:10,900
you mean I don't have to configure it? No. It will run pretty much whatever I want it

108
00:09:11,540 --> 00:09:17,260
to, which, again, very nice. The last thing or the last architecture picture we'll look

109
00:09:17,260 --> 00:09:24,100
at today is in the machine learning inference space. So your training data is there, you've

110
00:09:24,100 --> 00:09:28,100
done all of the work, you've followed all of your procedures to go ahead and update

111
00:09:28,100 --> 00:09:32,460
that. So when your training update happens, what you're doing is really deploying that

112
00:09:32,460 --> 00:09:37,500
code artifact to something that's readable by all of the different entities involved.

113
00:09:37,500 --> 00:09:42,700
So whether you send that out and then have it deploy across a fleet of instances or

114
00:09:42,700 --> 00:09:48,460
a bunch of containers or function as a service lambda code, all of that data is reading from

115
00:09:48,460 --> 00:09:54,760
a common file mount. And you can do that in AWS using the EFS system. So that training

116
00:09:54,760 --> 00:09:59,460
update goes out that way. Your code update, on the other hand, gets deployed directly

117
00:09:59,460 --> 00:10:05,500
into lambda, and then your change orders kind of follow what the DevOps path for all of

118
00:10:05,500 --> 00:10:10,500
those who are more of an application or a web background are. So it kind of brings things

119
00:10:10,500 --> 00:10:16,380
together. We don't have to worry about what that looks like, which is really nice. The

120
00:10:16,380 --> 00:10:21,140
really cool thing that if you're in anything that has model compliance associated with

121
00:10:21,140 --> 00:10:27,000
it, which I think everybody does if you're doing data models, you have versioned asset

122
00:10:27,000 --> 00:10:31,900
numbers for what that training data is that you're pushing into here because there's no

123
00:10:31,900 --> 00:10:36,460
way for it not to be versioned. It's kind of a cool way to invert that a little bit,

124
00:10:36,460 --> 00:10:41,180
and it forces something that I think is a cool practice. So let's talk a little bit

125
00:10:41,180 --> 00:10:46,500
about the serverless mindset. Hopefully you're a little bit intrigued by the concept of serverless

126
00:10:46,500 --> 00:10:50,100
or you probably would have spent time doing some of the other cool events that are going

127
00:10:50,100 --> 00:10:56,940
on in this time slot. We're glad you're here. But as you're looking at this, there are tradeoffs.

128
00:10:56,940 --> 00:11:03,500
When everything is an event, so is your logging, so is your observability, so are your deployments.

129
00:11:03,500 --> 00:11:08,840
And all of these events that are coming through have to be processed and handled correctly

130
00:11:08,840 --> 00:11:14,500
by using things that are totally ephemeral, where the code exists while it needs to run

131
00:11:14,500 --> 00:11:19,260
and then goes away. And this can happen in milliseconds, in fact, single digit milliseconds

132
00:11:19,260 --> 00:11:24,580
to come in. You're not going to have the classic constructs where you're going to SSH into

133
00:11:24,580 --> 00:11:29,780
an instance and go splunk through the logs using said or ack or writing a script to be

134
00:11:29,780 --> 00:11:35,980
able to do that. You need to be able to exhaust that correctly. So having ephemeral code is

135
00:11:35,980 --> 00:11:40,420
one of those things that requires a little bit of a shift from what you may be used to.

136
00:11:40,420 --> 00:11:44,860
The code is also isolated. I touched on this with the idea of, hey, I may want to do this

137
00:11:44,860 --> 00:11:49,580
strategically for security reasons because my git has a different footprint than my post

138
00:11:49,580 --> 00:11:54,440
does, but I could also isolate different elements of my code in different ways. For example,

139
00:11:54,440 --> 00:12:00,040
if I'm streaming data in a realtime interface, I may serve that customer traffic immediately

140
00:12:00,040 --> 00:12:04,680
and then fire off another event that then streams it into my data lake. My customer's

141
00:12:04,680 --> 00:12:08,880
experience is improved because they got their answer faster, and I have guaranteed delivery

142
00:12:08,880 --> 00:12:14,320
on that data into my data lake, which means I don't lose anything if an instance goes

143
00:12:14,320 --> 00:12:18,920
down, I don't have to recover it or potentially have to mount volumes or anything along those

144
00:12:19,720 --> 00:12:24,760
lines. Code self-contained. This is one of the things that's really nice because you

145
00:12:24,760 --> 00:12:30,600
get automatic versioning of this. As you go through the code and as you start doing deployments,

146
00:12:30,600 --> 00:12:35,320
all of the code is wrapped into individual segments and elements so that you don't have

147
00:12:35,320 --> 00:12:40,480
anything that's bleeding through. And last, it scales without intervention. Because you're

148
00:12:40,480 --> 00:12:45,080
paying for what you use and because those call volumes are what are determining what's

149
00:12:45,800 --> 00:12:49,280
you're doing. You have the ability to allow the service to auto scale out and you don't

150
00:12:49,280 --> 00:12:53,200
really have to think about it. If you start running into things where you're finding a

151
00:12:53,200 --> 00:12:57,560
limit or you're finding something else, it's observable. It's something that you can alarm

152
00:12:57,560 --> 00:13:01,560
on, it's something that you can proactively work through, and the really neat thing if

153
00:13:01,560 --> 00:13:07,360
you're into anything on cost controls or anything like that, your QA environment, which tends

154
00:13:07,360 --> 00:13:11,080
to provide a little bit less traffic unless you're under perf testing, which is for a

155
00:13:11,080 --> 00:13:16,920
short duration of time, three hours a day, something like that, has very, very low cost

156
00:13:16,920 --> 00:13:21,240
because you're not processing traffic through it. It's a really nice way to kind of drive

157
00:13:21,240 --> 00:13:27,040
that back to really a shift left in how do you think about where does that come from.

158
00:13:27,040 --> 00:13:31,880
So again, I mentioned tradeoffs. What should we consider here? How do you observe your

159
00:13:31,880 --> 00:13:38,200
application? How do you get those logs, metrics, and performance numbers that really drive

160
00:13:38,360 --> 00:13:42,920
what you should be doing? How do you test, build, and deploy that application? How do

161
00:13:42,920 --> 00:13:48,120
you architect it? Is Lambda the right service to use here or is any function as a service

162
00:13:48,120 --> 00:13:55,120
the right thing to use here? As you look at these kinds of questions, one of the things

163
00:13:55,120 --> 00:14:00,420
that I'll say keep in mind is this is not a silver bullet. It's not a magic solution.

164
00:14:00,420 --> 00:14:04,300
It works in more cases than it doesn't, but there are exceptions you need to be aware

165
00:14:04,300 --> 00:14:09,400
of. If you need specialized hardware, like a graphics card, to accelerate the performance

166
00:14:09,400 --> 00:14:13,820
of what you're doing, you probably don't want to be doing that in an ephemeral instance

167
00:14:13,820 --> 00:14:20,420
that's going to die and go away every few seconds as soon as it processes an event.

168
00:14:20,420 --> 00:14:23,820
Or if you're going in and you're doing kernel tweaks and kernel optimization, if you're

169
00:14:23,820 --> 00:14:27,980
doing something like that, this is probably not what you need. If you have long-running

170
00:14:27,980 --> 00:14:33,060
workers and processes that you can't decouple and put into something that's working behind

171
00:14:33,060 --> 00:14:37,260
other events or firing those events, that's not something that's going to work well in

172
00:14:37,260 --> 00:14:42,940
a serverless context because it's got to be running all the time. Another consideration

173
00:14:42,940 --> 00:14:50,140
is if you have large payloads, five-plus megabytes of data, there are things that you can do

174
00:14:50,140 --> 00:14:56,680
to make that effective and efficient, but you probably are in a case where you're processing

175
00:14:57,680 --> 00:15:03,080
an enormous amount of data and it may not be the right thing for it. At its heart and

176
00:15:03,080 --> 00:15:08,200
soul, try and cut with the grain, not against it. It's certainly a paradigm shift to think

177
00:15:08,200 --> 00:15:12,680
about things as events, especially API calls when we tend to think of those in terms of,

178
00:15:12,680 --> 00:15:17,460
well, how many can one thing handle before it falls over as opposed to how many calls

179
00:15:17,460 --> 00:15:23,400
did I get? I don't care if that's running on one or 100,000 instances. It runs. We're

180
00:15:23,400 --> 00:15:29,720
good. Understand it's a little bit uncomfortable when you first get started with it and there

181
00:15:29,720 --> 00:15:36,720
are places where there's an edge, but to go back to a point I mentioned a second ago,

182
00:15:37,480 --> 00:15:41,480
this works more often than not and more often than we find people think it does.

183
00:15:41,480 --> 00:15:48,480
One thing I would add too, if you have any questions afterwards, Dana are going to be

184
00:15:48,480 --> 00:15:51,400
at the CAF 1 booth, so hit us up with more than half an inch after.

185
00:15:51,400 --> 00:15:58,400
If you want to talk to use cases, we'll find a whiteboard. Let's go through and try and

186
00:15:59,640 --> 00:16:04,120
give you some insights into how Brian and I tend to think about this at very minimum.

187
00:16:04,120 --> 00:16:09,120
How do you know what this application's doing? The rules of observability change. I've alluded

188
00:16:09,120 --> 00:16:13,400
to this because when everything's an event and everything's ephemeral, it's no longer

189
00:16:13,400 --> 00:16:17,600
something you're going to SSH into. It's not something that's going to be outputting physical

190
00:16:17,600 --> 00:16:22,440
files that you're going to file beat and aggregate into some downstream aggregator.

191
00:16:22,440 --> 00:16:26,480
It's something that's happening in real time because logs are events too. Same thing for

192
00:16:26,480 --> 00:16:32,040
telemetry. Anything that comes in on the tracing side, that's individual events that you want

193
00:16:32,040 --> 00:16:36,840
to stitch together for what's there. Same thing is true of metrics. When we output things

194
00:16:36,840 --> 00:16:41,560
that tell us about the health of our functional requirements or our non-functional requirements,

195
00:16:41,560 --> 00:16:45,960
those are things you want to track and you want to make them available for you. Enough

196
00:16:45,960 --> 00:16:52,960
with the questions. Here's the advice. Structure your logs. This is a big one. JSON is really

197
00:16:53,560 --> 00:16:58,600
nice for this. The reason for that is across everything that we've looked at in function

198
00:16:58,600 --> 00:17:03,120
as a service, JSON can be natively interpreted, which means you can dive through it, you can

199
00:17:03,120 --> 00:17:08,600
get very fast search speeds, you can work through all of that. It's a really nice way

200
00:17:08,600 --> 00:17:12,560
to make that work and you can always send it downstream to an aggregator, whether you're

201
00:17:12,560 --> 00:17:18,160
using an elk stack, a product like Splunk, something else, you can always send JSON data

202
00:17:18,160 --> 00:17:22,560
forward, which is really nice, as opposed to unstructured data, which then somebody's

203
00:17:22,560 --> 00:17:26,480
got to parse and you've got to put compute power behind.

204
00:17:26,480 --> 00:17:30,200
Set a retention period for your logs. This is one of those things that we tend not to

205
00:17:30,200 --> 00:17:35,500
think about too much in the world of long-running hard drives, but you probably don't need data

206
00:17:35,500 --> 00:17:40,440
that's seven years old. Or if you do, you probably have a really defined reason for

207
00:17:40,520 --> 00:17:44,280
you don't want to keep it on some instance that may die at any point in time or is running

208
00:17:44,280 --> 00:17:48,520
as a spinning hard drive. You probably want that in something that's got a lot of redundancy

209
00:17:48,520 --> 00:17:55,280
built in. You want to log what's necessary for debugging. You want to be able to analyze

210
00:17:55,280 --> 00:17:59,960
what your application's doing, where it is, how it's running, but you don't want to log

211
00:17:59,960 --> 00:18:05,740
more information than really is needed. Now, why? Well, because it makes the logs a little

212
00:18:05,740 --> 00:18:12,300
harder to search. Imagine a stack trace of thousands of lines of data tracing across

213
00:18:12,300 --> 00:18:19,300
a bunch of things into packages and out of packages. Maybe useful at one level of code,

214
00:18:19,300 --> 00:18:21,300
probably less useful at others.

215
00:18:51,300 --> 00:18:58,300
Absolutely. Yeah. So you want to make sure that we're trimming that down and not keeping

216
00:19:01,060 --> 00:19:05,180
more than what we need. And the log retention period helps with this, by the way. You don't

217
00:19:05,180 --> 00:19:09,420
have to keep it in one service. There may be a threshold that, say, after 60 days when

218
00:19:09,420 --> 00:19:14,820
your customer discussion period and research period is over, you want to keep those for

219
00:19:14,820 --> 00:19:19,760
another six months so you can see what normal looks like and get a data-backed picture of

220
00:19:19,920 --> 00:19:24,200
what normal is for your app. That's great. It doesn't need to be in the log management

221
00:19:24,200 --> 00:19:28,920
service. Drop that into a data lake, put it somewhere where you can query across it, and

222
00:19:28,920 --> 00:19:33,600
you have the ability to actually keep moving at that point.

223
00:19:33,600 --> 00:19:39,160
Custom metrics are absolutely vital to most of these applications. Now, we're used to

224
00:19:39,160 --> 00:19:43,960
this in non-serverless context, being something that's done by a third-party aggregator or

225
00:19:43,960 --> 00:19:49,120
something downstream that's reading logs and trying to extract intelligent information

226
00:19:49,120 --> 00:19:54,280
out of it. We can shift that not only left, we can toss it right next to the point where

227
00:19:54,280 --> 00:19:58,440
the application is running. And when you're emitting things like the embedded metrics

228
00:19:58,440 --> 00:20:03,500
format as a standard, this is asynchronous. It doesn't impact your customer experience.

229
00:20:03,500 --> 00:20:08,680
It is guaranteed to be captured and caught. You can work with that data, and that's how

230
00:20:08,680 --> 00:20:14,400
you can build really business-centric dashboards in code, which is really cool, because now

231
00:20:14,400 --> 00:20:17,840
you don't have to worry about what are the metrics and what are the namespaces that we

232
00:20:17,840 --> 00:20:23,120
need, how do we compose that together. That dashboarding becomes much simpler.

233
00:20:23,120 --> 00:20:27,480
Now that said, there are different types of metrics and statistics that should be used

234
00:20:27,480 --> 00:20:33,520
here. If you're looking at the average amount of every transaction coming in, for example,

235
00:20:33,520 --> 00:20:39,320
to find out how did the day go, say you're running a fantastic coffee shop and you want

236
00:20:39,320 --> 00:20:43,040
to know what your average ticket price is, well, that would make a lot of sense for an

237
00:20:43,960 --> 00:20:49,040
If I was looking at the average response time for my application, though, well, I don't want

238
00:20:49,040 --> 00:20:52,660
an average of averages. I want to know what the customer impact was. That means I need

239
00:20:52,660 --> 00:20:58,480
long tails. Which brings us to metric math. You can do all kinds of different functions

240
00:20:58,480 --> 00:21:02,840
on this. You can pull the statistical anomalies and you can pull the correct data, and you

241
00:21:02,840 --> 00:21:09,100
can do this by actually running that information through EMF and then letting it be aggregated

242
00:21:09,100 --> 00:21:15,340
immediately as soon as it's exhausted. Touched on this briefly, but I'm going to emphasize

243
00:21:15,340 --> 00:21:21,260
it because I think it's important. Don't look at your averages. It's a good tool to know

244
00:21:21,260 --> 00:21:26,420
where you are, but look at P90. Look at P99. When it comes to response time, when it comes

245
00:21:26,420 --> 00:21:30,940
to error rates, when it comes to high end latencies, this is what impacts your customer

246
00:21:30,940 --> 00:21:37,700
and this is where their experience comes from. If you have something that's responding in

247
00:21:37,700 --> 00:21:44,300
100 milliseconds at the average, but the P99 is four and a half seconds, that's a really

248
00:21:44,300 --> 00:21:50,340
inconsistent customer experience. Sometimes your app is faster than humans can perceive.

249
00:21:50,340 --> 00:21:54,940
That threshold is around 250 milliseconds for what I'm using, and if anyone has better

250
00:21:54,940 --> 00:22:01,940
information, especially data science and bio people in the room, please tell me later.

251
00:22:02,060 --> 00:22:09,060
But we move off of that and we go to the 14 second line. Well, that's a long thing for

252
00:22:10,300 --> 00:22:17,300
an API call to come back and update information on a page. We can do better. Last tip, use

253
00:22:17,300 --> 00:22:22,660
AWS Lambda Power Tools. This is a fantastic open source project. It's published on PyPI,

254
00:22:22,660 --> 00:22:27,940
and it's really a stand on the shoulders of giants moment. The best practices that we're

255
00:22:27,940 --> 00:22:33,060
talking about here have been codified within this Python package, and they're really easy

256
00:22:33,060 --> 00:22:40,060
to implement. Don't take my word for it. Let's look at it. Inside of logging and trying to

257
00:22:40,980 --> 00:22:46,740
get some metrics and logging into here, this is actual code. Admittedly, it's a little

258
00:22:46,740 --> 00:22:51,540
bit of a simple example because it would fit very nicely on this slide, but we're doing

259
00:22:51,540 --> 00:22:56,040
just a couple things here. We're importing a logger and a metrics utility. We're grabbing

260
00:22:56,040 --> 00:23:01,440
some standard types of metric units. We're doing some typing to better involve your developer

261
00:23:01,440 --> 00:23:05,800
experience and make sure that we're getting good auto completion from whatever your IDE

262
00:23:05,800 --> 00:23:10,160
of choice happens to be, and then we're just doing a couple annotations. We're going to

263
00:23:10,160 --> 00:23:14,600
inject the context so we know if this is something that is a brand new scale out event. You may

264
00:23:14,600 --> 00:23:20,980
have heard of this as a cold start, or if it's something that is working from an existing

265
00:23:20,980 --> 00:23:25,840
container that's done processing an event right now. We can also go through and we can

266
00:23:25,840 --> 00:23:30,100
add custom data to this. You can see that we're collecting payment info right under

267
00:23:30,100 --> 00:23:34,540
the handler method and just kind of telling us what's going on, or you can do something

268
00:23:34,540 --> 00:23:39,180
far more advanced where we're actually giving a dictionary of data here that's going to

269
00:23:39,180 --> 00:23:45,200
be interpreted and exhausted to logs. We can even add success charge metrics here. These

270
00:23:45,200 --> 00:23:50,180
are arbitrary things. We define them as we go, and it's entirely defined in code. Those

271
00:23:50,180 --> 00:23:53,940
are picked up and become things that we can search on, which we'll go through a little

272
00:23:53,940 --> 00:23:59,420
bit more. Tracing and tracking work the same way. Power tools seamlessly does this. You

273
00:23:59,420 --> 00:24:03,760
annotate your functions, you instrument your packages, you inject your keys, and those

274
00:24:03,760 --> 00:24:09,140
can be any arbitrary key you like. If you have some kind of ID to identify transactions

275
00:24:09,140 --> 00:24:15,340
across a distributed system, that would be a great thing to use as that metric itself.

276
00:24:15,340 --> 00:24:20,860
Kind of abstract in bullet points, but let's look at the code. Again, this is a pretty

277
00:24:20,860 --> 00:24:26,140
sparse amount of code, fair amount of comments in there for being as big as it is. But to

278
00:24:26,140 --> 00:24:30,840
implement the tracer, we just go ahead and implement it. And then by using the annotation

279
00:24:30,840 --> 00:24:35,900
tracer capture method, we're going to know exactly how long these things take. How long

280
00:24:35,900 --> 00:24:42,460
is it taking you to generate a UUID for? Is that really the thing you want to do for that?

281
00:24:42,460 --> 00:24:45,980
Kind of a silly example. I know it's part of standard library. I know it's pretty fast

282
00:24:45,980 --> 00:24:49,820
for that kind of stuff. But if you're trying to benchmark two different things, this is

283
00:24:49,820 --> 00:24:54,980
a way to grab data around it, which is really cool. Capturing methods that are much more

284
00:24:54,980 --> 00:25:00,300
complicated, like actually processing a payment, it's a nice thing to just add that in as a

285
00:25:00,300 --> 00:25:09,660
subsegment. By, you know, 18 character strokes, it's in there. It's ready to go.

286
00:25:10,540 --> 00:25:14,060
We've talked a little bit about what this is, but we're at PyCon. You're not here to

287
00:25:14,060 --> 00:25:20,620
look at slides. You're here to look at code. So I'm going to walk away, and Brian is going

288
00:25:20,620 --> 00:25:31,620
to start walking through some of the code examples within this. There you go.

289
00:25:32,260 --> 00:25:44,700
Thank you very much, Dan. So have a sample project in my account. It's called PyCon 2023,

290
00:25:44,700 --> 00:25:52,980
you know, because I'm creative. The project itself is built using cookie cutter. If you

291
00:25:52,980 --> 00:25:58,140
haven't worked with cookie cutter before, it's really, really good at templating things.

292
00:25:58,140 --> 00:26:06,820
The other nice thing is that the project uses the AWS SAM CLI. So I'm being really

293
00:26:06,820 --> 00:26:12,740
opinionated here. The examples that I have here use SAM, which is an open source project

294
00:26:12,740 --> 00:26:17,780
from AWS. It stands for the serverless application model. You can think of it like cloud formation

295
00:26:17,780 --> 00:26:23,260
for serverless specific things. But there's also a corresponding CLI that lets you work

296
00:26:23,460 --> 00:26:29,420
with serverless applications locally, or you can use it to deploy to AWS as well.

297
00:26:29,420 --> 00:26:34,940
I'll add in that if you're looking at this from the perspective of, I don't use cloud

298
00:26:34,940 --> 00:26:38,900
formation. We use Terraform. We use Ansible. We use something else in our business right

299
00:26:38,900 --> 00:26:43,660
now. SAM's not going to force you to use the cloud formation template. One of the things

300
00:26:43,660 --> 00:26:48,500
that Brian's going to talk through here is that SAM can actually be used for local testing

301
00:26:48,500 --> 00:26:53,980
environments. So just like you stand up a local environment for Flask or Fast API, you

302
00:26:53,980 --> 00:26:59,220
have the ability to do that with SAM and process those events in real time. We'll go through

303
00:26:59,220 --> 00:27:00,220
that.

304
00:27:00,220 --> 00:27:05,540
So because I'm big on architectural diagrams, we have another one in here. So this represents

305
00:27:05,540 --> 00:27:12,540
an API, and our API is comprised of two Lambda functions. One that handles payment requests

306
00:27:12,940 --> 00:27:19,940
via the post HTTP method, and one that lets you get your balance using a GET. If you're

307
00:27:27,020 --> 00:27:34,020
following along at home, there is a make file within the repository that we can use to make

308
00:27:34,900 --> 00:27:41,900
some of this work a little bit easier. So make install will basically install the necessary

309
00:27:42,220 --> 00:27:47,940
Python package dependencies locally for the project, and it'll also go through a separate

310
00:27:47,940 --> 00:27:54,940
target where it'll install pipenv as well as the requisite modules. I'm using pipenv

311
00:27:57,900 --> 00:28:04,900
here. Not super opinionated. You can use whatever Python package manager you like. The one constraint

312
00:28:05,860 --> 00:28:12,860
for those of you who are using AWS with Python Lambda functions, your dependencies have to

313
00:28:14,540 --> 00:28:20,180
be in a requirements.txt file. So again, just keep that in mind. Pipenv has a really easy

314
00:28:20,180 --> 00:28:27,180
way to convert your pipenv file into a requirements file, but it's not a hard, that in and of

315
00:28:27,180 --> 00:28:30,020
itself is not a hard rule.

316
00:28:30,020 --> 00:28:34,300
I'll add in, if you find yourself using a different environment manager, go for it.

317
00:28:34,300 --> 00:28:40,060
We literally added this on top of SAM. This is not a SAM-directed thing. And all of these

318
00:28:40,060 --> 00:28:46,520
practices that are kind of made and codified by the AWS Lambda power tools stand on the

319
00:28:46,520 --> 00:28:50,700
shoulders of the Python community. When we talk about data validation, we have Pydantic

320
00:28:50,700 --> 00:28:55,220
at our fingertips. We have fast JSON scheme at our fingertips. When we're talking about

321
00:28:55,740 --> 00:29:00,700
this is a best practices implementation of standard library logging. None of this is

322
00:29:00,700 --> 00:29:07,060
magic. This is cookie cutter. This is a way to bring that stuff forward. The goal here

323
00:29:07,060 --> 00:29:11,660
is to not be opinionated about what's there. It's to make sure we're providing the best

324
00:29:11,660 --> 00:29:18,340
way possible to do that. And by centralizing that into one API and one package, we get

325
00:29:18,340 --> 00:29:24,460
the ability to do that as broad level developer adoption, which frankly makes people happy.

326
00:29:25,020 --> 00:29:30,820
That's one of the reasons I keep coding Python. It makes me happy.

327
00:29:30,820 --> 00:29:37,820
So in this case here, we are using SAM. Sam, if you're not familiar with it, quick show

328
00:29:38,380 --> 00:29:42,940
of hands, how many folks in the room have heard of SAM, the serverless application model?

329
00:29:42,940 --> 00:29:49,100
Okay. For those of you who haven't heard of it, basically it's infrastructure as code

330
00:29:49,100 --> 00:29:56,100
using AWS under the covers. Think of either YAML or JSON files being used to represent

331
00:29:57,540 --> 00:30:03,900
AWS resources. That's all. It has a reputation for being verbose. I'm not going to fight

332
00:30:03,900 --> 00:30:10,380
it. SAM makes it easier to do serverless specific things. One of the things you can do with

333
00:30:10,380 --> 00:30:17,380
the SAM CLI is make sure you're working with a valid template. The point of the validation

334
00:30:18,340 --> 00:30:25,060
here is to shift left so you don't have to wait for a build to fail. If you're using

335
00:30:25,060 --> 00:30:31,380
a shared resource to deploy your cloud assets, it may take time. Well, you don't have to

336
00:30:31,380 --> 00:30:37,040
spend that time determining whether or not your template's valid or not. Use the tools

337
00:30:37,040 --> 00:30:41,780
that you have at your disposal. It actually does use CFN Lint as well under the covers.

338
00:30:41,780 --> 00:30:45,360
So you have a sense of whether the template itself is valid and whether it's actually

339
00:30:45,360 --> 00:30:51,320
adhering to best practices. Like any good baking show, I have instructions for what

340
00:30:51,320 --> 00:30:58,320
to do. So I have a make target for all that will actually build your serverless application

341
00:31:01,040 --> 00:31:06,120
and get it ready for deployment. Now, we're not going to actually deploy yet because...

342
00:31:06,120 --> 00:31:13,120
Conference Wi-Fi. Well, and we're also on the readme where we're interacting with our

343
00:31:13,280 --> 00:31:20,280
API locally. So there are a few different ways that you can approach testing your API

344
00:31:20,440 --> 00:31:27,440
locally or working with your API locally. One is to actually pass in an event. So as

345
00:31:27,880 --> 00:31:32,360
Dan mentioned, everything in Lambda is an event. Events can come from a number of different

346
00:31:32,360 --> 00:31:39,360
sources, whether it's Amazon's API Gateway, S3, SQS, SNS, name it. In this case, we're

347
00:31:39,520 --> 00:31:46,520
basically... We have the option to pass in a valid event that's actually generated by

348
00:31:46,520 --> 00:31:51,680
an API Gateway endpoint into our Lambda function. Now, when you're doing this sort of work,

349
00:31:51,680 --> 00:31:57,000
this early experimentation, it's really, really good to test the happy path, things that you

350
00:31:57,000 --> 00:32:04,000
expect to work well and iterate as you learn, but also test for failure as well. So again,

351
00:32:04,080 --> 00:32:10,280
I do have a couple of make targets as well for sending a valid balance request as well

352
00:32:10,280 --> 00:32:17,280
as an invalid balance or invalid balance request, valid payment options as well as invalid payment

353
00:32:17,320 --> 00:32:22,120
options for any number of reasons. So I'll show you what that looks like. Again, we do

354
00:32:22,120 --> 00:32:29,120
have make targets to make this look a little bit easier. So...

355
00:32:34,000 --> 00:32:41,000
Thanks. Got you. So is that clear for everybody? Clear enough? So in this case here, I've

356
00:32:52,080 --> 00:32:59,080
already built my Lambda function locally and I'm passing the event through. Now, this is

357
00:33:00,040 --> 00:33:06,480
actually representing what the Lambda function is actually doing. So I can see that a request

358
00:33:06,480 --> 00:33:12,400
is actually coming in. I can see the event. Like in the Lambda function itself, I'm emitting

359
00:33:12,400 --> 00:33:16,000
the event. But I'm actually doing something with the event itself.

360
00:33:16,000 --> 00:33:22,400
By the way, events are dictionaries. So while there are different forms for each one, one

361
00:33:22,400 --> 00:33:26,040
of the cool things about Power Tools is it includes type hinting for each of those different

362
00:33:26,040 --> 00:33:30,640
event types and it includes the ability for us to generate those events from the local

363
00:33:30,640 --> 00:33:34,840
CLI. So you don't have to guess at what these things are. They're auto-generated for you,

364
00:33:34,840 --> 00:33:40,040
which is really nice. And the SAM CLI has that capability for any

365
00:33:40,040 --> 00:33:46,560
number of different services that can act as events for Lambda. So in case you're wondering

366
00:33:46,560 --> 00:33:53,560
what an event might look like, as Dan mentioned, it's essentially a JSON payload. This is representative

367
00:33:53,880 --> 00:34:00,480
of a real API gateway request to a Lambda function. So you can, because it's JSON, it

368
00:34:00,480 --> 00:34:07,480
can be parsed and interpreted. So let's take a peek now. We know that we can test local

369
00:34:12,320 --> 00:34:17,520
valid local interactions. Let's actually take a look at sending an invalid payment request.

370
00:34:17,520 --> 00:34:24,520
So in this example, a valid payment is one that has a user ID associated with it, a

371
00:34:26,800 --> 00:34:32,280
payment date that is later than or equal to today, where the dollar value is between one

372
00:34:32,280 --> 00:34:34,080
and $10,000. Anything?

373
00:34:34,080 --> 00:34:37,840
This may be a little bit simplified.

374
00:34:37,840 --> 00:34:44,840
This is not really how Capital One does things. So let's take a peek at what that same interaction

375
00:34:45,840 --> 00:34:52,840
looks like when we send, let's say, an invalid payment amount. So again, we have a make target.

376
00:35:07,840 --> 00:35:12,640
If you go in and download this repo, it has the instructions for how to actually instantiate

377
00:35:12,640 --> 00:35:17,160
the whole project yourself. It'll scaffold it just like Brian has here. So you'll have

378
00:35:17,160 --> 00:35:21,160
these make commands and you can look at what it is behind it. We could walk you through

379
00:35:21,160 --> 00:35:24,920
Sam in depth and we could probably talk about it for what, two, three hours, not more than

380
00:35:24,920 --> 00:35:29,040
that, but that would be really boring. So we built a make file.

381
00:35:29,040 --> 00:35:34,520
So long story short, we're able to do input validation here. You may not be able to see

382
00:35:34,520 --> 00:35:41,520
that so clearly, but we can see that the request ID, or that the request failed because the

383
00:35:42,720 --> 00:35:48,920
amount that I passed in wasn't between one and $10,000.

384
00:35:48,920 --> 00:35:53,000
And this is a good example of what we're talking about when we reference what do you need to

385
00:35:53,000 --> 00:35:57,400
debug the application? Do I need to know which specific validation failed? Do I need to know

386
00:35:57,400 --> 00:36:02,640
what field failed? I need enough to identify the application. I don't necessarily need

387
00:36:02,640 --> 00:36:08,440
to know every call made by this application.

388
00:36:08,440 --> 00:36:13,280
So I mentioned we can pass these events into our Lambda function directly. If we want to,

389
00:36:13,280 --> 00:36:20,280
though, we can actually start a local, a Docker container that represents an Amazon API gateway.

390
00:36:22,600 --> 00:36:27,080
So in this way, rather than interacting at the command line and directly passing in an

391
00:36:27,080 --> 00:36:32,600
event, you can use any HTTP client at your disposal. So what this means is if you like

392
00:36:32,600 --> 00:36:37,920
Postman, if you like Curl, if you like anybody else using anything else like requests, you

393
00:36:37,920 --> 00:36:38,920
can do it.

394
00:36:38,920 --> 00:36:44,440
Anyone heard of requests? Yeah. So if your acceptance testing, for example, is running

395
00:36:44,440 --> 00:36:49,720
just with requests because you're making API calls today, you do the exact same thing.

396
00:36:49,720 --> 00:36:53,560
And instead of having it be something that's locked in a cloud provider or locked in a

397
00:36:53,560 --> 00:36:58,240
service provider for functions as a service, you can run it right from your desktop. So

398
00:36:58,240 --> 00:37:03,080
it's a full shift left. We have yet to touch anything that's related to the cloud deployment.

399
00:37:03,080 --> 00:37:05,360
We still have confidence that it works.

400
00:37:05,360 --> 00:37:11,920
So this is a sample request being made to my local Docker container for a valid payment.

401
00:37:11,920 --> 00:37:17,640
So I'm passing in a username, and I can see the output of that whole thing. This is what

402
00:37:17,640 --> 00:37:24,640
my Lambda is emitting. Similarly, you know, like I could go through and do the same for

403
00:37:26,060 --> 00:37:30,600
an invalid payment. So I know I showed you an invalid date before, or an invalid payment

404
00:37:30,600 --> 00:37:37,600
amount. Let's take a look at invalid date in the local. Thank you, Dan. Invalid date.

405
00:37:40,200 --> 00:37:46,400
And we can see ultimately the same outcome. The request is going to fail. The HTTP client

406
00:37:46,400 --> 00:37:52,160
is going to get a 400 because now this isn't being returned to the client. The client's

407
00:37:52,160 --> 00:37:59,160
only saying it's an invalid payment. But because we're, you know, like good serverless developers,

408
00:37:59,160 --> 00:38:02,960
we can actually go back after the fact and determine why it failed. In this case, it's

409
00:38:02,960 --> 00:38:04,960
an invalid date.

410
00:38:04,960 --> 00:38:10,920
And again, what makes it invalid here, and you'll see this in the code, and you know,

411
00:38:10,920 --> 00:38:13,600
you're welcome to dig through it in as much depth as you like. You're welcome to question

412
00:38:13,600 --> 00:38:15,840
us at the end and interrogate us.

413
00:38:15,840 --> 00:38:20,080
Just don't judge the code too harshly.

414
00:38:20,080 --> 00:38:26,600
Fair point. We're standing on the shoulders of Pydantic here. You know, whether you love

415
00:38:26,640 --> 00:38:31,160
or hate it, it's a really powerful tool within the community, and we happen to really like

416
00:38:31,160 --> 00:38:38,160
it. So it's a nice way to bring that in. Again, this is not about picking tools that are out

417
00:38:38,240 --> 00:38:43,960
of nowhere or that you've never heard of. This is running Pydantic under the scenes,

418
00:38:43,960 --> 00:38:50,080
and you can do the exact same thing with FastJSON API, or sorry, FastJSON schema, rather, in

419
00:38:50,080 --> 00:38:55,140
order to make sure that that's working, which is, again, kind of nice.

420
00:38:55,460 --> 00:39:02,460
In the testing portion, we're actually making use of PyTest. Again, that's an opinion in

421
00:39:02,460 --> 00:39:07,840
this repository. You can use, you know, like I'm not telling you how you should test your

422
00:39:07,840 --> 00:39:13,140
Python code. What I am saying is that you do have the ability to test your serverless

423
00:39:13,140 --> 00:39:19,900
application because it's a Python application. That's it. It's running in a, you know, like

424
00:39:19,900 --> 00:39:24,820
in a virtual execution environment, but it's still Python code at the end of the day. So

425
00:39:24,940 --> 00:39:31,940
we can do things like, you know, make test, right? Now, this make target is literally

426
00:39:33,380 --> 00:39:40,380
running a PyTest command where my tests are really nothing more than standard Python code.

427
00:39:47,500 --> 00:39:54,500
So here's what I'm doing. Like, I'm importing my lambda function. I'm passing it to the

428
00:39:54,820 --> 00:40:00,420
testing in an event and context, and I'm checking if the expected amount matches what I actually

429
00:40:00,420 --> 00:40:05,500
got. Standard testing procedures. I'm not doing anything special just because it's a

430
00:40:05,500 --> 00:40:06,780
lambda function.

431
00:40:06,780 --> 00:40:11,140
In fact, in testing, you don't even need to know that it's a lambda function. You pass

432
00:40:11,140 --> 00:40:16,580
in an HTTP request, you get back an HTTP response, assert that that worked, you've got your outside-in

433
00:40:16,580 --> 00:40:21,540
tests now covered. As you find you're developing the application more and more, cascade into

434
00:40:21,580 --> 00:40:26,060
more detailed tests, whatever framework you happen to use on that.

435
00:40:26,060 --> 00:40:31,100
So up to this point, we've essentially written our code, we've interacted with our code,

436
00:40:31,100 --> 00:40:38,100
we've tested our code locally. We haven't done anything in AWS yet. I'll highlight how

437
00:40:38,100 --> 00:40:44,100
to do that using Sam, but ultimately, you know, I'm kind of going to, I'll gloss over

438
00:40:44,100 --> 00:40:51,100
this a little bit. Sam does have the ability to both build and deploy your serverless application.

439
00:40:52,500 --> 00:40:57,620
There's a mechanism that you can use to actually store values that make subsequent deployments

440
00:40:57,620 --> 00:41:04,620
easier where you don't have to be prompted for information. And there's sample output

441
00:41:05,100 --> 00:41:08,660
here for what a guided deployment might look like.

442
00:41:08,660 --> 00:41:15,660
Again, we gave you the make file command mostly because we're not teaching Sam CLI on this.

443
00:41:15,860 --> 00:41:21,420
But, let's assume we ran it, because I did last night and it actually worked. We actually

444
00:41:21,420 --> 00:41:28,420
have a live API based on our underlying code here. So at this point, we have a URL that

445
00:41:29,180 --> 00:41:36,180
we can use to both get credit card balances as well as postpayments. Once it's deployed,

446
00:41:38,780 --> 00:41:44,160
we can actually use our HTTP clients of choice, just like we were when we were working locally

447
00:41:44,160 --> 00:41:49,920
against our Docker container. So in this case here, you can postpayments using curl,

448
00:41:49,920 --> 00:41:54,920
postman, whatever. I'm not opinionated. But the point is, up to this point, it's really

449
00:41:54,920 --> 00:42:01,920
easy to go from ideation in using Python code to actual deployment. Like this is real. Remember

450
00:42:03,280 --> 00:42:07,000
the first time I saw this actually happen. It was pretty cool.

451
00:42:07,000 --> 00:42:11,440
And it's worth noting too that this is not AWS specific. If you're using something else

452
00:42:11,440 --> 00:42:17,480
to deploy, whether any kind of CI, CD, these are local commands that are great for hacking

453
00:42:17,480 --> 00:42:21,960
around in Brian's personal account, my personal account. When you're doing this at enterprise

454
00:42:21,960 --> 00:42:26,960
scale, you probably have something that's CI, CD in the middle. It may be a cloud specific

455
00:42:26,960 --> 00:42:33,960
system like code deploy. It may be Jenkins. It may be GitHub Actions. All of these things

456
00:42:34,720 --> 00:42:40,400
work. It's just a matter of plugging in, hey, run this for me, and you're ready to go.

457
00:42:40,640 --> 00:42:44,640
And some actually have really, really good integration with SAM as well specifically.

458
00:42:44,640 --> 00:42:50,120
So regardless of whether you are using other tooling, like if you happen to be using SAM,

459
00:42:50,120 --> 00:42:56,040
just know for a fact that there's really good support with various CI, CD tools.

460
00:42:56,040 --> 00:43:01,320
So let's get to the fun stuff. Now our application has been deployed. Let's talk about observability

461
00:43:01,320 --> 00:43:06,980
in your app. So as Dan mentioned, things change a little bit with serverless applications.

462
00:43:06,980 --> 00:43:13,980
In many ways we have to be more disciplined. You can't rely on SSHing into a host or running

463
00:43:14,380 --> 00:43:20,300
a Docker command to execute a bash shell. This is a short-lived function that won't

464
00:43:20,300 --> 00:43:27,300
be around. So you have to be mindful in how you emit your observability data, logs, traces,

465
00:43:27,380 --> 00:43:28,940
and metrics.

466
00:43:28,940 --> 00:43:33,820
By the way, your cyber people will thank you for this because there's nothing to log into.

467
00:43:34,220 --> 00:43:39,280
Therefore it's much harder to get in and do something bad in it.

468
00:43:39,280 --> 00:43:45,580
So in our case here, we're actually using two different services that AWS offers. Again,

469
00:43:45,580 --> 00:43:52,500
I'm using AWS here because I'm deploying to AWS. Not super opinionated. If you're a fan

470
00:43:52,500 --> 00:43:57,780
of Prometheus or if you're a fan of Grafana, you can take advantage of these open tools.

471
00:43:57,780 --> 00:44:03,300
In this application, we're emitting data to X-ray to capture tracing data. We're also

472
00:44:03,300 --> 00:44:09,100
using CloudWatch as a service. So we're using both CloudWatch logs and CloudWatch metrics.

473
00:44:09,100 --> 00:44:12,780
By the way, even if you're using CloudWatch logs and metrics and for some reason you don't

474
00:44:12,780 --> 00:44:17,780
like the console, you have additional security around console access, pipe it to Grafana,

475
00:44:17,780 --> 00:44:23,860
pipe it to Prometheus, pipe it into Splunk, run it in Elk. Whatever you want to do, you

476
00:44:23,860 --> 00:44:30,620
can move that data forward. What it does is it drives it left. You've got the sensor milliseconds

477
00:44:30,700 --> 00:44:33,100
in the application, which is really nice.

478
00:44:33,100 --> 00:44:40,100
So earlier Dan called out, there were a couple different places where we added a few decorators

479
00:44:40,260 --> 00:44:47,260
to our code in the hopes of essentially providing more valuable information. Let's take a look

480
00:44:47,420 --> 00:44:54,420
at some examples of where we do that in code. So this is our handler for post-payment.

481
00:44:54,820 --> 00:45:01,740
We can see that we're importing the logger. We're also importing lambda context. Now what

482
00:45:01,740 --> 00:45:08,740
this means is that we can wrap our handler and say, hey, for every downstream call from

483
00:45:09,060 --> 00:45:16,060
the handler on, wrap contextual data. Now what does contextual data mean? Well, things

484
00:45:16,060 --> 00:45:21,500
like the ARN, the unique identifier for the function including either the alias or the

485
00:45:21,980 --> 00:45:25,900
things like the amount of memory allocated to the function. Whether it was a cold start

486
00:45:25,900 --> 00:45:30,300
or not, all this information can be emitted in your logs just based on context, which

487
00:45:30,300 --> 00:45:32,100
is really cool.

488
00:45:32,100 --> 00:45:36,660
And just to drive a little bit more there, because that's an easy thing to say, the implications

489
00:45:36,660 --> 00:45:42,060
kind of require us to drop the mic in the sound people yell at us if we did it. Because

490
00:45:42,060 --> 00:45:49,060
the answer is, as you're doing this, all of that information, what version of it was deployed,

491
00:45:49,140 --> 00:45:52,540
what version of it executed it. If you're in the middle of a canary deployment, was

492
00:45:52,540 --> 00:45:55,740
it the new thing that failed or was it something in the old thing that failed? You don't want

493
00:45:55,740 --> 00:46:00,780
to prematurely trigger a rollback, especially if it's just a bad request coming in every

494
00:46:00,780 --> 00:46:06,060
once in a while from somebody. Those things are automatically instrumented for you here

495
00:46:06,060 --> 00:46:09,340
with those three lines of code. That's it.

496
00:46:09,340 --> 00:46:16,340
And what's really, really cool too, the actual act of logging is pretty straightforward,

497
00:46:17,100 --> 00:46:20,980
literally just a call to the logger. That's it.

498
00:46:20,980 --> 00:46:22,100
And give it a dictionary.

499
00:46:22,100 --> 00:46:22,460
Yeah.

500
00:46:22,460 --> 00:46:24,500
And it's serialized as a JSON.

501
00:46:24,500 --> 00:46:29,380
Yeah. Like here, we're just passing in value that came in from the event. So if I want

502
00:46:29,380 --> 00:46:36,060
to know user ID, amount, outcome of the payment, payment date, time, timestamp, I can have

503
00:46:36,060 --> 00:46:41,980
that automatically structured in my log. Now many log aggregators or log service providers

504
00:46:41,980 --> 00:46:46,820
will consume JSON really, really well. You don't have to write the regex from hell in

505
00:46:46,820 --> 00:46:51,220
order to make sense of that log, which is awesome. I've written enough of those. It's

506
00:46:51,220 --> 00:46:55,900
not fun. If you haven't, you're lucky.

507
00:46:55,900 --> 00:47:01,020
The important point to note about logging is you can capture all the relevant information

508
00:47:01,020 --> 00:47:05,100
and have it discoverable. So when bad things happen at two in the morning, your mean time

509
00:47:05,100 --> 00:47:10,900
to resolution hopefully becomes a lot shorter because you're not hunting through a lot of

510
00:47:10,940 --> 00:47:17,100
log information. It's all structured there for you. The other point too is that the amount

511
00:47:17,100 --> 00:47:22,940
of code that you need to write to add this instrumentation is minimal. Like as a developer,

512
00:47:22,940 --> 00:47:27,940
you don't want to spend time figuring out how am I going to emit this information to

513
00:47:27,940 --> 00:47:32,380
my logging service, to my metric service, to my tracing service. You want that overhead

514
00:47:32,380 --> 00:47:39,380
to be minimal. The nice thing is using AWS Lambda Power Tools, it's really easy. There

515
00:47:39,500 --> 00:47:44,640
is support. We're obviously at PyCon, but don't think it's just limited to Python. There's

516
00:47:44,640 --> 00:47:49,940
a Java version and a Node-type script version as well.

517
00:47:49,940 --> 00:47:53,700
That's really profound. I've found that to be really cool because while this package

518
00:47:53,700 --> 00:47:58,700
codifies best practices and gives you an easy way to download, import them, and get working

519
00:47:58,700 --> 00:48:04,780
on them, this means if you're a polyglot programmer and you spend part of your time in Java, you

520
00:48:05,780 --> 00:48:08,940
spend part of your time in front-end work doing Node, maybe you're writing back-end

521
00:48:08,940 --> 00:48:14,580
code using Node, it's the same patterns, but they're idiomatic to each language. You're

522
00:48:14,580 --> 00:48:18,380
not going to be dealing with weird things that don't feel like they fit here. You're

523
00:48:18,380 --> 00:48:22,020
going to be having a very similar pattern as you keep working forward.

524
00:48:22,020 --> 00:48:26,900
I'll add one thing to the tracing point because I forgot to mention it earlier. Instrumenting

525
00:48:26,900 --> 00:48:32,100
individual function calls is great if you have a complex function there. Let's say I'm

526
00:48:32,100 --> 00:48:37,620
making a database connection through PsychoPG. If I said that wrong, I apologize. I've only

527
00:48:37,620 --> 00:48:44,140
ever seen it written, not heard. I do follow them on Twitter, but they don't include pronunciation

528
00:48:44,140 --> 00:48:51,140
guides. If you're running requests or AIO HTTP, it's as simple as just saying when you declare

529
00:48:51,700 --> 00:48:55,840
your tracer, instrument these packages too. Anything they happen to do in the code, please

530
00:48:55,840 --> 00:49:01,140
track that as well. You're not having to put that implementation on every function, especially

531
00:49:01,140 --> 00:49:07,580
if all it's doing is doing retries and making sure you get a good response out of it.

532
00:49:07,580 --> 00:49:14,580
This is just a really, really quick example showing that all of these new fields that

533
00:49:14,940 --> 00:49:20,940
we're adding to our log can be automatically discovered. Here, again, this is an image

534
00:49:20,940 --> 00:49:26,340
representing AWS CloudWatch Logs Insights. Essentially, it's a way to query across different

535
00:49:26,380 --> 00:49:33,380
log groups and actually do some really cool things. The important point is that those

536
00:49:35,540 --> 00:49:41,460
fields that we didn't add are automatically discovered. Things that we did add are automatically

537
00:49:41,460 --> 00:49:47,260
discovered as well. The discovered fields list is actually quite long. We can use all

538
00:49:47,260 --> 00:49:54,260
those fields when doing comparisons, looking for values in specific fields. It's easy.

539
00:49:54,260 --> 00:49:58,500
If you have a one customer example of something that's gone wrong and people can't figure

540
00:49:58,500 --> 00:50:03,500
out what it is, you can search that customer's transaction directly and you can see all of

541
00:50:03,500 --> 00:50:07,860
the different things that hit it, not only in your function, but across the service mesh,

542
00:50:07,860 --> 00:50:13,980
which is really a cool way to approach debugging and troubleshooting.

543
00:50:13,980 --> 00:50:20,420
Using this example here, how many of you support APIs? Quick show of hands. How many of you

544
00:50:20,460 --> 00:50:25,500
have had to troubleshoot, like I had a request come in, I know this is the request ID, what

545
00:50:25,500 --> 00:50:30,780
the heck happened afterwards? What happens once it left the API? It's hard. It's hard

546
00:50:30,780 --> 00:50:35,440
unless you have a correlation ID, some way to associate that initial request with what

547
00:50:35,440 --> 00:50:41,340
happens downstream. Now, that can be done via trace, but it can even be done in logs.

548
00:50:41,340 --> 00:50:47,180
Different services may have the ability to create a correlation ID and allow you to use

549
00:50:47,180 --> 00:50:54,180
it. In this example here, I'm basically looking across multiple log groups. My API, I'm making

550
00:50:55,100 --> 00:50:59,580
sure I'm logging my API requests. My API, and I want to see what happens to a particular

551
00:50:59,580 --> 00:51:04,900
downstream request in my Lambda function. I can do that pretty easily. Now, I have a

552
00:51:04,900 --> 00:51:11,140
request ID in my API gateway that corresponds to a correlation ID. I can search for that

553
00:51:11,140 --> 00:51:18,140
same value in multiple log groups and see what happened. In this example, I have my

554
00:51:18,140 --> 00:51:20,740
API gateway request. This is the log.

555
00:51:20,740 --> 00:51:26,700
This is what we mean by the way of JSON being really nice. I'm just going to jump in and

556
00:51:26,700 --> 00:51:32,620
show this. Not only do we have this native data from the ingestion timestamp at epoch,

557
00:51:32,620 --> 00:51:38,340
we've got the message itself in JSON format, we've got the HTTP method, what came in in

558
00:51:38,340 --> 00:51:42,780
the IP. All of these are searchable as is this message field. This is just a nice way

559
00:51:42,780 --> 00:51:46,300
to display it for this query.

560
00:51:46,300 --> 00:51:51,700
I have this information for my API gateway log. I also have this information in my Lambda

561
00:51:51,700 --> 00:51:58,700
logs as well. I can find out what happened. I can see the amount, outcome, payment date,

562
00:51:58,700 --> 00:52:05,700
timestamp. I can correlate that initial API request to what happened downstream. Really,

563
00:52:09,020 --> 00:52:15,380
really powerful. Makes your troubleshooting a lot easier. Now, that can be done in code

564
00:52:15,380 --> 00:52:18,700
by setting a correlation ID.

565
00:52:18,700 --> 00:52:25,700
In this case here, I'm using a correlation path defined in Lambda power tools. We're

566
00:52:28,860 --> 00:52:35,860
really cool. I don't have to understand all the different places that that request ID

567
00:52:35,860 --> 00:52:40,420
can come from. Because again, in the AWS world, it could be an application load balancer.

568
00:52:40,420 --> 00:52:44,980
It could be an HTTP API gateway. It could be a REST API gateway. It could be a native

569
00:52:44,980 --> 00:52:46,380
Lambda function API.

570
00:52:46,380 --> 00:52:48,020
It could be a cron job.

571
00:52:48,020 --> 00:52:54,060
Yeah. But in this case, the request themselves, this is just on the HTTP side. Now, there

572
00:52:54,060 --> 00:53:01,020
are ways that you can look at correlation paths across all these different services,

573
00:53:01,020 --> 00:53:04,980
but you don't have to think about it. It's already been done for you, which is really

574
00:53:04,980 --> 00:53:11,340
nice. I know we're running out of time, so I want to be mindful of that.

575
00:53:11,340 --> 00:53:16,780
Let's talk about traces as well. Dan mentioned we have the ability... Actually, let's jump

576
00:53:16,780 --> 00:53:22,380
into metrics. This will be really, really quick. With metrics, you should be using the

577
00:53:22,620 --> 00:53:29,620
embedded metrics format. Good news is there's support for it in Python, Node, and Java

578
00:53:29,620 --> 00:53:36,620
through the power tools or utility. This lets you emit metrics, custom metrics, business

579
00:53:37,100 --> 00:53:42,300
level metrics asynchronously. There's no additional cost to you. It's just the actual cost of

580
00:53:42,300 --> 00:53:48,460
a log event being stored. That's it. You don't put your users in the critical path of emitting

581
00:53:48,460 --> 00:53:54,620
custom metrics. Quick show of hands, anybody ever try to emit a custom metric and have

582
00:53:54,620 --> 00:53:59,380
things hang for a little while because your metrics provider couldn't handle the request?

583
00:53:59,380 --> 00:54:03,140
Or how did Damon start bogging down and then crash your system because it ran into memory

584
00:54:03,140 --> 00:54:10,140
overflow? This can't happen here. You're inoculated against it because the best practices implementation,

585
00:54:11,140 --> 00:54:12,220
it's already here.

586
00:54:12,220 --> 00:54:18,300
And it's asynchronous, which is really, really nice. This is what EMF output looks like.

587
00:54:18,300 --> 00:54:25,300
You don't have to worry about writing this. All you have to know is essentially the namespace

588
00:54:26,620 --> 00:54:33,620
that you want to use, the name of the metric, the unit of the metric, and the value of the

589
00:54:34,660 --> 00:54:41,660
metric. What this gives you is the ability to have this show up in CloudWatch. So CloudWatch

590
00:54:42,660 --> 00:54:49,660
matrix. I can see the number of successful payments that have been made over a one hour

591
00:54:50,140 --> 00:54:54,860
window in a five minute bucket.

592
00:54:54,860 --> 00:54:58,740
And if you have an ops center watching this, it doesn't need to be them logging into your

593
00:54:58,740 --> 00:55:05,220
specific account to see it. You can pipe it into any of the tools we mentioned before

594
00:55:05,220 --> 00:55:10,460
from Grafana to Elk to other things that are looking at this. They have a centralized view

595
00:55:10,500 --> 00:55:16,700
of any region you choose to use. So if you distribute this closest to your clients and

596
00:55:16,700 --> 00:55:22,500
go across all 20 plus regions worldwide, then you have the ability to bring all of those

597
00:55:22,500 --> 00:55:27,380
together. If something's happening in one region, you can auto recover and fail over

598
00:55:27,380 --> 00:55:34,380
and essentially your ops people stay asleep. The cloud itself is capable of helping you

599
00:55:34,380 --> 00:55:40,060
recover using these best practices, not waking you and having you go take manual resolution

600
00:55:40,380 --> 00:55:47,380
actions. I like sleep. It's a good metric for me of how good is this code base?

601
00:55:47,380 --> 00:55:52,460
One of the other things you can do with some of the time that we have left, you do have

602
00:55:52,460 --> 00:55:58,460
the ability to capture traces as well. So again, ideally you're minimizing the amount

603
00:55:58,460 --> 00:56:03,540
of code that you have to write to observe your application. By taking advantage of power

604
00:56:03,540 --> 00:56:09,060
tools and using function decorators, you can basically get tracing for free. So in other

605
00:56:09,100 --> 00:56:12,340
words, you don't have to instrument your code if you're using something like x-ray or even

606
00:56:12,340 --> 00:56:18,620
O-Tel. There will be a degree of tracing built in. Obviously you can make those traces more

607
00:56:18,620 --> 00:56:25,620
meaningful if you want by adding things like annotations and metadata. In the case of annotations,

608
00:56:26,140 --> 00:56:31,940
annotations are indexable by the x-ray service, meaning you have a list of traces. You want

609
00:56:31,940 --> 00:56:37,860
to find out what a particular, show me a list of traces for a particular user ID. You can

610
00:56:37,860 --> 00:56:44,420
do that. Metadata you can add. Metadata is not indexed. So what might this look like

611
00:56:44,420 --> 00:56:51,420
in your x-ray dashboard? Well, in this case, with x-ray, you actually get a service map.

612
00:56:55,900 --> 00:57:01,700
So you can understand at a high level the amount of services that are being used in

613
00:57:01,700 --> 00:57:07,360
a given period of time. Now if you want to dive down into traces, you can see all the

614
00:57:07,360 --> 00:57:11,520
different traces that have been recorded for a given period of time.

615
00:57:11,520 --> 00:57:15,880
And for anyone who's really data focused in the room, you can see what's that abnormal

616
00:57:15,880 --> 00:57:20,200
transaction that threw my four nines value. If that's what you're measuring at and that's

617
00:57:20,200 --> 00:57:26,720
what your application looks for, you can find that one transaction and look line by line

618
00:57:26,720 --> 00:57:32,560
where you implemented telemetry to find where your opportunity for improvement is.

619
00:57:32,560 --> 00:57:37,080
So in this case here, I have a sense of what's happening for this one trace. API Gateway

620
00:57:37,080 --> 00:57:41,640
is one of the services that can add a trace ID header. So all downstream AWS services

621
00:57:41,640 --> 00:57:48,640
can potentially take advantage of it. I can see end to end how long that interaction took.

622
00:57:48,960 --> 00:57:53,600
But what's cool is each downstream service in turn can add its own values as well. So

623
00:57:53,600 --> 00:58:00,600
in the case of Lambda, Lambda can be an active source of tracing or a passive source. It

624
00:58:01,600 --> 00:58:07,040
can create its own trace, trace ID if one is not present or it can consume one and add

625
00:58:07,040 --> 00:58:13,280
to it if it's already present. I can see the invocation time in Lambda itself. So I can

626
00:58:13,280 --> 00:58:20,280
see the overhead of API Gateway. I can see the overhead at each point of the function

627
00:58:20,520 --> 00:58:25,760
execution as well. Now in this case, really simple API. If you're looking at the code,

628
00:58:25,760 --> 00:58:28,920
you're probably laughing a little bit. I'm basically generating mock responses. I'm not

629
00:58:28,960 --> 00:58:33,720
interacting with anything downstream. But if you were, if you were interacting with

630
00:58:33,720 --> 00:58:39,720
let's say DinoDB or a Postgres database or a Mongo database.

631
00:58:39,720 --> 00:58:45,160
Or a third party vendor running a mainframe. You can see how long each of those things

632
00:58:45,160 --> 00:58:46,240
take.

633
00:58:46,240 --> 00:58:52,440
Like if you're using requests or any number of other modules, you can actually see what's

634
00:58:52,440 --> 00:58:55,480
happening downstream. Really, really powerful stuff.

635
00:58:55,520 --> 00:58:58,760
Now if this graph looks a little off, I'm just going to draw your attention to one thing

636
00:58:58,760 --> 00:59:03,240
here as we go through. These are actually running in Brian's personal account right

637
00:59:03,240 --> 00:59:08,000
now. And while these lines look like they've got some big gaps in them, remember our baseline

638
00:59:08,000 --> 00:59:13,960
for the length of this transaction is 17 milliseconds. There aren't a lot of things that respond

639
00:59:13,960 --> 00:59:18,360
in 17 milliseconds today. And when you put containers in front of them and you deal with

640
00:59:18,360 --> 00:59:24,040
auto scaling and things like that, maybe you beat this metric. Maybe you don't. But again,

641
00:59:24,040 --> 00:59:30,240
this is a Python application. You know, for a reputation, right or wrong earned in the

642
00:59:30,240 --> 00:59:37,240
past of being slow to execute, 17 milliseconds is pretty respectable for anything I write,

643
00:59:38,240 --> 00:59:39,280
certainly.

644
00:59:39,280 --> 00:59:46,280
So I'll tell you what. We have covered a lot of ground here. So congrats for hanging in.

645
00:59:47,420 --> 00:59:49,680
We applaud you guys. Thank you.

646
00:59:49,720 --> 00:59:54,480
So I'll say there are a couple takeaways I want you to leave with. One, when you're developing

647
00:59:54,480 --> 00:59:58,640
serverless applications, shift left as much as possible. Try to get your local developers

648
00:59:58,640 --> 01:00:03,760
to have a good experience locally before you even worry about deployment. You want to make

649
01:00:03,760 --> 01:00:09,320
sure you test your serverless applications. Those rules don't change. Just because ultimately

650
01:00:09,320 --> 01:00:16,320
it may be running in Lambda, it doesn't get you off the hook. Still test your code. Use

651
01:00:17,280 --> 01:00:21,760
infrastructure as code. AWS makes it really easy to get started in the console. Don't

652
01:00:21,760 --> 01:00:28,760
fall for it. In this repo, we're using SAM. Not super opinionated. There's great support

653
01:00:28,760 --> 01:00:35,760
for Lambda in Terraform, CDK or the serverless framework. Use what works. Make sure your

654
01:00:35,760 --> 01:00:41,160
artifacts can be deployed consistently. Another thing is use automated mechanisms to deploy

655
01:00:41,160 --> 01:00:46,200
your serverless app. So as Dan mentioned here, like in this example, we're just doing a deploy

656
01:00:46,280 --> 01:00:53,280
from a laptop. In the real world, you don't do that. You use services like CodeDeployer,

657
01:00:53,280 --> 01:00:59,360
Jenkins or GitHub Actions. Also ensure you have good visibility into your serverless

658
01:00:59,360 --> 01:01:05,160
applications. The tools are there. Take advantage of them. Don't let observability be a blind

659
01:01:05,160 --> 01:01:09,080
spot for you. There's no reason you shouldn't understand what's happening in your serverless

660
01:01:09,080 --> 01:01:10,520
app.

661
01:01:10,520 --> 01:01:15,080
And it's probably fair to add that we've seen lots of different observability. We talk about

662
01:01:15,120 --> 01:01:20,040
this a lot within our company at Capital One. We talk about it in the cloud in general.

663
01:01:20,040 --> 01:01:25,680
There are dozens of blog articles on it and probably more upcoming. Observability is a

664
01:01:25,680 --> 01:01:32,080
big thing. By having a library that codifies this and either as reference material or something

665
01:01:32,080 --> 01:01:36,320
you actually choose to use, this can help inform how the rest of your systems work as

666
01:01:36,320 --> 01:01:41,400
well. Because when we understand the best practices, when it's one event executing in

667
01:01:41,400 --> 01:01:47,280
a couple hundred milliseconds or in a couple dozen milliseconds, that scales pretty well

668
01:01:47,280 --> 01:01:50,600
when you start talking about something that, okay, well, this is going to run longer and

669
01:01:50,600 --> 01:01:55,240
it's going to be able to go through. Can you get that same level of observability and visibility?

670
01:01:55,240 --> 01:01:59,560
How hard is it? Bringing some of these things in, because we're already talking about standard

671
01:01:59,560 --> 01:02:04,720
Python conventions and standard Python tools, use it as a reference implementation. Bring

672
01:02:04,720 --> 01:02:09,480
that in. Set your own standards for your organization and drive what makes sense for you.

673
01:02:10,160 --> 01:02:11,640
Big, big thanks to everybody for hanging in.

674
01:02:11,640 --> 01:02:13,640
Yeah, really appreciate you coming.

