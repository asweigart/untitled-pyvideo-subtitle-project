1
00:00:00,000 --> 00:00:06,000
Good morning. Our next speaker is Burt Wagner, and he will be talking to us today about

2
00:00:06,000 --> 00:00:10,000
crossover data drawings on flow networks with Python.

3
00:00:10,000 --> 00:00:15,000
Thanks, Emma, for that.

4
00:00:15,000 --> 00:00:21,000
Alright, awesome.

5
00:00:21,000 --> 00:00:27,000
Well, welcome everyone. Hope you're having a good morning so far. I'd like to start off

6
00:00:27,000 --> 00:00:35,000
today's talk with a, talking about my relationship probably many of you have had similar

7
00:00:35,000 --> 00:00:41,000
relationships with data and networks in the past. I'm going to kind of do it through the decades.

8
00:00:41,000 --> 00:00:46,000
And so kind of the first one I want to talk about is what I call the mom don't use the phones

9
00:00:46,000 --> 00:00:51,000
era of my upbringing. You hear the laughter, which is good.

10
00:00:51,000 --> 00:00:57,000
Obviously this is talking about the days of dialogue, right, where our networks' data transfer

11
00:00:57,000 --> 00:01:03,000
speeds were very important, right, and I'd be talking to friends. And if someone picked up that

12
00:01:03,000 --> 00:01:09,000
phone, it was a very binary decision. It went from sending data to not sending data. And even

13
00:01:09,000 --> 00:01:15,000
though that data was really small, right, text chat, it made a big difference. You had to be

14
00:01:15,000 --> 00:01:21,000
conscious of that. Also, there was a time of free music. I don't know if many people know that.

15
00:01:21,000 --> 00:01:27,000
It's the case for music now, but it used to be free. And that used to be really important to think

16
00:01:27,000 --> 00:01:33,000
about with your dialogue connections. You'd be waiting an hour to download a song and you'd find out

17
00:01:33,000 --> 00:01:39,000
eventually that it was mislabeled or whatever, but you'd want that song to download. And so making

18
00:01:39,000 --> 00:01:45,000
sure you're not abusing your bandwidth so that you can focus on the download was really important.

19
00:01:45,000 --> 00:01:51,000
The web at the time also had lots of interesting things to look at. That's what made the web great.

20
00:01:51,000 --> 00:01:57,000
So for what little bandwidth and speed I had, I wanted to spend it on fun things like that. And

21
00:01:57,000 --> 00:02:03,000
those era, that era of the web, it had lots of fan sites, where band or hobby or whatever we can go

22
00:02:03,000 --> 00:02:09,000
check out. Your favorite images, if they ever decided to load, they'd be really slow. But yeah,

23
00:02:09,000 --> 00:02:15,000
bandwidth, whether I was thinking about it or not, was a really important part of my life, even back

24
00:02:15,000 --> 00:02:21,000
then during the dialogue days. Sorry, Joey, we're not going to wait for you to load.

25
00:02:21,000 --> 00:02:27,000
All right, so back forward a decade, we upgraded to DSL, cable modems,

26
00:02:27,000 --> 00:02:33,000
Internet's a lot faster. Things are a lot better. Websites are in this perfect state where they have

27
00:02:33,000 --> 00:02:39,000
good content, they're interesting, but they're not like bloated single page JavaScript applications.

28
00:02:39,000 --> 00:02:45,000
They don't have all this privacy fracking junk embedded in them that are slowing things down.

29
00:02:45,000 --> 00:02:51,000
You know, the web was nice. We had a lot of bandwidth. Things were fast. Websites were good.

30
00:02:51,000 --> 00:02:57,000
Our media consumption, our gaming was pretty good for all the security flaws, like macro media flash.

31
00:02:57,000 --> 00:03:05,000
They made for really great games. But they really need a lot of networking and processing power in the

32
00:03:05,000 --> 00:03:11,000
background to get them to work. And then, if you were missed, but I mentioned in the early days of

33
00:03:11,000 --> 00:03:19,000
YouTube, where you can go stream 240p video on a DSL connection, and you can go to the

34
00:03:19,000 --> 00:03:25,000
DSL connection, it would be fun. It would be totally fun. And this is why I think this is the golden era.

35
00:03:25,000 --> 00:03:33,000
Unfortunately, history repeats itself. And now we're back to this era which I like to call the stop streaming era.

36
00:03:33,000 --> 00:03:39,000
I'm sure many of you can guess, but nowadays we work from home. Maybe we've got something going on in the

37
00:03:39,000 --> 00:03:45,000
background. A lot of streaming services. We have kids, family members. They may be watching the TV.

38
00:03:45,000 --> 00:03:53,000
Suddenly our bandwidth is becoming a lot more precious. We're all on video calls, especially if we're working from

39
00:03:53,000 --> 00:03:59,000
home. We may be having spouses or other people in the house who are on video calls. Once again, more bandwidth

40
00:03:59,000 --> 00:04:06,000
used up for things that maybe aren't that important to us. Gaming, bandwidth now has become much more

41
00:04:06,000 --> 00:04:13,000
important. Latency is a huge deal. So I want to make sure my home network is as good as possible to be able to

42
00:04:13,000 --> 00:04:19,000
give myself that competitive advantage. Or maybe I don't and then I just blame how bad I am at games on my

43
00:04:19,000 --> 00:04:25,000
internet connection. That's fine too. What we just had, if you were in this room before here, is a home

44
00:04:25,000 --> 00:04:31,000
automation talk. But I can't even turn a light bulb on without having the Wi-Fi working in my house.

45
00:04:31,000 --> 00:04:36,000
This has become such an integral thing that now to turn on the light I have to think about the network

46
00:04:36,000 --> 00:04:43,000
capacity in my house. So finally, the reason why we're all here today is probably because we needed to do our

47
00:04:43,000 --> 00:04:49,000
jobs and do the fun things we like doing. We like to program. We probably like to use data that's not on our

48
00:04:49,000 --> 00:04:55,000
local laptops, on our local computers at home. Maybe we're connecting through our corporate networks and dealing

49
00:04:55,000 --> 00:05:01,000
with data. Maybe we're connecting to clouds or party members, wherever. Data is an important part of the

50
00:05:01,000 --> 00:05:07,000
things we do. And being able to work with that data when it's having to compete for network resources with all

51
00:05:07,000 --> 00:05:15,000
these other things isn't always easy. And so my goal for this talk is hopefully we all walk away today being a

52
00:05:15,000 --> 00:05:23,000
little bit more cognizant about how our data is traversing our network and what we can do about it to make things

53
00:05:23,000 --> 00:05:33,000
faster. So my name is Burt Wagner. I'm a data scientist by day. I also like to blog and make YouTube videos about

54
00:05:33,000 --> 00:05:40,000
data performance concepts. So if you're into that, check that out. I've already uploaded these slides on my

55
00:05:40,000 --> 00:05:47,000
GitHub. I've created a quick link there. Feel free to take a look at them afterwards if you want. And if you have

56
00:05:47,000 --> 00:05:55,000
follow-up questions after this session. But today's agenda, we're going to be talking about how do we join data sets

57
00:05:55,000 --> 00:06:03,000
across the network efficiently, across networks that maybe are bloated by all those streaming apps and everything

58
00:06:03,000 --> 00:06:10,000
else. And I'm going to share three solutions, right? It's only going to be three. We're not going to get into too much

59
00:06:10,000 --> 00:06:16,000
detail. We've got 25 minutes left here. There's not that much time. But we'll start with kind of a great solution.

60
00:06:16,000 --> 00:06:24,000
Solutions I want to be using 90 plus percent of the time, or can use 90 percent of the time. We'll get into not bad

61
00:06:24,000 --> 00:06:30,000
solutions, but you'll see there's some downsides. And then we'll end with the ugly solutions, which are the ones that you

62
00:06:30,000 --> 00:06:38,000
hope you never have to use. But they're good to have in kind of a pocket knife utility in your back pocket for when

63
00:06:38,000 --> 00:06:45,000
the situation arises where you do need them. Let's talk about our scenario, right? If we have these two data sets

64
00:06:45,000 --> 00:06:53,000
and I'll take this example where I have a CSV file. Let's think of it. It's got some customers in it and it doesn't

65
00:06:53,000 --> 00:06:58,000
have all of the columns of data that we maybe want with our customers. We need some additional attributes, because

66
00:06:58,000 --> 00:07:05,000
whatever you're doing, whatever, some modeling or whatever else with it. So the rest of those customer attributes are in,

67
00:07:05,000 --> 00:07:13,000
let's say, a data warehouse, some kind of relational database. I've been denoted with a little cloud. It could be in the

68
00:07:13,000 --> 00:07:21,000
data center owned by our company or whatever. And our goal here is, we want to join these two data sets together.

69
00:07:21,000 --> 00:07:31,000
The CSV file, maybe 100,000 rows, two gigs in size on my local laptop. And let's say 30 million rows, 20 gigs of data in a

70
00:07:31,000 --> 00:07:42,000
database somewhere, not local to my laptop. So does anyone have any, what's everyone's go-to method for this? That would be

71
00:07:42,000 --> 00:07:54,000
a good method for this. Shout it out. Select star. All right. Well, we'll talk about that. Reload it into the data center.

72
00:07:54,000 --> 00:07:59,000
I like that. You know how to network any clock? We're talking about moving data to the data center. So let's jump to that.

73
00:07:59,000 --> 00:08:08,000
That's actually my favorite solution, right? This is the one you want to use if you can. And so the way it kind of looks

74
00:08:08,000 --> 00:08:16,000
at this is we take our CSV file data, customer data, and we move that across the network, right? That's our smallest piece of

75
00:08:16,000 --> 00:08:21,000
data that we're working with. Let's move that to our data center or to the cloud or wherever the bigger data is, right?

76
00:08:21,000 --> 00:08:28,000
This kind of technique is called moving your smaller data to where the bigger data is. And so maybe that means we're

77
00:08:28,000 --> 00:08:33,000
going to create like a staging table in our, if it's a database or something like that, maybe we can create like a

78
00:08:33,000 --> 00:08:42,000
temporary memory table. It's kind of up to you how you do it, right? But you create that and then you do your joins, right?

79
00:08:42,000 --> 00:08:49,000
And relational databases, whatever flavor you choose to use, you know, they're really good at doing step-based operations.

80
00:08:49,000 --> 00:08:54,000
They're really good at doing joins, right? So you're going to be using the right tool for the job in that case. You're going to be able to

81
00:08:54,000 --> 00:09:00,000
join that data, especially if you index it and everything else. You're going to get really great performance out of that.

82
00:09:00,000 --> 00:09:08,000
Once that's done, you can output your data. So here in this example, I'm doing some image join. I'm leaving some records,

83
00:09:08,000 --> 00:09:14,000
but it's only outputting 80,000 rows once again. That travels back across the network to our laptops or wherever.

84
00:09:14,000 --> 00:09:22,000
And then we can go ahead and process that data, transform it, do whatever we want with it. But at that point, right, we're not

85
00:09:22,000 --> 00:09:32,000
working with this big massive set. We're just working with kind of our final subset. Now, this solution, there's a lot of things I like about it, right?

86
00:09:32,000 --> 00:09:41,000
First of all, you're moving your smaller data to where the larger data is. You're really minimizing how much network traffic you're going to be having,

87
00:09:41,000 --> 00:09:51,000
and that's awesome. It's also, you know, I'm going to assume here that your database is on some kind of server with some better specs than what your

88
00:09:51,000 --> 00:09:58,000
laptop would have. You're going to be able to process that data well, using the right tools for the job. That's going to be a really good thing.

89
00:09:58,000 --> 00:10:15,000
Additionally, let's see, the one thing that this solution is not great at is the fact that if you need to clean and transform your data right before you do your join,

90
00:10:15,000 --> 00:10:25,000
it's going to be a little tough. Like, sure, you can do, you know, some replace functions in your database if you need to clean some data up, but that's not really the right place to do it.

91
00:10:25,000 --> 00:10:35,000
Databases aren't necessarily the best tool for you to actually do that. But as an example, maybe make it more concrete. Imagine we're joining these two customer databases

92
00:10:35,000 --> 00:10:45,000
on the street address. Maybe one of the files we got 123 main street fully spelled out, and then the other file we got 123 main ST, right, and abbreviated for the street.

93
00:10:45,000 --> 00:10:55,000
You're not going to be able to join on those columns on their own. You're going to have to do a little bit of cleanup on them, because you will look up, make a match, something like that.

94
00:10:55,000 --> 00:11:05,000
And so in this scenario, that can sometimes get tricky. It's not always the best place you'd be able to do that. So we have other options.

95
00:11:05,000 --> 00:11:13,000
Before we get to them, though, I just want to remind everyone of kind of the goal of this session. I think about where your data is moving.

96
00:11:13,000 --> 00:11:23,000
In this case, let's say here we are in Salt Lake City, and I'm moving that 2 gigabyte file to a corporate VPN endpoint, maybe somewhere in Los Angeles.

97
00:11:23,000 --> 00:11:31,000
It's going out to the East Coast in Virginia to some cloud provider. Maybe that's where all my data processing is happening. Great.

98
00:11:31,000 --> 00:11:37,000
When that data comes back, it's a small subset, right, and has to make all those talks back, not to mention all the hop in between.

99
00:11:37,000 --> 00:11:49,000
So just think about that. And when you're kind of working with data with large data sets, think about how much network traversal is having to happen in order for this to be effective.

100
00:11:49,000 --> 00:12:01,000
So just to summarize this, this kind of moving the small data where your big data lives minimizes network movement and maximizes the tools for what they're good for.

101
00:12:01,000 --> 00:12:10,000
The downsides are you need to be able to write your data to that kind of larger database. So maybe a database administrator should really lock the database down.

102
00:12:10,000 --> 00:12:16,000
You're not allowed to create additional tables on it. It could be problematic. If you're working with a little vendor or something,

103
00:12:16,000 --> 00:12:26,000
read-only, it's going to be a little bit more challenging. And also, this isn't the greatest solution if you have to do a lot of transformations.

104
00:12:26,000 --> 00:12:36,000
I talked a lot about this topic in the past in way more detail than 30 minutes to handle, so if you're interested in that, I've linked to some other conference talks I've had for additional tips

105
00:12:36,000 --> 00:12:44,000
kind of making that process a lot more speedy. But we will for now continue to the next topic.

106
00:12:44,000 --> 00:12:54,000
Those are not bad solutions. In this case, we are assuming we have a big heavy duty server in the middle there that's going to help us do our joins.

107
00:12:54,000 --> 00:13:02,000
So we have our small data set in the bottom left. We've got our database with our large amount of data in the right.

108
00:13:02,000 --> 00:13:12,000
We can't move our small data to our large data. So what are we going to do? Or we are going to do this join on some big heavy duty application server with hundreds of gigs of memory,

109
00:13:12,000 --> 00:13:22,000
dozens of CPUs that's going to be able to handle it. And this is traditionally what you maybe would see in like an ETL pipeline solution.

110
00:13:22,000 --> 00:13:32,000
You know, it's great. It gets the job done. You need to be able to have a server and procure it. For some of us, it's pretty easy.

111
00:13:32,000 --> 00:13:42,000
Spin up some instance in the cloud, you're ready to go. For others, there's lots of corporate red tape maybe to get that server.

112
00:13:42,000 --> 00:13:50,000
So it's kind of a key hero adventure of how easy or hard that is. Thinking about network traversal again.

113
00:13:50,000 --> 00:13:57,000
We are going to have to move that giant database, you know, 30 gigs or whatever I said it was.

114
00:13:57,000 --> 00:14:05,000
So it would be beneficial to still have this big heavy duty server as close to your database server as possible.

115
00:14:05,000 --> 00:14:12,000
So maybe that's in a corporate data center that understands flight. They're going to be moving across fibers and then it'll be really fast.

116
00:14:13,000 --> 00:14:22,000
It won't be a big deal. You know, if your big heavy duty server is in your basement, maybe you have one of those because you're a nerd like me.

117
00:14:22,000 --> 00:14:29,000
That's awesome. But it's going to be really slow competing with all the Netflix bandwidth and everything else that you need to your house.

118
00:14:29,000 --> 00:14:33,000
So that's a big caveat of this. You need to have that data there.

119
00:14:33,000 --> 00:14:39,000
Another plus of this solution though is if we have that situation again where we need to do our join,

120
00:14:39,000 --> 00:14:43,000
we have our street address data and it's not exactly lining up right.

121
00:14:43,000 --> 00:14:48,000
One, two, three main street spelled out versus an abbreviated street in our other data set.

122
00:14:48,000 --> 00:14:55,000
We can process that data really easily, really nicely with something like Python here before we do our joins.

123
00:14:55,000 --> 00:15:01,000
So that's really cool. And if we're doing our joins, you know, for a lot of us, we might be using something like Pandas.

124
00:15:01,000 --> 00:15:09,000
Having plenty of memory and disk space available to do that join is a really nice option.

125
00:15:09,000 --> 00:15:17,000
You could also in this scenario, I have plenty of other Python packages out there, so if you use something like LID and DAS sometime,

126
00:15:17,000 --> 00:15:25,000
so DAS will allow you to parallelize it and kind of distribute some computations to across lots of machines.

127
00:15:25,000 --> 00:15:33,000
So once again, if you have more than one machine available to kind of do this data processing, great.

128
00:15:33,000 --> 00:15:47,000
That's a great place to be. But obviously not everyone will have that available to them, so kind of want to focus too much on that.

129
00:15:47,000 --> 00:15:57,000
So, right, let's summarize this solution. If large data is on the same network as our application servers,

130
00:15:57,000 --> 00:16:02,000
or we're maybe running Python to do some joins, performance is getting pretty good.

131
00:16:02,000 --> 00:16:08,000
If you have a pool of machines, even better. There's lots of packages out there, but you can utilize to make that work.

132
00:16:08,000 --> 00:16:16,000
If you've ever checked out where there's a good documentation page in the Pandas documentation, I'll include a link to it down below.

133
00:16:16,000 --> 00:16:24,000
Where they talk about all different kind of packages that are better dealing with large data sets and maybe the Amazon itself.

134
00:16:24,000 --> 00:16:28,000
So depending on your individual use case, it may be a better option there.

135
00:16:28,000 --> 00:16:36,000
You know, the downsides here are obviously servers are hard to get sometimes. And like we said, we want to be on the same network.

136
00:16:36,000 --> 00:16:43,000
So, let's see. Any quick questions before we continue on with the other solutions?

137
00:16:43,000 --> 00:16:45,000
So, what are the solutions that you want to get to the ugly solutions?

138
00:16:45,000 --> 00:16:51,000
These two solutions, this is what you want to be doing when you're dealing with data.

139
00:16:51,000 --> 00:16:56,000
Alright, great. I think we have a couple of questions at the end there.

140
00:16:56,000 --> 00:17:01,000
Alright, so let's get to the ugly solution.

141
00:17:01,000 --> 00:17:08,000
We're going to write our own half join. So, for those of you who are kind of maybe from computer science degrees,

142
00:17:08,000 --> 00:17:12,000
or you're a computer engineer, if that's not your background, totally cool.

143
00:17:12,000 --> 00:17:18,000
Well, it's my background either. I have a degree in political science.

144
00:17:18,000 --> 00:17:23,000
But, this doesn't mean you can't enjoy a little theory. If it goes over your head, don't worry about it.

145
00:17:23,000 --> 00:17:28,000
You don't need to know it all. You can't be able to use the Python code that I include at the end.

146
00:17:28,000 --> 00:17:35,000
So, what we want to do with a half join, at a high level, we've got our key data sets, right?

147
00:17:35,000 --> 00:17:39,000
That's going to be represented by these two columns with colorful data.

148
00:17:39,000 --> 00:17:44,000
We have our build input data sets. That would be our smaller data set.

149
00:17:44,000 --> 00:17:49,000
And we have our probe input data set. That's going to be our large data set.

150
00:17:49,000 --> 00:17:56,000
And the way a half join works, this is how a lot of the relational databases kind of implement this and a lot of other tools too.

151
00:17:56,000 --> 00:18:03,000
We're just going to be running it with all the control that we want in order to kind of make things reliable.

152
00:18:03,000 --> 00:18:08,000
So, what happens is we take our data. Here we're going to take data B.

153
00:18:08,000 --> 00:18:13,000
We're going to run it through a hash function. Then it's going to go get stored in a hash bucket or a dictionary.

154
00:18:13,000 --> 00:18:21,000
We're doing the tasking. We're using this hash function on the columns that we're going to be using in our join, right?

155
00:18:21,000 --> 00:18:30,000
And so, we're hashing that data to be able to quickly look it up and see if there's any matches in a minute, which we'll see.

156
00:18:30,000 --> 00:18:41,000
So, we like to do that for all our data. Once we take the next probe data, we have the columns that we're joining on, put that in a phone bucket, do that again for the next data, and we keep doing that.

157
00:18:41,000 --> 00:18:48,000
Now, the interesting thing with hash functions is, you'll see here, we got two pieces of data that fell into the same bucket.

158
00:18:48,000 --> 00:18:57,000
So, in the implementation, I'm going to show you in a minute, I'm using Python's hash method, which I believe you can see hash under the covers,

159
00:18:57,000 --> 00:19:07,000
which means collision resistant, but not collision proof. And so, what that means is, when it comes to hashing, what that means is you get two very different inputs,

160
00:19:07,000 --> 00:19:12,000
and you pass them through the hash function, and they end up outputting the same exact value.

161
00:19:12,000 --> 00:19:20,000
That's what's known as collision. We need to check for those. So, this is kind of an example here of a collision happening, even though A and F are two different pieces of data,

162
00:19:20,000 --> 00:19:25,000
they're generating the same hash, and they're getting put in the same hash bucket.

163
00:19:25,000 --> 00:19:32,000
So, something to be aware of, but we'll need to check for that in a minute, and finally do that with our last piece of data there.

164
00:19:32,000 --> 00:19:40,000
Now, all of this data, just in our hash bucket that was kind of in our build input phase, needs to fit into memory.

165
00:19:40,000 --> 00:19:46,000
So, if you are running this on a lower performance laptop, whatever, you've got to be able to store that in memory,

166
00:19:46,000 --> 00:19:56,000
there are workarounds, write stuff to disk, page it out to disk, or take a buff if you want to, that's way beyond this tutorial and the packet that we're showing you.

167
00:19:56,000 --> 00:20:09,000
This is through your small data set that's in memory. And what that's going to enable you to do is now screen all of the rest of your data across the network

168
00:20:09,000 --> 00:20:18,000
to kind of compare it and see if we have a match. So, we screen that in from our SQL database, our giant SQL database, we hashed it,

169
00:20:18,000 --> 00:20:29,000
it could put in the hash bucket, now we can say, oh cool, these both have hash values of two, let's actually now iterate over them and check to see if they're a match.

170
00:20:29,000 --> 00:20:35,000
Cool, these values actually match, not just the hashes, but the values of our joint keys match.

171
00:20:35,000 --> 00:20:44,000
And we're doing that once again because in this case there was a collision so it didn't match with F, that's why we've got to check the values one by one.

172
00:20:44,000 --> 00:20:53,000
And so we do this again for the next row of data, we pass that in, we check to see if there are matches, we iterate and write, we save our matches off,

173
00:20:53,000 --> 00:20:59,000
maybe we save them for our local hard drive, maybe we save them somewhere else. And we kind of repeat this process for all the rest of our data.

174
00:20:59,000 --> 00:21:09,000
Now the cool thing with this is that if we get a network error, right, or our network blows up and we have to restart it, as long as we can go look at our output file

175
00:21:09,000 --> 00:21:22,000
and see kind of what our last match, match record was, we can re-query our database, you know, our large data set input and have it kind of resume from where it left off.

176
00:21:22,000 --> 00:21:32,000
That's the really nice thing about this. This is a slow solution, so thinking about again like network movement, we are streaming that gigantic database through our laptop,

177
00:21:32,000 --> 00:21:43,000
but it is very reliable. If that thing crashes, you can have it try, accept, you know, logic to say, hey, pick up where I left off, which is really nice.

178
00:21:43,000 --> 00:21:49,000
But if you get stuck in that scenario where you have to do that, it's really nice to be able to do that.

179
00:21:49,000 --> 00:22:00,000
So, how do we actually implement this in practice? We are done with the theory. So, I built a package for it, so I couldn't find anyone else who had done it.

180
00:22:00,000 --> 00:22:08,000
It's called Cross Screen, so you can install that. Two F's, and if you just search for my name on GitHub or whatever, you can find it that way too.

181
00:22:08,000 --> 00:22:18,000
Makes it really easy to use, so let's kind of look through one example. Because this is, I've spent a lot of time with coworkers and just on my own having to solve this problem

182
00:22:18,000 --> 00:22:27,000
when we get these kind of weird data sets that we can't easily move data around, we have to kind of do things this way. We don't want them to die prematurely.

183
00:22:27,000 --> 00:22:37,000
We need to join the finish. So, starting out at the top, pretty basic import the package. We're going to create our, we're going to read in our files.

184
00:22:37,000 --> 00:22:47,000
So here I'm using two CSV files in the demo, but the package will allow you to do an ODBC connection as well as you can just trace from a database or anything else

185
00:22:47,000 --> 00:22:55,000
that you can connect to as an ODBC driver. So that works pretty well. And this is just going to kind of do a lazy read. All we're doing here, this is similar to how

186
00:22:55,000 --> 00:23:05,000
packages like Dashboard, right, we're just going to get metadata in about our data sets, right, so what are the files, do they contain header columns, and what are the columns that we're going to be

187
00:23:05,000 --> 00:23:14,000
joining on between our two data sets. We're just kind of telling the program that's all the info we need. And then we're going to go ahead and do the same thing with the other

188
00:23:14,000 --> 00:23:26,000
data sets. So we're just going to get our output file set up, right, so maybe it's local, maybe it's on the network or cloud storage. We write out some header columns, and this is all just dealing with CSVs,

189
00:23:26,000 --> 00:23:37,000
keep things simple. And then we finally, right, this is kind of what we just looked at on that last slide which puts the animations on it. This is doing that, that inner hash join.

190
00:23:37,000 --> 00:23:55,000
And so it loads that first file in the memory, the small one, and then it's going to stream one row at a time from your other large data source, every single record, do that casting, do the joins, and, you know, eventually get your data to work in an unreliable network environment.

191
00:23:55,000 --> 00:24:05,000
And that's where I said you can wrap this up and you can try and accept and use some error handling logic if you think you're going to have problems with just finishing out.

192
00:24:05,000 --> 00:24:20,000
A couple other things I want to show here is, you know, we were talking about, hey, you have this street address where you have to convert 123 Main Street spelled out, 123 Main, you know, SQ with an abbreviation.

193
00:24:20,000 --> 00:24:30,000
How do we do that? Well, this solution actually works really well. I'm trying to minimize, you know, having only one row of data in memory at a time from the streaming side, from the probing side.

194
00:24:30,000 --> 00:24:40,000
And so what's in here is you can, you know, there's a function you can override that describes how you want to transform your data before you join it.

195
00:24:40,000 --> 00:24:58,000
So if you just create a simple function where you cast in the value of one of your columns that you're joining on and you tell it how you want to transform it, right, so here we're just doing a basic, you know, strip out the white space kind of transformation, but you can make this as complicated as you want, right?

196
00:24:58,000 --> 00:25:08,000
There's no limit to how complicated you want this object. It is slowing down your process. But you can do, you know, you can do these kind of geospatial stuff.

197
00:25:08,000 --> 00:25:18,000
There's some complicated transformations you may need to do before you join your data. It could be things like street addresses, the buggy matching, whatever you need, right, you can kind of secure that.

198
00:25:18,000 --> 00:25:30,000
And then so when you call your inner hash join function, you just pass in, as a named argument there, the name of your function that you created with your kind of custom join transformation logic.

199
00:25:30,000 --> 00:25:36,000
So that'll just kind of do it as the data streams through.

200
00:25:36,000 --> 00:25:46,000
And so finally, though the last thing I kind of want to point out with this solution is just how we can transform data before we do our join.

201
00:25:46,000 --> 00:25:58,000
There's also the capability to transform your data after your join happens. And I figure, oh, well for performance, that row of data is already in memory, this would be the perfect time to do something with it if we needed to, right?

202
00:25:58,000 --> 00:26:05,000
Just kind of get things to go quicker so we don't have to save this disk and read it in later. So this works very similarly.

203
00:26:05,000 --> 00:26:17,000
You can create your own, you know, functions for kind of how to process the data once it's matched. Here, right, I'm just adding a static weight value of 1.0 to every single row.

204
00:26:17,000 --> 00:26:32,000
And then so we're returning that row of data there as those pupils. And then so in our inner join function that we were calling, we can pass in that custom processing function as an override.

205
00:26:32,000 --> 00:26:44,000
Right? And so in conclusion, this ugly solution, even though it's not going to be as fast as our other two options, and it's not as good as our other two options, it is reliable.

206
00:26:44,000 --> 00:26:54,000
It is a reliable way to get this kind of problem of joining data sets across networks to work if you need to. It has a lot of retriability.

207
00:26:54,000 --> 00:27:06,000
You can program that in, and it's just a network failure and it's really flexible and customizable in terms of if you need to be able to do data transfer information in order to get your joins to work.

208
00:27:06,000 --> 00:27:21,000
I try to be pretty good about reading in data only once and kind of keeping memory usage as low as possible. In my case, when I created this, right, it was I only had my laptop and it was like, you know, whatever, 10 gigs of free RAM and like no disk space available.

209
00:27:21,000 --> 00:27:31,000
Sometimes that's the world we have to work with. So if that sounds like a problem you have, right, then check that out. But with that said, thank you very much.

210
00:27:31,000 --> 00:27:34,000
And I think we have two minutes if people have questions.

211
00:27:34,000 --> 00:27:59,000
Are there ways in the library to optimize the hash function?

212
00:27:59,000 --> 00:28:00,000
Great question.

213
00:28:00,000 --> 00:28:12,000
No. I was, you know, just trying to get the basics working. The story behind this is we were at a deadline and I needed to get this joined to work and nothing else was working.

214
00:28:12,000 --> 00:28:25,000
So this was a first pass kind of take at it. So there's no huge crazy optimizations. It was more focusing on the reliability aspect of it. Thank you, though.

215
00:28:25,000 --> 00:28:33,000
All right. Well, I want to make sure we have enough time for the next speaker. If anyone else has questions, they'll be hanging around in the back or outside. But thank you for coming.

