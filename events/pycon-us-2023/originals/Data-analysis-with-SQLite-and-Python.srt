1
00:00:00,000 --> 00:00:06,560
Okay, good afternoon everyone and welcome to PyCon.

2
00:00:06,560 --> 00:00:09,680
This is the largest conference I've been to in like three

3
00:00:09,680 --> 00:00:10,960
and a half years.

4
00:00:10,960 --> 00:00:12,520
It's very exciting.

5
00:00:12,520 --> 00:00:14,360
So today I'm going to be teaching you all

6
00:00:14,360 --> 00:00:18,040
about data analysis with SQLite and Python.

7
00:00:18,040 --> 00:00:21,440
And for this session I've prepared a very extensive handout

8
00:00:21,440 --> 00:00:23,000
which I'm going to be working through

9
00:00:23,000 --> 00:00:26,840
which is available right now online.

10
00:00:26,880 --> 00:00:33,760
So if you go to SQLite-tutorial-pycon-2023.readthedochs.io,

11
00:00:33,760 --> 00:00:36,000
I'm now regretting not getting a shorter URL for it.

12
00:00:36,000 --> 00:00:38,160
This will give you the exact notes that I'm going

13
00:00:38,160 --> 00:00:42,200
to be working through for the workshop today.

14
00:00:42,200 --> 00:00:45,760
We have two volunteers at the back, Felix and Nick,

15
00:00:45,760 --> 00:00:47,240
who are waving there.

16
00:00:47,240 --> 00:00:50,760
If you get stuck with anything, please put up a hand and one

17
00:00:50,760 --> 00:00:54,160
of them will scoop in and hopefully rescue you.

18
00:00:54,160 --> 00:00:56,560
I'm hoping this should all be quite straightforward.

19
00:00:56,760 --> 00:00:59,960
One of the great things about SQLite in general is

20
00:00:59,960 --> 00:01:02,520
that there are very little things that can go wrong with it.

21
00:01:02,520 --> 00:01:05,440
The development environment you'll be using today should be,

22
00:01:05,440 --> 00:01:08,040
I'm going to regret saying this, should be rock solid

23
00:01:08,040 --> 00:01:11,000
and hopefully won't cause us any problems.

24
00:01:11,000 --> 00:01:14,080
And yes, so the plan for today is we're going to talk

25
00:01:14,080 --> 00:01:17,840
about SQLite and Python and how to use those just straight

26
00:01:17,840 --> 00:01:19,320
out of the standard library.

27
00:01:19,320 --> 00:01:21,680
I'm going to introduce you to Dataset, the tool

28
00:01:21,680 --> 00:01:24,440
that I've been building for the past five years now to help

29
00:01:24,480 --> 00:01:28,720
with sort of data exploration and publishing.

30
00:01:28,720 --> 00:01:31,640
I'll also talk you through SQLite Utils,

31
00:01:31,640 --> 00:01:33,920
which is a library I've been building to make working

32
00:01:33,920 --> 00:01:35,920
with SQLite as productive as possible.

33
00:01:35,920 --> 00:01:38,120
We will publish a database onto the internet.

34
00:01:38,120 --> 00:01:39,440
That's going to be fun.

35
00:01:39,440 --> 00:01:42,840
I'll show you Dataset Lite, which is Dataset running inside

36
00:01:42,840 --> 00:01:45,200
of WebAssembly in a browser, which I still

37
00:01:45,200 --> 00:01:46,880
to this day can't believe that's possible.

38
00:01:46,880 --> 00:01:49,680
And then we'll talk about some advanced SQL topics towards the

39
00:01:49,680 --> 00:01:53,120
end as well and lots of demos and all sorts of bits

40
00:01:53,120 --> 00:01:54,400
and pieces like that.

41
00:01:55,400 --> 00:01:57,280
One thing I'm going to be sort

42
00:01:57,280 --> 00:02:00,720
of tangentially covering is SQL itself.

43
00:02:00,720 --> 00:02:03,400
And that's because I've decided to do this in sort

44
00:02:03,400 --> 00:02:05,960
of board game rules.

45
00:02:05,960 --> 00:02:08,280
And if you've ever played a new board game,

46
00:02:08,280 --> 00:02:09,600
there are two ways you can do it.

47
00:02:09,600 --> 00:02:12,320
You can dissect all of the rules at the very start

48
00:02:12,320 --> 00:02:14,600
and everyone figures them out and then you start playing.

49
00:02:14,600 --> 00:02:17,360
Or you can just start playing and pick up the rules as you go along.

50
00:02:17,360 --> 00:02:19,440
And I'm hoping that we can do that with SQL.

51
00:02:19,440 --> 00:02:21,760
So if you're familiar with SQL, you're going to be fine.

52
00:02:21,760 --> 00:02:23,320
If you're not familiar with SQL,

53
00:02:23,360 --> 00:02:26,560
hopefully you'll be able to pick up bits and pieces as

54
00:02:26,560 --> 00:02:30,080
and when we need them as we go through the tutorial.

55
00:02:30,080 --> 00:02:35,720
So in terms of what you'll need, for the first half of this tutorial,

56
00:02:35,720 --> 00:02:39,200
you will need nothing more than Python 3 running on your laptop,

57
00:02:39,200 --> 00:02:40,720
running on a device.

58
00:02:40,720 --> 00:02:41,240
That's it.

59
00:02:41,240 --> 00:02:41,840
Nothing else.

60
00:02:41,840 --> 00:02:45,560
Everything will be based on things in the standard library.

61
00:02:45,560 --> 00:02:50,000
Once we get past Python, you will need the ability to pip install stuff.

62
00:02:50,000 --> 00:02:52,120
So if you know how to use virtual environments

63
00:02:52,160 --> 00:02:54,520
and if you've got an environment you can pip install into,

64
00:02:54,520 --> 00:02:55,960
you'll be completely fine.

65
00:02:55,960 --> 00:02:58,400
Just in case anyone's not set up for that,

66
00:02:58,400 --> 00:03:02,720
I'm going to be running this tutorial myself using GitHub Codespaces,

67
00:03:02,720 --> 00:03:06,760
which offers a free development environment entirely in the browser.

68
00:03:06,760 --> 00:03:10,520
The thing I love about Codespaces is that if it breaks,

69
00:03:10,520 --> 00:03:12,520
you throw it away and you open up a new one.

70
00:03:12,520 --> 00:03:16,440
So it's sort of ideal for things like tutorials where the last thing

71
00:03:16,440 --> 00:03:18,520
you want to do is get into a state where your development environment

72
00:03:18,520 --> 00:03:21,040
isn't working and now you lose time trying to fix it

73
00:03:21,040 --> 00:03:23,680
when you could be getting on with the thing that you're trying to learn.

74
00:03:25,200 --> 00:03:28,640
So, first a show of hands,

75
00:03:28,640 --> 00:03:32,560
who here has Python 3 running on a laptop and is already happy with that?

76
00:03:33,400 --> 00:03:33,760
Okay.

77
00:03:33,760 --> 00:03:36,720
Is there anyone who doesn't and is going to join me in using Codespaces,

78
00:03:36,720 --> 00:03:37,800
which I'm going to use anyway?

79
00:03:39,200 --> 00:03:39,640
There we go.

80
00:03:39,640 --> 00:03:40,240
We have one.

81
00:03:40,240 --> 00:03:40,920
Fantastic.

82
00:03:40,920 --> 00:03:41,320
Okay.

83
00:03:41,320 --> 00:03:45,880
So what I'm going to do is I'm going to start with this repository here.

84
00:03:45,880 --> 00:03:48,600
This is a repository that GitHub themselves shared,

85
00:03:49,280 --> 00:03:53,080
that gives you a really quick way to get into a Codespaces environment

86
00:03:53,080 --> 00:03:55,560
with Jupyter Notebook setup as well.

87
00:03:55,560 --> 00:03:59,160
So I can say, use this template, open in a Codespace,

88
00:03:59,160 --> 00:04:01,600
and this churns away for a few seconds.

89
00:04:01,600 --> 00:04:04,720
It's somewhere it's launching a Kubernetes container and all sorts of stuff,

90
00:04:04,720 --> 00:04:06,760
but I don't really have to care about that.

91
00:04:06,760 --> 00:04:08,920
And now, here we go.

92
00:04:08,920 --> 00:04:13,720
This is a brand new development environment that I can start working in,

93
00:04:14,680 --> 00:04:18,480
at least once it finishes booting up Linux and so forth.

94
00:04:21,440 --> 00:04:22,760
I'll leave that.

95
00:04:22,760 --> 00:04:23,280
Here we go.

96
00:04:23,280 --> 00:04:27,800
Codespaces, Jupyter, problems, output terminal.

97
00:04:31,920 --> 00:04:35,760
There's a animated GIF here that shows the sort of faster version of this

98
00:04:35,760 --> 00:04:38,160
for people who want to try this out themselves.

99
00:04:39,200 --> 00:04:40,720
Now I've got a cursor.

100
00:04:40,720 --> 00:04:41,640
And here we are.

101
00:04:41,720 --> 00:04:47,840
This right here is a full-blown Linux installation with Python

102
00:04:47,840 --> 00:04:49,720
and all of the bits and pieces that I'm going to need.

103
00:04:49,720 --> 00:04:52,480
And I'm going to bump that font size up a lot.

104
00:04:52,480 --> 00:04:53,800
I'll get rid of this.

105
00:04:55,240 --> 00:05:01,240
Let's do export PS1 equals dollar space.

106
00:05:01,240 --> 00:05:01,880
There we go.

107
00:05:01,880 --> 00:05:04,600
So now if I run Python 3, I've got Python 3.10,

108
00:05:04,600 --> 00:05:07,640
which is everything that I should need for the first section of this tutorial.

109
00:05:08,120 --> 00:05:13,120
So I did export PS1 equals and then a dollar in the space.

110
00:05:20,040 --> 00:05:24,240
And that just renames the start of my thing so that when I've got a big font,

111
00:05:24,240 --> 00:05:26,920
I don't have half the screen taken up with the directory path.

112
00:05:28,320 --> 00:05:29,760
So what happens if I run Python?

113
00:05:29,760 --> 00:05:31,800
Oh, just running Python gets me Python 3 as well.

114
00:05:32,200 --> 00:05:39,440
So we're going to start by talking a little bit about SQLite itself.

115
00:05:39,440 --> 00:05:43,080
Does anyone here use SQLite on a regular basis at the moment?

116
00:05:43,080 --> 00:05:43,800
A few people.

117
00:05:43,800 --> 00:05:45,680
I'm going to turn the rest of you into converts.

118
00:05:45,680 --> 00:05:47,360
That's my goal for the day.

119
00:05:47,360 --> 00:05:51,160
So SQLite is a fascinating little piece of technology.

120
00:05:51,160 --> 00:05:55,440
One of the, I think it's often not given the respect that it deserves.

121
00:05:55,440 --> 00:05:58,040
I think part of that's because the name suggests

122
00:05:58,040 --> 00:05:59,960
that it's a lightweight database.

123
00:06:00,000 --> 00:06:03,560
It turns out it's actually named after stalactites and stalagmites.

124
00:06:03,560 --> 00:06:08,760
The LITE wasn't the goal when D. Richard Hitt released the software

125
00:06:08,760 --> 00:06:10,240
in the first place.

126
00:06:10,240 --> 00:06:15,240
But the unique thing about SQLite is it's been around for at least 20 years now.

127
00:06:15,240 --> 00:06:16,960
And it's an embedded database.

128
00:06:16,960 --> 00:06:19,480
It's not like Postgres and MySQL run on service somewhere

129
00:06:19,480 --> 00:06:21,240
and you have to talk them over a network.

130
00:06:21,240 --> 00:06:25,320
SQLite is a C library that you import into your program.

131
00:06:25,320 --> 00:06:27,000
And any time you're talking to the database,

132
00:06:27,040 --> 00:06:31,080
you're actually just making C-based function calls to a file on disk,

133
00:06:31,080 --> 00:06:33,040
which means it's screamingly fast.

134
00:06:33,040 --> 00:06:38,160
Like it doesn't have any of the overhead of sending SQL queries back and forth.

135
00:06:38,160 --> 00:06:42,120
And it's surprisingly powerful in terms of what you can do with it.

136
00:06:42,120 --> 00:06:44,320
There we go.

137
00:06:44,320 --> 00:06:48,080
It's also been built into Python since 2006.

138
00:06:48,080 --> 00:06:51,480
Like this is, and anyone who has installed Python anywhere in the world

139
00:06:51,480 --> 00:06:55,360
actually has SQLite as a database just sat there waiting for them to import

140
00:06:55,360 --> 00:06:56,160
and start using it.

141
00:06:56,160 --> 00:07:01,280
So this is not a new technology in the world of Python by a very long way.

142
00:07:01,280 --> 00:07:02,200
It's not just Python.

143
00:07:02,200 --> 00:07:06,160
It's got bindings for Go and Ruby and Perl and any programming language

144
00:07:06,160 --> 00:07:08,280
you like can talk to SQLite.

145
00:07:08,280 --> 00:07:12,080
And it's also got really strong backwards compatibility to the point

146
00:07:12,080 --> 00:07:15,400
that the Library of Congress has announced that SQLite is one

147
00:07:15,400 --> 00:07:19,840
of the few structured data formats that they consider of archival quality

148
00:07:19,840 --> 00:07:21,160
for the work that they do.

149
00:07:21,160 --> 00:07:24,240
If you put data in a SQLite database, future versions

150
00:07:24,280 --> 00:07:27,960
of SQLite are effectively guaranteed to be able to open it again.

151
00:07:27,960 --> 00:07:32,680
So it's actually just a great default place to put data.

152
00:07:32,680 --> 00:07:36,480
My absolute favorite thing about it, a SQLite database is a file,

153
00:07:36,480 --> 00:07:37,800
like data.db.

154
00:07:37,800 --> 00:07:39,160
It's a file on disk.

155
00:07:39,160 --> 00:07:41,400
It's a single file which will work on any platform.

156
00:07:41,400 --> 00:07:46,280
You can send it over to an ARM machine or a 486 or somebody reported a bug

157
00:07:46,280 --> 00:07:49,120
in data set on some IBM mainframe that I've never even heard of,

158
00:07:49,120 --> 00:07:52,560
but it could run SQLite.

159
00:07:52,880 --> 00:07:57,960
But that single file can hold pretty much anything you could possibly

160
00:07:57,960 --> 00:07:59,240
want to stick in there.

161
00:07:59,240 --> 00:08:03,000
You can put in, it does text and binary data,

162
00:08:03,000 --> 00:08:04,520
integers and floating point numbers.

163
00:08:04,520 --> 00:08:06,640
It's got really good support for JSON.

164
00:08:06,640 --> 00:08:09,640
And the end result is that any data of any size,

165
00:08:09,640 --> 00:08:13,000
anything you could possibly imagine, you could fit it into that single file.

166
00:08:13,000 --> 00:08:16,640
It's almost like a zip file that you can run SQL queries against.

167
00:08:16,640 --> 00:08:19,560
So super, super powerful.

168
00:08:19,600 --> 00:08:21,880
Also, these days the maximum size

169
00:08:21,880 --> 00:08:24,880
of a SQLite database is 2.8 terabytes.

170
00:08:24,880 --> 00:08:28,360
About a year ago it was 1.4 terabytes and then one of the,

171
00:08:28,360 --> 00:08:30,960
someone in the world has a database that's too big for that

172
00:08:30,960 --> 00:08:34,080
and they reported it to the SQLite maintainers who doubled

173
00:08:34,080 --> 00:08:35,360
that maximum size.

174
00:08:35,360 --> 00:08:38,080
So now if you've got less than 2.8 terabytes of data

175
00:08:38,080 --> 00:08:39,800
and a disk big enough to put it, you can stick

176
00:08:39,800 --> 00:08:41,120
that in a SQLite database.

177
00:08:41,120 --> 00:08:44,680
And it also has some very interesting characteristics of its own.

178
00:08:44,680 --> 00:08:47,840
One of my favorite examples of that, there's this essay

179
00:08:47,880 --> 00:08:51,520
on the SQLite database, many small queries are efficient in SQLite,

180
00:08:51,520 --> 00:08:54,800
which points out that with MySQL and Postgres, if you're,

181
00:08:54,800 --> 00:08:57,280
if you've got a webpage with 200 SQL queries on it,

182
00:08:57,280 --> 00:09:00,080
you're in trouble because that's 200 round trips back and forth

183
00:09:00,080 --> 00:09:01,400
to your database server.

184
00:09:01,400 --> 00:09:05,560
With SQLite, 200 SQL queries is 200 function calls.

185
00:09:05,560 --> 00:09:08,320
So actually once you get used to that, you start using it

186
00:09:08,320 --> 00:09:10,800
in a very different way from other databases.

187
00:09:10,800 --> 00:09:14,440
It's okay to have n plus one queries loop through all 200 things

188
00:09:14,440 --> 00:09:15,960
and do an extra query for each one.

189
00:09:16,000 --> 00:09:18,840
That kind of thing just works, which is fascinating.

190
00:09:18,840 --> 00:09:21,680
I'm still, I've been using SQLite for most of my work

191
00:09:21,680 --> 00:09:23,440
for the past five years and I still feel

192
00:09:23,440 --> 00:09:26,080
like I'm not taking full advantage

193
00:09:26,080 --> 00:09:28,920
of this bizarre characteristic of that database.

194
00:09:28,920 --> 00:09:31,520
But let's actually start using it.

195
00:09:31,520 --> 00:09:35,680
So I'm going to suggest we start with an example database.

196
00:09:35,680 --> 00:09:39,400
If you run wget or curl or you click it in the browser

197
00:09:39,400 --> 00:09:42,640
to grab dataset.io slash content.db,

198
00:09:42,640 --> 00:09:44,440
that will give you the SQLite database

199
00:09:44,480 --> 00:09:46,960
that powers the official dataset website.

200
00:09:46,960 --> 00:09:50,560
So this website right here has news and plugins and all sorts

201
00:09:50,560 --> 00:09:51,880
of bits and pieces.

202
00:09:51,880 --> 00:09:53,200
Those are sat there in a SQLite database,

203
00:09:53,200 --> 00:09:56,120
which you can just download right now to start working with it.

204
00:09:56,120 --> 00:09:59,840
And so I'm going to do that in my code space.

205
00:09:59,840 --> 00:10:01,120
Where is it?

206
00:10:01,120 --> 00:10:02,440
There we go.

207
00:10:02,440 --> 00:10:07,160
Let's do wgetdataset.io slash content.db.

208
00:10:07,240 --> 00:10:08,080
Now.

209
00:10:17,000 --> 00:10:17,840
That's a great tip.

210
00:10:17,840 --> 00:10:21,200
Yeah, curl dash capital O will get you, hold on.

211
00:10:21,200 --> 00:10:22,480
I need to shrink my window

212
00:10:22,480 --> 00:10:24,640
so it's not behind my little sticking out.

213
00:10:24,640 --> 00:10:27,360
Curl dash O, will that work for,

214
00:10:27,360 --> 00:10:29,600
they said that without the HTTP, I wonder.

215
00:10:31,640 --> 00:10:32,480
Oops.

216
00:10:33,240 --> 00:10:37,240
I think it didn't work because I already had the file.

217
00:10:37,240 --> 00:10:39,320
But anyway, that will give you a SQLite database.

218
00:10:39,320 --> 00:10:42,760
It's, oh no, I overrode it.

219
00:10:42,760 --> 00:10:44,520
Hang on, let's do that one more time.

220
00:10:46,880 --> 00:10:49,520
Aha, if you do curl without the HTTPS,

221
00:10:49,520 --> 00:10:52,520
it downloads the HTTP redirect page, I think,

222
00:10:52,520 --> 00:10:54,240
which is blank.

223
00:10:55,120 --> 00:10:57,840
HTTPS curl slash slash.

224
00:11:00,080 --> 00:11:01,400
There we go.

225
00:11:01,400 --> 00:11:05,640
And that has given us a 17 megabyte file.

226
00:11:05,640 --> 00:11:09,360
So now I'm going to start playing with it in Python.

227
00:11:09,360 --> 00:11:10,720
We can fire it Python again.

228
00:11:11,840 --> 00:11:13,560
Import SQLite 3.

229
00:11:13,560 --> 00:11:16,240
That's straight out of the Python standard library.

230
00:11:16,240 --> 00:11:19,400
And then to connect to a database file,

231
00:11:19,400 --> 00:11:20,920
you just pass it as the path

232
00:11:20,920 --> 00:11:23,720
to the SQLite3.connect method.

233
00:11:23,720 --> 00:11:25,680
And this gives us a SQLite connection.

234
00:11:26,880 --> 00:11:29,840
Actually, I'm gonna go off script a moment.

235
00:11:29,840 --> 00:11:31,840
The first thing I always do with these things

236
00:11:31,840 --> 00:11:33,720
is I want to see what the tables are

237
00:11:33,720 --> 00:11:34,920
that are present in there.

238
00:11:34,920 --> 00:11:39,920
I can do that by saying select star from SQLite master,

239
00:11:41,600 --> 00:11:43,960
which is the metadata database

240
00:11:43,960 --> 00:11:46,240
that SQLite gives you for free.

241
00:11:46,240 --> 00:11:47,440
Fetch all.

242
00:11:48,920 --> 00:11:51,960
Actually, I think I probably want to do select name

243
00:11:53,840 --> 00:11:58,840
from SQLite master where type equals table.

244
00:12:00,040 --> 00:12:01,680
There we go.

245
00:12:01,680 --> 00:12:04,000
And that right there is a list of all of the tables

246
00:12:04,000 --> 00:12:05,200
in my database.

247
00:12:07,680 --> 00:12:09,160
Oh, we'll do this line here too.

248
00:12:09,160 --> 00:12:11,760
This is the SQLite version function.

249
00:12:11,760 --> 00:12:14,840
This demonstrates that SQLite has functions.

250
00:12:14,840 --> 00:12:17,320
We'll tell you what version of SQLite you're running.

251
00:12:17,320 --> 00:12:20,400
I'm on 3.31.1, which I think is pretty recent.

252
00:12:20,400 --> 00:12:23,000
That's within the last 12 months that release came out.

253
00:12:25,440 --> 00:12:28,520
When I'm doing here, when I call db.execute,

254
00:12:28,520 --> 00:12:29,720
I give it a SQL query.

255
00:12:29,720 --> 00:12:32,120
It gives me back a cursor object.

256
00:12:32,120 --> 00:12:33,680
One of the things you can do with the cursor object

257
00:12:33,680 --> 00:12:36,680
is called .fetch all, which then fetches a list

258
00:12:36,680 --> 00:12:37,920
of all of the rows from that.

259
00:12:37,920 --> 00:12:39,720
And then each of these lists,

260
00:12:39,720 --> 00:12:42,280
each of those rows comes back as a Python tuple.

261
00:12:42,280 --> 00:12:44,120
So I've got a Python tuple with the result

262
00:12:44,120 --> 00:12:47,840
of that single function call right there.

263
00:12:50,640 --> 00:12:52,160
And we can also do this.

264
00:12:52,160 --> 00:12:56,640
So I'm gonna do, the cursor object acts as an iterator.

265
00:12:56,640 --> 00:12:58,560
So you can loop through it directly

266
00:12:58,560 --> 00:13:00,520
and print out each of those rows.

267
00:13:00,520 --> 00:13:02,120
So let's do that now.

268
00:13:05,200 --> 00:13:06,040
There we go.

269
00:13:06,040 --> 00:13:08,160
And again, each of those came back as a tuple.

270
00:13:08,160 --> 00:13:10,480
The column, the database table I'm selecting here

271
00:13:10,480 --> 00:13:11,480
just has three columns.

272
00:13:11,480 --> 00:13:13,000
So we get three values back.

273
00:13:14,360 --> 00:13:17,000
Tupes aren't particularly readable

274
00:13:17,000 --> 00:13:18,760
if you're mucking around trying to figure out

275
00:13:18,760 --> 00:13:19,600
what's going on.

276
00:13:19,600 --> 00:13:23,800
So you can instead say db.row underscore factory equals,

277
00:13:23,800 --> 00:13:25,520
and then give it a factory function

278
00:13:25,520 --> 00:13:28,600
that we used to create the returned objects

279
00:13:28,600 --> 00:13:30,280
from those rows.

280
00:13:30,280 --> 00:13:32,720
SQLite3.row is a built-in mechanism

281
00:13:32,720 --> 00:13:35,040
that gives you a weird sort of half tuple,

282
00:13:35,040 --> 00:13:37,360
half dictionary object that you can work with.

283
00:13:37,360 --> 00:13:40,320
So if I put that in and then do this one again,

284
00:13:41,640 --> 00:13:43,640
oh, now I'm getting row objects,

285
00:13:43,640 --> 00:13:46,440
which that's not crazy useful,

286
00:13:46,440 --> 00:13:51,440
but you can call the dict function on those rows

287
00:13:51,640 --> 00:13:52,800
and get back dictionaries.

288
00:13:52,800 --> 00:13:54,680
So this is a very lightweight way

289
00:13:54,680 --> 00:13:56,480
to start turning those SQL results

290
00:13:56,480 --> 00:13:57,920
into lists of dictionaries,

291
00:13:57,920 --> 00:14:00,400
which you can then start working with really easily.

292
00:14:05,280 --> 00:14:07,080
Let's take this a step further

293
00:14:07,080 --> 00:14:10,920
and actually create a database table of our own.

294
00:14:10,920 --> 00:14:15,760
So SQLite's version of SQL is heavily inspired

295
00:14:15,760 --> 00:14:16,680
by Postgres.

296
00:14:16,680 --> 00:14:18,640
So if you're familiar with Postgres,

297
00:14:18,640 --> 00:14:20,320
a lot of the things you know from Postgres

298
00:14:20,320 --> 00:14:22,560
will work more or less the same in SQLite.

299
00:14:22,560 --> 00:14:24,680
One of the biggest differences with SQLite

300
00:14:24,680 --> 00:14:28,480
is it has a much more restricted set of column types,

301
00:14:28,480 --> 00:14:30,640
which I really love.

302
00:14:30,640 --> 00:14:31,880
When I'm working with databases,

303
00:14:31,880 --> 00:14:33,360
one of my least favorite bits

304
00:14:33,360 --> 00:14:34,320
is when you think to yourself,

305
00:14:34,320 --> 00:14:37,280
should I have the titles of varchar 100

306
00:14:37,280 --> 00:14:40,000
or maybe it needs to be a varchar 255?

307
00:14:40,000 --> 00:14:41,640
All of those kinds of questions.

308
00:14:41,640 --> 00:14:42,920
With SQLite, those just go away.

309
00:14:42,920 --> 00:14:46,000
You can say everything is going to be either,

310
00:14:46,000 --> 00:14:47,280
got a section down at the bottom here,

311
00:14:47,280 --> 00:14:50,360
it's either gonna be text or an integer or a wheel,

312
00:14:50,360 --> 00:14:51,840
which is a floating point number,

313
00:14:51,840 --> 00:14:54,120
or a blob for binary data.

314
00:14:54,120 --> 00:14:54,960
And that's the whole thing.

315
00:14:54,960 --> 00:14:58,440
So it's very, very easy to make decisions

316
00:14:58,440 --> 00:15:02,360
about how we're going to create those tables.

317
00:15:02,360 --> 00:15:04,000
So the table I'm gonna create right now

318
00:15:04,000 --> 00:15:07,200
is one to hold Python PEPs, P-E-P's,

319
00:15:07,200 --> 00:15:11,800
the documents that define how Python works

320
00:15:11,800 --> 00:15:14,160
and how the Python community operates.

321
00:15:14,160 --> 00:15:16,040
And I've just selected the subset.

322
00:15:16,040 --> 00:15:17,280
I'm gonna give them an ID,

323
00:15:17,280 --> 00:15:19,760
a title, author, status, type, created, and body.

324
00:15:19,760 --> 00:15:22,280
We'll see the data for those in a moment.

325
00:15:22,280 --> 00:15:24,520
So I can paste that into here.

326
00:15:25,880 --> 00:15:27,920
And that's created my table.

327
00:15:28,880 --> 00:15:32,920
And if I was to run that table thing from earlier on,

328
00:15:33,960 --> 00:15:35,960
this one here, oh, look at that.

329
00:15:35,960 --> 00:15:37,720
It's showing me rows now.

330
00:15:38,600 --> 00:15:41,760
Let's do dict r for r in.

331
00:15:43,240 --> 00:15:44,080
There we go.

332
00:15:44,080 --> 00:15:46,600
We now have a brand new table at the very end called PEPs.

333
00:15:48,080 --> 00:15:49,440
So now that we've created a table,

334
00:15:50,080 --> 00:15:51,560
we're going to drop some data into it.

335
00:15:51,560 --> 00:15:54,120
I decided it would be more interesting if,

336
00:15:54,120 --> 00:15:57,720
is there a way to hide this navigation, I wonder?

337
00:15:57,720 --> 00:15:59,200
Doesn't look like there is.

338
00:15:59,200 --> 00:16:00,040
I decided it would be more fun

339
00:16:00,040 --> 00:16:01,760
to actually work with real data for this.

340
00:16:01,760 --> 00:16:05,800
So I got GPT-4 to write me a quick function

341
00:16:05,800 --> 00:16:10,440
to turn a PEP into a Python dictionary.

342
00:16:10,440 --> 00:16:12,480
And this is perfect example of the kind of code

343
00:16:12,480 --> 00:16:14,480
that I could have written in two minutes,

344
00:16:14,480 --> 00:16:16,760
but GPT-4 wrote in 15 seconds.

345
00:16:16,760 --> 00:16:19,840
So it saved me a whole minute and a half of time.

346
00:16:20,800 --> 00:16:22,240
So I'm going to paste that function in.

347
00:16:22,240 --> 00:16:25,560
This is my parse underscore PEP function.

348
00:16:25,560 --> 00:16:28,400
And then I'm going to grab a copy

349
00:16:28,400 --> 00:16:30,640
of one of the greatest PEPs of all time,

350
00:16:30,640 --> 00:16:34,240
the Zen of Python by Tim Peters.

351
00:16:34,240 --> 00:16:37,720
Published in 2004, wow, it's nearly 20 years old now.

352
00:16:37,720 --> 00:16:42,720
That's, huh, have to have a birthday party for it next month.

353
00:16:44,120 --> 00:16:45,840
And as you can see, the format of this file

354
00:16:45,840 --> 00:16:48,480
is a bunch of key value pairs at the top,

355
00:16:48,480 --> 00:16:50,360
and then the rest of the file comes afterwards.

356
00:16:50,360 --> 00:16:52,960
So it's very easy to write a parser for it.

357
00:16:54,360 --> 00:16:57,800
I'm going to import URLlib and do this

358
00:16:57,800 --> 00:17:00,480
to get that PEP into a dictionary.

359
00:17:01,440 --> 00:17:06,440
Ooh, did I mean, what happened there?

360
00:17:06,440 --> 00:17:07,940
I'm going to do that.

361
00:17:13,800 --> 00:17:16,120
URLlib has no attribute request.

362
00:17:17,320 --> 00:17:18,520
That doesn't sound right.

363
00:17:19,600 --> 00:17:20,440
Okay, it worked.

364
00:17:20,440 --> 00:17:22,960
I had to change that to import URLlib.request.

365
00:17:24,080 --> 00:17:29,080
So now I've got the contents of that text file.

366
00:17:29,120 --> 00:17:31,200
I just printed out a summary version of it

367
00:17:31,200 --> 00:17:32,500
so that we could see that.

368
00:17:33,500 --> 00:17:37,500
And if I run pass PEP on the Zen,

369
00:17:39,060 --> 00:17:41,020
I've now got a Python dictionary

370
00:17:42,300 --> 00:17:45,940
with keys PEP, title, author, status, type, content type.

371
00:17:45,940 --> 00:17:50,460
So our next step is to insert that into our database.

372
00:17:50,460 --> 00:17:53,660
And this is, we will not be doing much more of this

373
00:17:53,660 --> 00:17:54,880
for the durations tutorial

374
00:17:54,880 --> 00:17:58,060
because my SQLite utils library puts a sort of layer

375
00:17:58,060 --> 00:18:00,260
of syntactic sugar over the top of this.

376
00:18:00,260 --> 00:18:02,460
But generally, when you're working with SQLite,

377
00:18:03,460 --> 00:18:04,380
the way you run queries like this is you have,

378
00:18:04,380 --> 00:18:06,860
the query is a string with placeholders

379
00:18:06,860 --> 00:18:09,340
for the values that you want to inject into that.

380
00:18:09,340 --> 00:18:10,780
And then you pass those values

381
00:18:10,780 --> 00:18:13,260
as a list or tuple afterwards.

382
00:18:13,260 --> 00:18:14,660
So in this case, we want to insert

383
00:18:14,660 --> 00:18:16,580
into the ID, title, author, status, type,

384
00:18:16,580 --> 00:18:18,820
created body columns, the values

385
00:18:18,820 --> 00:18:20,740
that we got back from that PEP.

386
00:18:20,740 --> 00:18:22,380
The reason we do it like this

387
00:18:22,380 --> 00:18:25,020
is to avoid SQL injection problems

388
00:18:25,020 --> 00:18:28,220
where if we tried to glue the string together

389
00:18:28,220 --> 00:18:30,860
with double quotes or single quotes,

390
00:18:31,220 --> 00:18:33,340
inside of those values might have interfered with that

391
00:18:33,340 --> 00:18:34,260
and broken our SQL.

392
00:18:34,260 --> 00:18:36,900
So you should always parameterize queries in this way

393
00:18:36,900 --> 00:18:39,540
anytime you're working with data from another source.

394
00:18:41,900 --> 00:18:43,900
So here we go, I'm gonna paste that in.

395
00:18:47,100 --> 00:18:51,860
And now we should have a PEP in our database.

396
00:18:54,140 --> 00:18:58,540
Let's run select star db.execute,

397
00:18:58,580 --> 00:19:02,580
select star from PEPs.fetchall.

398
00:19:03,940 --> 00:19:05,780
And we'll wrap that in a dict.

399
00:19:05,780 --> 00:19:07,780
And there it is, our PEP is now inside

400
00:19:07,780 --> 00:19:10,140
of our SQLite database, which is kind of cool.

401
00:19:10,140 --> 00:19:12,620
That was a pretty lightweight way

402
00:19:12,620 --> 00:19:15,860
of getting that data into a format where we can query it.

403
00:19:17,220 --> 00:19:19,980
I do want to demonstrate another,

404
00:19:19,980 --> 00:19:22,340
an alternative way of doing these parameters.

405
00:19:22,340 --> 00:19:23,980
Here we had to put question marks

406
00:19:23,980 --> 00:19:26,060
and then pass things in as a sequence.

407
00:19:26,060 --> 00:19:30,140
You can also use colon syntax for named parameters

408
00:19:30,140 --> 00:19:34,380
and then pass a dictionary directly to c.execute as well.

409
00:19:34,380 --> 00:19:36,580
So I'm going to delete that file.

410
00:19:36,580 --> 00:19:39,100
I'll delete the PEP that I just inserted,

411
00:19:39,100 --> 00:19:40,180
check that's been deleted.

412
00:19:40,180 --> 00:19:43,820
Yep, the SQL query there failed to retrieve it.

413
00:19:47,980 --> 00:19:49,860
So for triple, so the question is,

414
00:19:49,860 --> 00:19:51,900
when do you use triple quotes versus single?

415
00:19:51,900 --> 00:19:54,380
So triple quotes let you split your string

416
00:19:54,380 --> 00:19:55,340
over multiple lines.

417
00:19:55,340 --> 00:19:58,140
That's the only difference between triple quotes

418
00:19:58,140 --> 00:20:00,700
and single quotes is you get multi-line strings.

419
00:20:00,700 --> 00:20:03,380
And I find just SQL queries, it's more readable

420
00:20:03,380 --> 00:20:05,580
if you can see it spread across multiple lines

421
00:20:05,580 --> 00:20:07,340
than if it's just a single line.

422
00:20:10,940 --> 00:20:12,500
Okay, I'm going to run that again.

423
00:20:12,500 --> 00:20:15,540
And once more, we've inserted our, so we've now inserted

424
00:20:15,540 --> 00:20:18,100
that dictionary directly into that query

425
00:20:18,100 --> 00:20:20,340
and we've inserted it back into our table.

426
00:20:23,100 --> 00:20:24,380
The interesting, an interesting thing

427
00:20:24,380 --> 00:20:27,700
about SQL is that to be honest, there's very little

428
00:20:27,700 --> 00:20:28,740
that you need to know in order

429
00:20:28,740 --> 00:20:31,460
to use it productively.

430
00:20:31,460 --> 00:20:33,660
We've done create table, that's obviously important.

431
00:20:33,660 --> 00:20:35,740
You need to create tables to put your data in.

432
00:20:35,740 --> 00:20:37,660
We've seen what an insert looks like.

433
00:20:37,660 --> 00:20:39,100
There are two more things to know.

434
00:20:39,100 --> 00:20:40,700
There's updates and deletes.

435
00:20:40,700 --> 00:20:44,340
So an update is a way of updating an existing row.

436
00:20:44,340 --> 00:20:47,340
In this case, I'm going to think the row

437
00:20:47,340 --> 00:20:53,580
that we've got right now has Tim Peters as the author.

438
00:20:53,620 --> 00:20:55,940
Let's see, pet.keys.

439
00:20:59,380 --> 00:21:02,580
So yeah, it's got Tim Peters with his email address.

440
00:21:02,580 --> 00:21:06,580
I'm going to update that to remove the email address.

441
00:21:06,580 --> 00:21:08,820
I'll do that by pasting this in here.

442
00:21:10,300 --> 00:21:11,140
There we go.

443
00:21:11,140 --> 00:21:16,140
And now if I do db.execute, select author from paps.fetchall

444
00:21:20,340 --> 00:21:21,180
dict.

445
00:21:24,180 --> 00:21:27,300
And you can see that now I've updated the author

446
00:21:27,300 --> 00:21:31,140
to be Tim Peters instead of Tim Peters with the Gmail address.

447
00:21:31,140 --> 00:21:33,860
This example here does use one other trick.

448
00:21:33,860 --> 00:21:37,780
SQLite is a relational database that supports transactions.

449
00:21:37,780 --> 00:21:40,860
So you can bundle up changes in a block of code

450
00:21:40,860 --> 00:21:42,940
and then make them all available at once

451
00:21:42,940 --> 00:21:45,500
to other people who might be querying that database.

452
00:21:45,500 --> 00:21:48,380
And there's a nice little thing in the Python SQLite module

453
00:21:48,380 --> 00:21:52,380
where you can use the context manager and say with db colon

454
00:21:52,420 --> 00:21:54,620
and then anything indented inside of that block

455
00:21:54,620 --> 00:21:56,700
will be run inside the same transaction.

456
00:21:56,700 --> 00:21:58,700
So this is a good habit to get into

457
00:21:58,700 --> 00:22:00,700
when you're making modifications to a database

458
00:22:00,700 --> 00:22:03,500
because it means that you don't really have to think very hard

459
00:22:03,500 --> 00:22:05,140
about how transactions are working.

460
00:22:05,140 --> 00:22:06,780
You just sort of stick to this pattern

461
00:22:06,780 --> 00:22:08,700
and everything should work okay.

462
00:22:08,700 --> 00:22:10,260
The update there, I'm saying update paps

463
00:22:10,260 --> 00:22:13,980
set author equals blah where id equals blah

464
00:22:13,980 --> 00:22:15,900
and again, using those placeholders.

465
00:22:15,900 --> 00:22:19,460
So this ends up being update paps set author to Tim Peters

466
00:22:19,460 --> 00:22:21,180
where the id is 20.

467
00:22:22,820 --> 00:22:24,140
The width is not.

468
00:22:24,140 --> 00:22:27,140
No, the width there is the, that's the Python context manager.

469
00:22:27,140 --> 00:22:32,140
If you were using SQL, I think you'd do commit semicolon

470
00:22:32,140 --> 00:22:34,020
but to be honest, I've never done it in Python

471
00:22:34,020 --> 00:22:36,220
because I always use this mechanism instead.

472
00:22:36,220 --> 00:22:39,660
And then finally, to run deletes, very similar.

473
00:22:39,660 --> 00:22:43,300
Delete from paps where id equals question mark.

474
00:22:43,300 --> 00:22:45,460
So I'm going to delete row 20.

475
00:22:47,140 --> 00:22:50,460
And having done that, now if I run that query again,

476
00:22:50,660 --> 00:22:52,180
I don't get back any records.

477
00:22:52,180 --> 00:22:54,180
And if you want, you can delete with where clorys

478
00:22:54,180 --> 00:22:56,780
and delete everything and things like that as well.

479
00:22:56,780 --> 00:22:59,740
But this, what I find, what I like about this is

480
00:22:59,740 --> 00:23:06,740
that we've basically covered 90% of the data manipulation language

481
00:23:06,740 --> 00:23:08,420
aspect of SQL that we need to care about.

482
00:23:08,420 --> 00:23:10,940
You create tables, you insert rows into them,

483
00:23:10,940 --> 00:23:13,100
you update them, you delete them.

484
00:23:13,100 --> 00:23:16,420
Almost everything else you do will be in the much more rich

485
00:23:16,420 --> 00:23:18,780
and complicated area of writing selectively.

486
00:23:18,780 --> 00:23:23,580
And that's the bit that we'll be applying board game rules to

487
00:23:23,580 --> 00:23:26,100
and picking up bits and pieces of that as we go along.

488
00:23:26,100 --> 00:23:33,860
So this is because when you have question marks in your query,

489
00:23:33,860 --> 00:23:38,300
that's saying that you're going to pass a list of parameters to it.

490
00:23:38,300 --> 00:23:41,340
And in this case, normally, most of the time,

491
00:23:41,340 --> 00:23:42,780
we've used multiple question marks.

492
00:23:42,780 --> 00:23:45,580
So it makes sense to pass a list of multiple ones.

493
00:23:45,580 --> 00:23:46,860
You know what?

494
00:23:47,340 --> 00:23:49,300
This will work without being in a list.

495
00:23:49,300 --> 00:23:51,380
I've not actually tried.

496
00:23:51,380 --> 00:23:53,180
Let's see.

497
00:23:57,820 --> 00:24:00,060
It's purely out of habit.

498
00:24:00,060 --> 00:24:02,100
You absolutely could do that in this case

499
00:24:02,100 --> 00:24:04,620
where ID equals 20 would work fine.

500
00:24:04,620 --> 00:24:06,420
But then you're getting to a stage where, okay,

501
00:24:06,420 --> 00:24:10,740
do we start gluing strings together to construct that SQL query,

502
00:24:10,740 --> 00:24:13,140
which is a dangerous pattern because if you do

503
00:24:13,180 --> 00:24:19,020
that with non-integers, you might end up accidentally injecting bits

504
00:24:19,020 --> 00:24:21,660
of SQL you didn't mean to into your query.

505
00:24:21,660 --> 00:24:27,700
And I mentioned transactions earlier.

506
00:24:27,700 --> 00:24:31,020
We, oh, hold on.

507
00:24:31,020 --> 00:24:34,740
Oh, no, that is working.

508
00:24:34,740 --> 00:24:38,220
If you're interested in exploring transactions a little bit further,

509
00:24:38,220 --> 00:24:43,060
there's an exercise here where you can see how those transactions are only

510
00:24:43,060 --> 00:24:45,740
visible to other processes after they've been committed.

511
00:24:45,740 --> 00:24:48,140
I'm going to skip over that now because I think it's more fun

512
00:24:48,140 --> 00:24:51,380
to get onto the next section.

513
00:24:51,380 --> 00:24:54,700
So that's Python SQLite.

514
00:24:54,700 --> 00:24:56,500
Oh.

515
00:24:56,500 --> 00:25:04,380
Exactly. Yes.

516
00:25:04,380 --> 00:25:05,700
Yeah, oh, that's a really good point.

517
00:25:05,700 --> 00:25:09,060
Yeah. One of the interesting things about transactions is you can have

518
00:25:09,060 --> 00:25:11,660
multiple connections to a SQLite database

519
00:25:11,700 --> 00:25:16,300
from within the same Python program and changes in one will only be visible

520
00:25:16,300 --> 00:25:20,020
to the others after you've committed those transactions.

521
00:25:23,380 --> 00:25:25,780
Let's do something a whole lot more fun.

522
00:25:25,780 --> 00:25:30,820
I'm going to, I'm going to drop us out of regular Python and we're going

523
00:25:30,820 --> 00:25:32,540
to start using the tool that I've been building

524
00:25:32,540 --> 00:25:34,780
for the past few years called Dataset.

525
00:25:34,780 --> 00:25:38,300
So the way you install this is you run pip install Dataset.

526
00:25:38,300 --> 00:25:40,780
Or if you prefer pipx, you can use pipx.

527
00:25:40,820 --> 00:25:42,100
There's home group packages.

528
00:25:42,100 --> 00:25:44,420
There are many different ways that you can install it.

529
00:25:44,420 --> 00:25:48,020
Pip install Dataset will work just fine for the most part.

530
00:25:48,020 --> 00:25:54,180
And having done that, you get a new command called Dataset.

531
00:25:54,180 --> 00:25:56,260
Oh, hold on.

532
00:25:56,260 --> 00:26:00,220
Let's do Dataset version just to make sure that worked.

533
00:26:00,220 --> 00:26:01,540
There we go.

534
00:26:01,540 --> 00:26:04,660
So I've just installed Dataset version 0.64.2.

535
00:26:04,660 --> 00:26:08,780
So Dataset is a command line utility that starts a web server running

536
00:26:08,780 --> 00:26:10,580
against a SQLite database.

537
00:26:10,620 --> 00:26:14,300
In this case, we're going to say Dataset content.db and hit enter.

538
00:26:14,300 --> 00:26:19,220
And this will then start a web server running on port 8001.

539
00:26:19,220 --> 00:26:23,420
And in GitHub Codespaces, it has a feature where it spots when that happens

540
00:26:23,420 --> 00:26:26,500
and gives you the option to open that in a new tab.

541
00:26:26,500 --> 00:26:28,940
So this is one of the reasons I love GitHub Codespaces.

542
00:26:28,940 --> 00:26:31,260
It makes all of this stuff work.

543
00:26:31,260 --> 00:26:36,500
And so now, I click that URL and I've now got access to Dataset.

544
00:26:36,540 --> 00:26:40,460
Except, huh.

545
00:26:40,460 --> 00:26:41,940
I was expecting this to break, actually.

546
00:26:41,940 --> 00:26:45,620
I just remember that there's one extra step that I need to do.

547
00:26:45,620 --> 00:26:46,780
But we'll continue.

548
00:26:46,780 --> 00:26:50,580
And if it breaks, I'll fix it when that happens.

549
00:26:50,580 --> 00:26:53,740
So this right here is the Dataset web application.

550
00:26:53,740 --> 00:26:58,260
By default, it gives you an interface to explore any SQLite database

551
00:26:58,260 --> 00:26:59,900
that you care to give it.

552
00:26:59,900 --> 00:27:03,100
So this is our content database that we've been working with.

553
00:27:03,100 --> 00:27:05,540
And whereas previously, we were running code inside of Python

554
00:27:05,580 --> 00:27:09,140
and sort of squinting at lists of strings, now we can scroll down

555
00:27:09,140 --> 00:27:12,060
and see all of the tables that are in that database.

556
00:27:12,060 --> 00:27:14,860
Oh, there's our Peps table, except it hasn't got any data in it.

557
00:27:14,860 --> 00:27:18,740
That's annoying because I deleted the data and forgot to put it back.

558
00:27:18,740 --> 00:27:22,540
So I'm going to pop back to Codespaces.

559
00:27:22,540 --> 00:27:25,380
I'm going to run Python again.

560
00:27:25,380 --> 00:27:30,660
Oh, this is frustrating.

561
00:27:30,660 --> 00:27:33,500
Drop in my past Pep thing.

562
00:27:36,500 --> 00:27:37,500
Go.

563
00:27:39,420 --> 00:27:41,500
Import urllib.request.

564
00:27:44,780 --> 00:27:46,700
Import urllib.request.

565
00:27:48,820 --> 00:27:52,580
db equal import SQLite3.

566
00:27:54,700 --> 00:27:57,260
3.connect.

567
00:27:57,260 --> 00:28:00,540
What was it called? Content.db.

568
00:28:00,540 --> 00:28:03,980
And now I can run this thing here,

569
00:28:04,460 --> 00:28:06,380
and we should be back to where we started.

570
00:28:08,700 --> 00:28:09,700
There we go.

571
00:28:09,700 --> 00:28:10,940
OK.

572
00:28:10,940 --> 00:28:14,340
So now if I run data on content.db again,

573
00:28:15,820 --> 00:28:21,100
this time I forgot to commit.

574
00:28:22,580 --> 00:28:24,460
I forgot to commit my change to the database.

575
00:28:25,620 --> 00:28:27,340
You know what? I'm going to leave that.

576
00:28:27,340 --> 00:28:30,340
And if you didn't delete the data, then you should have that row

577
00:28:30,340 --> 00:28:32,980
in your local database as well.

578
00:28:33,020 --> 00:28:34,780
But I've got all of these other tables.

579
00:28:34,780 --> 00:28:38,780
So these right here are the tables that power the official data set website.

580
00:28:39,500 --> 00:28:44,140
One of the most interesting tables in there is one called repos,

581
00:28:44,460 --> 00:28:46,140
which is all of the...

582
00:28:46,140 --> 00:28:47,140
Aha.

583
00:28:47,980 --> 00:28:52,740
So the reason I got that error is I forgot to install one additional plugin.

584
00:28:52,740 --> 00:28:56,260
This should be in the documentation.

585
00:28:58,700 --> 00:29:01,500
Yes, in GitHub Codespaces, there's an additional plugin

586
00:29:01,500 --> 00:29:04,620
you need to install to make sure you avoid those kinds of errors.

587
00:29:05,740 --> 00:29:08,980
That's this one here, data set install, data set-codespaces.

588
00:29:10,060 --> 00:29:12,060
And having done that, now...

589
00:29:14,140 --> 00:29:15,660
This should...

590
00:29:18,180 --> 00:29:19,180
Here we go.

591
00:29:20,300 --> 00:29:24,540
So this right here is a database of all of the GitHub repositories

592
00:29:24,540 --> 00:29:26,580
that form the overall data set project.

593
00:29:27,340 --> 00:29:31,540
And it's 158 because there's a lot of different plugins and tools

594
00:29:31,540 --> 00:29:33,060
and associated things like this.

595
00:29:33,660 --> 00:29:36,540
The data in here is actually created by another tool I wrote called

596
00:29:36,540 --> 00:29:41,420
GitHub to SQLite, which can import data from your GitHub account

597
00:29:41,660 --> 00:29:43,660
and use it to populate a SQLite database.

598
00:29:43,980 --> 00:29:46,140
And this is a pattern that I use a lot.

599
00:29:47,580 --> 00:29:50,300
If you go to the data set website and look in the tools database,

600
00:29:50,620 --> 00:29:54,700
you will find things like Swarm to SQLite and Apple Notes to SQLite

601
00:29:54,700 --> 00:29:55,980
and GitHub to SQLite.

602
00:29:56,340 --> 00:30:00,140
Because once you realize how flexible SQLite is,

603
00:30:00,340 --> 00:30:02,540
you kind of want everything in your life to be in it

604
00:30:02,700 --> 00:30:05,780
so that you can run queries against it and join it against things

605
00:30:05,780 --> 00:30:07,540
and run data set against it.

606
00:30:07,740 --> 00:30:11,140
So I've spent the last few years accumulating all sorts of weird

607
00:30:11,140 --> 00:30:15,420
little tools that will take things from some API or some source

608
00:30:15,420 --> 00:30:18,020
of information and stick them in a SQLite database

609
00:30:18,140 --> 00:30:21,780
so I can then start using them with data set and other tools like that.

610
00:30:22,300 --> 00:30:25,340
I've actually got one in here called Genome to SQLite,

611
00:30:25,340 --> 00:30:28,740
which can import a copy of your genome into a SQL database,

612
00:30:29,220 --> 00:30:31,980
which means I know what color my eyes are with a SQL query,

613
00:30:31,980 --> 00:30:34,340
which is super useful, really exciting.

614
00:30:34,980 --> 00:30:41,340
But yeah, so we're going to grab another database for the next section.

615
00:30:41,540 --> 00:30:42,540
I'm going to...

616
00:30:42,540 --> 00:30:43,540
Uh-huh?

617
00:30:43,540 --> 00:30:54,900
So I don't have a route to go from here to Postgres yet,

618
00:30:55,060 --> 00:30:56,660
but I have a route to go in the other direction.

619
00:30:56,660 --> 00:30:59,940
So I have a tool I wrote called db2sqlite,

620
00:31:00,220 --> 00:31:04,940
which uses SQL Alchemy and could be pointed at a Postgres or MySQL

621
00:31:04,940 --> 00:31:09,860
database and will create a SQLite database as a backup of those databases.

622
00:31:10,140 --> 00:31:12,940
And I've got a demo I'll show you later on where I've done exactly that

623
00:31:12,940 --> 00:31:16,340
with a Postgres database that I do a lot of work with.

624
00:31:18,340 --> 00:31:20,300
So we're going to grab one more database.

625
00:31:20,340 --> 00:31:22,260
We're going to grab this one here.

626
00:31:22,260 --> 00:31:26,740
This is a database of congressional legislators.

627
00:31:26,780 --> 00:31:31,860
It's people who are Congress people or senators or presidents

628
00:31:31,860 --> 00:31:32,860
and vice presidents.

629
00:31:33,140 --> 00:31:36,500
It's this wonderful open source project where people have been gathering

630
00:31:36,940 --> 00:31:41,660
information on how Congress has been structured right back to 1793, I think.

631
00:31:42,020 --> 00:31:45,620
So if you're building demos for database software, this is absolute...

632
00:31:45,660 --> 00:31:46,660
This is absolute...

633
00:31:46,660 --> 00:31:47,900
An absolute gold mine.

634
00:31:48,500 --> 00:31:53,620
And what we're going to do again is we're going to grab a link to that database.

635
00:31:54,100 --> 00:31:56,340
This one is just 5.2 megabytes.

636
00:31:57,780 --> 00:32:00,180
And you know what?

637
00:32:00,180 --> 00:32:02,780
I'm going to open up an additional terminal so I can leave data

638
00:32:02,780 --> 00:32:03,780
set running.

639
00:32:04,500 --> 00:32:06,060
Export PS1.

640
00:32:08,740 --> 00:32:09,740
There we go.

641
00:32:12,580 --> 00:32:13,740
So I'm going to grab that as well.

642
00:32:14,780 --> 00:32:22,100
And then I'm going to restart data set to run against my new legislators database.

643
00:32:23,260 --> 00:32:27,820
So having done that, I've now got...

644
00:32:30,780 --> 00:32:32,580
Oh, what happened there?

645
00:32:34,340 --> 00:32:35,340
Surprising.

646
00:32:41,660 --> 00:32:42,660
Here we go.

647
00:32:42,660 --> 00:32:45,820
So I've now got a content database and another one called legislators.

648
00:32:46,140 --> 00:32:50,820
And that's got tables for legislators and executives, that's presidents and vice

649
00:32:50,820 --> 00:32:56,500
presidents, and when they were in office and their names and their birthdays and

650
00:32:56,500 --> 00:32:57,820
all sorts of bits and pieces like that.

651
00:32:57,820 --> 00:33:02,700
It's a really fun example database for playing around and learning things like

652
00:33:02,700 --> 00:33:03,700
SQL.

653
00:33:05,900 --> 00:33:08,420
So in a moment, we'll...

654
00:33:08,860 --> 00:33:09,860
Yeah, that's a good one.

655
00:33:09,860 --> 00:33:16,300
So for the next step, if there's a link from this document here to one of the

656
00:33:16,300 --> 00:33:21,620
official data set tutorials, which effectively introduces data set in the

657
00:33:21,620 --> 00:33:25,740
data set interface in terms of this congressional legislators data.

658
00:33:26,820 --> 00:33:29,100
You're welcome to run this on your own copy of the database.

659
00:33:29,100 --> 00:33:33,940
You can also go straight to the hosted copy of the database here as well if you

660
00:33:34,060 --> 00:33:35,700
haven't managed to download that file.

661
00:33:36,260 --> 00:33:43,140
And this will take you through a whole bunch of useful features of data set

662
00:33:43,140 --> 00:33:44,940
itself. So I'll review that right now.

663
00:33:46,460 --> 00:33:51,980
One of those features is that data set will give you a table of data and you

664
00:33:51,980 --> 00:33:56,220
can do things like sort that by district or...

665
00:33:57,860 --> 00:34:00,780
Actually, it probably makes more sense to sort by start date.

666
00:34:00,980 --> 00:34:03,780
So I can sort start date, sending.

667
00:34:04,100 --> 00:34:08,260
I can see that in this database here, we've got 44,000 legislative terms.

668
00:34:08,420 --> 00:34:14,020
The earliest was 1789 when the anti-administration party had...

669
00:34:14,660 --> 00:34:16,500
That's a fun first party for America.

670
00:34:16,500 --> 00:34:17,500
That feels...

671
00:34:18,620 --> 00:34:20,500
Oh, there's the pro-administration party as well.

672
00:34:21,340 --> 00:34:23,100
So Richard Bassett right here.

673
00:34:23,380 --> 00:34:27,540
Now, you'll notice that this is a table, but that right there is a hyperlink.

674
00:34:27,580 --> 00:34:28,660
The Richard Bassett...

675
00:34:29,060 --> 00:34:31,900
Richard Bassett, there is a link through to another page.

676
00:34:32,260 --> 00:34:38,500
And this is using a SQLite feature of foreign keys, which are defined

677
00:34:39,620 --> 00:34:43,660
in this... When you create a table, you can say that a column references

678
00:34:43,660 --> 00:34:45,460
another column on another table.

679
00:34:45,740 --> 00:34:49,700
Data set notices that and it turns those things into hyperlinks.

680
00:34:49,980 --> 00:34:53,980
So this is getting in the sort of relational part of relational databases

681
00:34:53,980 --> 00:34:55,940
where data can link through to other data.

682
00:34:58,060 --> 00:35:01,420
Let's see if we sort the other way.

683
00:35:01,980 --> 00:35:04,380
I'm trying to find a legislator who's a bit more interesting.

684
00:35:07,020 --> 00:35:09,140
Wow, these are all one term legislator. Here we go.

685
00:35:09,140 --> 00:35:13,820
So this one here, Jake Alayze, is actually referenced from two other tables.

686
00:35:13,860 --> 00:35:16,540
They're referenced from legislator terms and they're also referenced

687
00:35:16,540 --> 00:35:20,020
from a social media table because we've got a table of data here

688
00:35:20,020 --> 00:35:21,700
that tells us what their Twitter account is.

689
00:35:23,260 --> 00:35:26,100
But this is a very important concept in databases in general

690
00:35:26,300 --> 00:35:29,380
and in data sets specifically, the idea to link these records together

691
00:35:29,660 --> 00:35:32,980
so that you end up with more of a sort of graph of data that you can start

692
00:35:33,220 --> 00:35:35,860
traversing between and really understanding how these things are all

693
00:35:35,860 --> 00:35:36,860
related to each other.

694
00:35:39,700 --> 00:35:43,140
My absolute favorite feature of data set is something called faceting.

695
00:35:44,020 --> 00:35:49,180
And that gives... That's the ability to spot columns like the type column

696
00:35:49,180 --> 00:35:51,340
and the state column and say, you know what?

697
00:35:52,060 --> 00:35:56,140
Of these 40... We've got 44,000 rows here, but it's kind of interesting

698
00:35:56,140 --> 00:35:58,860
to count them based on the value in a specific column.

699
00:35:59,100 --> 00:36:03,420
So if we look at states, we'll see that New York has had 4,189

700
00:36:03,420 --> 00:36:06,220
of these legislative terms and is the... Presumably because it's one

701
00:36:06,220 --> 00:36:08,900
of the oldest states. It's the state that's had the...

702
00:36:09,460 --> 00:36:12,140
Well, also it's going to be the number of Congress people they get

703
00:36:12,140 --> 00:36:14,140
in their... For their state, isn't it?

704
00:36:14,780 --> 00:36:17,780
We can look at like representative versus senator.

705
00:36:17,980 --> 00:36:19,500
So then I can drill down and say, you know what?

706
00:36:19,500 --> 00:36:23,260
Just show me the senators from Massachusetts.

707
00:36:25,140 --> 00:36:26,220
And then I can say, you know what?

708
00:36:26,260 --> 00:36:29,900
Let's facet by party as well and see that the state of Massachusetts

709
00:36:30,300 --> 00:36:34,260
in the Senate has had 38 terms served by Republicans, 32 by Democrats,

710
00:36:34,460 --> 00:36:38,980
11 by Whigs. That pro-administration party from other one shows up.

711
00:36:39,260 --> 00:36:43,740
But the key thing about this is that when you're faced with 44,000 rows

712
00:36:43,740 --> 00:36:48,060
of data, it can be kind of difficult to know what to do next.

713
00:36:48,060 --> 00:36:50,500
You know, okay, great. I've got 44,000 rows.

714
00:36:50,860 --> 00:36:53,740
My favorite thing to do with it is to start digging into it using

715
00:36:53,740 --> 00:36:56,700
these facets and saying, okay, well, what can we learn about the data

716
00:36:56,700 --> 00:37:00,340
at the macro level based on slicing and dicing it in different ways?

717
00:37:00,980 --> 00:37:03,180
That's exactly what I've done here. I've just found out something about

718
00:37:03,180 --> 00:37:06,900
the state of Massachusetts Senate population based on clicking a few

719
00:37:06,900 --> 00:37:09,860
different lengths. You'll notice there's...

720
00:37:10,580 --> 00:37:15,220
This interface at the top here is pretty much exactly what you would

721
00:37:15,220 --> 00:37:18,620
expect it to be. This is a query building interface.

722
00:37:18,940 --> 00:37:21,700
So you can select a column like state and say you want it where state

723
00:37:21,740 --> 00:37:26,380
equals MA, where type equals Sen. You know what? I'm going to say where...

724
00:37:26,860 --> 00:37:30,860
Is there a name in here? I'm going to say where party

725
00:37:34,340 --> 00:37:35,340
contains...

726
00:37:37,060 --> 00:37:38,060
Let's do Rep.

727
00:37:39,940 --> 00:37:43,260
And that's given us back party, the Republican Party, the Democratic

728
00:37:43,260 --> 00:37:46,740
Republican Party, the Independent Republican Democrat Party, and the Liberal

729
00:37:47,020 --> 00:37:50,260
Republican Party... And the Liberal Republican Party as well.

730
00:37:50,620 --> 00:37:53,740
So you can use this interface to build up pretty complicated filters

731
00:37:53,740 --> 00:37:54,740
onto your data.

732
00:37:58,540 --> 00:37:59,540
Does it... What's that?

733
00:38:02,860 --> 00:38:07,340
Yes, it runs the full query. So if you scroll down, it's showing you

734
00:38:07,620 --> 00:38:11,700
all of the rows that match those criteria. And then if it was on multiple

735
00:38:11,700 --> 00:38:15,180
pages, which this one isn't, so let's get rid of that.

736
00:38:18,300 --> 00:38:19,300
The query builder...

737
00:38:20,260 --> 00:38:21,260
No.

738
00:38:24,820 --> 00:38:28,180
It doesn't... It would be useful if it could autocomplete things.

739
00:38:28,180 --> 00:38:30,300
That's not a feature that I've figured out yet.

740
00:38:33,260 --> 00:38:37,300
Here's where things get really fun. You can export this data back out again

741
00:38:37,300 --> 00:38:43,220
in multiple different formats. So if you want a CSV file of the Republican

742
00:38:43,220 --> 00:38:47,460
Party candidates from the state of Massachusetts, you can click CSV.

743
00:38:48,020 --> 00:38:52,180
And there's that data with CSV ready to be loaded into Excel or

744
00:38:53,100 --> 00:38:56,340
Tableau or things like that. You can also get it back out as JSON.

745
00:38:57,380 --> 00:39:00,940
This right here is that data that we were just looking at as a...

746
00:39:00,940 --> 00:39:03,380
And in fact, I think if I do raw data and then pretty print,

747
00:39:03,380 --> 00:39:08,300
you can see this is JSON data here, which is specifically set up so that

748
00:39:08,300 --> 00:39:12,580
you can pull it into JavaScript or Python applications running elsewhere.

749
00:39:12,940 --> 00:39:16,580
So you've effectively... This interface here is actually an API.

750
00:39:16,620 --> 00:39:19,420
In fact, everything in data set works like that. If you can see it

751
00:39:19,420 --> 00:39:22,060
in data set, you can get it out as JSON as well.

752
00:39:23,620 --> 00:39:25,740
And so when you start looking at the URL, you realize that,

753
00:39:25,740 --> 00:39:31,340
okay, now we've got an API where we can specify in that URL that we want

754
00:39:31,340 --> 00:39:34,940
people from California, for example. And so now if I'm hitting that from

755
00:39:34,940 --> 00:39:38,140
JSON, I've got an API here where I can say, you know what?

756
00:39:38,140 --> 00:39:42,740
Instead of California, let's do Florida. And now I'm getting things back

757
00:39:42,780 --> 00:39:50,220
from Florida. So this is effectively a very powerful API interface onto

758
00:39:50,220 --> 00:39:53,660
anything that you can fit in a SQLite database, which as we discussed

759
00:39:53,660 --> 00:39:56,740
earlier is everything. There was nothing that you could not stick in

760
00:39:56,740 --> 00:39:59,700
a SQLite database, put up online with this interface on top,

761
00:39:59,700 --> 00:40:02,460
and then start doing API automations against it.

762
00:40:05,220 --> 00:40:08,140
So we've talked about facets and we've talked about filters.

763
00:40:12,740 --> 00:40:20,460
So at the moment, the only way to get data into data set is to have it

764
00:40:20,460 --> 00:40:24,500
as a SQLite database. So if your data is in some other form,

765
00:40:24,500 --> 00:40:27,620
you'll need to write tools to get it into SQLite. If it's in Postgres

766
00:40:27,620 --> 00:40:31,540
or MySQL, you'll have to export it out as a SQLite database file.

767
00:40:31,540 --> 00:40:35,820
I've been experimenting with ways of getting data set to talk to other

768
00:40:35,820 --> 00:40:39,140
databases directly. There's actually someone in the data set community

769
00:40:39,180 --> 00:40:43,660
built a DuckDB plugin, which Monkey patched it around so that you could

770
00:40:43,660 --> 00:40:47,820
run it against DuckDB as well. I haven't committed to this as a feature

771
00:40:47,820 --> 00:40:51,700
yet because building software that speaks to multiple databases slows

772
00:40:51,700 --> 00:40:55,460
you down a lot, you know, in terms of you have to test it against all

773
00:40:55,460 --> 00:40:58,740
of the databases and write features twice and so forth. But I'm thinking

774
00:40:58,740 --> 00:41:03,220
it might happen maybe for data set 2.0, which I hope to work on in the next

775
00:41:03,220 --> 00:41:04,460
sort of 12 to 18 months.

776
00:41:09,140 --> 00:41:27,220
Well, we'll get into that in a little bit. For more advanced stuff,

777
00:41:27,220 --> 00:41:30,540
you actually end up writing your own SQL queries directly in the data set

778
00:41:30,540 --> 00:41:34,260
interface. And I'll jump into it. You know, we can jump into that right now.

779
00:41:34,260 --> 00:41:37,260
This is quite a good point for that. So right here, we're looking at

780
00:41:38,100 --> 00:41:41,300
954 rows where the party contains rep and the state equals CA.

781
00:41:41,300 --> 00:41:46,540
And if I click view and edit SQL down here, it will switch me into an

782
00:41:46,540 --> 00:41:50,540
interface where I can see that underlying SQL query. Let's click format

783
00:41:50,540 --> 00:41:54,900
SQL to tie that up a little bit as well. So now it's exactly the same data

784
00:41:54,900 --> 00:42:00,180
coming back, but we're now in a situation where we can start editing that

785
00:42:00,180 --> 00:42:02,780
SQL query. And so I can actually make changes to it. I can say,

786
00:42:02,820 --> 00:42:08,580
you know what, I don't care about those columns or that one or those.

787
00:42:08,580 --> 00:42:13,580
I just want those ones right here. And click run. And now I get back

788
00:42:13,580 --> 00:42:18,060
that subset of columns that I've defined in that SQL query. And I can then say,

789
00:42:18,060 --> 00:42:22,620
you know what, let's do, instead of rep, we'll do dem. The percentage symbols

790
00:42:22,620 --> 00:42:27,100
there are because we're using a like operator. And the like operator does

791
00:42:27,100 --> 00:42:31,940
wildcard matching. So here we're saying wildcard match where the party column

792
00:42:31,980 --> 00:42:35,580
contains any number of characters, dem any other number of characters at the

793
00:42:35,580 --> 00:42:44,140
end. Why are these ends? Oh, that's because end, I believe, is a reserved

794
00:42:44,140 --> 00:42:50,140
word in SQLite. So if you take the square brackets, oh, okay, I was expecting

795
00:42:50,140 --> 00:42:54,780
that to break. I think if the column was called select, it would definitely

796
00:42:54,780 --> 00:42:58,620
break. So data set plays it cautious. And if it thinks something's a reserved

797
00:42:58,620 --> 00:43:02,740
word, it escapes it. And actually, you can escape it using double quotes

798
00:43:02,740 --> 00:43:06,140
or you can escape it using square brackets. I've been a bit inconsistent

799
00:43:06,140 --> 00:43:09,060
in using both of them. I should probably just stick to the double quotes

800
00:43:09,060 --> 00:43:10,060
in that case.

801
00:43:17,740 --> 00:43:22,340
Yes, which is just messy. That's unnecessary. In this particular case,

802
00:43:22,340 --> 00:43:24,700
I could get rid of these and it would work just fine.

803
00:43:28,660 --> 00:43:35,300
So that there is because this is the named parameter trick that we used

804
00:43:35,300 --> 00:43:39,300
earlier on with the insert query. What we're saying here is that I want

805
00:43:39,300 --> 00:43:42,500
party to be like something called P0 and I can actually change that to be

806
00:43:42,500 --> 00:43:49,060
party. And I want state to be like colon state. And data set then notices

807
00:43:49,060 --> 00:43:52,140
that those are there and it adds them as form fields at the bottom.

808
00:43:53,100 --> 00:43:57,580
So a trick that you can do with this, which is kind of fun, is that this

809
00:43:57,580 --> 00:44:01,100
actually becomes a way of building little custom applications for people.

810
00:44:01,100 --> 00:44:04,500
There's a hide button at the top that hides the SQL query. And so now,

811
00:44:04,500 --> 00:44:08,820
if you've got a friend who desperately wants to be able to look at...

812
00:44:08,820 --> 00:44:14,420
And let's get rid of this one here. So just where state equals state.

813
00:44:15,340 --> 00:44:18,940
So if you have a friend who desperately wants lists of legislators from the

814
00:44:18,940 --> 00:44:22,980
state of cap from any state they like, you can send them this bookmark,

815
00:44:22,980 --> 00:44:26,220
like literally copy and paste that URL, send it to somebody else and when

816
00:44:26,260 --> 00:44:29,540
they land on it, they'll get this page here with a form field where they

817
00:44:29,540 --> 00:44:34,820
can change CA to FL and to run the query again. I kind of think of this

818
00:44:34,820 --> 00:44:40,060
as a way of building entire interactive database applications as a bookmark.

819
00:44:40,060 --> 00:44:42,620
And so it's something you can share on Slack or you can send somebody

820
00:44:42,620 --> 00:44:46,900
a message or email. And it's a really fun and powerful way of working

821
00:44:46,900 --> 00:44:51,500
with data, especially in a collaborative environment. Data set, the initial

822
00:44:51,500 --> 00:44:55,300
inspiration for this was data journalism. I was working for newspapers where

823
00:44:55,340 --> 00:44:58,340
we were dealing with all kinds of weird and interesting shapes of data.

824
00:44:59,860 --> 00:45:03,180
And you want to make that available to the reporters in the newspaper

825
00:45:03,460 --> 00:45:07,380
so that they can look at the latest COVID figures or look up what

826
00:45:07,380 --> 00:45:11,380
some senator has been doing, things like that. And so the idea here was

827
00:45:11,380 --> 00:45:15,740
to have a few people in the newsroom who know enough to load data

828
00:45:15,740 --> 00:45:19,980
into this system and sort of construct these secret queries. And then everyone

829
00:45:19,980 --> 00:45:22,380
else just walks off a bookmarks. If somebody says, hey, have you got

830
00:45:22,420 --> 00:45:25,540
the latest COVID numbers for New York? You can send them a link.

831
00:45:25,740 --> 00:45:28,620
And when they click that link, they will get that data come back at them

832
00:45:28,620 --> 00:45:33,260
in this format. As before, as with the table view, you can get the stuff

833
00:45:33,260 --> 00:45:38,180
out of CSV and you can also get the stuff out as JSON. So what we've

834
00:45:38,180 --> 00:45:42,820
got here, which is kind of fascinating, is we've got a...

835
00:45:44,060 --> 00:45:47,700
We have an API. Where's the wrap option?

836
00:45:53,140 --> 00:45:55,500
I totally messed that up, haven't I?

837
00:45:58,220 --> 00:46:03,660
Here we go. This right here is a JSON API that takes an argument

838
00:46:03,660 --> 00:46:09,300
of SQL equals and then takes a SQL query. So effectively, we've built

839
00:46:09,300 --> 00:46:14,780
a JSON API that a JavaScript application can use to run SQL queries directly

840
00:46:14,780 --> 00:46:19,020
against a database, which... Oh, that's fun. Copilot's trying to suggest what...

841
00:46:19,780 --> 00:46:24,340
Wow. GitHub Copilot just guessed that the response of that would be

842
00:46:26,260 --> 00:46:29,460
the Federalist Party in Florida. That's quite a surprising completion it pulled

843
00:46:29,460 --> 00:46:35,620
off there. But yes, so we've built an API that you can send SQL through,

844
00:46:35,620 --> 00:46:39,860
which for most developers will tell you that this sounds like an appallingly

845
00:46:39,860 --> 00:46:45,260
bad idea. In web application security, there's a concept of a SQL injection

846
00:46:45,300 --> 00:46:49,340
vulnerability where people can use tricks to run their own SQL against your

847
00:46:49,340 --> 00:46:53,260
database. And normally, you don't want that at all. But Dataset was specifically

848
00:46:53,260 --> 00:46:57,900
designed for a world in which you're publishing entire databases. So you don't

849
00:46:57,900 --> 00:47:00,540
mind if people can query any of it because that was the whole point.

850
00:47:00,540 --> 00:47:05,180
You wanted to put that data out there. And everything... The SQL queries are run

851
00:47:05,180 --> 00:47:09,460
against read-only databases. They can't cause any damage to them. And there's

852
00:47:09,460 --> 00:47:13,580
also a one second time limit that's enforced on those queries, which means

853
00:47:14,060 --> 00:47:20,980
despite this being a wildly unexpected thing to do, having an endpoint that

854
00:47:20,980 --> 00:47:24,340
people can just run their own SQL queries against is both safe and really

855
00:47:24,340 --> 00:47:29,460
flexible. I started building... I built some applications when I worked at

856
00:47:29,460 --> 00:47:33,700
Eventbrite a few years ago. I built an application that used this in JavaScript

857
00:47:33,700 --> 00:47:36,940
basically as a joke. I was trying to wind up the other developers at the

858
00:47:36,940 --> 00:47:40,940
company by saying, hey, look, I used JavaScript to glue together a SQL query.

859
00:47:40,980 --> 00:47:43,740
Only it turned out it was just a really productive way of working,

860
00:47:43,740 --> 00:47:47,660
especially for rapid prototyping. So I've ended up building all sorts of software

861
00:47:47,660 --> 00:47:51,940
on top of this since then. And I'll show you a much more advanced example of

862
00:47:51,940 --> 00:47:58,580
that in a moment. But yeah, so that's the core of the Dataset web application.

863
00:47:58,580 --> 00:48:02,380
It's got tables in it. You can browse those tables. You can filter them and

864
00:48:02,380 --> 00:48:05,820
facet them and explore them in that way. You can get the data back out as

865
00:48:05,820 --> 00:48:10,900
JSON or CSV. And actually, this one here is running a plugin called Copyable,

866
00:48:11,380 --> 00:48:15,020
which lets you get the data back in all sorts of other formats too.

867
00:48:15,020 --> 00:48:19,260
If you're writing an academic paper and you need a LaTeX table of all

868
00:48:19,260 --> 00:48:23,140
of the... What is this? The Florida representatives since 1821,

869
00:48:23,940 --> 00:48:26,620
you can click that. You can click the LaTeX button and copy and paste

870
00:48:26,620 --> 00:48:29,580
this right back out again. Actually, I use the GitHub one here all the

871
00:48:29,580 --> 00:48:34,180
time because it lets you get things out as a markdown table that's formatted

872
00:48:34,220 --> 00:48:43,660
for GitHub issues. But let's jump back to the demo that I was running just

873
00:48:43,660 --> 00:48:48,220
off my own machine. And I'm gonna install... I'm gonna demonstrate plugins

874
00:48:48,220 --> 00:48:54,060
a little bit more. Hang on a second. What's happening there?

875
00:48:57,780 --> 00:49:00,060
My Ctrl C shortcut isn't working.

876
00:49:04,620 --> 00:49:11,700
Okay, I killed that. I'm gonna say... Oh, it's because I switched out

877
00:49:11,700 --> 00:49:16,260
of Dvorak mode. That's why that wasn't working. So I'm gonna say Dataset...

878
00:49:16,260 --> 00:49:20,980
I'm gonna say Dataset install. Now, I'm going to install a new plugin

879
00:49:20,980 --> 00:49:26,820
called Dataset-ClusterMap. So Dataset is very, very heavily reliant on

880
00:49:26,820 --> 00:49:31,300
plugins. There are 115 plugins available at the moment in the Dataset

881
00:49:31,340 --> 00:49:35,780
plugin directory. And as an open source maintainer, one of the things I love

882
00:49:35,780 --> 00:49:40,820
about this is that people can add new features to my software without me

883
00:49:40,820 --> 00:49:44,420
even having to talk to them or even review a pull request. I can wake up

884
00:49:44,420 --> 00:49:47,980
in the morning and somebody has shipped a new Dataset plugin that adds

885
00:49:47,980 --> 00:49:50,820
some kind of feature and I have nothing to do with it at all,

886
00:49:50,820 --> 00:49:56,180
which is so liberating in terms of me getting to see my software expand out

887
00:49:56,180 --> 00:49:59,740
in interesting ways. A lot of the work I do around Dataset is writing plugins

888
00:49:59,740 --> 00:50:04,660
myself because the beautiful thing about plugin is that if it's rubbish,

889
00:50:04,660 --> 00:50:10,140
it doesn't harm the purity of the core project. I can have a weird idea

890
00:50:10,140 --> 00:50:14,980
for a feature, knock out a very quick poorly constructed plugin to try it

891
00:50:14,980 --> 00:50:18,980
out and then iterate on that over time and figure out if that's what I really

892
00:50:18,980 --> 00:50:23,500
wanted to do. Dataset-ClusterMap is one of the... I think it might even be

893
00:50:23,500 --> 00:50:27,860
the very first plugin that I ever wrote and you can install it by saying

894
00:50:27,900 --> 00:50:32,540
Dataset-Install-Dataset-ClusterMap and that's actually the same thing as running

895
00:50:32,540 --> 00:50:37,460
pip install Dataset-ClusterMap. The reason I have a Dataset-Install command

896
00:50:37,460 --> 00:50:40,780
is it makes absolutely sure that the plugin gets installed in the same

897
00:50:40,780 --> 00:50:44,780
virtual environment as Dataset itself. So if you install Dataset through

898
00:50:44,780 --> 00:50:48,460
Homebrew or pipx, it'll go in the right place and you won't have to think

899
00:50:48,460 --> 00:50:52,900
about how to install it. So I'll run that plugin, run that installer.

900
00:50:52,900 --> 00:50:57,660
I can type Dataset-Plugins to see a list of available plugins and this says

901
00:50:57,700 --> 00:51:02,420
I've got Dataset-X-Forwarded-Host, Leaflet-Codespaces and Dataset-ClusterMap

902
00:51:02,420 --> 00:51:06,780
is the one that I just installed. Now if I run Dataset-Legislators again,

903
00:51:08,180 --> 00:51:12,700
what this plugin does is it looks out for any table that has a latitude

904
00:51:12,700 --> 00:51:16,500
and longitude column on it and if it finds one of those, it draws them

905
00:51:16,500 --> 00:51:21,980
on a map. And so in this case, the offices table, this is the congressional

906
00:51:21,980 --> 00:51:25,860
offices of all of these different legislators, has latitude and longitude

907
00:51:26,420 --> 00:51:29,500
and so the plugin kicks in and it draws this map at the very top

908
00:51:29,500 --> 00:51:33,420
of the page. And again, straight away we see some interesting things.

909
00:51:33,420 --> 00:51:39,100
What's this one here? This is... Is this American Samoa maybe?

910
00:51:39,860 --> 00:51:46,380
This is one of those far, far out tiny bits of the United States,

911
00:51:46,660 --> 00:51:49,500
but there's a congressional office there, so instantly that pops right up.

912
00:51:49,500 --> 00:51:54,180
You can see that Hawaii's got three, all of them in Honolulu it looks like.

913
00:51:55,940 --> 00:51:58,500
And is it... Was it Puerto Rico is the other one over here?

914
00:51:59,940 --> 00:52:03,460
Oh no, that's not Puerto Rico, that's the US Virgin Islands.

915
00:52:05,580 --> 00:52:09,140
One of the fun things about playing with these tools is I always learn

916
00:52:09,140 --> 00:52:12,340
new things about the world just through little things that pop up

917
00:52:12,340 --> 00:52:16,380
while I'm doing demos. Yeah, so that's a... I think that's a...

918
00:52:16,900 --> 00:52:20,500
It's a good example of the thing I'm trying to get to with these plugins.

919
00:52:20,500 --> 00:52:25,060
Like my ideal for plugins is that they'll grow more and more abilities

920
00:52:25,100 --> 00:52:27,580
like this one, where they can look at your data and say,

921
00:52:27,940 --> 00:52:31,940
oh, that's interesting. This appears to be mentioning countries.

922
00:52:31,940 --> 00:52:34,620
I could draw a heat map or this is latitude and longitudes or

923
00:52:35,020 --> 00:52:38,100
all sorts of things like that. And I think there's a lot of scope

924
00:52:38,100 --> 00:52:42,900
for really interesting data exploration features that are built around this

925
00:52:42,900 --> 00:52:46,580
idea of plugins that you can run in data set that then get exposed

926
00:52:46,580 --> 00:52:52,820
to your data. We talked about exporting data.

927
00:52:53,700 --> 00:52:57,900
It's also worth noting that you can export the current page of data

928
00:52:57,900 --> 00:53:00,980
that you're looking at. So if I'm looking at legislator terms,

929
00:53:00,980 --> 00:53:03,940
I can get out, I think it's 100 records at a time.

930
00:53:04,540 --> 00:53:07,620
Or if you scroll to the bottom of the page, you can click stream all

931
00:53:07,620 --> 00:53:13,300
rows on the CSV export. And this will actually generate a CSV file

932
00:53:13,540 --> 00:53:19,580
with everything, with all 45,000 rows in it. And I just clicked on that.

933
00:53:19,580 --> 00:53:22,300
I think it's gonna open in numbers. Here we go.

934
00:53:22,340 --> 00:53:28,660
So now I've exported the full 44,551 rows from that table in a format

935
00:53:28,660 --> 00:53:31,260
where I can load it into any other tool that knows how to open

936
00:53:31,260 --> 00:53:43,460
a CSV file. Oh, yes. No, that's a good call. It's worth talking about

937
00:53:45,100 --> 00:53:51,540
primary keys in SQLite. So when you create a SQLite table,

938
00:53:51,860 --> 00:53:56,020
you can give it a primary key. So you can say, this one's got an

939
00:53:56,020 --> 00:53:59,060
integer primary key, for example. But I think some of these,

940
00:54:00,620 --> 00:54:04,340
this one here does not have a primary key at all. It's got a

941
00:54:04,340 --> 00:54:08,380
legislature ID, type text, et cetera, but it's missing that primary key.

942
00:54:08,700 --> 00:54:12,860
And this is a SQLite specific feature where if you don't specify

943
00:54:12,860 --> 00:54:17,780
primary key, SQLite will add one called row ID that's essentially an

944
00:54:17,820 --> 00:54:21,820
incrementing, it's an automatically incrementing number. So this is kind

945
00:54:21,820 --> 00:54:24,900
of convenient when you're just sort of quickly importing data. You don't

946
00:54:24,900 --> 00:54:28,180
have to think about if it's got a primary key or not. You can import

947
00:54:28,180 --> 00:54:32,860
it in and SQLite will automatically assign a unique identifier climbing

948
00:54:32,860 --> 00:54:37,300
from one going up and make it available as that row ID. And when you

949
00:54:37,300 --> 00:54:43,340
do select star, I think select star will not give you the row IDs,

950
00:54:43,900 --> 00:54:49,820
but if you do select row ID, star, then you can see that column. So it's

951
00:54:49,820 --> 00:54:53,820
a bit of a weird column and then it's hidden from select star. But if

952
00:54:53,820 --> 00:54:56,020
you know about it and you know how to call it, you can get it

953
00:54:56,020 --> 00:55:00,180
back in. And data set knows about this row ID thing and so it will

954
00:55:00,180 --> 00:55:05,820
automatically include the row ID in situations where there isn't a defined

955
00:55:05,820 --> 00:55:06,580
primary key.

956
00:55:06,580 --> 00:55:18,540
Now, we sort of touched a little bit on that next, on the next sort of

957
00:55:18,540 --> 00:55:22,500
tutorial segment, which is the idea that the data set is actually a really

958
00:55:22,500 --> 00:55:27,020
useful tool for learning SQL itself. And this is one of the reasons I don't

959
00:55:27,020 --> 00:55:31,660
plan to spend a great amount of time on sort of SQL fundamentals today,

960
00:55:31,660 --> 00:55:34,460
is that a lot of the stuff that you need to do with SQL, you need to know

961
00:55:34,500 --> 00:55:38,660
about creating tables, inserting and updating data, and then you need to be

962
00:55:38,660 --> 00:55:42,100
able to start running select queries. But data set will do a lot of that

963
00:55:42,100 --> 00:55:45,260
work for you, right? It will let you say, you know what, I want everything

964
00:55:45,260 --> 00:55:49,740
where this and this and this. And then clicking on that view and edit SQL

965
00:55:50,220 --> 00:55:53,660
button will get you into an interface where you can see how that SQL query

966
00:55:53,660 --> 00:55:57,460
was working. So, and I've heard from a few... There's a couple of universities

967
00:55:57,460 --> 00:56:00,660
that I've heard from who are now using this as part of their sort of

968
00:56:00,700 --> 00:56:04,300
database education, because it's just a really quick way for the students

969
00:56:04,300 --> 00:56:08,460
to start exploring data and getting a feel for what relational databases

970
00:56:08,460 --> 00:56:12,900
feel like, and then dipping then... Starting to dip into SQL as and when

971
00:56:12,900 --> 00:56:16,180
they want to get the hang of that. So, we'll talk about some advanced

972
00:56:16,180 --> 00:56:20,540
SQL tricks in a little bit, but for basic SQL, you can get a very

973
00:56:20,540 --> 00:56:23,820
long way just noodling around in data set with the filters and then seeing

974
00:56:23,820 --> 00:56:30,980
what query that generated. So, I think we'll... So, this tutorial here,

975
00:56:30,980 --> 00:56:34,580
this is the Learn SQL with data set tutorial. This goes into a little

976
00:56:34,580 --> 00:56:38,980
bit more detail. So, this will talk about select from where order by

977
00:56:38,980 --> 00:56:42,900
limit, the sort of key areas of a SQL query. And there's a bit of

978
00:56:42,900 --> 00:56:46,060
material here as well about the named parameters that we talked about,

979
00:56:46,060 --> 00:56:52,300
where you can say, you know, where start like colon P0 and type equals colon

980
00:56:52,340 --> 00:56:57,740
P1, and then feed those in as separate fields. And there's a little bit

981
00:56:57,740 --> 00:57:02,140
about SQL like queries and joins in here as well. You know, I think we

982
00:57:02,140 --> 00:57:05,460
probably should talk about joins in a little bit more detail, because that's

983
00:57:05,460 --> 00:57:11,980
something we're going to see again later on. Does... Is anyone here super

984
00:57:11,980 --> 00:57:14,900
happy and comfortable joins and left joins and right joins things all the

985
00:57:14,900 --> 00:57:19,060
time? I'm not putting my hand up either, because I still have to look...

986
00:57:19,100 --> 00:57:22,940
I'll be honest, I don't think I've ever used a right join in my entire

987
00:57:22,940 --> 00:57:27,140
career. And SQLite itself only grew right join support like six months

988
00:57:27,140 --> 00:57:30,260
ago maybe, because everything you can do with a right join, you can do

989
00:57:30,260 --> 00:57:34,980
with a left join and so on and so forth. But we'll review the most basic

990
00:57:34,980 --> 00:57:39,060
kind of join just to make sure that we've seen what one of these looks

991
00:57:39,060 --> 00:57:42,740
like. And the problem that we want to solve here is that we've got this

992
00:57:42,740 --> 00:57:48,540
table of executive terms. So, this is saying that there was the president

993
00:57:48,540 --> 00:57:54,180
from 1789 to 1793 was from this party and it was executive ID equals one.

994
00:57:54,180 --> 00:57:59,500
That right there is a reference to another table. So, I can say select

995
00:57:59,500 --> 00:58:05,660
star from executives where ID equals one and say that that means George

996
00:58:05,660 --> 00:58:09,980
Washington. So, we've got a reference to George Washington in this table and

997
00:58:09,980 --> 00:58:13,900
we've got details about George Washington here, but it would kind of be

998
00:58:13,900 --> 00:58:17,220
nicer if we could see the name George Washington in the table that we're

999
00:58:17,220 --> 00:58:18,460
looking at right there.

1000
00:58:22,820 --> 00:58:26,260
Ah, that's a great call. This right here is not an assignment.

1001
00:58:26,260 --> 00:58:29,460
This is a... It would be nicer if it was equals equals.

1002
00:58:30,580 --> 00:58:34,300
Exactly. Yeah. This in Python terms, this is select star from executives

1003
00:58:34,300 --> 00:58:38,620
where ID equals equals one. Unfortunately, SQL is not designed like that.

1004
00:58:38,620 --> 00:58:45,540
So, SQL confuses equals as assignment with equals as comparison.

1005
00:58:48,180 --> 00:58:53,100
You could actually, yeah. So, I could say where that equals colon ID and then I

1006
00:58:53,100 --> 00:58:56,500
could stick a one in here and that'll have the same effect.

1007
00:59:01,300 --> 00:59:06,100
So, we want to see George Washington's name on this table right here.

1008
00:59:07,140 --> 00:59:11,220
And the way we're gonna do that is we're going to do a join. So, I'll copy and

1009
00:59:11,220 --> 00:59:16,460
paste this query in here so we can explore it. So, first we'll run that and

1010
00:59:16,500 --> 00:59:19,860
you'll see that now we've got the same data we had before, but instead of

1011
00:59:19,860 --> 00:59:25,060
executive ID, we have a name and the name is that information pulled in from

1012
00:59:25,060 --> 00:59:30,580
that other table. Maybe we could do select star instead to see if that...

1013
00:59:31,900 --> 00:59:35,860
So, if I do select star, I now get back everything from both tables.

1014
00:59:35,860 --> 00:59:39,820
So, I'm seeing the executive terms columns that we saw earlier and then I'm

1015
00:59:39,820 --> 00:59:43,620
saying all of that stuff from the executives table. And the way this works

1016
00:59:43,660 --> 00:59:48,300
is through what's called a join. I can say where we're selecting from the

1017
00:59:48,300 --> 00:59:53,980
executive terms table and we're joining against the executives table on executive

1018
00:59:53,980 --> 00:59:59,580
terms dot executive ID equals executives dot ID. This is the structure of a join.

1019
00:59:59,580 --> 01:00:04,100
You say this table and this table are gonna get combined together and the way

1020
01:00:04,100 --> 01:00:08,460
we're gonna do it is on this criteria, the on criteria, saying anything where

1021
01:00:08,860 --> 01:00:13,380
the ID in executives matches the executive ID in executive terms.

1022
01:00:15,300 --> 01:00:19,380
These, you can actually nest, you can do multiple joins against multiple tables.

1023
01:00:19,380 --> 01:00:23,100
There's a lot more sophistication to this, but most of the time I find that

1024
01:00:23,100 --> 01:00:27,220
I'm using the most basic kind of join. I think this is called an inner join

1025
01:00:27,220 --> 01:00:30,780
or I just call it a join. But it gives us what we want, right?

1026
01:00:30,780 --> 01:00:34,540
This gives us the ability to start saying, let's combine

1027
01:00:35,380 --> 01:00:39,900
party and then executives dot name.

1028
01:00:41,340 --> 01:00:46,740
And when you're constructing queries like this, you can leave the tables off

1029
01:00:46,740 --> 01:00:50,140
in this bit here, but then it gets kind of confusing because when I look at

1030
01:00:50,140 --> 01:00:54,860
this, it's kind of like an import star in Python. I don't instantly know which

1031
01:00:54,860 --> 01:00:59,900
table these different things came from. So personally, I like to always include

1032
01:00:59,900 --> 01:01:04,860
the full table names. So I'll do that dot that dot that dot that dot that

1033
01:01:05,500 --> 01:01:10,860
and then executives dot name there. And that gives us the exact same results.

1034
01:01:11,180 --> 01:01:15,020
It's just easier to read that query and understand how it works and what it's

1035
01:01:15,020 --> 01:01:21,940
doing. That's surprising. I don't know why my... Oh, my format button is

1036
01:01:21,940 --> 01:01:24,140
gonna confuse by end being a reserved word.

1037
01:01:24,900 --> 01:01:32,700
But yeah, so that's... Honestly, this... I recommend if you want to spend...

1038
01:01:32,700 --> 01:01:36,180
If you're not familiar with SQL, I recommend spending a little bit more time

1039
01:01:36,580 --> 01:01:42,860
with this document. But as you can see, it's relatively straightforward and hope...

1040
01:01:42,860 --> 01:01:45,740
And we'll get into the more advanced SQL stuff in a little bit,

1041
01:01:45,740 --> 01:01:52,100
but it's a pretty clean language to start working with. Something I really like

1042
01:01:52,100 --> 01:01:59,300
about SQL is that when I started learning to be a programmer nearly 25

1043
01:01:59,300 --> 01:02:04,860
years ago now, I learned a whole... I learned Perl and PHP and SQL and action

1044
01:02:04,860 --> 01:02:08,300
script, all sorts of different bits and pieces. SQL is the one thing I learned

1045
01:02:08,300 --> 01:02:12,060
25 years ago that's still serving me today. I feel like it's one of those

1046
01:02:12,060 --> 01:02:17,380
skills which kind of like bash on Linux, pick it up once and it will be paying

1047
01:02:17,380 --> 01:02:21,620
for itself for decades to come. So... And I mean, as somebody who writes

1048
01:02:21,620 --> 01:02:26,140
software around SQL, I have some bias here, but I do think it's worth spending

1049
01:02:26,140 --> 01:02:28,900
time with SQL and getting to a point where you're really comfortable and

1050
01:02:28,900 --> 01:02:32,380
familiar with it, because it will apply to SQLite, but also so many other

1051
01:02:32,380 --> 01:02:36,820
technologies as well you can then tap into. And the syntax for basic select

1052
01:02:36,820 --> 01:02:40,980
statements is effectively the same across all of the different database engines.

1053
01:02:44,380 --> 01:02:49,220
Let's get into something else. Let's start to... So I've shown you a bunch

1054
01:02:49,260 --> 01:02:53,700
of stuff so far that assumes that you've got data in SQLite already. If your

1055
01:02:53,700 --> 01:02:57,140
data is in SQLite, then you can run data set against it and you can do all

1056
01:02:57,140 --> 01:03:01,180
of these things and that's super fun. But obviously the vast majority of data

1057
01:03:01,180 --> 01:03:05,100
in the world is currently not in SQLite, which is something that I'm on a

1058
01:03:05,100 --> 01:03:10,620
one-man quest to try and fix. But so the other major project I've been working

1059
01:03:10,620 --> 01:03:15,820
on for the past two years is a tool called SQLiteUtils. So this tool here,

1060
01:03:15,860 --> 01:03:20,300
it's up to version 3.30, so you can see I've shipped a lot of changes and fixes

1061
01:03:20,300 --> 01:03:27,100
to this over the years. And SQLiteUtils is two things in one. It's a command line

1062
01:03:27,100 --> 01:03:33,300
interface, it's a CLI tool, and it's a Python library for manipulating SQLite

1063
01:03:33,300 --> 01:03:37,380
databases. The specific problem I'm trying to solve with it is what is the

1064
01:03:37,380 --> 01:03:42,220
fastest way to get from here's some data to now that data's in SQLite and I can

1065
01:03:42,220 --> 01:03:47,620
start running queries against it. And so you install it exactly the same way as

1066
01:03:47,620 --> 01:03:53,500
data set. It's a pip install or pipx or brew or various other things like that.

1067
01:03:53,500 --> 01:04:04,660
I'm going to do that in here. Actually let's fire up another shell again. Here

1068
01:04:04,660 --> 01:04:06,740
we go.

1069
01:04:06,780 --> 01:04:19,900
Export. I need to figure out how to set this up as the default. There we go. Okay,

1070
01:04:19,900 --> 01:04:24,580
so I'm going to do pip install SQLiteUtils and this will give me a Python

1071
01:04:24,580 --> 01:04:28,100
library that I can import but it also gives me a new command called SQLite-Utils.

1072
01:04:28,100 --> 01:04:35,380
I'll run a version on that. There we go, I've got version 3.30 of that

1073
01:04:35,380 --> 01:04:42,700
installed now. And what we're going to do is we're going to use this to create

1074
01:04:42,700 --> 01:04:46,660
ourselves a brand new database from a new set of a new data source. This is

1075
01:04:46,660 --> 01:04:52,180
following another of the official data set tutorials. We're going to take a look

1076
01:04:52,180 --> 01:04:59,420
at data about manatees in Florida and specifically where manatees have

1077
01:04:59,420 --> 01:05:03,900
come a cropper, manatee carcass recovery locations. This is data put out by the

1078
01:05:04,060 --> 01:05:08,180
Florida Fish and Wildlife Conservation Commission and they have been collecting

1079
01:05:08,180 --> 01:05:13,620
information on where dead manatees have shown up since April of 1974. So there's

1080
01:05:13,620 --> 01:05:20,500
50 years of manatee carcass locations represented in this CSV file.

1081
01:05:20,500 --> 01:05:26,380
And it's got 14,341 records in it. It's available to CSV. I've actually got a

1082
01:05:26,380 --> 01:05:33,100
copy of that that I archive just in case they change their file format, which

1083
01:05:33,180 --> 01:05:39,980
we can download from here. So I'm going to right click on this, copy link, and

1084
01:05:39,980 --> 01:05:50,380
then back in here. I'm going to do wget of that. So this is giving me the file

1085
01:05:50,380 --> 01:05:56,140
manatee carcass recovery locations in Florida.csv. It's a 2.7 megabyte file.

1086
01:05:56,140 --> 01:06:02,020
Next, I'm going to load that into a SQLite database using the SQLite-Utils tool.

1087
01:06:02,060 --> 01:06:07,740
So, I wonder if there's a way I can pin this so it's easier to find. So I'm going

1088
01:06:07,740 --> 01:06:11,300
to run a command, SQLite-Utils insert, because we're inserting data. I'm going

1089
01:06:11,300 --> 01:06:15,220
to create a new database file called manatees.db. I'm going to create a table

1090
01:06:15,220 --> 01:06:19,100
called locations. I'm feeding it the path to that file. And then at the end,

1091
01:06:19,100 --> 01:06:24,580
I'm saying, this is CSV data. And then dash D means try and figure out the types

1092
01:06:24,580 --> 01:06:28,100
of that data. So if a column looks like it's all integers, treat that as an

1093
01:06:28,100 --> 01:06:30,820
integer column. If you don't do that, you'll get everything as strings,

1094
01:06:30,820 --> 01:06:38,020
which isn't quite as useful. Nulls. So this is the problem with CSV is that

1095
01:06:38,020 --> 01:06:43,060
a null and a blank string are completely indistinguishable. So SQLite-Utils will

1096
01:06:43,060 --> 01:06:46,340
always import them as blank strings. If you want to then turn them into

1097
01:06:46,340 --> 01:06:49,700
nulls later, you have to do that yourself. This is one of the reasons I prefer

1098
01:06:49,700 --> 01:06:54,060
JSON over CSV as a way of passing this data around, because JSON does have

1099
01:06:54,060 --> 01:06:57,460
nulls. It has nulls, a floating point number, and JSON is represented as a

1100
01:06:57,460 --> 01:07:02,260
floating point number. It's generally, I think, better. But most of the data in

1101
01:07:02,260 --> 01:07:06,900
the world is published as CSV, so we have to have tooling for working with that.

1102
01:07:06,900 --> 01:07:14,740
So we now have a file called manatees.db. It's 5.4 megabytes. And I'm going to open

1103
01:07:14,740 --> 01:07:24,460
that up in dataset as well. Yes. So that's in this tutorial here. It's SQLite-Utils

1104
01:07:24,460 --> 01:07:31,220
insert manatees.db locations. So that's the command, the database file, the table.

1105
01:07:31,220 --> 01:07:35,180
This is the name of the CSV file. And then at the end, I'm saying treat this as a

1106
01:07:35,180 --> 01:07:39,980
CSV file and do the type detection as well.

1107
01:07:39,980 --> 01:08:01,660
That's a great question. Is there a user-friendly interface for SQLite? Or are you

1108
01:08:01,660 --> 01:08:06,460
stuck in the command line? There are many. One of the great things about SQLite is

1109
01:08:06,460 --> 01:08:11,220
it's been around for 20 years. And so there are Windows apps and Mac apps and GUI apps.

1110
01:08:11,220 --> 01:08:15,860
And there are dataset plugins that actually add the ability to upload CSV files

1111
01:08:15,860 --> 01:08:19,580
directly into dataset as well. So there's lots and lots of different options for

1112
01:08:19,580 --> 01:08:27,580
this. I tend to do most of my analysis on most of my data manipulation on the

1113
01:08:27,580 --> 01:08:31,420
command line because I like that I can automate it. So I can figure out a recipe

1114
01:08:31,420 --> 01:08:34,980
that works for manipulating data in a certain way. And then I can stick that in a

1115
01:08:34,980 --> 01:08:38,980
bash script. Or a lot of the work I do, and I'll show some examples of this later

1116
01:08:38,980 --> 01:08:43,620
on, is in GitHub actions. So I have GitHub actions automations that download data

1117
01:08:43,620 --> 01:08:47,100
from here and then convert it and then do this and clean it up and all of that kind

1118
01:08:47,100 --> 01:08:52,060
of stuff. But yeah, so SQLite utils, you can actually use in a Jupyter notebook as

1119
01:08:52,060 --> 01:08:58,180
well, which I'll demonstrate in a moment. But I kind of, there's an aspect of this

1120
01:08:58,180 --> 01:09:02,500
where I'm enjoying the intellectual challenge of coming up with a command line

1121
01:09:02,500 --> 01:09:07,500
interface version of every Python method that I'm writing. So a lot of this is me

1122
01:09:07,500 --> 01:09:11,620
just having fun trying to explore the intersection of command line tools and

1123
01:09:11,620 --> 01:09:17,740
Python libraries. Oh, let's run a different command actually. We're going to do

1124
01:09:17,740 --> 01:09:27,260
SQLite utils schema, manatees.db. And that dumps out the schema. So we can see the

1125
01:09:27,260 --> 01:09:31,140
schema of that database that we just created. And actually looking at that

1126
01:09:31,140 --> 01:09:35,260
scheme is kind of interesting because you see things like there's an X and a Y

1127
01:09:35,260 --> 01:09:40,540
column that are floating point. There's a lat and a long column that are floating

1128
01:09:40,540 --> 01:09:45,580
point as well. So we've got potentially four different ways of representing those

1129
01:09:45,580 --> 01:09:52,340
locations. But it's better to fire it up in data set itself. So here is that table

1130
01:09:52,340 --> 01:09:57,980
that we just imported. It's got 13,000 rows in it. And it's a table. And as with

1131
01:09:57,980 --> 01:10:01,100
all tables, it's kind of boring when you first start looking at it. You know, it's

1132
01:10:01,100 --> 01:10:05,900
got these X and Y columns, the year that they were found, the state, the county,

1133
01:10:05,900 --> 01:10:10,540
mortality. This is an interesting one. We can facet by that and see the most

1134
01:10:10,540 --> 01:10:15,260
common reasons that a manatee meets their end. And unsurprisingly, it's

1135
01:10:15,260 --> 01:10:21,580
watercraft collisions. That's taken 2,700 manatees in the state of Florida.

1136
01:10:21,580 --> 01:10:29,340
Cold stress beginning in 1986, 1,233. And then floodgates and canal locks have

1137
01:10:29,340 --> 01:10:33,780
killed 250 full manatees. So that's something you should watch out for if

1138
01:10:33,780 --> 01:10:37,820
you're a manatee. But what we're going to do now is we're going to make some

1139
01:10:37,820 --> 01:10:42,380
changes to this data. Because that schema is a little bit untidy as a sort of

1140
01:10:42,380 --> 01:10:48,340
starting point. So, oh, actually, I'll show one quick other thing that you can

1141
01:10:48,340 --> 01:10:54,660
do. So with SQLiteUtils, you can run SQL queries. Here I'm saying SQLiteUtils

1142
01:10:54,660 --> 01:11:00,940
against manatees.db. I want to select those columns and out it comes. And

1143
01:11:00,940 --> 01:11:04,260
actually, I think I need to... There we go. If I drop the font size down, you can see

1144
01:11:04,260 --> 01:11:12,580
that that came out as a tab-separated table. I think if I, instead of dash T, by

1145
01:11:12,580 --> 01:11:17,740
default, without any options, it'll output JSON, which is useful for piping

1146
01:11:17,740 --> 01:11:22,740
into the commands. You can also say dash, dash format. And then I think it might

1147
01:11:22,780 --> 01:11:26,500
even let me format it. There we go. This is LaTeX on the command line, should that

1148
01:11:26,500 --> 01:11:30,620
be a thing that is useful for you. So there's a lot of stuff you can do with

1149
01:11:30,620 --> 01:11:38,540
SQLiteUtils just as a command line tool for querying data. But let's make some

1150
01:11:38,540 --> 01:11:44,380
changes. So when we were looking at this table earlier, it became apparent that...

1151
01:11:44,420 --> 01:11:45,380
Where has it gone?

1152
01:11:50,380 --> 01:11:55,660
Oh, manatee's gone. So some of these columns are kind of pointless. Like,

1153
01:11:55,660 --> 01:11:59,660
field ID doesn't look very interesting. The rep month and rep day is already

1154
01:11:59,660 --> 01:12:06,020
represented in the date. Every single one of them was last edited by user FWC,

1155
01:12:06,020 --> 01:12:10,580
so we don't really need those columns since they're all the same values. So we're

1156
01:12:10,620 --> 01:12:15,420
gonna run this column here. This is a... This command here. This is the transform

1157
01:12:15,420 --> 01:12:20,500
command, and what this can do is apply a whole bunch of alterations to the table

1158
01:12:20,500 --> 01:12:25,660
you specify. It's effectively a very fancy alter table statement. And this is

1159
01:12:25,660 --> 01:12:32,220
actually making up for a limitation of SQLite itself. Databases have a query

1160
01:12:32,220 --> 01:12:36,620
you can run for alter table that can make changes to a table. The SQLite one's

1161
01:12:36,660 --> 01:12:40,660
actually quite limited. There are things that it can't do. It can't reassign the

1162
01:12:40,660 --> 01:12:44,740
primary key. It used to not be able to rename columns, but I think they might

1163
01:12:44,740 --> 01:12:49,700
have fixed that. So this command here does a whole bunch of wild things under

1164
01:12:49,700 --> 01:12:56,940
the hood to make more complicated alter statements available to you. The way it

1165
01:12:56,940 --> 01:13:00,660
actually works is when you run this, it will create a brand new table called

1166
01:13:00,660 --> 01:13:05,620
locations temp in the new format. It'll copy the old data to the new table.

1167
01:13:05,660 --> 01:13:09,780
It'll drop the old table and then rename the new one back to the old one.

1168
01:13:09,780 --> 01:13:14,460
So there's a lot of activity hidden by that command, but the end result of this

1169
01:13:14,460 --> 01:13:17,620
command is that you can make all of the alterations to your table that you want

1170
01:13:17,620 --> 01:13:21,260
to. So we're gonna do a few things. We're gonna rename the last column to

1171
01:13:21,260 --> 01:13:25,820
latitude and rename that long underscore column to longitude. That's so that our

1172
01:13:25,820 --> 01:13:29,700
cluster map plugin can pick up those columns and show them on a map. We're

1173
01:13:29,700 --> 01:13:34,060
gonna drop those created user last user x, y, state. We're dropping the state

1174
01:13:34,060 --> 01:13:37,580
column because it's all in Florida. We don't need to know which state it is.

1175
01:13:37,580 --> 01:13:41,980
We'll drop object ID and we're gonna reassign the primary key to be this field

1176
01:13:41,980 --> 01:13:45,500
ID column as well. So that's a lot of stuff that's gonna happen.

1177
01:13:47,420 --> 01:13:51,340
And if I run that right now, here we go.

1178
01:13:52,300 --> 01:13:52,820
Oops.

1179
01:13:55,580 --> 01:14:00,900
It's done. So that should have changed all of those different things that we wanted

1180
01:14:00,940 --> 01:14:06,980
to change. So now if I refresh this page here, there we go. We've now got a map

1181
01:14:06,980 --> 01:14:10,660
because we've got latitude and longitude columns. We've dropped a whole bunch

1182
01:14:10,660 --> 01:14:15,500
of those columns that we didn't need. And having that map, and we can actually...

1183
01:14:15,500 --> 01:14:20,060
The map defaults showing 1,000 points. You can actually load all 14,000 points

1184
01:14:20,060 --> 01:14:24,100
on. So now this is a map of every single time a manatee carcass has been

1185
01:14:24,100 --> 01:14:28,060
discovered in the state of Florida since 1974. And oh, that's interesting.

1186
01:14:28,100 --> 01:14:32,100
I wonder what happened to... Huh. Not many manatees died around there.

1187
01:14:32,100 --> 01:14:36,420
That poor manatee got very unlucky. Also, it's nice to note that the Florida

1188
01:14:36,420 --> 01:14:40,340
keys look a lot safer than the rest of Florida. There seems to be a lot

1189
01:14:40,340 --> 01:14:43,500
less manatee deaths down there than up around the edges.

1190
01:14:45,220 --> 01:14:49,020
So again, it's a classic demonstration of how you start with a CSV file,

1191
01:14:49,300 --> 01:14:51,900
you run a couple of commands, and now you can tell stories.

1192
01:14:51,900 --> 01:14:56,100
You can start understanding that data in much more detail than if you

1193
01:14:56,140 --> 01:14:58,180
were just squinting at it in Microsoft Excel.

1194
01:15:00,060 --> 01:15:02,260
There is one more change that we want to make to this.

1195
01:15:02,940 --> 01:15:07,540
So we've visualized the locations. The date columns. So

1196
01:15:08,740 --> 01:15:12,900
I'm European, and I think that the way Americans do dates is completely...

1197
01:15:13,900 --> 01:15:17,460
The whole month, day, year thing is not good. At least in this case,

1198
01:15:17,740 --> 01:15:20,620
the dates are slightly better. This is year, year, year, year, month, month,

1199
01:15:20,620 --> 01:15:25,100
day, day, which is closer to my European sensibilities. But it's not quite

1200
01:15:25,100 --> 01:15:29,380
the format that I like. And so another command that SQLiteUtils has

1201
01:15:29,860 --> 01:15:35,140
is called SQLiteUtilsConvert, and this can be run to convert the values

1202
01:15:35,540 --> 01:15:39,940
in one or more columns to some other format. And I'm gonna run that

1203
01:15:39,940 --> 01:15:40,460
right now.

1204
01:15:45,340 --> 01:15:48,020
There we go. This one actually shows you a little progress bar as it whizzes

1205
01:15:48,020 --> 01:15:53,220
through. And having run that, if we refresh this table here,

1206
01:15:53,540 --> 01:15:57,300
there we go. This is much more pleasing to my eyes. We're now in ISO

1207
01:15:57,300 --> 01:16:02,380
formatted dates for all of that data there. You'll notice that the

1208
01:16:02,380 --> 01:16:05,300
command I ran there actually has a little snippet of code in it.

1209
01:16:05,300 --> 01:16:09,300
I'm saying, convert the values... Look at the locations table.

1210
01:16:10,180 --> 01:16:13,460
Look at data in these three columns. So the rep date, created date,

1211
01:16:13,460 --> 01:16:16,620
and last edited date columns. And for each of those, I want you to

1212
01:16:16,620 --> 01:16:21,340
run this little Python expression. And this is actually a snippet of Python.

1213
01:16:21,380 --> 01:16:24,620
You can put any Python expression you like in there. In this case,

1214
01:16:24,620 --> 01:16:29,420
I'm using a baked in function called pass date time, which is using...

1215
01:16:29,860 --> 01:16:32,420
I think it's using date util under the hood. But it basically,

1216
01:16:32,420 --> 01:16:34,980
you can give it any string you like, and it will try and convert

1217
01:16:34,980 --> 01:16:39,540
that into an ISO formatted string. So this is a really powerful way

1218
01:16:39,540 --> 01:16:45,220
of rewriting data on the fly. I often find myself, starting with the CSV

1219
01:16:45,220 --> 01:16:48,740
file, I import it. I spot that some of the values need to be converted

1220
01:16:48,820 --> 01:16:52,260
lowercase, or have their dates processed or whatever. And I can do all

1221
01:16:52,260 --> 01:16:56,420
of that as these command line invocations of that SQLite utils command.

1222
01:16:59,220 --> 01:17:07,060
So R stands for... It's a namespace. And I can't remember why I called

1223
01:17:07,060 --> 01:17:12,020
it R. Let's have a quick look. I think you're right. Yes, it's recipes.

1224
01:17:12,100 --> 01:17:19,260
Yep, command line tool. It's the convert. Converting data from columns.

1225
01:17:20,220 --> 01:17:24,500
Yes. Yep, it was R for recipes. So there are a bunch of other recipes

1226
01:17:24,500 --> 01:17:27,940
that are available on that object. There's like a JSON split one that

1227
01:17:27,940 --> 01:17:32,580
splits comma separated values into a JSON array, pass date, pass date time.

1228
01:17:32,980 --> 01:17:36,860
Oh, that's it. There's three built in recipes that you can use.

1229
01:17:36,860 --> 01:17:41,180
And then most of the time, I will use little Python expressions like this,

1230
01:17:41,220 --> 01:17:46,860
like value.upper will convert things to uppercase. Float value plus 0.5 will

1231
01:17:46,860 --> 01:17:51,180
convert it to floating point number and add 0.5 on the end. SQLite utils

1232
01:17:51,180 --> 01:17:56,100
has extremely extensive documentation, both for the Python library and the

1233
01:17:56,100 --> 01:18:00,180
command line tools. So there's a lot of stuff that I'm not going to...

1234
01:18:00,180 --> 01:18:03,620
There's a huge amount of stuff that I'm not gonna cover with us today.

1235
01:18:03,620 --> 01:18:09,220
So how many times have you covered Python with the command line tool?

1236
01:18:09,260 --> 01:18:12,180
Right. So yeah, the question is, how do you decide when to use Python and

1237
01:18:12,180 --> 01:18:18,260
when to use SQL? SQLite, SQL is very limited. SQLite can do string

1238
01:18:18,260 --> 01:18:21,260
concatenations and adding numbers together, but it can't even do

1239
01:18:21,980 --> 01:18:26,700
mathematic... It can't do math operations beyond multiplying and addition and

1240
01:18:26,700 --> 01:18:31,660
such. It's missing more advanced string manipulations aren't in there.

1241
01:18:32,580 --> 01:18:36,180
If you're used to Postgres, Postgres has a very rich SQL language that

1242
01:18:36,220 --> 01:18:40,260
can do regular expression extractions and just almost any string operation

1243
01:18:40,260 --> 01:18:43,380
you want is built into Postgres. That's not the case with SQLite.

1244
01:18:45,220 --> 01:18:49,820
But what SQLite does let you do is it lets you register Python functions

1245
01:18:50,100 --> 01:18:54,460
with names in SQLite so that they can be called as SQL functions.

1246
01:18:54,860 --> 01:18:59,220
And Dataset does that quite often. SQLite utils can do that as well.

1247
01:18:59,460 --> 01:19:04,100
And when you actually... When you use SQLite utils in this way,

1248
01:19:04,100 --> 01:19:09,700
when you give it that conversion function, it actually defines a new Python

1249
01:19:09,700 --> 01:19:15,220
function called convert. It registers that. It runs the SQL query of update

1250
01:19:15,220 --> 01:19:19,300
table set something equals convert bracket something else. And that's how

1251
01:19:19,300 --> 01:19:23,180
it all works. So your Python code is being executed effectively inside of

1252
01:19:23,180 --> 01:19:27,380
SQLite as a custom function. That's how the internals of the system work

1253
01:19:27,380 --> 01:19:32,420
anyway. There's actually other stuff you can do with SQLite utils too for

1254
01:19:34,940 --> 01:19:43,420
for registering functions. Yeah, you can do things like this. You can say...

1255
01:19:44,460 --> 01:19:48,860
That gets the right place. SQLite, when you run a query, one of the things

1256
01:19:48,860 --> 01:19:54,180
you can pass is you can pass dash, dash functions and actually pass it a block

1257
01:19:54,180 --> 01:19:58,380
of Python code that defines some additional Python functions. And those will

1258
01:19:58,380 --> 01:20:01,860
then become available to your SQLite query. So that's a sort of advanced

1259
01:20:01,860 --> 01:20:05,540
feature that I wasn't going to go into today. But if you want to use Python

1260
01:20:05,540 --> 01:20:09,460
functions as part of SQL, it's got you covered. That's a feature that's

1261
01:20:09,460 --> 01:20:12,580
available in the library too. That's kind of fun.

1262
01:20:16,580 --> 01:20:21,020
Okay, we're gonna do one more operation on our Manatees database. And that's

1263
01:20:21,020 --> 01:20:25,380
that when we take a look at the... When we take a look at our Manatees,

1264
01:20:25,540 --> 01:20:28,900
one of the things we notice is that this data is not particularly...

1265
01:20:32,100 --> 01:20:39,460
Great. Yeah, so this mortality column is duplicate data. It's the same string,

1266
01:20:39,460 --> 01:20:44,420
verified not necropsy, repeated thousands and thousands of times. In database

1267
01:20:44,420 --> 01:20:48,580
design, this is bad. If you ever hear people talking about database

1268
01:20:48,580 --> 01:20:52,700
normalization, one of the things that that means is that you don't duplicate

1269
01:20:52,700 --> 01:20:56,580
the same string thousands of times just because that's the way the data came

1270
01:20:56,580 --> 01:21:00,300
in. Another thing you'll notice is that there's mortality and there's a thing

1271
01:21:00,340 --> 01:21:05,580
called decode. And if we facet by both of those, so facet by decode and

1272
01:21:05,580 --> 01:21:11,500
then facet by mortality, those numbers are exactly the same because it turns

1273
01:21:11,500 --> 01:21:16,820
out decode of one means human related water group condition, four means

1274
01:21:16,940 --> 01:21:22,060
perinatal, eight means that. These are duplicate data. We've got the decode

1275
01:21:22,060 --> 01:21:25,460
and we've got the mortality and we really shouldn't need both of those.

1276
01:21:26,020 --> 01:21:31,580
So what I'm gonna do is I'm going to extract that... I'm gonna create myself

1277
01:21:31,580 --> 01:21:39,220
a new table that's just those mortality codes and I'm gonna rewrite my existing

1278
01:21:39,220 --> 01:21:44,100
table to reference that one instead. I think what I'll do is I'll demonstrate

1279
01:21:44,100 --> 01:21:50,620
that first and then we will... Let's go back up to here. So our Malaties database

1280
01:21:50,620 --> 01:21:54,620
currently has a single table called locations. I'm gonna run this command

1281
01:21:54,620 --> 01:22:00,980
here. SQLiteUtils extract. I'm extracting from the locations table. I'm saying

1282
01:22:00,980 --> 01:22:06,260
pull out the decode and the mortality, rename mortality to just name and put

1283
01:22:06,260 --> 01:22:11,140
them in a table called mortality. So I run that and now when I hit refresh

1284
01:22:11,140 --> 01:22:15,660
here, I've got two tables. I've got my locations table and I've got a new

1285
01:22:15,660 --> 01:22:20,180
table called mortality. If I go in here, I can see that those are all of those

1286
01:22:20,180 --> 01:22:23,980
unique values that we had a moment ago. And if I pop back into the locations

1287
01:22:23,980 --> 01:22:28,940
table, I no longer have those columns. I've now got a new column called

1288
01:22:28,940 --> 01:22:33,580
mortality underscore ID and that's one of these foreign keys. That's now a

1289
01:22:33,580 --> 01:22:37,740
foreign key reference to this new table. We can scroll down to the bottom and see

1290
01:22:37,740 --> 01:22:42,980
that the table is now defined in terms of mortality ID as an integer. There's a

1291
01:22:42,980 --> 01:22:47,660
foreign key of mortality ID referencing that other table. So SQLiteUtils has

1292
01:22:47,660 --> 01:22:51,700
pulled that data out, created a new table, populated that table, rewritten this

1293
01:22:51,700 --> 01:22:56,220
table to have a reference to it. It's done, again, a lot of work under the hood

1294
01:22:56,220 --> 01:22:59,860
to get you to a point where now we've got a database that is a table that is

1295
01:22:59,860 --> 01:23:03,820
much more normalized. We're no longer duplicating that data. We've got a foreign

1296
01:23:03,820 --> 01:23:09,700
key reference that we can follow. Features like faceting still work. You can say

1297
01:23:09,700 --> 01:23:19,220
facet by, in this case, mortality ID. And when you do that, it's data set smart

1298
01:23:19,220 --> 01:23:26,180
enough to pull in the official name of that. But that's kind of... This is... As

1299
01:23:26,180 --> 01:23:30,740
somebody who works with databases a lot, I haven't actually seen this particular

1300
01:23:30,740 --> 01:23:34,780
kind of refactoring tool for anything else. And I find that with... I use this

1301
01:23:34,780 --> 01:23:39,300
with my SQLite databases all the time. I'll start with manatee carcass locations

1302
01:23:39,300 --> 01:23:42,620
and then with just a few additional steps, I can get to the point where I've

1303
01:23:42,620 --> 01:23:47,420
actually got a clean relational... I've got a relational scheme of that. Multiple

1304
01:23:47,420 --> 01:23:51,060
tables, like foreign keys, all of those bits and pieces are all set up and

1305
01:23:51,060 --> 01:23:56,100
working together. So yeah, there's a whole bunch of other stuff you can do with

1306
01:23:56,100 --> 01:24:02,540
SQLite utils. This documentation here links to additional information about

1307
01:24:02,540 --> 01:24:06,460
those conversion functions and refactoring and all of that sort of stuff.

1308
01:24:06,460 --> 01:24:10,820
But hopefully this gives you a good feel for what you can start doing with that

1309
01:24:10,820 --> 01:24:13,500
as a command line tool for manipulating that data.

1310
01:24:17,420 --> 01:24:21,620
Can you foreign key from one to another?

1311
01:24:21,620 --> 01:24:24,860
That's a great question. So the question is, if you've got two CSV files,

1312
01:24:24,860 --> 01:24:30,300
can you foreign key from one to another? Yes, you can. I don't have a good demo

1313
01:24:30,300 --> 01:24:35,660
for that right now, but the way you would do that is you'd import the two and then

1314
01:24:35,660 --> 01:24:41,620
there's a command in here called adding foreign key constraints. So you can

1315
01:24:41,620 --> 01:24:47,220
basically say, SQLite utils add foreign key on my books.db database to the books

1316
01:24:47,220 --> 01:24:52,500
table, tell it that the author ID column is a foreign key to the author's table

1317
01:24:52,500 --> 01:24:56,100
and the ID column. So it's a bit of a mouthful, but it can do that.

1318
01:24:56,100 --> 01:24:59,540
And that's another example of something which SQLite out of the box doesn't have

1319
01:24:59,540 --> 01:25:04,940
the ability to do. You actually have to do some very weird things to the SQLite

1320
01:25:04,940 --> 01:25:09,060
metadata to get that to work, but SQLite utils has all of those patterns baked in.

1321
01:25:09,060 --> 01:25:13,220
So yeah, one of the capabilities it gives you is the ability to add foreign keys to

1322
01:25:13,220 --> 01:25:20,340
tables that don't have them yet, but should. So we are almost exactly halfway

1323
01:25:20,340 --> 01:25:25,220
through. I think we should, we take a 10 minute break now and then we'll, when we

1324
01:25:25,220 --> 01:25:29,860
get back, we're going to dive into some of the more advanced, well firstly we're

1325
01:25:29,860 --> 01:25:35,220
going to create a database of PEPs and publish that to the internet and then

1326
01:25:35,220 --> 01:25:40,540
we'll start diving into some advanced SQL as well. But yeah, let's reconvene it

1327
01:25:40,540 --> 01:25:49,300
at 3.05. Okay, we'll kick off again in just a moment. We are going to be on this

1328
01:25:49,300 --> 01:25:55,340
section here, the using SQLite utils as a Python library to import all of the PEPs.

1329
01:25:55,340 --> 01:26:00,980
So it should be linked to, I guess linked to from the index page of the tutorial

1330
01:26:00,980 --> 01:26:10,500
workshop. Okay, so actually what I'm going to do now is I'm going to switch to a

1331
01:26:10,500 --> 01:26:16,140
Jupyter notebook. And again, this is one of the things I like about code space,

1332
01:26:16,140 --> 01:26:20,180
it makes it nice and easy to do this. I'm going to go in notebooks, right click, a

1333
01:26:20,180 --> 01:26:28,980
new file, I'm going to call it SQLite.ipynb and this takes a little while

1334
01:26:28,980 --> 01:26:33,660
to start running because it has to detect kernels and things like that.

1335
01:26:33,660 --> 01:26:42,860
Select kernel, I want a Python environment, I want Python 3.10. Okay, there we go,

1336
01:26:42,860 --> 01:26:49,100
that's worked, great. So the next thing we're going to do is I'm going to create

1337
01:26:49,100 --> 01:26:53,860
a, I'm going to use the Python library version of SQLite utils. Everything that

1338
01:26:53,860 --> 01:26:57,860
you saw in the command line tool do earlier, you could also do by importing

1339
01:26:57,860 --> 01:27:01,740
SQLite utils into Python and doing things there. So I'm going to import

1340
01:27:01,820 --> 01:27:11,100
SQLite utils and then do db equals SQLite utils dot database. I'm going to create a

1341
01:27:11,100 --> 01:27:17,020
database called peps dot db. Okay, here we go, this is our peps database. If I say

1342
01:27:17,020 --> 01:27:22,740
db dot tables, it shows me an empty list because there aren't any tables yet. And

1343
01:27:22,740 --> 01:27:27,300
I'm actually going to do, I'm going to create a database of all 400 plus peps.

1344
01:27:28,140 --> 01:27:32,100
The easiest way to get that data is that you can git clone it. So I'm going to do

1345
01:27:32,100 --> 01:27:37,420
git clone of the Python slash peps repo and I'm going to put it in slash temp

1346
01:27:37,420 --> 01:27:42,420
slash peps. This is a habit I have for all sorts of things. I'm very lazy about

1347
01:27:42,420 --> 01:27:45,620
deleting files so anytime I don't think I'm going to want to keep something, I

1348
01:27:45,620 --> 01:27:49,780
stick it in my temp directory so next time I reboot my computer I don't have

1349
01:27:49,780 --> 01:27:54,860
to worry about it anymore. So I'm going to run that, I'm running it in a Jupyter

1350
01:27:54,860 --> 01:28:00,220
notebook cell so I put an exclamation mark at the beginning. There we go,

1351
01:28:00,220 --> 01:28:07,580
cloned into slash temp slash peps and there they are. That's all of those pep files.

1352
01:28:07,580 --> 01:28:13,860
So now what I'm going to do is I'm going to grab the, I'm going to grab that,

1353
01:28:13,860 --> 01:28:18,940
remember that code we wrote earlier, the the Paras pep function that can turn a

1354
01:28:18,940 --> 01:28:25,740
pep into a dictionary. So I've got that and now I'm going to use pathlib to get

1355
01:28:25,740 --> 01:28:30,700
a list of all of the files in that peps checkout that match the the file pattern

1356
01:28:30,700 --> 01:28:40,060
pep hyphen star dot txt. So I'll do that. Now if we look at files, len files, comma

1357
01:28:40,060 --> 01:28:47,980
files colon five, there we go. So we've now got 429 peps as files on disk and

1358
01:28:47,980 --> 01:28:56,540
each of those files looks something like that. Yes, so the code for this is all in

1359
01:28:56,540 --> 01:29:02,540
the handout for the workshop here. And now I'm going to pass them. So I'm going to

1360
01:29:02,540 --> 01:29:07,460
loop through every file and for each one I'm going to call pass pep on the text

1361
01:29:07,460 --> 01:29:16,940
contents of that file and stick it in the list. So we'll do this. And now if I look at

1362
01:29:17,100 --> 01:29:23,580
len of peps is 429 and each of those peps is a dictionary and it's got a pep id,

1363
01:29:23,580 --> 01:29:27,900
the title and the version, author status, all of those different fields that came out of those

1364
01:29:27,900 --> 01:29:33,820
those text files. So now we want to turn this into a SQLite database. So we can do

1365
01:29:33,820 --> 01:29:39,980
import SQL, oh we actually did this already right, we did, I'll run it again. So import

1366
01:29:39,980 --> 01:29:49,260
SQLite utils and I can do db equals SQLite utils dot database peps dot db.

1367
01:29:51,420 --> 01:29:56,380
Now I'm going to insert them. So I can say db square brackets peps, this is a shortcut for

1368
01:29:56,380 --> 01:30:02,620
getting a reference to a table in that database. SQLite utils will, if the table doesn't already

1369
01:30:02,620 --> 01:30:09,660
exist it will create it. So I can say that dot insert all peps and then we also, we know that

1370
01:30:09,660 --> 01:30:18,700
we've got a primary key which is the PEP column. So I'm going to tell it to insert all 420 of those

1371
01:30:18,700 --> 01:30:25,980
peps into a new table called peps and give it a primary key. And this failed with an error.

1372
01:30:25,980 --> 01:30:31,820
The error was table peps has no column named pep delegate. So what's happened here is that

1373
01:30:31,820 --> 01:30:37,100
we've got a list of dictionaries and it creates a table to match the first dictionary and then

1374
01:30:37,100 --> 01:30:41,340
tries to insert all of the data. But it turns out later on in that list there's a dictionary

1375
01:30:41,340 --> 01:30:46,700
with an additional key in it which breaks everything. So we're going to drop our table

1376
01:30:46,700 --> 01:30:52,860
and we're going to try this again. So let's do db peps dot drop to get rid of it. And now I'm

1377
01:30:52,860 --> 01:31:00,140
going to add alter equals true. And alter equals true means when you're inserting this, if you find

1378
01:31:00,140 --> 01:31:05,580
a new column that you haven't seen before, alter the table to add that column. So running this

1379
01:31:07,100 --> 01:31:13,980
that worked. I've now got a peps table. I can run db peps dot schema with a print to see what

1380
01:31:13,980 --> 01:31:19,180
that there we go. That's the column. That's the table that we've just created. And if I

1381
01:31:20,220 --> 01:31:27,180
switch back over to data set, where's data set? It should be in the terminal.

1382
01:31:28,140 --> 01:31:31,980
I'm going to stop data set and I'm going to start it again and add peps dot db to it.

1383
01:31:32,460 --> 01:31:39,260
And now if we go back to here, what happened there?

1384
01:31:44,300 --> 01:31:48,060
Service unavailable. Has it crashed or is it just taking a while to start up?

1385
01:31:48,060 --> 01:31:53,500
Oh, path peps dot db does not exist. That's because it's actually in the notebooks folder.

1386
01:31:54,620 --> 01:31:55,260
There we go.

1387
01:31:55,820 --> 01:32:03,660
All right. Now we've got our peps. And so this right here is the database table that we just

1388
01:32:03,660 --> 01:32:09,180
created. It's called peps. It's got all of the columns that we do want it. You'll see that it's

1389
01:32:09,180 --> 01:32:14,860
actually got a bunch of weird columns towards the end, like bdf l delegate is apparently a column in

1390
01:32:14,860 --> 01:32:21,500
one of these things. We've got the pep ID is that primary key. And so if I want to go and find pep

1391
01:32:21,500 --> 01:32:27,660
20, I can say where pep equals 20. And there it is. There's the Zenith Python in our SQLite database.

1392
01:32:27,660 --> 01:32:32,380
I can get an as a JSON, I can get as a CSV. All of those features we had before were available.

1393
01:32:32,380 --> 01:32:39,260
But crucially, creating this table, all we did was effectively this one liner. We said

1394
01:32:39,260 --> 01:32:42,780
create a table called peps, insert these things, set the primary key,

1395
01:32:43,740 --> 01:32:47,500
add any columns that you need to add. That's the whole thing. That's done.

1396
01:32:47,500 --> 01:32:52,380
So most of the, I showed you work that I do with command line tools earlier on.

1397
01:32:52,380 --> 01:32:56,380
Anytime I'm doing anything more sophisticated, I do it in a Jupyter Notebook. I import SQLite

1398
01:32:56,380 --> 01:33:01,180
utils. I fire up a Jupyter Notebook and I use exactly this workflow I'm showing you here,

1399
01:33:01,180 --> 01:33:07,340
where you grab files from Git or you download JSON over HTTP or anything like that and just

1400
01:33:07,340 --> 01:33:14,460
start splatting things into these, almost treating the database itself like a dictionary

1401
01:33:15,180 --> 01:33:18,060
where each of the keys in the dictionaries will end up being a table.

1402
01:33:19,660 --> 01:33:24,860
This is an amazingly productive way of working with data because normally when I'm working with

1403
01:33:24,860 --> 01:33:30,380
a relational database, I spend 90% of my time sweating over the schema. I'm like, okay,

1404
01:33:30,380 --> 01:33:33,900
I need to know what columns I've got to advance and what types of I should have in those columns

1405
01:33:33,900 --> 01:33:39,180
and think about the primary keys and all of that. And here I'm skipping everything in terms of

1406
01:33:39,180 --> 01:33:44,860
database design and just saying, I've got an array of dictionaries, make a table. And it makes

1407
01:33:44,860 --> 01:33:50,780
a table. It derives the correct schema. All of that sort of tedious database creation work is

1408
01:33:50,780 --> 01:33:57,500
done for me. And now I've got, again, I've got an API, right? This is now an API for PEPs where I can

1409
01:33:57,500 --> 01:34:02,780
filter and search and do faceting. And actually let's do some faceting now. So there's a thing

1410
01:34:02,780 --> 01:34:11,820
called status, which out of our, what, 429 PEPs, 220 are final, 96 are rejected, 31 are deferred.

1411
01:34:11,820 --> 01:34:17,180
There's a type column. I'm going to fast on that. Standard track informational process.

1412
01:34:17,180 --> 01:34:24,460
There's a PEP that's an April Fool PEP, which you click on. And this joke hasn't aged very well.

1413
01:34:24,460 --> 01:34:30,300
This is from April 2009. It's a joke announcement that Guido van Rossum was retiring. I think he

1414
01:34:30,300 --> 01:34:34,300
actually did retire a few years after that. And then he came back again. But you know, this is

1415
01:34:34,300 --> 01:34:42,540
preserved forever in the Python enhancement proposals GitHub repository. Yeah. So it's very,

1416
01:34:42,540 --> 01:34:49,180
very little work to get from data in some shape or form to a database table that we can start doing

1417
01:34:49,180 --> 01:34:57,740
all of the fun stuff that we've seen so far. Oh, that's a great question. So the question is,

1418
01:34:57,740 --> 01:35:02,540
what do you do if you want to create a data frame from this? I will be honest, I don't use

1419
01:35:02,540 --> 01:35:07,740
pandas habitually enough to just run that straight off the top of my head, but let's have a go.

1420
01:35:07,740 --> 01:35:15,100
So if I do pip install pandas, I think pandas has a thing where you can do, what is it?

1421
01:35:15,100 --> 01:35:21,340
And put pandas as PD, PD dot. But you can actually do from SQL, can't you, in pandas?

1422
01:35:21,980 --> 01:35:25,660
Well, there's a CSV thing, but isn't there a pandas?

1423
01:35:29,020 --> 01:35:35,260
That's it. Read SQL. This is the one. This will do it. Okay. So I can say pandas

1424
01:35:36,940 --> 01:35:46,780
dot read SQL brackets, DB dot con. Oh no, it's the SQL first, isn't it? Select star from peps, DB dot

1425
01:35:46,860 --> 01:36:02,300
con. Yep. Yep. PD. Did that work? Yeah, there we go. There we go. So what I did there is

1426
01:36:05,020 --> 01:36:12,060
the pandas read, the PD dot read SQL method takes a SQL query and a SQLite database connection.

1427
01:36:12,060 --> 01:36:16,940
Our DB object isn't a SQLite database connection. It's a special SQLite utils one,

1428
01:36:16,940 --> 01:36:21,580
but DB dot con, which I don't know if it's documented. I should make sure that's documented.

1429
01:36:21,580 --> 01:36:29,020
DB dot con is that underlying SQLite query. So yeah, this is how you would get that same data

1430
01:36:29,020 --> 01:36:33,420
into pandas. And you could do it otherwise as well. Pandas, you can give it a list of JSON objects and

1431
01:36:33,420 --> 01:36:38,380
all of that kind of stuff. But I think this is probably the cleanest way to get from SQLite or

1432
01:36:38,380 --> 01:36:43,340
SQLite utils into pandas. And it's kind of neat that we, again, we started with that list of

1433
01:36:43,340 --> 01:36:48,460
objects. We round tripped them through that SQLite database and now we've got them in pandas again.

1434
01:36:48,460 --> 01:36:56,220
So that's kind of cool. So this bit here talks about the error. So we're going to do one more

1435
01:36:56,220 --> 01:37:03,660
trick here. Wouldn't it be neat if we had a proper search engine for our peps? Now SQLite, one of the

1436
01:37:03,660 --> 01:37:08,620
many, many features of SQLite is it actually has a really robust search engine built into the

1437
01:37:08,620 --> 01:37:15,980
database. It's called the SQLite FTS5 extension, where FTS stands for full text search. And this

1438
01:37:15,980 --> 01:37:22,300
is a shockingly capable piece of software. It's actually, there's an aspect where this is better

1439
01:37:22,300 --> 01:37:28,060
than what Postgres gives you, because Postgres has a very good, it's got full text search baked in,

1440
01:37:28,620 --> 01:37:33,420
but the Postgres full text search can only work based on the sort of information about individual

1441
01:37:33,420 --> 01:37:38,860
rows. So it can give you, like it can give you matches for a term and it can use stop words and

1442
01:37:38,860 --> 01:37:43,820
all of that kind of stuff. But Postgres doesn't have ranking built in. Like when you're using a

1443
01:37:43,820 --> 01:37:49,580
search engine like Elasticsearch or Lucene, the ranking algorithm takes into account the,

1444
01:37:50,620 --> 01:37:55,980
how common the term is in the whole corpus. And so if you search for something which is very rare

1445
01:37:55,980 --> 01:38:01,180
overall, but is super common in a few rows, you'll get those rows back. SQLite actually has that

1446
01:38:01,180 --> 01:38:09,260
baked in. It does full blown BM25 search ranking, which again puts it a little bit above Postgres

1447
01:38:09,260 --> 01:38:14,380
in terms of the sort of out of the box search for search capabilities. But anyway, what we're going

1448
01:38:14,380 --> 01:38:20,140
to do is we're going to set up a search engine. And I'm going to do it by saying dbpeps, that's our table,

1449
01:38:22,620 --> 01:38:30,540
dot configure FTS for configure full text search. I'm going to make it searchable by the title

1450
01:38:31,260 --> 01:38:37,420
and the body columns. So configure full text search and we feed it a list of those columns.

1451
01:38:38,220 --> 01:38:49,500
And if I run that, now if we run db.tables, db.tables, actually let's do db.tablenames.

1452
01:38:51,180 --> 01:38:51,900
Oh, hold on.

1453
01:38:51,900 --> 01:39:00,860
That's surprising, there should be another table in there.

1454
01:39:04,700 --> 01:39:05,500
What went wrong?

1455
01:39:09,580 --> 01:39:12,540
Oh, it said configure FTS, didn't I? Or did I?

1456
01:39:12,540 --> 01:39:23,900
I don't know. It should be dot enable FTS. So okay, I've run enable FTS. That took 0.3 seconds.

1457
01:39:23,900 --> 01:39:28,460
If I look at the list of table names, here we go, here we go. So now we've got that peps table that

1458
01:39:28,460 --> 01:39:35,100
we started with. We also have one, two, three, four, five new tables relating to this full text

1459
01:39:35,100 --> 01:39:39,420
search index that we've just created. You do not have to worry about the details of these tables

1460
01:39:39,420 --> 01:39:43,580
at all. They're sort of effectively an implementation detail of how SQLite full

1461
01:39:43,580 --> 01:39:49,180
text search works. But now in data set, if we refresh this page, it spots them and it adds a

1462
01:39:49,180 --> 01:39:54,540
search box. So this is a data set feature. It knows about full text search in SQLite and it knows how

1463
01:39:54,540 --> 01:39:59,420
to tell if a table's got it set up and it adds a search box to it. So now if we look for say generators

1464
01:40:00,940 --> 01:40:07,900
and run that search, there are 42 peps where the title of the body matches the search term

1465
01:40:08,860 --> 01:40:14,380
generators. And that looks about right to me. We've got range literals. Something I'm going to do here

1466
01:40:14,380 --> 01:40:21,020
is I'm going to hide the body column just because it's taking up a lot of space. So yeah, this looks

1467
01:40:21,020 --> 01:40:26,460
spot on to me. Coroutines generators, continuations, simple generators. We've just built a search engine

1468
01:40:26,460 --> 01:40:30,220
on top of Python peps and it was a single line of Python code to get that to happen.

1469
01:40:31,180 --> 01:40:35,420
There is one missing feature here. You'll notice that this is showing a search results

1470
01:40:35,420 --> 01:40:43,820
for generators sorted by that ID column. So 204, 205, 219, 220. It's a search engine. We kind of

1471
01:40:43,820 --> 01:40:49,100
want to sort by relevance. Data set doesn't have search by relevance as a built-in feature yet

1472
01:40:49,660 --> 01:40:53,100
and that's just because I haven't quite figured out how I'm going to build it. But what you can

1473
01:40:53,100 --> 01:40:59,980
do instead is you can run a raw SQL query that does that ranking. We'll do that. We'll go to

1474
01:40:59,980 --> 01:41:10,540
view and edit SQL. Stick that in and run this query. And this, oh, hold on. Actually, I'm going

1475
01:41:10,540 --> 01:41:15,820
to put rank first and get rid of the body column just so we can see that it worked.

1476
01:41:19,980 --> 01:41:24,940
There we go. Look at this. Asynchronous generators is the top result. Then generators,

1477
01:41:24,940 --> 01:41:32,620
attributes and exceptions. This column here is the SQLite scoring column for how relevant it

1478
01:41:32,620 --> 01:41:37,580
thinks that document is. We scroll down to the bottom. That looks about right to me. Asynchronous

1479
01:41:37,580 --> 01:41:42,700
IOS support presumably mentions generators but it's not as relevant to our search term as all of

1480
01:41:42,700 --> 01:41:48,060
those other things are. And as before, we can click hide on this and then send somebody else a link.

1481
01:41:48,060 --> 01:41:52,140
And now we've built a search engine and there's a search interface. They can type the search

1482
01:41:52,140 --> 01:42:00,060
query in here. They get the results back ordered by rank. The query here, I won't go into detail,

1483
01:42:00,060 --> 01:42:05,100
but you can see that the way this works is it's a join. So it's joining the peps column against

1484
01:42:05,100 --> 01:42:11,820
the peps FTS column on that magic and visible row ID thing. So when we created the search index,

1485
01:42:11,820 --> 01:42:17,740
we actually created it such that the row ID in the FTS table matches the row ID in the other table.

1486
01:42:17,740 --> 01:42:25,100
And then when you say where peps FTS match colon search, match is a magic SQLite operator

1487
01:42:25,100 --> 01:42:30,700
that does all of the work of running those full text queries and stemming things and all of that

1488
01:42:30,700 --> 01:42:37,260
kind of stuff. And then peps FTS isn't actually a real table. It's a virtual table that exposes

1489
01:42:37,260 --> 01:42:45,420
that relevant score right there. So it's one of those things. You can just trust me that this is

1490
01:42:45,420 --> 01:42:49,820
the weird incantation that you need to do relevance-based searching, but it does make

1491
01:42:49,820 --> 01:42:54,700
a sort of logical sense once you start digging into how the internals of this search system works.

1492
01:42:57,980 --> 01:43:03,820
Search exactly. Well, if you say peps FTS, so that's the name of the table,

1493
01:43:04,380 --> 01:43:10,940
it will search against all of the columns in that table that are, it's all of the columns in that

1494
01:43:10,940 --> 01:43:16,380
table that have search enabled on them. I think you can say peps FTS dot title.

1495
01:43:17,660 --> 01:43:21,420
How many results did we get? We got, I'm going to get rid of that limit as well.

1496
01:43:22,860 --> 01:43:29,580
So if we run this without a limit clause, we get back 42 results. If we say peps FTS dot title,

1497
01:43:30,780 --> 01:43:36,060
now we get back eight results. Yeah. So you can optionally tell it that you just want to match a

1498
01:43:36,060 --> 01:43:40,780
specific column. And now it's just the ones where the title column mentioned that keyword.

1499
01:43:40,780 --> 01:43:44,220
Whereas previously we were seeing everything where either the title column or the body column

1500
01:43:44,220 --> 01:43:52,540
mentioned it. Basically, this is if you want to build a really advanced search interface,

1501
01:43:52,540 --> 01:43:55,900
if you want to say to Pete, if you want to have actually in your UI something that says

1502
01:43:55,900 --> 01:44:01,900
search everything or just search the title or just search the author. I very rarely use it.

1503
01:44:01,900 --> 01:44:06,860
For the most part, I'll just run my search against all of the available columns,

1504
01:44:06,860 --> 01:44:09,100
but it's a capability that's there if you need it.

1505
01:44:11,980 --> 01:44:18,060
But yeah, so this is, so let's see what we've got to then. We started out with a, we had that

1506
01:44:18,060 --> 01:44:24,060
GitHub repository with all of the peps in it. We had to write a little bit of custom Python code.

1507
01:44:24,060 --> 01:44:29,580
This thing here, and again, I didn't write this. I cheated and I got chat GPT to write it for me

1508
01:44:29,580 --> 01:44:33,100
because it's the kind of code that I can't be bothered to write these days now that there are

1509
01:44:33,100 --> 01:44:41,660
robots that'll do it instead. If anyone's interested, I might be doing a open space

1510
01:44:41,660 --> 01:44:45,900
session later in the conference talking about all of the weird things you can do to get

1511
01:44:46,460 --> 01:44:50,940
GPT-4 to write code for you. This will be one of the examples I'll demonstrate then.

1512
01:44:50,940 --> 01:44:57,260
But yeah, so we did this. We looped through all of those files and we turned them into a list.

1513
01:44:58,060 --> 01:45:03,820
We inserted them to a database table. We enabled full text search. We basically ran three lines of

1514
01:45:03,820 --> 01:45:10,220
Python and now we've got a search engine against all of the Python peps. And that's, I do this kind

1515
01:45:10,220 --> 01:45:15,020
of thing honestly on a daily basis. If I've got any data about anything that I think looks

1516
01:45:15,020 --> 01:45:21,580
interesting, I'll probably slurp it down, fire it through SQLite utils and start exploring it in this

1517
01:45:21,580 --> 01:45:27,100
way. It's kind of my, it's one of those things when all you have is a hammer, everything's a nail.

1518
01:45:27,100 --> 01:45:31,820
When all you've got is SQLite utils, SQLite Python, Jupyter Notebooks and Datasette,

1519
01:45:32,380 --> 01:45:34,940
everything is a problem that fits into a SQLite database.

1520
01:45:41,180 --> 01:45:47,340
Exactly. Yeah, I'm not a very heavy pandas user because I find that I prefer SQL,

1521
01:45:47,340 --> 01:45:51,420
especially with Datasette giving me an interface on the top of it. It's just, it's what I do

1522
01:45:51,420 --> 01:45:58,620
habitually. And it's partly, I mean, it's mainly because what I build is almost a competitor to

1523
01:45:58,620 --> 01:46:02,220
pandas in terms of there are problems that you could solve with SQLite and Datasette or you could

1524
01:46:02,220 --> 01:46:06,620
solve them with pandas. And so I tend to lean towards the ones that I'm building. Like a lot of

1525
01:46:06,620 --> 01:46:10,620
the stuff I'm showing you here, you could solve very effectively in pandas and you'd be, it would

1526
01:46:10,620 --> 01:46:18,460
be a sensible thing for you to use it for that. Why do I need, oh, if I'm using SQL instead,

1527
01:46:19,420 --> 01:46:29,180
what do you mean? Why do I need Python? It's for the manipulation. The bit that you couldn't do in

1528
01:46:29,180 --> 01:46:34,380
SQL very easily is this bit here, the bit where you take these text files on disk and turn them

1529
01:46:34,380 --> 01:46:40,940
into a format and then get them in. So Python is my data manipulation and sort of cleaning layer,

1530
01:46:40,940 --> 01:46:48,300
and then SQL is how I do the querying at the end of it. Let's publish what we've done to the

1531
01:46:48,700 --> 01:46:54,460
internet. So I've been playing around in GitHub Codespaces and a frustrating thing about GitHub

1532
01:46:54,460 --> 01:47:02,060
Codespaces is that it is private. So if I try and visit this URL, here we go. So I try to visit that

1533
01:47:02,060 --> 01:47:06,060
in a different browser and it says, now you've got to sign in to GitHub for this. Because GitHub,

1534
01:47:06,780 --> 01:47:09,820
they're giving us free development environments, but they're not giving us a free hosting

1535
01:47:09,820 --> 01:47:17,340
environment for us to publish our stuff with the world. But Datasette's entire reason of existing

1536
01:47:17,340 --> 01:47:21,740
is to help people publish data. It's all about trying to get data into a format where you can

1537
01:47:21,740 --> 01:47:27,180
share it with the rest of the world as easily as possible. So I'm going to try something. I'm going

1538
01:47:27,180 --> 01:47:35,660
to publish this Peps database that we created, peps.db. Oh, it's in notebooks, isn't it?

1539
01:47:37,500 --> 01:47:43,900
So I've got a 10 megabyte database and I'm going to publish it to the world. And the way I'm going

1540
01:47:43,900 --> 01:47:55,900
to do that is I'm going to use, aha, interesting, npm install-g-versel. Is it Versel or Versel CLI?

1541
01:47:58,060 --> 01:48:05,900
Versel, just Versel, npm install-g-versel. So I'm going to use a hosting platform called Versel.

1542
01:48:05,900 --> 01:48:11,660
And Datasette works with a whole bunch of different hosting platforms. The thing I like about Versel

1543
01:48:11,660 --> 01:48:17,820
is that it's the least steps to get it set up. And this is something which I would invite people

1544
01:48:17,820 --> 01:48:22,940
in the room to try doing yourself. Even if you haven't used Versel before, I'm crossing my fingers

1545
01:48:22,940 --> 01:48:29,260
and hoping that this will just work for you anyway. So what I've done is I've done npm install

1546
01:48:29,260 --> 01:48:36,380
Versel and the handout has all of the other ways that you can install this. There we go. I've got

1547
01:48:36,380 --> 01:48:41,740
the Versel command. I then have to write, type Versel login and that'll give me options to sign

1548
01:48:41,740 --> 01:48:49,500
in. I'm going to sign in with GitHub. It wants me to visit this webpage where I think I'll get to

1549
01:48:49,500 --> 01:48:54,140
authenticate and go back again. Oh, it's got a little code for me to copy and paste in.

1550
01:48:55,260 --> 01:49:00,380
Okay, there we go. I have now authenticated with my Versel account. And so now I can install another

1551
01:49:00,380 --> 01:49:09,100
plugin, dataset install dataset publish Versel. And this is a plugin that teaches dataset how to

1552
01:49:09,100 --> 01:49:15,420
deploy things onto the Versel environment. And now I could say dataset publish Versel notebook

1553
01:49:15,420 --> 01:49:22,140
slash peps.db dash dash. I have to give it a project name. I'm going to call it project peps.

1554
01:49:25,260 --> 01:49:29,740
There we go. So what this command is doing is it's uploading my 10 megabyte SQLite database

1555
01:49:29,740 --> 01:49:36,940
to Versel. It's then going to deploy a copy of the full dataset web application to Versel with,

1556
01:49:36,940 --> 01:49:41,580
and it gives me this little link here I think. This will actually give me a progress bar so I

1557
01:49:41,580 --> 01:49:48,300
can see what's going on. Here we go. It's building. So it's installing some stuff. It's going to run

1558
01:49:48,300 --> 01:49:54,540
pip install and install dataset. It's generating build outputs. It's deploying it to Washington,

1559
01:49:54,540 --> 01:50:01,340
DC for some reason. It's going to run some checks, assign some domains, and it's done.

1560
01:50:01,340 --> 01:50:08,060
And so now if I click on peps-simonw.versell.app, this is that database that we just created

1561
01:50:09,100 --> 01:50:13,420
complete with its search engines so I can search for generators again right here. But it's now

1562
01:50:13,420 --> 01:50:18,380
running live on the internet. It's on peps-simonw.versell.app. And you can actually,

1563
01:50:18,380 --> 01:50:23,020
you can point custom domain names at it or assign aliases. There are a bunch of flexibility there.

1564
01:50:23,820 --> 01:50:31,340
But that was kind of the, that was, I mean, that was what? About 30 seconds including installing

1565
01:50:31,340 --> 01:50:36,060
the software to get a database that I've been working just locally in my own environment and

1566
01:50:36,060 --> 01:50:41,340
put it online where other people can use it. And again, this is an API. So if you want to build

1567
01:50:41,340 --> 01:50:48,540
a application right now, the second that runs, that needs a JSON API for searching all of the

1568
01:50:48,540 --> 01:50:53,420
Python paps, that's what this is. I'm going to stick shape equals array on the end because

1569
01:50:53,420 --> 01:51:01,020
it gives you a nicer format back. Yeah, this is every pep that matches generators as a JSON

1570
01:51:01,020 --> 01:51:06,460
API that anyone in the world can now start using. A neat thing about Vercell is that under the hood,

1571
01:51:06,460 --> 01:51:12,540
it's based on Amazon Lambda functions. So it's very much a scale to zero thing where if your

1572
01:51:12,540 --> 01:51:17,260
project isn't getting any traffic, it's not costing anyone any money to use. Vercell themselves have

1573
01:51:17,260 --> 01:51:23,180
a very generous free tier. So it's effectively a free hosting plan for structured data. There is

1574
01:51:23,180 --> 01:51:29,340
one catch, which is that on Vercell, the maximum file size is I believe 50 megabytes. So if you're

1575
01:51:29,340 --> 01:51:34,060
doing, if you've got a gigabyte database, this won't work for you. If you do have a gigabyte

1576
01:51:34,060 --> 01:51:38,780
database, Google Cloud Run works more or less the same way. It can handle up to a gigabyte quite

1577
01:51:38,780 --> 01:51:45,020
easily. But yeah, I think this is kind of cool. This is, this was, this was actually the feature

1578
01:51:45,020 --> 01:51:49,580
that inspired me to create data set in the first place is I'd been looking at Vercell, which back

1579
01:51:49,580 --> 01:51:55,020
then was called Zite Now, it was before their rebranding and thinking, okay, this is kind of

1580
01:51:55,020 --> 01:52:00,700
interesting that this is, that you can host applications on this. But is that, can this solve

1581
01:52:00,700 --> 01:52:05,020
that problem I had beforehand back from when I was working at newspapers of wanting a really

1582
01:52:05,020 --> 01:52:11,580
good way to deploy, to deploy structured data in a way that people could start using it? And yeah,

1583
01:52:11,740 --> 01:52:15,980
it works really well. I've deployed hundreds of applications to Vercell in this way. They've

1584
01:52:15,980 --> 01:52:22,140
cost me like at the most $20 a month, I think I'm paying for a Vercell plan with hundreds of

1585
01:52:22,140 --> 01:52:26,220
different projects on it. It's a really, really powerful way of sharing data in that way.

1586
01:52:29,260 --> 01:52:34,940
Okay. So yeah, that's, so as I mentioned, there are other options. There's a data set out of the box

1587
01:52:34,940 --> 01:52:39,580
can publish to both Google Cloud Run. Oh, that, those two, there's the wrong way around, but it

1588
01:52:39,580 --> 01:52:44,140
can publish to Heroku, it can publish to Google Cloud Run. Heroku used to be a great option and

1589
01:52:44,140 --> 01:52:49,820
they canceled their free plan and so now it's a lot less exciting. There's my favorite hosting

1590
01:52:49,820 --> 01:52:54,860
provider was Fly. I built a plugin called data set published Fly that can push things up there.

1591
01:52:54,860 --> 01:52:59,580
And again, because this is based around the plugin system, if there's a hosting provider that you

1592
01:52:59,580 --> 01:53:03,820
like that has good API driven hosting, you can write a plugin for it.

1593
01:53:10,220 --> 01:53:14,780
Yes.

1594
01:53:17,340 --> 01:53:23,340
That's when I'm running the deploy, when you run data set publish, it actually, it bundles the

1595
01:53:23,340 --> 01:53:29,820
database and the application together. So it's kind of, it's actually a complete cheat. And I

1596
01:53:29,820 --> 01:53:35,100
was going to talk about this later on, but I'll talk about this right now. The Vercell and Amazon

1597
01:53:35,100 --> 01:53:40,620
Lambda and Google Cloud Run, they're all designed as stateless hosting providers. Like they will run

1598
01:53:40,620 --> 01:53:45,660
code for you on the internet, but if you want a database, they're no good. Like they will tell

1599
01:53:45,660 --> 01:53:51,020
you, yeah, you have to pay extra for Amazon RDS or Google, whatever Google's database thing is,

1600
01:53:51,020 --> 01:53:56,140
or whatever. Because hosting code that doesn't write to disk, that doesn't make any changes is

1601
01:53:56,140 --> 01:54:00,540
really easy. You wrap up a little container, you launch it when a new request comes in, you

1602
01:54:00,540 --> 01:54:07,020
shut it back down again, you scale it by running multiple copies. It's very, very easy in 2023

1603
01:54:07,020 --> 01:54:13,820
to host a stateless application online. Stateful applications with disk access and backups and so

1604
01:54:13,820 --> 01:54:21,020
on, those are expensive and complicated. The big realization I had is that for the stuff that I do,

1605
01:54:21,020 --> 01:54:26,540
I only care about read only data. Like all of these things right here, none of them are accepting

1606
01:54:26,540 --> 01:54:32,700
rights at all. And if it's read only, you can essentially abuse these free hosting providers

1607
01:54:32,700 --> 01:54:39,020
by sticking up a full database and saying, yeah, it's SQLite, it's just built in, it runs in read

1608
01:54:39,020 --> 01:54:44,780
only mode. So effectively you're abusing their tools that were supposed to be for hosting little

1609
01:54:44,780 --> 01:54:50,300
simple applications, and you're sticking five megabytes of binary SQLite data in the container

1610
01:54:50,300 --> 01:54:54,780
with all of the other stuff, and it just works. And this is, I'm going to talk about this at length

1611
01:54:54,780 --> 01:55:00,700
in a moment, I call it the baked data pattern because you're baking data into your application.

1612
01:55:00,700 --> 01:55:05,980
But yeah, this is the sort of perfect example of why that's an interesting pattern. And with this,

1613
01:55:06,620 --> 01:55:11,900
a new PEP is published maybe once or twice a month. You just redeploy the whole thing.

1614
01:55:13,420 --> 01:55:19,740
I have many, many projects which they deploy once a day because that's often enough to have the data

1615
01:55:19,740 --> 01:55:24,780
that they need. And yeah, I don't worry about trying to update them in place. I just overwrite

1616
01:55:24,780 --> 01:55:30,220
the application with a completely fresh deploy with a new copy of the database with any changes baked

1617
01:55:30,220 --> 01:55:39,020
into that. Where did we get to? So yeah, lots of other publishing options. The dataset documentation

1618
01:55:39,020 --> 01:55:46,540
has extensive coverage of this stuff as well. I'm going to switch gears entirely and talk about a

1619
01:55:46,540 --> 01:55:54,220
very, very different way of running datasets. This is a sort of experiment I did last year,

1620
01:55:54,780 --> 01:56:01,180
again, kind of as a joke, right? So I was looking at, so there's this amazing project

1621
01:56:01,180 --> 01:56:06,140
Piodide which lets you run Python entirely in a web browser by compiling the Python interpreter

1622
01:56:06,140 --> 01:56:10,940
web assembly and running it in the browser. And then this project called JupiterLight is, I think,

1623
01:56:10,940 --> 01:56:18,300
one of the most incredible pieces of engineering I've ever seen. This right here is the full

1624
01:56:18,300 --> 01:56:24,540
Jupiter Notebook stack, like everything that the Jupiter Notebooks do on your computer. But again,

1625
01:56:24,540 --> 01:56:30,460
running purely in the browser. This is astonishing to me. This is running Python's web assembly,

1626
01:56:30,460 --> 01:56:35,660
all of Jupiter, Pandas is in there, NumPy and data frames and Matplotlib and all of this stuff.

1627
01:56:36,300 --> 01:56:41,740
And I thought dataset is a lot simpler than that. Wouldn't it be interesting if dataset itself

1628
01:56:41,740 --> 01:56:47,660
could run inside the browser? And again, it was kind of a joke because dataset is very opinionated

1629
01:56:47,660 --> 01:56:52,620
in that it's not a JavaScript application. There's very little JavaScript in dataset itself. There's

1630
01:56:52,620 --> 01:56:59,180
like some syntax highlighting stuff here. But when you click sort in dataset, it reloads the

1631
01:56:59,180 --> 01:57:04,780
whole page. Everything is just boring old HTML because it's simpler and it's faster to load and

1632
01:57:04,780 --> 01:57:09,100
it's got better accessibility and all sorts of reasons that I built it in that way. So I thought

1633
01:57:09,100 --> 01:57:15,180
it would be funny if I could run a pure server-side web application entirely in the browser using web

1634
01:57:15,180 --> 01:57:20,700
assembly. And so I built this thing called Dataset Light. And actually what I'm going to do is I'm

1635
01:57:20,700 --> 01:57:30,620
going to pop open, I'm going to pop open my browser network tab so you can see what this thing does.

1636
01:57:31,500 --> 01:57:38,140
And now if I go to light.dataset.io, you can see that it's pulling down a bunch of stuff.

1637
01:57:38,140 --> 01:57:46,540
Most excitingly, it just downloaded two megabytes of pyodides.asm.js and five megabytes of pyodide.asm.data.

1638
01:57:46,540 --> 01:57:51,180
That's the Python interpreter compiled to web assembly. That's the Python standard library,

1639
01:57:51,180 --> 01:57:57,180
I believe, compiled to web assembly and such like as well. And so it downloaded those two things

1640
01:57:57,820 --> 01:58:07,020
and then it hit the Python package index and ran a pip install of Dataset itself. And that's not

1641
01:58:07,020 --> 01:58:10,460
actually showing up there. Let's see if I, maybe it's cached. I'll try refreshing the page.

1642
01:58:12,380 --> 01:58:18,540
Loading setup tools. Oh, that's surprising. I'm not seeing it. Well, you'll have to take

1643
01:58:18,540 --> 01:58:23,100
my word for it. This actually does a pip install in your browser and pulls the latest data set and

1644
01:58:23,100 --> 01:58:27,740
its dependencies in from the Python package index and now we've got Dataset. And again,

1645
01:58:27,740 --> 01:58:33,580
it's full Dataset. It's got all of the features you'd expect. It's got faceting and it's got the

1646
01:58:33,580 --> 01:58:38,700
ability to run SQL queries and all of that kind of stuff. And if you look at the URL, you'll see

1647
01:58:38,700 --> 01:58:44,620
that it's, there's a little fragment hash in it and then everything after that point is the regular

1648
01:58:44,620 --> 01:58:50,220
Dataset URLs that you'd be using. So I built this, like I said, as a bit of a joke. I thought it would

1649
01:58:50,220 --> 01:58:55,260
be funny. And I also thought nobody is going to use this because it has to load nearly 10 megabytes

1650
01:58:55,260 --> 01:58:59,980
of stuff before, you sit there with a loading spinner for quite a while before this becomes

1651
01:58:59,980 --> 01:59:06,060
usable. I forgot that these days your average React app is like five megabytes of stuff. So,

1652
01:59:06,060 --> 01:59:10,780
you know, waiting for a few extra megabytes to get a full Python interpreter isn't actually nearly

1653
01:59:10,780 --> 01:59:15,820
as restrictive as I thought it was going to be. And it's also, but the best thing about this

1654
01:59:15,820 --> 01:59:21,180
is that this is genuinely serverless and that there is no server-side code running anywhere

1655
01:59:21,180 --> 01:59:25,980
at all. This entire thing you're seeing is just static HTML, some JavaScript, some binary web

1656
01:59:25,980 --> 01:59:31,660
assembly files. Nothing can break about this. There is no server to fill up with log files or

1657
01:59:31,660 --> 01:59:35,260
crash or anything like that. It's actually being served off GitHub pages. So it doesn't even cost

1658
01:59:35,260 --> 01:59:41,020
me any money, but it gives you that full interactive experience. And so I ended up building a couple of

1659
01:59:41,020 --> 01:59:47,260
extra features for it. I built a feature where you can give it the URL to a CSV file or a JSON

1660
01:59:47,260 --> 01:59:53,340
file or a SQLite database. And if your browser can fetch it, it will fetch that file in and

1661
01:59:53,340 --> 01:59:59,340
load it into Dataset Lite directly. And that turns out to suddenly be really, really useful.

1662
02:00:00,620 --> 02:00:07,980
So I'll show you a couple of examples. Let's skip that one. We'll go straight to this one down here.

1663
02:00:08,940 --> 02:00:16,380
So 538, the blog, does this wonderful thing where they publish their underlying data on GitHub.

1664
02:00:19,020 --> 02:00:24,860
Let's do, here we go. It's the 538 slash data repository. And they've got one of the files in

1665
02:00:24,860 --> 02:00:32,780
here is called fight songs. And it's data about fight songs from various American college university

1666
02:00:33,500 --> 02:00:40,060
and what year they were written and all of that kind of stuff. So this is a CSV file. If you copy

1667
02:00:40,060 --> 02:00:47,020
and paste that CSV URL and hit the load CSV button in Dataset Lite, paste it in, Dataset will fire up

1668
02:00:47,020 --> 02:00:52,700
again. It will fetch that data from GitHub directly and it'll load it to a table. So now we've got that

1669
02:00:52,700 --> 02:00:59,740
Dataset interface on top of data that was fed in from a CSV file via a URL. And I can do things like

1670
02:00:59,740 --> 02:01:06,700
facet by official song, yes, no, student writer. I can run SQL queries, all of that kind of stuff.

1671
02:01:06,700 --> 02:01:10,780
And again, it's also bookmarkable. So if I send that link to anyone else in the world,

1672
02:01:10,780 --> 02:01:15,740
their browser will install Dataset in Python and get a Python to running and download CSV file and

1673
02:01:15,740 --> 02:01:23,020
load it to SQL. All of that bizarre crazy junk in about 15 seconds and then they can start

1674
02:01:23,020 --> 02:01:29,420
interacting with the data in the same way. So I used to spend a lot of time building projects

1675
02:01:30,060 --> 02:01:35,420
download some data, put it in SQL and publish it to Versel or to Cloud Run as a dedicated

1676
02:01:37,100 --> 02:01:42,540
project. These days, most of the time when I've got an ad hoc data project, I'll do it by gathering

1677
02:01:42,540 --> 02:01:48,380
the CSV data or the JSON data, sticking it in GitHub gist and then linking to a Dataset Lite

1678
02:01:48,380 --> 02:01:55,580
instance that loads that data from GitHub from gist. The reason I'm always using GitHub gist and

1679
02:01:55,580 --> 02:02:03,500
GitHub for this is that JavaScript can only fetch files from servers that have a special HTTP cause

1680
02:02:03,500 --> 02:02:09,340
header allowing that access. Most hosting doesn't have this. So if you feed this URL to most of the

1681
02:02:09,340 --> 02:02:13,500
things on the internet, it'll just give you an error. But GitHub do have that header turned on

1682
02:02:13,500 --> 02:02:19,180
for all of the content in their repositories and in their gist. So GitHub is effectively the world's

1683
02:02:19,180 --> 02:02:25,500
quickest cause friendly hosting provider for any kind of data that you want to throw in there.

1684
02:02:25,820 --> 02:02:33,980
I'll show you one more example of this. That's a great question. Is there an attribution thing?

1685
02:02:33,980 --> 02:02:41,740
There is not and there should be. So Dataset itself has quite has attribution built in as a

1686
02:02:41,740 --> 02:02:48,540
metadata mechanism where you can configure it to show data source about license information.

1687
02:02:48,540 --> 02:02:52,620
Dataset doesn't have that yet because I'm lazy and I haven't built it and I should build it.

1688
02:02:52,620 --> 02:02:58,700
It's kind of rude that I've not done that yet. So I'm going to show you a project I did a few

1689
02:02:58,700 --> 02:03:06,700
days ago using Dataset Lite. One of my hobbies is digging into the training data behind weird

1690
02:03:07,340 --> 02:03:13,580
AI models to try and understand how they work. So this new training set came out two days ago

1691
02:03:13,580 --> 02:03:19,980
called Red Pajama and they basically released 2.6 terabytes of data that you can use to train

1692
02:03:20,060 --> 02:03:25,900
language model like all of Wikipedia's in there and all of Archive and lots of GitHub data and

1693
02:03:25,900 --> 02:03:30,780
Stack Exchange all of this stuff. And so I wanted to do a little bit of an investigation about this

1694
02:03:30,780 --> 02:03:36,380
data but I didn't want to download 2.7 terabytes of stuff from the internet. The way they released

1695
02:03:36,380 --> 02:03:44,540
that is they released a text file with 2,000 URLs in and each of these URLs is a one gigabyte blob

1696
02:03:44,540 --> 02:03:52,300
of data. So if you want all the data you run WGET and you download all 2,000 URLs and you take up

1697
02:03:52,300 --> 02:03:57,500
2.7 terabytes of disk space. But I realized that what you can do instead is you can do an HTTP

1698
02:03:57,500 --> 02:04:03,740
head request against each of those URLs to get back just the size in bytes of each one. And that

1699
02:04:03,740 --> 02:04:07,980
you can run in 15 seconds. You can knock up a quick script, it'll churn through all 2,000,

1700
02:04:07,980 --> 02:04:16,540
get their sizes and now you've got URLs and sizes. So what I did then is I

1701
02:04:20,620 --> 02:04:25,980
turned that into a JSON file. Where is it?

1702
02:04:30,140 --> 02:04:32,140
This JSON file here.

1703
02:04:32,140 --> 02:04:47,580
So yeah, I dumped that out into this JSON file. So this JSON file has 2,000 records in and each one has the URL and the size and then I made it in megabytes and gigabytes as well just because I was feeling lazy.

1704
02:04:47,580 --> 02:04:53,980
And it's got the path and then I split out the top folder because that tells you what the source of that data was.

1705
02:04:54,540 --> 02:05:00,220
And so now I've got that gist. I can go to data set light. I can feed it in that URL.

1706
02:05:01,580 --> 02:05:13,420
And now I've got a table. And so this is very, very quick route from a list of those files to a list of those files in those sizes to a JSON gist. And now it's an interface that I can start playing with.

1707
02:05:13,420 --> 02:05:22,700
And I can do things like facet by top folder and see that in this data, Wikipedia was a single file because they put all of it in one Wikipedia file.

1708
02:05:23,100 --> 02:05:36,620
C4 and common crawl are the two most represented things in that list of data. Again, I can start with from raw data to starting to tell stories and understand that data in as few steps as possible is the goal here.

1709
02:05:36,620 --> 02:05:43,580
And I'll use this demo again in a moment when we start talking about some advanced SQL queries.

1710
02:05:44,300 --> 02:05:56,700
But yeah, so data set light is actually it started as a joke. It's now my default solution for when I want to start publishing and working with anything that's sort of, you know, a few thousand records, anything small.

1711
02:05:56,700 --> 02:06:04,540
I'll do it in a gist and I'll load that into data set light. I won't even have to think about provisioning servers or deploying a server side Python web application anyway.

1712
02:06:04,540 --> 02:06:05,180
So, yeah.

1713
02:06:07,660 --> 02:06:18,620
Oh, one last feature that's kind of fun. Data set light supports plugins. You can construct when you construct a URL to data set light, you can include question mark install equals.

1714
02:06:18,620 --> 02:06:25,500
And then pass it the name of a plugin and you can do that multiple times. You can say install equals this and install equals that.

1715
02:06:25,580 --> 02:06:31,420
And it will then on startup, it will pip install that plugin. And so now we've got a new feature.

1716
02:06:31,420 --> 02:06:40,940
Here's that the copyable plugin I showed you earlier on. So again, if I want latech from this test table right here, I can do that.

1717
02:06:40,940 --> 02:06:48,860
I can do that directly using the use just by passing install equals data set copyable in the URL. I think the here we go.

1718
02:06:48,940 --> 02:06:56,300
The further reading documentation here includes links to a blog entry where I talk about how the plugin support works and all of that kind of stuff.

1719
02:06:56,300 --> 02:07:04,060
But really the goal here, this is an ecosystem thing, right? I've got data set. I've got people building plugins for data set.

1720
02:07:04,060 --> 02:07:17,100
If data set light can also benefit from those plugins, then it just sort of multiplies the value that the ecosystem can provide by making sure that bits written for this aspect of it work here and all of those kinds of things.

1721
02:07:18,860 --> 02:07:31,180
Yeah, so the question is how much data can your browser hold? So I have not tested the upper limits of this yet.

1722
02:07:31,180 --> 02:07:39,180
I think it's gigabytes. And the reason I think it's gigabytes is that the other day,

1723
02:07:39,180 --> 02:07:44,780
somebody released, so I'm fascinated by large language models, you know, chat, GPT and all of that kind of stuff.

1724
02:07:44,860 --> 02:07:57,420
A few days ago, somebody released this thing called Web LLM, which runs a chat GP style model entirely in your browser using not just WebAssembly, it's Web GPU, which is a new graphics card thing that Chrome's got.

1725
02:07:57,420 --> 02:08:05,100
And that thing, I ran it on my browser and it was a two gigabyte model file that was loaded into the browser and it worked.

1726
02:08:05,100 --> 02:08:14,700
So I've got to believe that two gigabyte SQLite file would work fine as well, which is kind of it's I mean, this this laptop has 64 gigabytes.

1727
02:08:14,700 --> 02:08:18,780
Of RAM, so you know, a two gigabyte browser tab isn't actually a big deal for it.

1728
02:08:21,100 --> 02:08:24,540
I'd assume so. Yeah, I mean, it's the download speed is the bigger problem.

1729
02:08:24,540 --> 02:08:28,860
But to be honest, my internet connection at home will load two gigabytes in like a minute or two.

1730
02:08:28,860 --> 02:08:35,580
So it's feasible. You know, it's it's it's it's all it's surprising how much you can get done in the browser.

1731
02:08:38,780 --> 02:08:41,580
OK, let's skip to advanced SQL.

1732
02:08:45,180 --> 02:09:03,260
We're going to talk about and I'm not going to teach you everything there is to know about SQL in the next sort of 20 minutes, but hopefully we can cover some of my favorite advanced SQL SQL features with enough details that you can sort of dive into that dig into them more later on.

1733
02:09:03,260 --> 02:09:10,940
The first of these is I mean, it's hardly even advanced, but it's something that's that it sort of helps build things up later on.

1734
02:09:11,020 --> 02:09:13,580
And that's SQL aggregations.

1735
02:09:13,580 --> 02:09:15,900
We've seen select queries, select queries are very straightforward.

1736
02:09:15,900 --> 02:09:22,620
And aggregation is a way of calculating, so doing calculations across groups of rows together.

1737
02:09:22,620 --> 02:09:24,220
The easiest example is.

1738
02:09:26,220 --> 02:09:37,740
Here we're looking at the executive terms table we saw earlier, so we're saying every executive, every presidential term, I want to know for each party how many presidential terms that they get.

1739
02:09:37,740 --> 02:09:44,220
So Democrat Party got five Democratic Party got 21 Democratic Republican, who I've never heard of, had seven.

1740
02:09:46,060 --> 02:09:58,620
But the key pattern here is you select you do a group by the end to say group those rows where so that into sort of chunks where on that matching on that matching column.

1741
02:09:58,620 --> 02:10:04,860
And then you can do aggregate functions like in this case count star to do a calculation across those individual groups.

1742
02:10:05,820 --> 02:10:09,900
There are a whole bunch of aggregate functions you can use count star is the most common.

1743
02:10:09,900 --> 02:10:19,260
The other ones that I use is that I tend to use a max for the maximum value min for the minimum value and some to add things up.

1744
02:10:19,260 --> 02:10:27,100
And I wanted to show you data set light first, because when we when I was exploring that training data, one of the.

1745
02:10:29,100 --> 02:10:30,220
Something's going wrong.

1746
02:10:30,220 --> 02:10:34,060
Wrong. Oh, look at this. There's ampersands where there should be.

1747
02:10:35,500 --> 02:10:38,780
Where there should be and yeah there's an escaping bug.

1748
02:10:39,820 --> 02:10:43,740
Okay, but yeah so here we go so here I'm running a.

1749
02:10:44,660 --> 02:10:59,980
Aggregation across the top folders column here and I'm summing the total number of gigabytes as total GB so I can see that Wikipedia responsible for 111 gigabytes in the data across just one file.

1750
02:11:00,460 --> 02:11:08,540
Stack exchanges 74 gigabytes in one file GitHub is 212 gigabytes across 98 files these are useful things to be able to calculate so.

1751
02:11:09,660 --> 02:11:13,900
it's worth exploring aggregates and knowing what they can do and how you can apply them.

1752
02:11:15,900 --> 02:11:19,900
I'm going to show you a much more fun one of these.

1753
02:11:19,900 --> 02:11:34,060
So sequel there's a concept in sequel called sub queries and I missed this for years because the first like 10 years of my career I just worked with my sequel and my sequel versions prior to I forget what.

1754
02:11:34,060 --> 02:11:42,220
Didn't support this feature so I didn't know that this was something you can do of course sequel light and postgres and all of the other sequels have really good support for this.

1755
02:11:42,380 --> 02:11:49,340
So what this lets you do is it lets you do things like this, you can say select something from so and so where repo in.

1756
02:11:49,820 --> 02:11:59,340
And then normally when you're doing an inquiry you give it like a comma separated list of values, but here instead i'm giving it another select query so i'm saying where the repository is in.

1757
02:11:59,660 --> 02:12:03,100
The list of ideas from right repos where full name is in.

1758
02:12:03,660 --> 02:12:14,220
The list of repos from plug-in repos so what i'm going to do is grab all of that run that here i'm actually going to run those as separate steps first.

1759
02:12:15,100 --> 02:12:23,100
So if we say select repo from plug-in repos you see you get back a list of names of repositories just as strings.

1760
02:12:23,980 --> 02:12:36,860
If I then do select ID from repos where full name in that full name has now been turned into an ID and actually let's do ID comma star.

1761
02:12:37,500 --> 02:12:45,500
Just to show you what else is going on in there so there's that name and i'm saying I want the IDs.

1762
02:12:45,900 --> 02:12:55,340
For the things in this where the name is in that other table this is the kind of thing that you could normally do with the join, but I actually find that joins across multiple tables are quite difficult to follow.

1763
02:12:55,580 --> 02:13:00,620
Like I feel as a like reading a sequel query anytime I get to a join I have to think about it.

1764
02:13:01,180 --> 02:13:10,700
If i'm using these bizarre weird nested sub queries I can actually follow this a lot easier I can say okay I get this it's the name of the repos list of those.

1765
02:13:11,100 --> 02:13:17,260
IDs a list of those and then this is select the URL from releases where the repo is in that.

1766
02:13:18,300 --> 02:13:26,060
And when you run this query what i'm getting back is the URLs to the releases the most recent release because it's ordered by created.

1767
02:13:26,780 --> 02:13:34,780
The most recent releases of the repositories that are in my list of plug-in repositories.

1768
02:13:34,780 --> 02:13:40,060
So this is the data set website has two types of things it cares about it cares about plugins and it cares about tools.

1769
02:13:40,700 --> 02:13:44,700
I switch that tools I will see maybe it's tool repos.

1770
02:13:46,780 --> 02:13:54,060
Now i'm seeing like swarm to sequel light and apple notes to sequel light and DB to sequel light and so forth if I if I do the same thing from plug-in repos.

1771
02:13:54,620 --> 02:14:04,460
I get it back as the latest releases of my plugins so it's a little bit of a convoluted example I probably should have broken this down into into a simpler example first.

1772
02:14:09,180 --> 02:14:09,820
what's the question.

1773
02:14:10,700 --> 02:14:14,860
I can do select star.

1774
02:14:21,580 --> 02:14:28,860
No no that that would be a neat feature that's a really good idea for a feature yeah if you could select just a portion of this and run it on its own.

1775
02:14:30,220 --> 02:14:38,060
But yeah that's not that's not something it can do so that's one type of sub query the other type of sub query I think is even cooler.

1776
02:14:38,220 --> 02:14:45,900
And that's this type here so what i'm doing here is i'm saying for I want for each of my.

1777
02:14:47,500 --> 02:14:55,580
Where are we for me each of my repositories I would like to get back the name of the repository and I want to get the url of the latest release in that repository.

1778
02:14:55,580 --> 02:15:03,900
But the latest latest releases is over in some other table I can actually in my select clause itself say I want back full name and then I want.

1779
02:15:04,540 --> 02:15:11,980
This complicated query here it has to be limit one because otherwise it'll actually what does happen if you get rid of that limit one let's have a look.

1780
02:15:14,620 --> 02:15:17,500
Oh I was expecting that to break but it didn't.

1781
02:15:19,500 --> 02:15:30,540
But the key idea is that for each of those repositories that we're listing we're running a second sequel query inside of the loop to try and fetch additional related data about it.

1782
02:15:31,100 --> 02:15:42,780
Looking at this my instinct is that this looks like a terrible idea because you're surely you're telling it to pull back 200 results and then do another 200 sequel queries on top of that to pull in extra details.

1783
02:15:42,780 --> 02:15:57,020
I believe sequel I optimizes that to a certain extent but more to the point if you've got a database table with 200 rows in it there is nothing you can do that will cause it to take longer than a fraction of a millisecond to run those queries.

1784
02:15:57,100 --> 02:16:05,420
So often I will deliberately write very inefficient queries of this kind of shape because I know that for the size of data I'm working with it honestly just doesn't matter.

1785
02:16:06,940 --> 02:16:25,820
When I was when I worked for a larger company I had access to hive and presto and various like big data warehouse things and a fun thing I found about those is you can actually write very inefficient sequel against them too because their whole reason for existing is to break up complex crews into like map reduces and all of those kinds of operations.

1786
02:16:25,820 --> 02:16:45,980
So actually inefficient queries work about work well against the big data warehouses as well and then for I would never do this in production code for like a high traffic web application that's where I care about optimization but for this kind of data size I think you can do a lot of cool stuff just with with laughably inefficient sort of sequel loops like this.

1787
02:16:46,940 --> 02:16:58,300
So yeah I spend I often default to weird sub queries almost just to try them out and see what works and what doesn't and so you always end up learning interesting new patterns from those.

1788
02:16:58,300 --> 02:17:02,140
I've got one more super fun sub query example coming up in just a moment.

1789
02:17:03,900 --> 02:17:10,620
This right here is my all time favorite sequel function feature that I don't think enough people are aware of.

1790
02:17:11,580 --> 02:17:35,100
I think it has a terrible name that calls this is a feature called CTE which stands for common table expressions but basically it's it's sequels answer to abstraction like when we're writing Python code if we've got duplicate code logic whatever we put it in function with a nice name and we and that that that helps it helps everything be more maintainable makes us more productive.

1791
02:17:35,740 --> 02:17:48,380
SQL queries are notorious for getting long like you'll hear from people who have like a 500 line SQL query that was left to them by the previous employee and nobody knows how it works and everyone's terrified to touch it.

1792
02:17:49,180 --> 02:18:03,900
The best way to make SQL more effective I believe is to break it up and using these common table expressions and basically what these are is a way to create a temporary table that exists purely for the duration of the current query.

1793
02:18:04,540 --> 02:18:12,780
So the example I'm going to show here is trying to answer the question how many presidents originally served as vice presidents.

1794
02:18:14,460 --> 02:18:17,580
And we were doing that against this table.

1795
02:18:18,140 --> 02:18:29,020
It's this executive terms table which mixes the two types of of office together so we've got presidents in here and we've got vice presidents in here.

1796
02:18:29,020 --> 02:18:38,220
So what I'm going to do is I'm going to start by creating a table called presidents and then say select staff and presidents here.

1797
02:18:38,780 --> 02:18:51,580
And this works right this is you could actually just do this without the CT at all you'd get the same result but we're the thing I'm trying to illustrate here is that we're giving our.

1798
02:18:51,580 --> 02:19:02,540
This select query here which is doing a join and it's doing a where clause so it's actually quite a complicated query we're giving it the alias presidents and then later on we can select stuff from it here.

1799
02:19:03,420 --> 02:19:12,300
The reason that's cool is that then we can add another one we can say that comma vice presidents as and then I'm going to select star from vice presidents.

1800
02:19:15,180 --> 02:19:19,100
And there's all of our vice presidents or I can select stuff from presidents and get just those.

1801
02:19:20,860 --> 02:19:25,260
But now that we've got these two virtual tables we can solve our problem we can say now I want

1802
02:19:25,340 --> 02:19:42,780
to select the distinct names to just duplicate the names from presidents where that name appears in the list of vice presidents and if I do that I get back 15 rows and each one of these people is a president who was previously a vice president.

1803
02:19:43,740 --> 02:19:58,060
Except I'm suspicious of George Bush because Americans are like reusing names and there was a George Bush and George HW Bush so I'd have to dig into that figure out of that that that's actually a proper relationship or not.

1804
02:19:58,060 --> 02:19:59,180
But the point stands that.

1805
02:20:00,380 --> 02:20:05,100
Using this technique you can take very complicated queries and at least turn them into something where.

1806
02:20:06,220 --> 02:20:09,980
If you spend a bit of time thinking about it you can start to understand what this is doing.

1807
02:20:10,940 --> 02:20:32,540
I love this I am I start I got a lot more ambitious with the problems I solve in SQL as opposed to Python. Once I figured out how to use this feature and how it allowed you to take much more complex logic and at least abstract it to the point where you can reason about it more reasonably and then do joins between them and unions and all of that kind of stuff.

1808
02:20:32,540 --> 02:20:38,700
So if you don't use CTEs I very strongly recommend at least exploring them and trying to figure out if that's.

1809
02:20:40,220 --> 02:20:46,700
Yep you can absolutely do that with the self join I think it would be less readable.

1810
02:20:47,740 --> 02:20:55,020
My hunch is that it would be harder to follow if you if you did it as self join although I'd be I'd be happy to be proved wrong by somebody doing that.

1811
02:20:57,340 --> 02:21:06,140
So yeah mostly CTEs are they're more of a readability hack than anything else they're a great way of making it easier to look at a query and understand what it's doing.

1812
02:21:06,380 --> 02:21:20,380
Let's talk about JSON I think I've got a quote so another feature that SQLite has baked in which is incredibly value valuable is it's got a bunch of functions that know how to deal with JSON objects that have been stored in the database.

1813
02:21:20,380 --> 02:21:29,980
Postgres has a JSON type there's a JSON and a JSON B type and it's all optimized and everything. SQLite doesn't it tells you to store JSON in a text column just like anything else.

1814
02:21:30,540 --> 02:21:37,180
But if you do store JSON in your text columns you can start to do all sorts of interesting things with it.

1815
02:21:38,300 --> 02:21:48,860
More importantly though SQLite can output JSON it can you can use functions to build up JSON values that you then then return back to people.

1816
02:21:48,940 --> 02:22:04,540
So this this query right here the thing I was trying to solve with this is I wanted to get back a list of all of my legislators and for each legislator I wanted to fetch all of the terms that they had served but all in one go right.

1817
02:22:04,540 --> 02:22:16,220
Normally if you were doing this you'd have to I mean you write might run Python code that D Jupes it or you may end up like pulling back duplicate versions of the top level thing to get one row per term.

1818
02:22:16,220 --> 02:22:29,740
So a bunch of ways you could do this it's kind of neat to be able to say you know what I want a single row per legislator and in one of the columns in that row should be a JSON list of objects representing everything from this other table.

1819
02:22:29,740 --> 02:22:45,260
There's actually there's two columns there's four columns there's the legislator's idea name there's that JSON list of all of the terms that they've served from a related table and then there's a number of terms as well which is one there and it's 13 here and it's just a count of what's going on.

1820
02:22:46,380 --> 02:23:11,820
So the way I'm doing this is two tricks basically this is an aggregate like I showed you earlier so it's select these things group by legislators dot ID so I'm D Juping and grouping on that ID of the legislator and it's a join against legislator terms so I've actually got duplicate records for in fact let's show you the duplicate records.

1821
02:23:11,900 --> 02:23:17,260
I'm going to do this as select star from legislators join that and get rid of that group by.

1822
02:23:19,900 --> 02:23:32,340
So the actual data that came by is full of jupes right like Watkins Abbott is represented 13 times because Watkins Abbott has been a has had 30 served 13 terms in Congress.

1823
02:23:33,220 --> 02:23:42,020
The first step is that we group by legislators dot ID to D Jup those things and we do a let's do a count star here.

1824
02:23:43,740 --> 02:23:50,260
And so that gives us back just the ID in the name and then the number of terms that they serve so 13 4 3 5 1 1 1 5.

1825
02:23:52,420 --> 02:23:56,620
Oh there's a limit 10 that's why i'm not getting back more than those first ones let's do this again.

1826
02:23:56,820 --> 02:24:08,820
So there we go there's all of our different legislators with all those counts and then the real magic here is i'm adding a additional aggregation called Jason group array so what this does is it says.

1827
02:24:09,900 --> 02:24:25,420
For every duplicate for each of those duplicate sort of chunks of rows build me an array that joins them all together and in that array i'm going to put a Jason object and the Jason object is these cut these labels and these columns.

1828
02:24:26,660 --> 02:24:38,020
So i'm building an object for each one with the type of the state and the start and the end and the party and you can see that in the output here we've got type state start and party in those Jason objects.

1829
02:24:38,340 --> 02:24:42,580
So the problem with solving here is one of the if you've been a web application developer for a long time.

1830
02:24:43,220 --> 02:24:54,620
This is something that's always is it's always frustrating like I want to pull back all of my blog entries and for each one I want a list of the tags on that blog entry and I don't want to have to run multiple sequel queries to retrieve back those tags.

1831
02:24:54,900 --> 02:25:02,700
Once you figure out how to use Jason aggregations like this, you can solve one of the sort of most frustrating problems in web development.

1832
02:25:03,220 --> 02:25:23,540
With a sequel query that does all of that fetching for you and I wrote about this quite extensively in this article linked here returning related rose in a single query using Jason and this actually shows you how to do the same thing in postgres as well, so if you're a postgres user it looks slightly different.

1833
02:25:24,700 --> 02:25:28,420
Here we go it's Jason and Jason build object what's that.

1834
02:25:29,540 --> 02:25:30,060
Yes.

1835
02:25:31,140 --> 02:25:42,780
Yeah so here's the exact same trick done in in postgres same idea since the syntax is very slightly different and as a coalescing thing that's explained in this article too.

1836
02:25:43,220 --> 02:25:54,540
But yeah this is it's a really powerful trick and it's one of my favorite sort of advanced sequel mechanism because it solves this problem that I've fought so many times as a developer just building standard web applications.

1837
02:25:59,820 --> 02:26:10,340
So we talked about CTAs we talked about Jason this is the final advanced feature that I wanted to mention today and this is something called window functions and.

1838
02:26:12,940 --> 02:26:13,540
Okay.

1839
02:26:14,500 --> 02:26:31,340
The problem these solve is this is sort of a much more powerful analytical problem where you want to do things like rolling averages over time or look at this is specifically important for things like time series data where you need to do rolling averages or some things by a sort of.

1840
02:26:33,820 --> 02:26:36,460
But in chunks of your data as it moves along.

1841
02:26:36,980 --> 02:26:44,220
And the best way to explain this is to start with this thing here.

1842
02:26:47,220 --> 02:26:49,140
What happens if I get rid of that.

1843
02:26:51,380 --> 02:26:55,980
Okay, so the problem I'm trying to solve here is I would like to return.

1844
02:26:56,980 --> 02:26:57,740
The.

1845
02:26:58,260 --> 02:27:15,260
I want to return for every repository in my database I want the name of the repository the date of the most recent release associated with that repository and the three most recent releases as a Jason array so previously I showed you the example with the Jason where we returning everything for each legislator.

1846
02:27:15,820 --> 02:27:35,060
We'd return all of that terms and we'd hope that none of them ever served a million terms because it would have broken our query here some of my projects have had over 100 releases I just want the three most recent returned in this query so we're going to start with this query here this is a.

1847
02:27:36,020 --> 02:27:48,460
Join we don't even need the CT for this here I'm joining the releases against the repost table and getting back all of that data and you see there's a lot of there's more than 1000 rows because data set itself has.

1848
02:27:49,700 --> 02:27:58,940
How many releases is data set had a lot like this is the sort of raw data that we're going to start working with but there's there's a there's a lot of de juping that we need to do here.

1849
02:28:00,260 --> 02:28:02,620
What I can do next is I can.

1850
02:28:05,620 --> 02:28:15,780
I'm going to add a rank so in order to select just the three most recent it would be useful if I have an indication of the the.

1851
02:28:17,100 --> 02:28:25,020
sort of incrementing integer for which release that was for those projects so i'm doing that by adding this little magic piece of syntax here.

1852
02:28:25,020 --> 02:28:33,540
And this is what sequel calls a window function it's a feature that was added to sequel like maybe three years ago inspired by its implementation in postgres.

1853
02:28:35,260 --> 02:28:43,140
What i'm doing is i'm saying for each of these releases, I want you to create a partition that's.

1854
02:28:43,140 --> 02:28:50,740
Unique for each of those repository IDs is ordered by that created date on those releases in descending order.

1855
02:28:51,420 --> 02:29:01,260
As well rank hard to sort of see what that means, but if you look at the output what we've just done is we've added a column here this well rank column right at the end.

1856
02:29:01,740 --> 02:29:10,380
And it keeps on going up for all of data sets releases data that has had 125 and then we switch to csv to sequel light and that's only had.

1857
02:29:10,380 --> 02:29:19,260
wow that's up 13 releases so it goes 12345 up to 13 now on data set cluster map that's had 12345678910 1112.

1858
02:29:20,220 --> 02:29:30,140
that's given us, so this is now given us in this single table for the single result we've now got something else that we can filter on to say I just want the three most the top three.

1859
02:29:30,780 --> 02:29:33,700
Although we're in the correct order here, I think we're in the wrong order.

1860
02:29:33,700 --> 02:29:42,460
No, we're in the we are in the correct order on we were with we're starting with the most recently going down, so now I can do is I can say select star from cta where well where.

1861
02:29:44,980 --> 02:29:45,580
Where.

1862
02:29:47,500 --> 02:29:49,380
I switch back to us keyboard layout.

1863
02:29:52,060 --> 02:29:56,060
Where well rank greater than.

1864
02:29:56,060 --> 02:29:59,580
Where well rank greater than or equal to three.

1865
02:30:00,820 --> 02:30:02,860
Well, that's less than or equal to three is now.

1866
02:30:05,180 --> 02:30:15,980
And this is almost solved our problem for us we're now saying for each of these repositories just show me the three most recent so for data set we get three rows for versions 64264.

1867
02:30:16,060 --> 02:30:25,780
Just show me the three most recent so for data set we get three rows for versions 64264164 csv to seek light we just got three rows three rows of cluster map.

1868
02:30:25,780 --> 02:30:31,900
Etc, etc, etc, somewhere in this a DBF to seek light has only ever had one release, so it only shows up once.

1869
02:30:32,740 --> 02:30:50,220
And, but most of the others we've we've had at least three a data set causes another one with just a single release, but this is almost what we need right, this is if we want the three most recent releases for each package we've now got that data and the last stage is to add that.

1870
02:30:50,220 --> 02:30:56,020
Here we go and again i'm taking advantage of my my my common table expression for this one.

1871
02:30:59,460 --> 02:31:09,460
i'm adding back in one of those Jason group raise with Jason object and that worked so now for someone is so for data set repo there's 125 releases total.

1872
02:31:09,780 --> 02:31:19,620
But there's only three that we care about the recent releases and that's those three that are showing up here with the ID and the name the created app again quite a complex solution.

1873
02:31:20,220 --> 02:31:41,580
So we're getting quite a lot of stuff done with this, but by combining the CT mechanism with the Jason group array mechanism with the windows window functions mechanism we're able to do a pretty get a pretty sophisticated result out that if we wanted to you know build a custom application that that shows us that sort of summarized data.

1874
02:31:42,060 --> 02:31:45,300
That's most of the work done for us just in this one SQL query right here.

1875
02:31:50,860 --> 02:31:56,180
That's a great question.

1876
02:31:59,980 --> 02:32:09,460
So the question is what's the computational overhead of doing the Jason construction in SQLite as opposed to in Python I have not measured it my hunch is that SQLite faster.

1877
02:32:10,180 --> 02:32:22,420
Because SQLite everything in SQL like SQLite is it's all see it's incredibly well optimized and the benchmarking they do and it's absolutely extraordinary my hunch is that.

1878
02:32:22,620 --> 02:32:38,380
And I've the thing I have benchmarked is things like how fast can the Jason functions in SQLite like crunch through gigabytes of data and it's just astonishingly fast at processing things I've actually got quote from Paul Ford that I dropped into the section here.

1879
02:32:38,940 --> 02:32:51,460
Where Paul Ford when talking about SQLite's JSON support said the JSON interface is like we save the text and when you retrieve it we pass the JSON several hundred megabytes a second and you can do path queries against it please stop overthinking it this is a filing cabinet.

1880
02:32:51,780 --> 02:33:01,820
Paul Ford's trick that he uses all the time is he gets JSON from somewhere and he literally just loads it into a SQLite table which has one row per object with a blob of JSON and that's it.

1881
02:33:02,180 --> 02:33:07,860
And then he uses SQLite's built in JSON features to extract bits of it out and convert things and all of that.

1882
02:33:08,100 --> 02:33:16,500
He'll often then save the results to a new database table but do all of his data processing with JSON functions inside of SQLite itself.

1883
02:33:17,060 --> 02:33:30,580
I'm not quite there yet I like I haven't embraced it to quite that extent but it is a phenomenally performant way of working with data and I feel like most data these days comes in comes in JSON shape.

1884
02:33:30,980 --> 02:33:37,820
If you've got a database engine that can do JSON really well and SQLite does do JSON really well you find yourself leaning on that more and more.

1885
02:33:38,180 --> 02:33:49,980
Like I always try and get things into a relational table eventually because then you can do faceting and filtering and all of that kind of stuff but there is a lot you can get done just using blobs of JSON if you've got a fast C library on top of it all.

1886
02:33:50,980 --> 02:34:04,340
Okay we are running close to time I can just finish this last section I wanted to show you some some fun some more fun demos the kind of things that I've been using I've been building data set with.

1887
02:34:04,860 --> 02:34:09,020
And basically this is a chance for me to show off some of my my my websites that I like.

1888
02:34:09,980 --> 02:34:20,660
So the first website I'm going to show you is this is the website for my personal my my main hobby and outside of technology I love I basically collect tiny museums.

1889
02:34:21,060 --> 02:34:27,820
So the goal is whenever you're in a new city and I mean I've never been Salt Lake City before and there are some very interesting tiny museums around here.

1890
02:34:27,820 --> 02:34:49,980
So try and find the smallest museums and go to those first because if it's a really tiny museum it doesn't matter what it's about the chances are the person who collected the stuff is there running the museum and then you get to have a conversation with somebody who collects evidence of Bigfoot or PES dispensers or little devil statues or whatever it is and it's super super rewarding.

1891
02:34:50,340 --> 02:35:06,180
So this website is where I put all of the tiny museums that I found and gone to and you can actually say use my location and they'll show you the Don and Memorial State Park visitor center 450 miles away is the closest that I've been to here so I've clearly not explored Utah and Colorado at all yet.

1892
02:35:06,180 --> 02:35:35,660
But anyway this is just data set this website right here is the same data set web application you saw earlier but data set lets you use custom templates and so here I've got a custom template for the home page that makes it look like a website about museums and I've got like this is a custom template that knows how to put things on a map and the search feature is just another canned query and you can tell that because if you go to nichemuseums.com slash browse it's data set this is this is the website there's the museums.

1893
02:35:35,660 --> 02:35:53,140
I think this one it's deployed on it's got plugins all of that kind of stuff is deployed on the cell so it costs me nothing to run every time I add a new museum to it it's close a new version of the site which works just fine but the one thing I did want to show you is that this website has an RSS feed.

1894
02:35:53,140 --> 02:36:23,060
So you can click that and you get XML back that you can subscribe to in a feed reader that's actually defined using a sequel query there's this query called browse slash feed it's this giant monstrosity of a sequel query here it's using Jason each and coalesce and all of that kind of stuff but the output of that query is this table it's got columns atom ID atom title atom updated and then there's a data set plugin called data set atom which only fires when a table has these columns.

1895
02:36:23,140 --> 02:36:53,140
And it adds the atom extension and it turns that into XML and I've actually got a few other plugins along those lines there's one called data set ICS that can output subscribable ICS calendar files it's kind of fun it's I'm effectively using sequel as a integration language where as developers we spend so much of our lives getting data in one format and transforming ever so slightly into another format sequels really good at that you can say select something as atom ID something as

1896
02:36:53,140 --> 02:37:12,020
atom link and it'll transform it into that other format and then if you've got plugins that know how to output things in different shapes you can sort of pipe it all together and now you've got a sort of an out of the box engine for reformatting data into different formats which is something I played with a whole bunch and I think is really neat.

1897
02:37:12,020 --> 02:37:23,140
I'm going to skip my other example from this section and jump to one last thing which is a project that sort of illustrates what happens when you pull all of this stuff together.

1898
02:37:23,140 --> 02:37:41,380
So I've got a blog this blog I've been running for 20 years it's been a Django app for most of its life it's a Django and postgres runs on heroku and that's really cool like it's it's it works it's got the Django admin and all of those features but these days I like to do things with sequel and data set.

1899
02:37:41,380 --> 02:38:04,660
So I have a copy of my blog running at data set Simon Willis and dot net this is my blog as a sequel like database and the way that works is I've got a GitHub action script running here which let's see this one's what 26 star slash two can anyone remember what that means is that every two hours.

1900
02:38:04,660 --> 02:38:32,700
Yes so this runs every two hours and every two hours it grabs a backup of my database from heroku as a postgres dump and it converts that into a sequel like database and it just gets those tables and it I think it would that's the password column because I don't want the password column out there runs a few little transforms drop a few tables bloody bloody bloody blah and then at the end it runs data set publish.

1901
02:38:32,700 --> 02:38:36,500
Where's the data publish bit somewhere way down here.

1902
02:38:38,700 --> 02:38:39,100
There we go.

1903
02:38:40,500 --> 02:38:58,980
Run Simon Willis and blog dot DB with a bunch of extra options and it stills a bunch of plugins but effectively every two hours the latest version of my blog gets published to a data set instance here which means I can run sequel against all of my stuff and run queries and searches and all of the sort of data set goodness is available to me.

1904
02:38:59,780 --> 02:39:19,380
But what I wanted to do is I wanted to stop I wanted to start a sub stack newsletter because I wanted people who only ever use email to be able to subscribe to my blog but I didn't want to do any extra work like I'm already putting stuff in my blog I wanted the simplest possible way of getting my blog's content into a into newsletter format.

1905
02:39:19,580 --> 02:39:29,780
Sub stack do not give you an API like sub stack they give you a text editor rich text editor to write content in they do not give you anything to help automate that process.

1906
02:39:30,580 --> 02:39:44,380
But you can copy and paste things into the sub stack editor so I'll go here I'll go new post here we go this right here is quite a good rich text editor it can do bold and links and headings and images and all of that kind of stuff.

1907
02:39:44,980 --> 02:40:01,380
So what I did is I built myself a tool using an observable notebook called blog to newsletter and this is basically a giant bundle of JavaScript which hits the data set API on my blog fetches back all sorts of different bits of content and all of that kind of stuff I think it's actually.

1908
02:40:01,380 --> 02:40:16,380
There's a terrifying sequel query in here somewhere here we go this is the sequel query that I put together which reads from five different tables and glues things together and sorts by and collect and all of the CTE tricks and all of that kind of stuff.

1909
02:40:16,380 --> 02:40:30,380
And based on running that thing there I get a I get this table of data so this has my titles my URLs and different types of content and so forth and then I've got a bit of JavaScript at the top that glues it together into markdown.

1910
02:40:30,380 --> 02:40:41,380
And so that generates HTML this previews the HTML and then this right at the very top I've got a big great copy button.

1911
02:40:41,380 --> 02:41:10,380
Actually let's do here we go so I can click copy and then I can go to sub stack and hit paste and that's my newsletter and done so I in this case I do that title there I'd stick that in the title at the top I had some kind of subheading and I click send and it's done and this has I mean this this is meant that sending out my newsletter which I do once or twice a week takes about a minute and a half like end to end.

1912
02:41:10,380 --> 02:41:23,380
But if you look at the moving pieces that are going on here this is my blogs and Django and I've got the GitHub actions that are pulling it from Postgres and converting that to SQLite and then I published that to Google Cloud Run and then I went JavaScript and Observable to pull it all together.

1913
02:41:23,380 --> 02:41:39,380
But it kind of just works you know once once I piece those bits and pieces together I didn't like one of the things I like about using observable notebooks this is I don't even have to like commit code to get her broken a text editor all of the all of the like the hard work that I've done.

1914
02:41:39,380 --> 02:41:54,380
Like the hard work that I didn't hear was done just directly in this notebook interface right here so it's kind of cool and I think that's a nice sort of illustration of what the kind of think projects you can do once you've got once you've got all of these different components working together.

1915
02:41:54,380 --> 02:41:58,380
So we've only got three minutes left and I've covered everything that I wanted to cover.

1916
02:41:58,380 --> 02:42:04,380
Let's do let's do three minutes of questions.

1917
02:42:04,380 --> 02:42:07,380
Yes, there's one that was back.

1918
02:42:07,380 --> 02:42:34,380
Yeah.

1919
02:42:34,380 --> 02:42:43,380
Yeah, there's something I didn't get into today is the data set actually sports or SQLite sports views right SQL views.

1920
02:42:43,380 --> 02:42:59,380
And there's actually there's an example of one here on the data set website. This is a view. This is the recent releases view, which does a join between the releases and the repost table a bunch of the stuff that we were playing around with earlier, but because it's defined as a view data set treats it just like another table.

1921
02:42:59,380 --> 02:43:13,380
So you've now got the same interface, you can like facet by topics and order by date and get things out as Jason all of that kind of stuff. So it behaves exactly like it was a table, except that it's defined as an as an extra SQL feature.

1922
02:43:13,380 --> 02:43:24,380
And that's super. That's something I find myself doing a lot is I'll get a bunch of raw data in a normalized form, and I'll use the views to create those sort of de normalized convenient let convenience layers.

1923
02:43:25,380 --> 02:43:43,380
And then you can get that out as Jason as well. So if you want a Jason API for very specific shape of data, one of the quickest ways to do that is to define a view and do it that way.

1924
02:43:43,380 --> 02:43:46,380
I got to stare at people until I get one more question.

1925
02:43:46,380 --> 02:43:56,380
Oh, yes, sir.

1926
02:43:56,380 --> 02:44:03,380
Oh yeah, so so if I don't want to build a database abstraction I what about using SQL alchemy. It's a good question.

1927
02:44:03,380 --> 02:44:19,380
I've never quite gelled with SQL alchemy like SQL alchemy is it's big and complicated. And so with data set, I've kind of been trying to minimize my dependencies. My biggest dependency for data set is ginger the templating language which I feel has sort of proved itself over many years.

1928
02:44:19,380 --> 02:44:31,380
And to be honest, SQL alchemy has proved itself as well. I don't think I'd be making a mistake to lock myself in SQL alchemy. The thing I'm worried about is dependencies where they might have backwards incompatible changes next year and all of that kind of stuff.

1929
02:44:32,380 --> 02:44:49,380
But yeah, it could be that SQL alchemy would help me help solve my database abstraction problem really well. But I've just not really dug into it enough. My hunch at the moment is that data set actually doesn't have many bits of it that would differ between different databases.

1930
02:44:49,380 --> 02:45:04,380
It's mainly the way the table page works, which is at least quite isolated like I'm refactoring that code at the moment. So I'm thinking maybe pluggable database backends wouldn't be as difficult as I'd initially thought they were. And if I do do pluggable database backends, the first two are going to be duck DB and postgres.

1931
02:45:04,380 --> 02:45:14,380
Because I feel like if you can do those two, that's 90% of the world of the sort of data problems in the world fit into SQLite or duck DB or postgres.

1932
02:45:14,380 --> 02:45:34,380
Okay, well, we're at time. So thank you all very much for coming along. I will be hanging around here for another 15 minutes. Happy to talk about this stuff more. The handout will stay available forever. I'm going to update it. I'll probably turn bits of it into more official data set tutorials as well.

1933
02:45:34,380 --> 02:45:46,380
And I'm super keen on feedback on this stuff. Like anything that didn't make sense, anything that could be easier, ideas for future projects, I want to hear all of them. And I'm here for the rest of PyCon. So again, thank you very much.

