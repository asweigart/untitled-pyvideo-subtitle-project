1
00:00:00,000 --> 00:00:04,160
Okay, I guess it's nine, so I'm gonna go ahead and get started.

2
00:00:04,160 --> 00:00:10,000
Well, if any stragglers come in, I may jump back to the startup instructions a bit.

3
00:00:10,000 --> 00:00:11,000
So welcome.

4
00:00:11,000 --> 00:00:21,820
Today's tutorial is gonna be on data of an unusual size with reference to Princess Bride.

5
00:00:21,820 --> 00:00:24,160
To give you some background, my name's Dar Haspatina.

6
00:00:24,160 --> 00:00:32,360
I'm originally an aerospace civil engineer, did a lot of work in computational fluid dynamics

7
00:00:32,360 --> 00:00:39,200
and hurricane modeling, and had to handle lots and lots of very, very big data sets.

8
00:00:39,200 --> 00:00:52,160
I've been part of the open source Python community for over a decade, and now I work for QuantSight,

9
00:00:52,160 --> 00:00:57,680
which is a company focused on consulting in the PyData and SciPy ecosystems.

10
00:00:57,680 --> 00:01:03,160
My co-presenter is Pavitra Ishamurthy, and she will be on the Jitsi chat.

11
00:01:03,160 --> 00:01:08,160
Unfortunately, we had some travel complications and she wasn't actually able to be here,

12
00:01:08,160 --> 00:01:13,520
but I do want to give her a big shout out because she created more than half of the

13
00:01:13,520 --> 00:01:15,040
materials we're going through today.

14
00:01:15,040 --> 00:01:17,280
So I just wanted to give that shout out.

15
00:01:17,280 --> 00:01:21,060
She's a developer advocate at QuantSight.

16
00:01:21,060 --> 00:01:29,900
The focus of today is to give you guys some intuition of how to handle large data sets.

17
00:01:29,900 --> 00:01:35,820
We're specifically going to be focusing on a set of tools, Dask and HP plot and a few

18
00:01:35,820 --> 00:01:42,660
other things, but the principles can apply to other big data tools, like if you use Ray

19
00:01:42,660 --> 00:01:46,540
or if you use Spark or anything else.

20
00:01:46,540 --> 00:01:51,660
So part of this is to get you familiar with these tools, which have a fairly low initial

21
00:01:51,660 --> 00:01:57,540
learning curve, but it's also to kind of get you thinking about what changes when you go

22
00:01:57,540 --> 00:02:04,540
from what fits in memory on your laptop to larger data sets.

23
00:02:04,540 --> 00:02:07,640
To give some quick setup instructions, I've already gone through this for a couple of

24
00:02:07,640 --> 00:02:10,180
you.

25
00:02:10,180 --> 00:02:17,100
Go to this website, nabari.quantsight.dev, and that is here in larger letters.

26
00:02:17,100 --> 00:02:19,380
Has anyone not registered here yet?

27
00:02:19,380 --> 00:02:24,820
I just wanted to get a show of hands.

28
00:02:24,820 --> 00:02:27,140
So I'll give folks a couple of minutes.

29
00:02:27,140 --> 00:02:30,900
In the meantime, go to this website.

30
00:02:30,900 --> 00:02:39,700
Once you go to nabari.quantsight.dev, you should see a key cloak page, and at the bottom,

31
00:02:40,220 --> 00:02:44,140
if you click on login with key cloak, you'll see a register.

32
00:02:44,140 --> 00:02:45,540
Just go ahead and register.

33
00:02:45,540 --> 00:02:46,620
You can put fake information in.

34
00:02:46,620 --> 00:02:47,620
That's fine.

35
00:02:47,620 --> 00:02:49,700
We're not checking the email addresses.

36
00:02:49,700 --> 00:02:53,140
And then once you log in, you'll see a start button.

37
00:02:53,140 --> 00:03:00,140
I'll walk you through this flow in a second.

38
00:03:00,140 --> 00:03:05,180
Before I get to that, just give a quick poll from the crowd.

39
00:03:05,180 --> 00:03:10,060
How many of you have used tools like Dask before?

40
00:03:10,060 --> 00:03:11,060
Anyone?

41
00:03:11,060 --> 00:03:12,060
Okay.

42
00:03:12,060 --> 00:03:15,820
How familiar are folks with Pandas?

43
00:03:15,820 --> 00:03:16,820
Okay.

44
00:03:16,820 --> 00:03:17,820
Good.

45
00:03:17,820 --> 00:03:22,100
So when we get to Dask, I'll talk a little bit about this more.

46
00:03:22,100 --> 00:03:25,940
If you're familiar with Pandas, we're going to show you how to take your knowledge from

47
00:03:25,940 --> 00:03:30,940
Pandas and then transition it to scaling up with some of these other tools.

48
00:03:30,940 --> 00:03:33,820
Okay.

49
00:03:33,820 --> 00:03:35,500
Let's see.

50
00:03:35,500 --> 00:03:41,820
So what we're going to do now is we will, I'm going to stop my server so I can walk

51
00:03:41,820 --> 00:03:43,940
you through what you guys will do.

52
00:03:43,940 --> 00:03:46,700
Is there anyone who's still registering?

53
00:03:46,700 --> 00:03:47,700
Okay.

54
00:03:47,700 --> 00:03:53,740
I'll give you guys actually, since we have a few more people.

55
00:03:53,740 --> 00:04:02,380
It would be nice if we had a whiteboard.

56
00:04:02,380 --> 00:04:05,060
Okay.

57
00:04:05,060 --> 00:04:10,460
So folks who came in new, if you can take a picture of this page.

58
00:04:10,460 --> 00:04:17,860
So this is where we're going to run today's tutorial.

59
00:04:17,860 --> 00:04:23,180
And once you go to this page, click on login with key cloak and then there's a register

60
00:04:23,180 --> 00:04:25,180
button, click on that and register.

61
00:04:25,180 --> 00:04:29,420
And then after that, the rest should be what I'm showing now.

62
00:04:29,420 --> 00:04:31,140
Okay.

63
00:04:31,140 --> 00:04:40,940
So I am going to shut down my, I'm going to stop my server.

64
00:04:40,940 --> 00:04:43,700
Okay.

65
00:04:43,700 --> 00:04:47,940
So when you log in, you should see something like this.

66
00:04:47,940 --> 00:04:54,260
We are doing today's tutorial on Nabari, which is an open source software that QuantSite

67
00:04:54,260 --> 00:04:55,260
develops.

68
00:04:55,260 --> 00:04:57,340
But think of this as Jupiter Hub.

69
00:04:58,260 --> 00:05:03,900
is open source and all the pieces you see put together are just all open source pieces

70
00:05:03,900 --> 00:05:05,940
that have been put together in the Nabari platform.

71
00:05:05,940 --> 00:05:10,180
So everything, and I'll talk about this at the end, how to get the various tools which

72
00:05:10,180 --> 00:05:11,980
we're going to use today.

73
00:05:11,980 --> 00:05:14,740
Click start my server.

74
00:05:14,740 --> 00:05:18,060
And we're going to choose, you will only see two instance types.

75
00:05:18,060 --> 00:05:19,900
You'll see a small and a medium.

76
00:05:19,900 --> 00:05:24,540
I actually have more because I'm logged in with my email address, including GPU instances.

77
00:05:24,540 --> 00:05:26,620
There's also a GPU instance in yours.

78
00:05:26,620 --> 00:05:31,180
I think you'll also see this K80 GPU instance.

79
00:05:31,180 --> 00:05:35,460
In the tutorial, if you want to play around with that and do something, you can, but we're

80
00:05:35,460 --> 00:05:38,260
not planning to do any GPU stuff today.

81
00:05:38,260 --> 00:05:43,700
So choose a medium instance and then click start.

82
00:05:43,700 --> 00:05:46,660
And what this is doing, this is running in the cloud.

83
00:05:46,660 --> 00:05:49,300
It's going and fetching an instance of that size.

84
00:05:49,300 --> 00:05:55,620
I think this is four CPUs and 16 GB of RAM, actually like 14 or 15 GB of RAM.

85
00:05:55,620 --> 00:06:01,260
So this is sort of equivalent to the power of your laptop.

86
00:06:01,260 --> 00:06:05,940
Most of our laptops are probably four to eight cores and 16 to 32 GB of RAM.

87
00:06:05,940 --> 00:06:10,060
So this is trying to give you a feel of what you could do locally on your laptop and then

88
00:06:10,060 --> 00:06:16,100
how we could transition to the cloud.

89
00:06:16,100 --> 00:06:22,320
Now the first thing we're going to do is we're going to clone the repository.

90
00:06:22,320 --> 00:06:29,920
So if we go back here, go to this URL.

91
00:06:29,920 --> 00:06:40,280
If you go to GitHub, you should see this URL.

92
00:06:40,280 --> 00:06:41,280
I'm going to do this slowly.

93
00:06:41,280 --> 00:06:44,280
So I was hoping to give everyone sticky notes, but I didn't forget to.

94
00:06:44,280 --> 00:06:45,280
I forgot to bring them.

95
00:06:45,280 --> 00:06:51,760
But over here, let's click on code and choose HTTPS.

96
00:06:51,760 --> 00:06:57,360
So if you click here, and we're going to choose the copy button.

97
00:06:57,360 --> 00:07:00,320
And then we're going to go back to JupyterLab.

98
00:07:00,320 --> 00:07:06,840
And over here, this icon here is Jupyter's Git plugin.

99
00:07:06,840 --> 00:07:09,920
So we're going to go to Jupyter's Git plugin.

100
00:07:09,920 --> 00:07:10,920
And oh.

101
00:07:10,920 --> 00:07:11,920
Okay.

102
00:07:12,920 --> 00:07:13,920
Okay.

103
00:07:13,920 --> 00:07:20,840
I don't remember how to get back to the original.

104
00:07:20,840 --> 00:07:29,080
Okay.

105
00:07:29,080 --> 00:07:33,020
For you guys, since you haven't already cloned something, you should actually see a place

106
00:07:33,020 --> 00:07:38,320
you can say clone and you can clone a new repo.

107
00:07:38,320 --> 00:07:43,480
I do not remember how to get back to the main folder on the Jupyter Git plugin.

108
00:07:43,480 --> 00:07:45,000
I don't use it that often.

109
00:07:45,000 --> 00:07:47,760
So I'm actually going to do this from the command line.

110
00:07:47,760 --> 00:07:50,400
So here, I'm going to hit the plus button.

111
00:07:50,400 --> 00:07:56,600
And I am going to open a terminal.

112
00:07:56,600 --> 00:08:02,560
And here, let me actually go back and I'm going to change this to, I'm going to use

113
00:08:02,560 --> 00:08:03,560
SSH.

114
00:08:13,560 --> 00:08:14,560
Okay.

115
00:08:14,560 --> 00:08:15,560
So I cloned it.

116
00:08:15,560 --> 00:08:18,880
And I should have this presentations folder now.

117
00:08:18,880 --> 00:08:21,560
For folks who are using the Git plugin.

118
00:08:21,560 --> 00:08:31,560
I really should have practiced this beforehand.

119
00:08:31,560 --> 00:08:34,560
Okay.

120
00:08:34,560 --> 00:08:35,560
Okay.

121
00:08:35,560 --> 00:08:40,560
Let's stop there.

122
00:08:40,560 --> 00:08:41,560
Okay.

123
00:08:41,560 --> 00:08:43,560
Who has successfully cloned the repo?

124
00:08:43,560 --> 00:08:44,560
Okay.

125
00:08:44,560 --> 00:08:45,560
Good.

126
00:08:45,560 --> 00:08:48,560
We all know how to clone things.

127
00:08:48,560 --> 00:08:53,360
Anyone still having trouble cloning?

128
00:08:53,360 --> 00:09:00,320
So as you can see, I don't usually use the Jupyter Git UI because I don't like it.

129
00:09:00,320 --> 00:09:02,880
But anyway, that's that.

130
00:09:02,880 --> 00:09:10,760
So now if we click on this folder icon at the top, you should see this presentations

131
00:09:10,760 --> 00:09:13,880
folder that you just cloned.

132
00:09:13,880 --> 00:09:17,040
Let's go in there and go into PyCon and go into our tutorial.

133
00:09:17,040 --> 00:09:20,840
And you should see something like this on the right.

134
00:09:20,840 --> 00:09:26,600
So I'm going to reopen our introduction notebook.

135
00:09:26,600 --> 00:09:30,480
And you should be, most of the notebooks, you should be able to walk through them.

136
00:09:30,480 --> 00:09:33,320
And at the end, there should be a link to the next notebook.

137
00:09:33,320 --> 00:09:36,920
Now I will say there's one thing I need you to be a little careful of.

138
00:09:36,920 --> 00:09:42,480
In case you are Jupyter experts, please don't hit run all cells.

139
00:09:42,480 --> 00:09:46,000
There's a couple of places where there's a race condition.

140
00:09:46,000 --> 00:09:52,160
So if you don't wait for the first cell to run, the other cells may not execute correctly

141
00:09:52,160 --> 00:09:56,120
because some of these tools inject JavaScript into your front end.

142
00:09:56,120 --> 00:10:03,000
There's also a, in some of the notebooks, I have a shutdown cluster command at the end.

143
00:10:03,000 --> 00:10:05,800
So if you run all, it's going to shut the cluster down and you're not going to be able

144
00:10:05,800 --> 00:10:06,800
to see anything.

145
00:10:06,800 --> 00:10:09,960
So just go slow.

146
00:10:09,960 --> 00:10:11,520
Okay.

147
00:10:11,520 --> 00:10:15,080
So roughly this is what the plan is.

148
00:10:15,080 --> 00:10:16,080
I'm going to be...

149
00:10:16,080 --> 00:10:17,080
Oh, thank you.

150
00:10:17,080 --> 00:10:22,240
Actually, if you can just pass them out.

151
00:10:22,240 --> 00:10:23,960
Yeah.

152
00:10:23,960 --> 00:10:30,000
So let's use the yellow as I completed things correctly and let's use the other colors as

153
00:10:30,000 --> 00:10:31,960
I'm having an issue or something like that.

154
00:10:31,960 --> 00:10:35,320
Just so I get a feel for if folks have completed stuff or not.

155
00:10:35,320 --> 00:10:36,320
Thank you very much.

156
00:10:36,320 --> 00:10:38,920
Yeah, just put it on the back of your laptop.

157
00:10:38,920 --> 00:10:42,440
That gives me a visual indicator of whether I need to wait longer or not because several

158
00:10:42,440 --> 00:10:44,000
of the things are going to be...

159
00:10:44,000 --> 00:10:46,160
Actually, everything's going to be hands-on.

160
00:10:46,160 --> 00:10:49,760
So overall, the plan is we're going to spend about 20 minutes just getting familiarized

161
00:10:49,760 --> 00:10:52,560
with the interface and making sure everyone's logged in or so.

162
00:10:52,560 --> 00:10:58,440
Then we're going to spend the rest of the first hour just doing Panda stuff that if

163
00:10:58,440 --> 00:11:01,840
everyone's done Panda stuff before should be fairly straightforward.

164
00:11:01,840 --> 00:11:07,160
We're also going to introduce you to a plotting tool called HVPlot.

165
00:11:07,160 --> 00:11:11,440
Who's familiar with Streamlit or Dash or any of these tools?

166
00:11:11,440 --> 00:11:20,000
So there's another set of tools called HoloViz which HVPlot is part of that is similar to

167
00:11:20,000 --> 00:11:21,000
those tools.

168
00:11:21,000 --> 00:11:26,200
But one of the nice things about the HoloViz things is it's very tightly integrated with

169
00:11:26,200 --> 00:11:28,320
the big data tools on the backend.

170
00:11:28,320 --> 00:11:32,800
So you can do very similar dashboards and stuff like you can do with Streamlit or something,

171
00:11:32,800 --> 00:11:35,720
but you can also then turn a switch on and say, okay, now I'm going to do this with a

172
00:11:35,720 --> 00:11:36,720
huge dataset.

173
00:11:36,720 --> 00:11:39,480
So that's why we're going to go through those.

174
00:11:39,480 --> 00:11:46,440
The second hour, we're going to do a deep dive, well, a dive into Dask.

175
00:11:46,440 --> 00:11:49,600
And with that, we're also going to talk about storage formats.

176
00:11:50,000 --> 00:11:55,920
And where your data is because once you start dealing with larger datasets, where the data

177
00:11:55,920 --> 00:11:59,800
is and how it's stored starts mattering a lot.

178
00:11:59,800 --> 00:12:01,060
And so we're going to go through that.

179
00:12:01,060 --> 00:12:05,800
And then we're going to actually do some really nice big compute.

180
00:12:05,800 --> 00:12:16,840
So the last hour, we're going to do visualization with larger datasets and building small applications.

181
00:12:16,840 --> 00:12:21,760
And then if we get to it, there's some stuff about sharing with people and stuff that I

182
00:12:21,760 --> 00:12:22,760
don't know.

183
00:12:22,760 --> 00:12:26,000
We have a lot of material, and this is the first time we're trying to teach this, so

184
00:12:26,000 --> 00:12:30,840
we'll see how far we get.

185
00:12:30,840 --> 00:12:31,840
Any questions?

186
00:12:31,840 --> 00:12:32,840
Has everyone logged in and cloned?

187
00:12:32,840 --> 00:12:38,600
Do I need to walk through the instructions again for anyone who just came in?

188
00:12:38,600 --> 00:12:41,240
Okay.

189
00:12:41,240 --> 00:12:45,040
So for those who just came in, we've still got about five minutes, so let me just do

190
00:12:45,040 --> 00:12:47,360
this again.

191
00:12:47,360 --> 00:12:52,600
Go to this first website, nabari.quantsite.dev.

192
00:12:52,600 --> 00:13:11,600
And on this website, you will see this sign in with key cloak.

193
00:13:11,920 --> 00:13:17,860
Once you click sign in with key cloak, ignore the sign in with Google, but click this register

194
00:13:17,860 --> 00:13:20,700
button at the bottom.

195
00:13:20,700 --> 00:13:35,560
And that register button will let you create an account.

196
00:13:35,600 --> 00:13:59,120
And then once you've created an account, we need to take this repo.

197
00:13:59,120 --> 00:14:04,640
And so we're going to start a server, and then we're going to actually walk through

198
00:14:04,720 --> 00:14:06,720
this again.

199
00:14:06,720 --> 00:14:11,400
I just realized I could do it with this, then we can...

200
00:14:11,400 --> 00:14:26,400
Pardon?

201
00:14:26,400 --> 00:14:31,400
Did it let you log in when you logged into the Gmail account?

202
00:14:31,400 --> 00:14:33,400
Oh, that's interesting.

203
00:14:34,160 --> 00:14:38,360
Well, we opened up the permissions to make it easy, so we may have accidentally enabled

204
00:14:38,360 --> 00:14:40,280
that, but that's fine.

205
00:14:40,280 --> 00:14:45,280
We're going to delete all of you after the tutorial anyway.

206
00:14:45,280 --> 00:14:54,280
Yeah, so don't tell any Bitcoin miners that this is open right now because...

207
00:14:54,280 --> 00:15:03,280
Oh, that already exists.

208
00:15:04,160 --> 00:15:17,160
Okay, so now if you click start my server, I'm going to pick a medium instance.

209
00:15:17,160 --> 00:15:21,160
I'm going to start.

210
00:15:22,040 --> 00:15:29,040
And let me find my...

211
00:15:29,040 --> 00:15:37,040
So if I go to the Git repo and I'm going to choose HTTPS, copy it, and now you'll see

212
00:15:37,040 --> 00:15:43,040
what the Git plugin looks like on this one.

213
00:15:43,040 --> 00:15:49,040
So we've also on the back end, so this is running on GCP, and we've upped our quotas

214
00:15:49,920 --> 00:15:57,920
so that all of you should be able to pull 10 medium instances at a time, and that should

215
00:15:57,920 --> 00:16:06,920
give each of you 40 cores and 16 into 10, like 160 or 200, I forget, whatever GB.

216
00:16:06,920 --> 00:16:14,920
Now, we tested that last week with about 10 of us each scaling up to 50 or 100, I don't

217
00:16:15,800 --> 00:16:20,800
remember, so hopefully this will all scale well today.

218
00:16:20,800 --> 00:16:26,800
Okay, anyone still cloning the Git repo?

219
00:16:31,800 --> 00:16:36,800
Okay, so for cloning, you click this, and this is the thing you should have seen earlier,

220
00:16:37,680 --> 00:16:45,680
and we can say clone repo, and we can put this in, clone, and then if you look here,

221
00:16:46,680 --> 00:16:48,680
we now have this nice presentations folder.

222
00:16:48,680 --> 00:16:52,680
So I'm going to shut down this server because I'm not going to use it.

223
00:16:52,680 --> 00:16:56,680
Stop my server.

224
00:16:56,680 --> 00:17:04,680
Okay, so I'm going to assume that everyone is doing well now.

225
00:17:07,680 --> 00:17:09,680
Okay.

226
00:17:17,680 --> 00:17:25,680
Okay, so one thing I'm going to do is if we, which stage are you on?

227
00:17:26,560 --> 00:17:32,560
If you're in Jupiter, do this.

228
00:17:32,560 --> 00:17:41,560
If we click on this little chat icon here, and if you join, if you click join on PyCon

229
00:17:41,560 --> 00:17:50,440
2023, you can turn off the video, and I'm going to leave my audio on so that my colleagues

230
00:17:50,440 --> 00:17:57,440
can listen in, but if you join meeting and go to chat, S Guild and Pavitra are online,

231
00:18:03,880 --> 00:18:08,160
and they can try and answer some questions if I'm not able to get to all of them, and

232
00:18:08,160 --> 00:18:11,160
we'll see how that works.

233
00:18:11,160 --> 00:18:18,160
Okay, let me go back here.

234
00:18:18,880 --> 00:18:25,880
Okay, so I'm going to close this for now.

235
00:18:33,880 --> 00:18:38,600
Okay, so if you look at the introduction, so two things.

236
00:18:38,600 --> 00:18:42,600
One is when we go to all the notebooks, run the first cell, wait.

237
00:18:42,600 --> 00:18:45,600
When it completes, run the next cells because in a couple of notebooks there's some race

238
00:18:45,600 --> 00:18:46,600
conditions.

239
00:18:47,040 --> 00:18:52,040
There are a couple of sections that say your turn, and the next cell is empty for you to

240
00:18:52,040 --> 00:18:55,800
try writing some code, and below that there's a cell with three dots.

241
00:18:55,800 --> 00:19:00,160
If you click the three dots, you'll see the solution or a solution because some of them

242
00:19:00,160 --> 00:19:04,800
are open-ended and we've given one solution there.

243
00:19:04,800 --> 00:19:11,800
For support, if you want, you can try and use the Jitsi plugin, the Meet.

244
00:19:12,800 --> 00:19:18,320
Also, if you want a longer discussion or more questions, we have a discussions page on our

245
00:19:18,320 --> 00:19:24,520
GitHub, so you can click there and you can open a discussion there as well.

246
00:19:24,520 --> 00:19:28,160
Put a red sticky, a colored sticky note up if you have a question.

247
00:19:28,160 --> 00:19:32,280
I'm going to stop it several times and ask for questions, but if something's not working,

248
00:19:32,280 --> 00:19:34,080
we'll go there.

249
00:19:34,080 --> 00:19:41,080
Okay, so I am actually going to go to the first actual data notebook.

250
00:19:42,200 --> 00:19:49,200
Okay, so this is, we're going to look at a data set which I pulled a few weeks ago.

251
00:19:52,840 --> 00:19:55,280
It's airline on-time performance.

252
00:19:55,280 --> 00:19:57,680
We're trying to think of a data set that everyone can understand.

253
00:19:57,680 --> 00:20:02,600
It's not finance, it's not geospatial, it's not weather, and everyone can get.

254
00:20:02,600 --> 00:20:06,560
We thought, okay, everyone's dealt with flight delays.

255
00:20:06,560 --> 00:20:13,560
We scraped this data from the Bureau of Transportation Statistics.

256
00:20:16,600 --> 00:20:22,600
The 2003 to 2022 is about 70 gig in terms of CSV.

257
00:20:22,600 --> 00:20:24,000
This is a medium-sized data set.

258
00:20:24,000 --> 00:20:29,320
The tools I'm showing you can go to much higher, but this is a data set that's not too big,

259
00:20:29,320 --> 00:20:34,240
so we can do something useful in this class because we're only giving you 10 nodes each.

260
00:20:34,280 --> 00:20:40,120
But also, it's bigger than you'd be comfortable doing on your laptop, or you could do it on

261
00:20:40,120 --> 00:20:42,800
your laptop, but it would be annoying.

262
00:20:42,800 --> 00:20:49,800
We've put this data on a GCS bucket, and it's divided into, okay, I'll just say one thing.

263
00:20:53,280 --> 00:20:58,400
The Bureau of Transportation Statistics website is very annoying.

264
00:20:58,400 --> 00:21:05,400
It took several days to scrape this data, and there's a folder in, if you're interested

265
00:21:06,040 --> 00:21:13,040
in this prep, there is the, my code to actually scrape the data.

266
00:21:17,320 --> 00:21:21,960
If you want to scrape it yourself and were willing to let it run for several days, you

267
00:21:21,960 --> 00:21:26,240
can look at that.

268
00:21:26,440 --> 00:21:29,560
Anyway, this is the data set.

269
00:21:29,560 --> 00:21:36,560
You should be able to run this cell, so this is just going to connect to the Google storage

270
00:21:38,760 --> 00:21:43,400
and list all the files we have.

271
00:21:43,400 --> 00:21:50,400
If I look at, actually, if we look at files, you can see these are the files that are,

272
00:21:50,400 --> 00:21:57,400
here, I just picked one year of data.

273
00:21:57,400 --> 00:22:03,080
I'm picking like 2022 data.

274
00:22:03,080 --> 00:22:09,080
So let's just import pandas.

275
00:22:09,080 --> 00:22:14,160
I also have a JSON file called Dtypes, which we wrote earlier, and if you want to look

276
00:22:14,160 --> 00:22:17,160
at that, it's in the prep folder.

277
00:22:17,160 --> 00:22:24,160
Look at that, and it's just what all the types are for the data set.

278
00:22:26,760 --> 00:22:29,480
We'll talk a little bit more about that in a minute, because it will become important

279
00:22:29,480 --> 00:22:31,520
later.

280
00:22:31,520 --> 00:22:34,800
This cell is going to take about three minutes to run.

281
00:22:34,800 --> 00:22:39,960
Pandas can only read one CSV at a time, so we kind of have to read them all in and then

282
00:22:39,960 --> 00:22:42,640
append them and then concat them.

283
00:22:42,640 --> 00:22:44,600
In the meantime, let's look at the data set.

284
00:22:45,080 --> 00:22:46,960
So we click on this link.

285
00:22:46,960 --> 00:22:49,600
This is the data set we're using.

286
00:22:49,600 --> 00:22:53,120
It has lots and lots and lots and lots and lots of fields.

287
00:22:53,120 --> 00:23:00,120
The main thing is for every carrier that flew in the U.S., it has every flight they flew,

288
00:23:01,040 --> 00:23:05,640
what the departure time was, what the departure delay was, what the arrival delay was, whether

289
00:23:05,640 --> 00:23:12,640
the flight was canceled or not, whether it was diverted or not, and which airline, and

290
00:23:13,200 --> 00:23:15,400
a fair number of statistics like that.

291
00:23:15,400 --> 00:23:18,920
So this is what we're going to use.

292
00:23:18,920 --> 00:23:25,920
It also tells you things like when the wheels off were, and it buckets the delays into 15-minute

293
00:23:26,280 --> 00:23:30,080
increments.

294
00:23:30,080 --> 00:23:37,080
Some other things it has is why it was canceled, and then what kinds of delay, was it a weather

295
00:23:37,560 --> 00:23:42,520
delay, was it a security delay, was it late aircraft arrival.

296
00:23:42,920 --> 00:23:45,560
Some of this data exists from 2003.

297
00:23:45,560 --> 00:23:48,680
Other data was added in 2008.

298
00:23:48,680 --> 00:23:50,760
Other fields were added.

299
00:23:50,760 --> 00:23:53,560
We have not done much cleanup to the data.

300
00:23:53,560 --> 00:23:58,640
We're using the direct CSVs without any cleanup.

301
00:23:58,640 --> 00:24:03,560
If you're doing this live, you'd probably want to do some cleanup.

302
00:24:03,560 --> 00:24:04,920
This is still running.

303
00:24:04,920 --> 00:24:07,920
While this is running, any questions so far?

304
00:24:07,920 --> 00:24:14,920
About three minutes.

305
00:24:17,920 --> 00:24:22,760
This is one of the reasons we don't use Pandas for large datasets.

306
00:24:22,760 --> 00:24:29,760
I'm showing this to you to show you the pain before we go to other things.

307
00:24:32,560 --> 00:24:36,080
If that didn't run, I would restart your kernel and try running it again.

308
00:24:36,080 --> 00:24:43,080
That was probably some kind of ... If this top cell didn't run, if this cell is still

309
00:24:43,480 --> 00:24:47,680
showing a star, just restart your kernel and try it again.

310
00:24:47,680 --> 00:24:52,680
You probably hit that race condition I was talking about.

311
00:24:52,680 --> 00:24:55,640
The top here, you see this little circle button?

312
00:24:55,640 --> 00:24:57,640
You can click that.

313
00:24:57,640 --> 00:25:00,560
Okay, do I have my ... Yeah, okay.

314
00:25:00,560 --> 00:25:01,560
Now I have my data.

315
00:25:02,040 --> 00:25:07,880
If I look at the top of the data, you can see I've got a year, quarter, month, month

316
00:25:07,880 --> 00:25:14,880
of day, day of week, the date, the carrier, tail number, wheels off.

317
00:25:15,640 --> 00:25:22,640
You see there's a lot of empty data as well towards the end of the data frame.

318
00:25:23,480 --> 00:25:25,760
I'm just going to make it simpler.

319
00:25:25,760 --> 00:25:27,440
I'm just going to take the first 60 columns.

320
00:25:27,480 --> 00:25:32,800
I think this data set has like 150 columns.

321
00:25:32,800 --> 00:25:37,360
This is the first 60.

322
00:25:37,360 --> 00:25:42,520
Let's see how much time people took to fly in 2022.

323
00:25:42,520 --> 00:25:49,520
We spent 1,600 years in aggregate flying around the country.

324
00:25:52,280 --> 00:25:53,600
First exercise.

325
00:25:53,600 --> 00:25:59,400
This should be simple pandas.

326
00:25:59,400 --> 00:26:03,240
Let's see what the maximum and average delays are in flight.

327
00:26:03,240 --> 00:26:08,240
If we look at the key ... You can either look here.

328
00:26:08,240 --> 00:26:10,040
Actually, no, actually.

329
00:26:10,040 --> 00:26:15,040
If you look at the JSON.D types, or if you look here, you can see what the delays are

330
00:26:15,040 --> 00:26:20,160
called.

331
00:26:20,240 --> 00:26:27,240
This is a depth delay and an ARR underscore delay.

332
00:26:28,400 --> 00:26:29,920
You can use that.

333
00:26:29,920 --> 00:26:36,920
I'm going to wait a couple of minutes to see.

334
00:26:43,960 --> 00:26:46,960
Go ahead.

335
00:26:46,960 --> 00:26:53,960
Go to this top URL.

336
00:26:57,040 --> 00:27:02,960
Then click on key cloak, and then click on register.

337
00:27:02,960 --> 00:27:09,960
Then you'll need to clone this repo.

338
00:27:10,040 --> 00:27:17,040
If you have finished the exercise, or if you're not planning to do the exercise, please put

339
00:27:18,440 --> 00:27:20,480
your little yellow tag up so I get a feel.

340
00:27:20,480 --> 00:27:27,480
Or if you're having trouble, do that too.

341
00:27:29,400 --> 00:27:36,400
Yellow means I'm done, or I'm good.

342
00:27:39,960 --> 00:27:46,960
If you hit ... If your kernel crashes, that is usually because you ran out of memory.

343
00:27:50,280 --> 00:27:54,440
If you have two notebooks open ... We have 16 GB.

344
00:27:54,440 --> 00:27:58,800
If you open one notebook and you've read in a bunch of stuff, and you open another notebook

345
00:27:58,800 --> 00:28:02,520
and you read in a bunch of stuff, one of the kernels will crash once you hit 16.

346
00:28:02,520 --> 00:28:05,440
That's one of the reasons we're going to start using Dask later.

347
00:28:05,560 --> 00:28:12,560
One thing you can do is you can go to this stop button here on the side, and you can

348
00:28:12,920 --> 00:28:18,640
see which kernels are running, and you can close any you don't want.

349
00:28:18,640 --> 00:28:19,640
We don't need these two.

350
00:28:19,640 --> 00:28:22,200
They don't have much data in them.

351
00:28:22,200 --> 00:28:25,600
Another thing you can see, and I'll do this a little later.

352
00:28:25,600 --> 00:28:30,000
Actually, I'll do that later.

353
00:28:30,000 --> 00:28:34,640
The person over there who was trying to get started, were you able to clone the Git repo,

354
00:28:34,640 --> 00:28:37,640
or do I need to show you that?

355
00:28:37,640 --> 00:28:44,400
Okay, yeah.

356
00:28:44,400 --> 00:28:48,720
If anyone wants to see what memory usage looks like while you're doing this stuff, look at

357
00:28:48,720 --> 00:28:50,680
this little cup thing here.

358
00:28:50,680 --> 00:28:53,280
If we click this, it says machine resources.

359
00:28:53,280 --> 00:29:00,280
If you click that, you'll get some graphs, and you can move that somewhere, and you can

360
00:29:00,600 --> 00:29:02,320
see what CPU memory.

361
00:29:02,320 --> 00:29:05,560
Right now, I'm using ... I don't know.

362
00:29:05,560 --> 00:29:12,560
I can't tell that's ... Yeah, right now, I'm at 11 GB of memory usage.

363
00:29:20,800 --> 00:29:21,800
I might keep that open.

364
00:29:21,800 --> 00:29:28,800
Okay, I'm going to move on.

365
00:29:30,000 --> 00:29:32,280
Let's see.

366
00:29:32,360 --> 00:29:37,360
This is just a little ... Just pandas.

367
00:29:37,360 --> 00:29:43,800
We're just saying what the mean and max delay is.

368
00:29:43,800 --> 00:29:50,800
The maximum departure delay is ... Is that 60 hours?

369
00:29:51,800 --> 00:29:54,320
Yeah, I think that's 60 hours.

370
00:29:54,320 --> 00:29:56,440
Yeah, 57 hours.

371
00:29:56,440 --> 00:30:01,440
The average is about 12 minutes.

372
00:30:01,440 --> 00:30:08,440
Okay, vote.

373
00:30:10,120 --> 00:30:12,640
Which airport do you think has the most delays?

374
00:30:12,640 --> 00:30:14,600
The most mean delays?

375
00:30:14,600 --> 00:30:16,960
Pick your least favorite airport.

376
00:30:16,960 --> 00:30:19,000
I would have said Dallas.

377
00:30:19,000 --> 00:30:20,000
JFK.

378
00:30:20,000 --> 00:30:21,000
JFK.

379
00:30:21,000 --> 00:30:22,000
Anyone else?

380
00:30:22,000 --> 00:30:27,000
Okay, let's see where it is.

381
00:30:27,000 --> 00:30:30,600
It's Atlanta.

382
00:30:30,600 --> 00:30:35,600
I would have thought it was DFW, though.

383
00:30:35,600 --> 00:30:37,880
DFW is massive, too.

384
00:30:37,880 --> 00:30:38,880
I don't know.

385
00:30:38,880 --> 00:30:39,880
LAX is massive.

386
00:30:39,880 --> 00:30:40,880
Is it?

387
00:30:40,880 --> 00:30:43,880
Yeah, I mean, you could.

388
00:30:43,880 --> 00:30:44,880
I don't know.

389
00:30:44,880 --> 00:30:48,400
This data set doesn't have ... I guess you could do it ... You have number of flights

390
00:30:48,400 --> 00:30:51,520
from each airport, so you could actually calculate that.

391
00:30:51,520 --> 00:30:56,080
See if there's a ... Do a scatter plot between ... If you want, you can try that.

392
00:30:56,080 --> 00:31:03,080
Do a scatter plot between the two, but yeah.

393
00:31:11,360 --> 00:31:12,640
Okay, I'm a little confused here.

394
00:31:12,640 --> 00:31:18,360
I think we put this solution in the wrong place.

395
00:31:18,360 --> 00:31:23,360
I'm just going to run this because I don't know what's in here, this section of code.

396
00:31:23,600 --> 00:31:25,800
Oh, okay.

397
00:31:25,800 --> 00:31:27,960
This was ... Yeah, airline.

398
00:31:27,960 --> 00:31:30,680
Do you want ... Okay, let's vote.

399
00:31:30,680 --> 00:31:35,680
Which airline do you think has the most delays?

400
00:31:35,680 --> 00:31:36,680
Spirit.

401
00:31:36,680 --> 00:31:37,680
Okay, let's see.

402
00:31:37,680 --> 00:31:38,680
Anyone else?

403
00:31:38,680 --> 00:31:41,680
We've got Spirit and Southwest as leading contenders.

404
00:31:41,680 --> 00:31:48,400
If you want, you guys can run this, or you can try writing this.

405
00:31:48,400 --> 00:31:50,920
This is just a simple group by delay.

406
00:31:50,960 --> 00:31:56,440
I'm going fast on these because this is just pandas right now, so there's plenty of really

407
00:31:56,440 --> 00:31:58,440
good pandas tutorials out there.

408
00:31:58,440 --> 00:32:05,440
Anyway, I'm just going to keep moving.

409
00:32:07,320 --> 00:32:14,320
It is ... Why is that ... Yeah, it is Southwest.

410
00:32:15,320 --> 00:32:22,320
I'm going to cancel that.

411
00:32:23,320 --> 00:32:30,320
Oops, that's not what I wanted to do.

412
00:32:30,320 --> 00:32:37,320
Okay, I'm going to move a little faster so we can get to the Dask things.

413
00:32:37,320 --> 00:32:44,320
I'm going to now try and show you why we need other tools than pandas.

414
00:33:00,160 --> 00:33:05,920
Here, I'm now getting the entire dataset, like all the CSVs.

415
00:33:05,920 --> 00:33:12,920
If we look at this, this is 240 files.

416
00:33:13,960 --> 00:33:20,960
Each one ... If I remember correct, each CSV file is about 200 to 300 MB, and the total

417
00:33:22,080 --> 00:33:24,480
dataset is about 70 gigabytes.

418
00:33:24,480 --> 00:33:31,480
I am now going to try and load all the data.

419
00:33:32,480 --> 00:33:39,480
If we look at this memory plot, what you should see is the memory starting to go up.

420
00:33:39,680 --> 00:33:46,680
One second.

421
00:33:55,640 --> 00:34:01,640
Just as a note, sometimes these plots are getting stuck, and if you tab away from them

422
00:34:01,640 --> 00:34:04,840
and then tab back, they refresh.

423
00:34:04,840 --> 00:34:11,840
We need to fix that.

424
00:34:11,840 --> 00:34:18,840
Okay, it's not ... I'm trying to work out why it's not reading right now.

425
00:34:42,640 --> 00:34:49,640
Has anyone tried running this last cell? Is it reading?

426
00:34:56,080 --> 00:35:02,360
I promise you this will crash your browser. It's not doing it right now for me, but I'm

427
00:35:02,360 --> 00:35:09,360
going to ... I'm going to stop this.

428
00:35:11,840 --> 00:35:18,840
Okay. Let's do a quick detour into plotting.

429
00:35:18,840 --> 00:35:25,840
Okay, so we're going to do some interactive plotting with HVPlot. To give you a little

430
00:35:25,840 --> 00:35:32,840
background, this is the ecosystem we're looking at. There's a tool called HoloView, and it's

431
00:35:55,840 --> 00:36:02,840
used that wraps several other tools, like Bokeh is a plotting tool, Matplotlib, and

432
00:36:02,840 --> 00:36:08,720
then you've heard of Plotly. There's also Altair. There's actually a really complicated

433
00:36:08,720 --> 00:36:12,840
diagram.

434
00:36:12,840 --> 00:36:19,440
DataShader is another tool for managing really big datasets. HVPlot is an API on top of

435
00:36:19,440 --> 00:36:25,120
these other tools to make it easy to use those other tools with a simpler API. But you can

436
00:36:25,120 --> 00:36:29,920
also ... All the plots I'm showing you, you can go down into the lower level libraries

437
00:36:29,920 --> 00:36:36,560
and plot it from there as well. This is a higher level tool, and it's designed to mimic

438
00:36:36,560 --> 00:36:43,560
the Pandas.plot API. If you've ever done a .plot in Pandas, HVPlot mimics that API to

439
00:36:44,800 --> 00:36:51,760
make it a very easy transition to get more features, and we'll show that. Let's just

440
00:36:51,760 --> 00:36:58,520
go Pandas, and then we're going to import HVPlot, and here I'm saying use the Bokeh

441
00:36:58,520 --> 00:37:04,560
backend for HVPlot. The Bokeh backend will give you interactive plots. If I put Matplotlib

442
00:37:04,560 --> 00:37:09,480
there, you'd get the same plots, but it would be static. You wouldn't be able to do any

443
00:37:09,480 --> 00:37:13,240
interaction.

444
00:37:13,240 --> 00:37:20,240
This command here, import HVPlot.Pandas, that means replace the Pandas.plot and add a ... Well,

445
00:37:23,760 --> 00:37:31,120
actually add a .hvplot to Pandas. Anything you do with .plot, you can now do .hvplot.

446
00:37:31,120 --> 00:37:37,920
I'm just going to read in one year of data, and I'll just show you how big this dataset.

447
00:37:37,920 --> 00:37:44,920
One year of data is about, I think, six million rows, and this will probably run out of memory

448
00:37:51,680 --> 00:37:57,560
on your medium-sized instance. Actually, I'm limiting that, yes, it's about six million

449
00:37:57,560 --> 00:38:04,560
rows, so I'm actually just going to limit it to United, American, Southwest, and Delta.

450
00:38:05,560 --> 00:38:11,680
That makes it about three million rows, and that kind of fits in memory here. You guys

451
00:38:11,680 --> 00:38:18,040
have probably done this before. This is just a group by the date, flight date, and look

452
00:38:18,040 --> 00:38:24,880
at the mean of the departure delay. This gives you your standard Matplotlib plot that you

453
00:38:24,880 --> 00:38:28,680
get with Pandas.

454
00:38:28,680 --> 00:38:35,680
I am now going to repeat that, and here, instead of .plot, I'm just going to change it to .hvplot.

455
00:38:39,480 --> 00:38:44,840
Now I get this, and it looks similar. It looks like styling differences, but now this is

456
00:38:44,840 --> 00:38:51,840
an interactive plot. I can zoom. I can pan, and I have hover tools, so I can look at each

457
00:38:51,840 --> 00:38:58,840
exact point. Simple. You don't need to know anything else. Just wherever you do .plot,

458
00:39:01,760 --> 00:39:08,760
do .hvplot. There are some slight differences in the keyword arguments because there's more

459
00:39:08,760 --> 00:39:14,760
features. In .hvplot, there's a hover column, so you can say which columns show up when

460
00:39:14,760 --> 00:39:21,760
you hover, which you obviously don't have in .plot because .plot doesn't have a hover

461
00:39:21,840 --> 00:39:28,840
ability. Let's try this. Let's see if you can take this plot. Take this plot you have

462
00:39:42,360 --> 00:39:47,240
here where we're group buying flight date and we're showing departure delay, and let's

463
00:39:47,240 --> 00:39:54,240
add arrival delay to this plot as well. You have both on here. I'll give you guys a few

464
00:39:56,000 --> 00:40:03,000
minutes to do that, and we'll take any questions while we're waiting. Actually, I guess I should

465
00:40:09,000 --> 00:40:16,000
look at chat. Oh. Disconnected from chat.

466
00:40:17,240 --> 00:40:24,240
Okay.

467
00:40:47,240 --> 00:40:54,240
While I'm waiting for you, I'm just going to check on our time.

468
00:41:17,240 --> 00:41:24,240
Feel free to change things if you don't want to do a mean and if you want to do a max or

469
00:41:24,240 --> 00:41:31,240
cum sum, cumulative sum, or something else.

470
00:41:43,240 --> 00:41:50,240
Okay.

471
00:41:54,240 --> 00:42:01,240
Yeah, if you just say what you're saying, instead of doing .mean, you can also do .ag

472
00:42:03,200 --> 00:42:10,200
and how equals mean max. It's just changing the plotting mechanism, but it's designed

473
00:42:12,080 --> 00:42:19,080
to mimic Panda's existing API. You guys ready for me to show the solution?

474
00:42:24,240 --> 00:42:31,240
Okay, go ahead. Probably go back to the top. That's that race condition I talked about.

475
00:42:37,200 --> 00:42:44,200
If you hit restart on the kernel, this button, and then run the first cell and make sure

476
00:42:46,440 --> 00:42:51,920
it completes fully, and then wait a few seconds and then run the second cell. Essentially,

477
00:42:51,920 --> 00:42:58,840
this command hvplot.extension injects some JavaScript into your Jupyter Notebook to

478
00:42:58,840 --> 00:43:05,840
enable interactivity. Currently, I filed a bug, but there's a bug with Jupyter Hub where

479
00:43:07,720 --> 00:43:14,960
there's a slight delay in the kernel being available and the JavaScript gets messed up.

480
00:43:14,960 --> 00:43:20,520
You can also see this if you restart a kernel and hit run all cells. Sometimes the first

481
00:43:20,560 --> 00:43:26,040
two cells don't get executed correctly. That's an issue in JupyterLab that I filed a bug

482
00:43:26,040 --> 00:43:33,040
on. Okay. I'm going to ... Anyone want me to wait or I'm going to go ahead and look

483
00:43:38,040 --> 00:43:45,040
at the solution here. Here, just a one-liner. I'm just adding arrival delay into the data

484
00:43:45,720 --> 00:43:51,360
frame, and now I'm going to plot both. Actually, this is interesting. If you look at the monthly

485
00:43:51,360 --> 00:43:57,960
arrival and departure delay, arrival delays are shorter than departure delays. That sort

486
00:43:57,960 --> 00:44:02,720
of makes sense because often they can make up time during the flight. They burn a little

487
00:44:02,720 --> 00:44:09,720
more fuel and go faster. You can also now click on these to shade them out. If you have

488
00:44:10,720 --> 00:44:16,840
lots of them, you can kind of emphasize which one you're doing. Again, HVPlot just gives

489
00:44:16,840 --> 00:44:23,840
you a lot more interactivity. Right now, I think ... Yeah. I won't go anything else

490
00:44:25,840 --> 00:44:31,480
with that. I'm going to run this histogram. Again, I'll just show you that it ... The

491
00:44:31,480 --> 00:44:36,040
whole point of this is this just gives you some extra powers over your existing plot

492
00:44:36,040 --> 00:44:43,040
command. Here, we have the histograms of the various ... Oops. We have the histograms of

493
00:44:45,560 --> 00:44:52,560
the various carriers. If I hover, it gives me actual information. Which bin it is, how

494
00:44:53,000 --> 00:45:00,000
many points are in this bin. Again, same plots, just a few extra features.

495
00:45:00,240 --> 00:45:07,240
The other thing that HVPlot gives you is widgets. Here ... Let me just show you something in

496
00:45:16,400 --> 00:45:23,400
the command here. Here, I gave it this keyword, subplots equals true. In this, I'm doing a

497
00:45:23,400 --> 00:45:30,400
group by carrier, and I'm saying subplots equals true. It gave me a plot for each carrier.

498
00:45:30,800 --> 00:45:37,800
If I leave that out and just do the group by ... I'm doing a histogram grouped by carrier,

499
00:45:39,920 --> 00:45:46,920
but I haven't put subplots equals true. Now, it'll give me a widget to select with. See,

500
00:45:47,920 --> 00:45:54,920
instead of having four plots ... This I did on the entire dataset. Here, now we have all

501
00:45:58,320 --> 00:46:05,320
the carriers. Let me look at delta. I click delta, and I'll see delta. Let me pick ... I

502
00:46:06,920 --> 00:46:13,920
don't even know what NK is. Anyway, it gives you easy widgets. We'll use these widgets.

503
00:46:14,920 --> 00:46:19,280
This is an automatic widget, but in one of the exercises later, we'll actually make our

504
00:46:19,280 --> 00:46:26,280
own. I would like you to go ... This next exercise, if you go up ... I think I have

505
00:46:31,480 --> 00:46:38,480
a link to the HVPlot doc gallery. Do I have a link to the HVPlot? Let's go here. If we

506
00:46:38,480 --> 00:46:45,480
go to HVPlot, and if we go to, I think, reference gallery, let's pick the violin plot, which

507
00:46:58,240 --> 00:47:05,240
is over here. I'm going to go to the reference gallery. I'm going to go to the reference

508
00:47:08,480 --> 00:47:15,480
gallery. Okay, I think our internet slowed down. Yeah. You'd think the conference center

509
00:47:17,640 --> 00:47:24,640
would ... Yeah, okay. Let's look at this violin plot, and see if you can take the code you

510
00:47:29,120 --> 00:47:36,120
have from your box, from one of these, and create a violin plot for the delays.

511
00:47:39,000 --> 00:47:46,000
In interest of time, there's three more exercises in this notebook. In interest of time, I'm

512
00:47:47,800 --> 00:47:52,720
going to go through the rest of the material, and then leave the exercises for you guys

513
00:47:52,720 --> 00:47:59,720
to do. We're going to take a break after this as well. You can work on it through the break.

514
00:48:00,040 --> 00:48:04,400
Right now, this is all just pandas. I want to spend less time on this and move to the

515
00:48:04,400 --> 00:48:09,920
big data parts. I think that's what I'm going to do. If you click the three dots, you'll

516
00:48:09,920 --> 00:48:16,920
see the code for the violin plot. Two last things I want to show with the HVPlot is we

517
00:48:17,880 --> 00:48:24,880
have the ability to very simply compose plots using plus and multiply operations.

518
00:48:26,120 --> 00:48:32,480
So here, I'm just going to look. I've got this group by for delays, and it's just this

519
00:48:32,560 --> 00:48:39,560
min-max with carrier. Now, I've got a plot, a min-max plot of areas, and a mean plot.

520
00:48:47,160 --> 00:48:50,640
If you look, this is one of the things. I made this min-max plot, but you didn't see

521
00:48:50,640 --> 00:48:57,640
it. If I type min-max now, min-max plot, now it'll display. This is my min-max plot. It's

522
00:48:57,640 --> 00:49:04,640
got a drop down. Then, let's see if we want to see this one. Mean plot. I have these two

523
00:49:14,880 --> 00:49:21,880
plots, nice two simple plots. Now, what I can do is I want to combine these two, and

524
00:49:22,880 --> 00:49:29,880
HVPlot has a simple way of composing plots. If you want to put two plots side by side,

525
00:49:29,880 --> 00:49:36,880
you can just add them, and you'll see them side by side, and their axes are linked. If

526
00:49:39,880 --> 00:49:44,880
I zoom in on this one, I zoom in on the other one, so the axes are linked. If I just want

527
00:49:44,880 --> 00:49:49,880
to zoom this way, I guess that axis won't link, but the main axis is linked. Now, instead

528
00:49:50,720 --> 00:49:55,480
of putting them side by side, if I want to overlay them on the same plot, you can do

529
00:49:55,480 --> 00:50:01,840
this. If I multiply them, both the plots are overlaid on the same thing, and with the same

530
00:50:01,840 --> 00:50:07,920
scale, so it looks like a straight line, but if I actually zoomed in down here, you'd see

531
00:50:07,920 --> 00:50:14,920
it is not really a straight line. It just looks like a straight line. Anyway. Another

532
00:50:15,920 --> 00:50:22,600
thing it gives you, I don't use this as much, but HVPlot, you can just give it a Pandas

533
00:50:22,600 --> 00:50:29,600
data frame, and it will give you a little Explorer interface on it. So if you click

534
00:50:29,600 --> 00:50:36,600
HVPlot.Explorer, it will give you, in a couple of seconds, a couple of seconds, and then

535
00:50:44,920 --> 00:50:51,920
you can see it. Okay, this usually doesn't take this long. I'm going to try kill it and

536
00:51:01,880 --> 00:51:08,880
try again. I have plenty of memory. Five here, five here. Okay. So, I'm going to try to get

537
00:51:14,920 --> 00:51:21,920
it. It looks like something's gone wrong. Did it come up for you? Okay. Okay. I do not

538
00:51:37,520 --> 00:51:44,520
know why it's not coming up for me, but let me try restarting. Okay. So, I'm going to

539
00:51:44,920 --> 00:51:51,920
try restarting. Actually, you know what? I have it up here. Yeah, I think we're hitting

540
00:51:51,920 --> 00:51:58,920
some of the Wi-Fi limitations here. This should not take this long to come up. Okay. So, I

541
00:52:21,920 --> 00:52:28,480
pre-ran this earlier just in case I had issues, but if you run the Explorer... Okay, this

542
00:52:28,480 --> 00:52:33,160
is what the violin plot would look like if you ran it. If you ran the Explorer, you'd

543
00:52:33,160 --> 00:52:38,880
get a little widget like this where it's got all the fields, and you could decide what

544
00:52:38,880 --> 00:52:45,320
your X axis, what your Y axis, whether you want... What kind of... What do you want to

545
00:52:45,320 --> 00:52:51,640
align, a bar plot, what do you want to group by, all that kind of stuff, and you can set

546
00:52:51,640 --> 00:52:56,720
your labels and axes and stuff. And then at the end, once you have the plot looking the

547
00:52:56,720 --> 00:53:03,720
way you like it, you can hit Save Plot Clote, and it'll give you the pandas code to do that.

548
00:53:14,080 --> 00:53:21,080
Last I'm going to... Actually, I killed this, I think. So... Okay. So, I'm going to go back

549
00:53:22,440 --> 00:53:29,440
and... Yeah. If you run the last one, I've just got... This shows you that HG plot also

550
00:53:30,280 --> 00:53:37,080
lets you do geographic stuff. So, I've read the... I've got the CSP that just gives me

551
00:53:37,080 --> 00:53:45,080
the lat long of all the airports, which we don't actually have in our dataset, and I'm

552
00:53:45,080 --> 00:53:52,080
doing a group by... I'm finding the mean departure delay. We did this earlier. We found out Atlanta

553
00:53:52,720 --> 00:53:59,600
was the worst place. But now I'm just doing the mean departure delay for all the airports.

554
00:53:59,600 --> 00:54:06,040
I'm merging it with this airport's dataset to get latitude and longitude, and now I'm

555
00:54:06,040 --> 00:54:13,040
plotting the points, and I'm saying geo equals true because I want a geospatial plot, and

556
00:54:13,200 --> 00:54:20,200
I'm saying I'm going to use these ESRI tiles mapping service. And I also just put the latitude,

557
00:54:20,200 --> 00:54:27,200
longitude limits for the US. And so now you have an interactive plot where you can actually

558
00:54:29,680 --> 00:54:36,680
zoom in and see what the delays are, and these points are color coded by how much delay.

559
00:54:37,000 --> 00:54:44,000
So... Okay. So, I was going to stop there and give folks time for a 10-minute break, or

560
00:54:47,480 --> 00:54:52,880
if folks have questions and want to try the exercises, we can... Folks who want to stick

561
00:54:52,880 --> 00:54:59,880
around can. But we'll start again at 10.10. You can install all this yourself. There's

562
00:55:00,880 --> 00:55:06,280
an excellent set of things called Zero to Jupiter Hub with Kubernetes, and it's got

563
00:55:06,280 --> 00:55:10,120
step-by-step, you know, do this, change this in Kubernetes, do this, do this, do this,

564
00:55:10,120 --> 00:55:16,080
do this. We got fed up of doing that lots and lots of times, and so we built a bunch

565
00:55:16,080 --> 00:55:22,840
of installation scripts that make it very easy to get everything you need running. And

566
00:55:22,840 --> 00:55:29,840
so we've called that NABARI. We've been working on it for about the last two years, and essentially

567
00:55:30,560 --> 00:55:37,560
if you have cloud credentials, and whichever cloud you care about, like DCP or AWS or whatever,

568
00:55:39,880 --> 00:55:46,400
you can get this entire platform up and running in about 30 minutes. You have a guided install

569
00:55:46,400 --> 00:55:50,480
that says, okay, which cloud are you on? Paste your credentials here, blah, blah, blah. It'll

570
00:55:50,480 --> 00:55:56,120
spit out a YAML file with, like, you know, which you can edit, like, which instances

571
00:55:56,200 --> 00:56:02,640
you want, GPU, non-GPU, and then you hit Deploy, you run Deploy, and it deploys everything

572
00:56:02,640 --> 00:56:09,640
on the cloud of your choice with, you know, all the stuff you're seeing today, and then

573
00:56:10,000 --> 00:56:15,280
it also integrates with GitHub or GitLab actions. So then if you want to make changes to your

574
00:56:15,280 --> 00:56:19,480
infrastructure, you just make changes to that YAML file, and it'll kick off a new install.

575
00:56:19,480 --> 00:56:25,560
So I don't know if any of you are old enough to remember when AWS first came around, there

576
00:56:25,560 --> 00:56:31,880
was a thing called StarCluster, and it was a simple script someone at MIT wrote to just

577
00:56:31,880 --> 00:56:37,160
launch your stuff in the cloud so you could run stuff. That's the vision behind this.

578
00:56:37,160 --> 00:56:40,960
Get your platform installed and get out of the way so you can actually get work done.

579
00:56:40,960 --> 00:56:47,960
And so that's what Navari is. It's essentially, let me see, where is it? Do I have a link?

580
00:56:48,960 --> 00:56:54,520
Oh, I don't have a link here. There's a brochure here which shows you more, but it essentially

581
00:56:54,520 --> 00:57:01,520
integrates, it's all the deployment scripts you need to install JupyterHub, Dask, Argo

582
00:57:01,520 --> 00:57:08,520
Workflows, ClearML, and a few other tools like in the cloud, and also we do, one thing

583
00:57:09,400 --> 00:57:16,400
I'll show at the end is we also give you a tool to manage environments. So that if you,

584
00:57:18,320 --> 00:57:24,720
like for example, this is the PyCon environment we're using right now, and you can see that

585
00:57:24,720 --> 00:57:31,720
over the last several days we made several versions of it. And so it's, the idea of Navari

586
00:57:33,200 --> 00:57:36,240
is to try and get everything installed and get out of the way so you can actually do

587
00:57:36,240 --> 00:57:43,240
work. So, but again, you don't have to use Navari to use any of the tools I'm showing

588
00:57:43,240 --> 00:57:49,320
today. That's just a way, easy way to install everything. And in one of the last notebooks

589
00:57:49,320 --> 00:57:55,320
I'll give you some other options if you want to use Dask and stuff. There are several options

590
00:57:55,320 --> 00:57:58,480
on how you get things installed.

591
00:57:58,480 --> 00:58:05,480
Okay, I'm going to go to this notebook three introduction to Dask. Okay. So, I'm going

592
00:58:07,240 --> 00:58:14,240
to go to this notebook three introduction to Dask. So, Dask is a tool to distribute compute.

593
00:58:16,960 --> 00:58:23,960
And it is very nice. One of the reasons I really like it is it mimics, it's a very

594
00:58:26,840 --> 00:58:33,840
PyData friendly tool. If you're used to using Python, it maps really well to the stuff you

595
00:58:34,200 --> 00:58:40,840
already know. So, there are other tools like Spark. And Spark came out of Java and came

596
00:58:40,840 --> 00:58:46,160
out of the concept of MapReduce. And Spark can be very powerful if you understand MapReduce

597
00:58:46,160 --> 00:58:52,160
and if you like Java. If you try and use PySpark from Python, you will get annoyed very quickly.

598
00:58:52,160 --> 00:58:57,440
Again, but PySpark does do certain things really well. So I'm not saying you shouldn't

599
00:58:57,440 --> 00:59:04,440
use it. Dask takes your computation, splits it up and executes it on a bunch of nodes

600
00:59:07,640 --> 00:59:13,680
for you and gives you back the answer. It has four APIs. I'm only going to go through

601
00:59:13,680 --> 00:59:20,680
one. It has, I'm not sure if we listed the APIs here. No, I don't think we did. It has

602
00:59:21,160 --> 00:59:28,160
an array API which mimics NumPy. So if you use NumPy arrays, there's a Dask array that

603
00:59:30,200 --> 00:59:34,760
is very similar to NumPy array. The main one we're going to use today is the Dask data

604
00:59:34,760 --> 00:59:41,760
frame API. And it mimics the pandas API. Then there's another one called Dask bag which

605
00:59:41,960 --> 00:59:46,120
is for unstructured data. Which is like if you're doing JSON files or something and it's

606
00:59:46,120 --> 00:59:51,800
not really, so Dask bag can be used for that. And then they have a low API called Dask delayed

607
00:59:51,800 --> 00:59:58,800
which you can use to do arbitrary parallel compute. Like for example, Spark is very specifically

608
00:59:59,760 --> 01:00:05,800
a MapReduce type algorithm. So you have to transform your, whatever your compute into

609
01:00:05,800 --> 01:00:11,840
the MapReduce paradigm to run it usually. Delayed lets you do kind of like say, hey,

610
01:00:11,840 --> 01:00:14,760
this function I want to run in parallel. This function I want to delay it. You know, and

611
01:00:14,760 --> 01:00:21,200
it just kind of lets you construct things. I'm going to run this first thing. So as I

612
01:00:21,200 --> 01:00:26,560
said, we're going to import the Dask data frame as DD. Like kind of like how you do

613
01:00:26,560 --> 01:00:33,560
import pandas as PD. If you notice, I said it's a similar API. It's not the same. And

614
01:00:34,800 --> 01:00:39,840
this, there's a reason for this. There's other projects like moden and I forget there's one

615
01:00:39,840 --> 01:00:46,840
more that try and give you the exact pandas API. The problem with it is then you're stuck

616
01:00:47,200 --> 01:00:51,480
with decisions pandas is made. Like one of the things we saw earlier, if you want to

617
01:00:51,480 --> 01:00:57,320
read multiple files in, you have to do this whole list concat business, which is fine

618
01:00:57,320 --> 01:01:00,520
when you're only reading a few data sets. But if you're doing big data and you're going

619
01:01:00,520 --> 01:01:07,520
to read 10,000 files, you don't want to do a list concat. And so while, and there's certain

620
01:01:08,320 --> 01:01:15,320
things, there's certain like, one thing we're going to go into is local memory versus remote

621
01:01:16,080 --> 01:01:21,640
memory. And there are some computations. So your local memory is limited. We have 16 GB

622
01:01:21,640 --> 01:01:28,640
in this medium instance type. We have lots of memory distributed over our entire cluster.

623
01:01:30,880 --> 01:01:35,200
Anything you bring locally has to fit in that local memory. And so there are some extra

624
01:01:35,200 --> 01:01:39,920
keywords for some functions. Like if you do a drop duplicates in pandas, fine, you do

625
01:01:39,920 --> 01:01:46,920
drop duplicates. If you do a drop duplicates in Dask, it can crash your local machine because

626
01:01:48,120 --> 01:01:52,000
drop duplicates is one of those things where you need to see all the data. And so it has

627
01:01:52,000 --> 01:01:58,880
some extra keywords in there to use an algorithm that does drop duplicates in pieces and then

628
01:01:58,880 --> 01:02:02,960
puts the pieces together. So it has to go through several cycles and you can control

629
01:02:02,960 --> 01:02:09,960
that. And so even though it's data frame inspired, it's not exactly the same as pandas data frame.

630
01:02:10,720 --> 01:02:17,720
Okay. Let me see. I think we are hitting Wi-Fi issues. Oh, it ran the second time for me.

631
01:02:17,720 --> 01:02:24,720
So, okay. Let's see. So now we're going to do the same thing. But now we're going to

632
01:02:34,360 --> 01:02:41,360
read in four months of data into a convention as we call Dask data frames, ddf instead of

633
01:02:42,760 --> 01:02:46,480
df, just so you know it's a distributed data frame. But you don't have to. So I'm going

634
01:02:46,480 --> 01:02:53,480
to read four months of data. Oh, that happened really quickly. Yeah. The star means it's

635
01:02:55,520 --> 01:03:02,200
executing or stuck, you know, whatever. So if you notice that ran really quickly. When

636
01:03:02,200 --> 01:03:09,200
you try to read a year of data in with pandas, it took a significant amount of time. Four

637
01:03:09,200 --> 01:03:15,960
minutes should have taken longer. But there's a reason. Dask actually hasn't read the file.

638
01:03:15,960 --> 01:03:22,960
If you look at it, Dask does what's called lazy evaluation. I've told it I want to read

639
01:03:24,120 --> 01:03:31,120
these four files. It's just put in its graph, I need to read four files. It actually hasn't

640
01:03:31,200 --> 01:03:38,200
read them. And so this is showing that it's gone to those files, it's looked at the structure

641
01:03:38,520 --> 01:03:45,520
of the files, and it's decided to divide those into seven pieces. Like you see the end partition

642
01:03:45,920 --> 01:03:52,920
but it hasn't done anything yet. It's just showing you the metadata. So if you actually

643
01:03:55,360 --> 01:04:01,560
want it to calculate, you actually have to say .compute. Again, now this is not something

644
01:04:01,560 --> 01:04:07,200
you have in pandas. Pandas does not have a .compute. Pandas is what's called eager. When you tell

645
01:04:07,200 --> 01:04:12,520
pandas to do something, it does it immediately. When you tell Dask to do something, it doesn't

646
01:04:12,560 --> 01:04:19,000
do it, it just adds nodes to its calculation graph. And one of the reasons is you might

647
01:04:19,000 --> 01:04:25,040
add several nodes, and by the end of it, you've done some filtering and done some other things,

648
01:04:25,040 --> 01:04:30,320
and so the data set's much smaller. So maybe it doesn't need to read all the files. So

649
01:04:30,320 --> 01:04:37,320
by being lazy, you have some advantages. But now I'm going to run compute, and now it's

650
01:04:37,320 --> 01:04:42,520
actually going to chug, and it's going to take a few seconds. Right now it's going to

651
01:04:42,520 --> 01:04:48,200
use the cores which I have on this instance, because I haven't started. So now I did a

652
01:04:48,200 --> 01:04:55,200
unique, and we've got four months of data. There are certain things like head, which

653
01:04:58,120 --> 01:05:04,280
so earlier I told you, I said we have to use .compute. There are certain commands that

654
01:05:04,280 --> 01:05:11,160
will implicitly compute. So if you do a .head, it has to read the data to show you the beginning

655
01:05:11,160 --> 01:05:18,160
of the data. So I'm going to run .head, and it gave me an error. Okay. Let's read this

656
01:05:25,480 --> 01:05:32,480
error. Column 77 and 84 has mixed types. Okay. So I'm going to run .head, and it's

657
01:05:34,480 --> 01:05:40,840
okay. You don't have this problem with pandas, okay, because you're reading one file. And

658
01:05:40,840 --> 01:05:47,680
when you read pandas, it looks at every column, and it makes a decision on what data type

659
01:05:47,680 --> 01:05:52,880
to give that column. And sometimes it may give a D type that you didn't like. Maybe

660
01:05:52,880 --> 01:05:57,940
it took an integer and made it a float, or maybe it took something and made it a string.

661
01:05:57,940 --> 01:06:01,640
But you can kind of specify D types. But often the first thing you do with pandas is just

662
01:06:01,640 --> 01:06:06,200
say read CSV without specifying D types, and then if you care, you might transform them

663
01:06:06,200 --> 01:06:13,200
or you might put the D types in. Now, as I told you, Dask is lazy. If you give it 10,000

664
01:06:14,160 --> 01:06:21,160
files, if it had to open each file to check the D types, and, you know, that would take

665
01:06:22,000 --> 01:06:29,000
forever. So what it does is it looks at the first file and a certain number of rows in

666
01:06:29,000 --> 01:06:35,040
the first file, and it guesses what the D types are. And if you have a good data set

667
01:06:35,040 --> 01:06:42,040
and it's fairly uniform, everything works fine. In our case, the first file had a bunch

668
01:06:44,280 --> 01:06:51,280
of, you know, no data in one column, and one of the other files had floats or a missing

669
01:06:52,280 --> 01:06:59,280
NA or something. So it made a guess based on the first file, I think this column is

670
01:07:00,440 --> 01:07:05,080
a string column. But when you did head and now trying to read it and it tried to read

671
01:07:05,080 --> 01:07:10,080
the second file, assuming that column was a string column, and it said, no, that didn't

672
01:07:10,080 --> 01:07:15,720
work. So that's what this had. This is why we'll get to storage formats, why when you

673
01:07:15,720 --> 01:07:22,720
do big data, storage formats matter. So this is a big horrible error. Now, instead, what

674
01:07:24,560 --> 01:07:31,560
we're going to do, you know, I spent some time actually looking through the data and

675
01:07:32,800 --> 01:07:39,800
creating this JSON file, and I gave it very explicit storage types. Now, if you look,

676
01:07:40,240 --> 01:07:43,280
if you want, you can just make it simple. You can just say integer, string, string,

677
01:07:43,280 --> 01:07:47,560
integer. I actually went ahead and did like integer 16, integer 8, you know, and these

678
01:07:47,560 --> 01:07:51,360
are different sizes. Because when you're reading one file, this doesn't matter. But when you're

679
01:07:51,360 --> 01:07:58,360
reading 10,000 files, an integer 32 or an integer 64 is four times as big as an integer

680
01:07:58,360 --> 01:08:04,080
8. So if you know your number is only like a 0 and 1, use a Boolean. If you know the

681
01:08:04,080 --> 01:08:09,320
numbers between this and this. So you can save a lot of memory, which doesn't matter

682
01:08:09,320 --> 01:08:14,040
when you're just doing one or two files. But when you're doing 10,000 or 20,000, that adds

683
01:08:14,040 --> 01:08:19,000
up to a lot of memory. And so taking some time to look at the data, explore it, and

684
01:08:19,000 --> 01:08:24,880
think through, okay, what can I get away with, will really impact how well you can do big

685
01:08:24,880 --> 01:08:31,720
data. One of the things I want to kind of reinforce as we go through this is everyone

686
01:08:31,720 --> 01:08:37,720
wants a magic scale button. I want to take my data set and hit scale, and it just scales.

687
01:08:38,520 --> 01:08:44,640
But what you'll see through this is you get some initial benefits with the automatic scale

688
01:08:44,640 --> 01:08:48,760
button initially. Like, you know, oh, I can go from this data set to this bigger data

689
01:08:48,760 --> 01:08:55,680
set. But eventually you have to think through the fact that your data is not local. It's

690
01:08:55,680 --> 01:09:02,480
on multiple machines, and moving things between machines costs a lot of time. And so you have

691
01:09:02,480 --> 01:09:07,400
to start thinking about that if you want to effectively do larger data sets. The magic

692
01:09:07,400 --> 01:09:14,200
scale button will work until it doesn't. You know? And that's why some tools like this,

693
01:09:14,200 --> 01:09:17,880
you know, there are a couple of tools. Moden is one of them. There's a couple of others

694
01:09:17,880 --> 01:09:22,000
which try and give you a magic scale button and try and hide everything. And you can use

695
01:09:22,000 --> 01:09:26,280
them effectively to a certain data set size, and then you're stuck. You know, you have

696
01:09:26,280 --> 01:09:33,160
to really think about what you're doing. Okay.

697
01:09:33,160 --> 01:09:53,400
So, I'm going to read in that same data set with the D types which I just read in. And

698
01:09:53,400 --> 01:09:59,000
if you also look, now I'm also reading in the data with a glob. I don't actually have

699
01:09:59,000 --> 01:10:04,320
to limit myself to one data set. Or you can use a glob and do some whatever regex you

700
01:10:04,320 --> 01:10:09,040
want to get some complicated list, and you can pass the list of files in as well. So

701
01:10:09,040 --> 01:10:14,920
very simple. We don't have to do the read each one, concatenate. And so now when we

702
01:10:14,920 --> 01:10:20,500
run head, we shouldn't get any warnings or errors. Okay? So now we've got the beginning

703
01:10:21,260 --> 01:10:31,580
of the data. Same thing we saw earlier. We got five rows, 109 columns. Now, here, I'm

704
01:10:31,580 --> 01:10:38,740
creating an ad. Actually, is this large enough? I think, okay. I'm saying DF.sum. What do

705
01:10:38,740 --> 01:10:42,380
you think when I hit enter, what do you think this will look like? What will I get any numbers

706
01:10:42,380 --> 01:10:51,580
out of this? No. This is just going to give me a graph. So this basically says, I've

707
01:10:51,580 --> 01:10:59,420
set a sum. To do a sum, Dask needs to do three or four things. Okay? And we're going to look

708
01:10:59,420 --> 01:11:07,440
at this. We're going to visualize this. And this is one of the reasons why we're only

709
01:11:07,520 --> 01:11:12,120
reading four files, because if we read all the files, this visualization would look kind

710
01:11:12,120 --> 01:11:23,120
of messy. So I'm going to, well, since this is taking a long time, so if you actually

711
01:11:23,400 --> 01:11:29,080
look at it, this is what the visualization kind of looks like. Oops. That's not what

712
01:11:29,080 --> 01:11:36,080
I wanted to do. So if you look, we gave it four files. So there are four read.csvs. So

713
01:11:37,080 --> 01:11:43,200
it's reading each of the four files. It's doing this sum chunk. So it's summing all

714
01:11:43,200 --> 01:11:50,200
the data in each of the four files. And then it's aggregating that sum from each of the

715
01:11:50,200 --> 01:11:55,200
four files. So this is the graph. And then it's giving you back the answer. Yeah.

716
01:11:55,200 --> 01:12:02,200
0 to 6. What do you mean? Oh, okay. Yeah. Well, so also the partitions don't have to match

717
01:12:13,680 --> 01:12:20,120
the files always. It's making some logic on what the best size to put each piece is. So

718
01:12:20,120 --> 01:12:27,120
yeah. Okay. So now we're... Okay. So now we're... So now we're... So now we're... So

719
01:12:50,120 --> 01:12:57,120
now we're... So I computed it. This took about 20 seconds to compute the sum. Now internally

720
01:12:59,160 --> 01:13:05,400
this is what's happening. Internally, each partition has an individual Pandas data frame.

721
01:13:05,400 --> 01:13:11,160
Okay? And if we're using the array, if we're using the Dask array, each partition would

722
01:13:11,160 --> 01:13:16,100
have a NumPy array. There's also another tool which is Dask compatible called XArray, which

723
01:13:16,100 --> 01:13:19,000
for n-dimensional data if you're doing geospatial

724
01:13:19,000 --> 01:13:22,040
or large field, you know, like the symmetry

725
01:13:22,040 --> 01:13:25,780
or weather data, that also has x-array,

726
01:13:25,780 --> 01:13:28,140
non-py-array chunks put together.

727
01:13:28,140 --> 01:13:30,900
So the Dask data frame is a collection

728
01:13:30,900 --> 01:13:32,860
of individual Pandas data frames,

729
01:13:32,860 --> 01:13:36,200
and then it automatically works out a parallel set

730
01:13:36,200 --> 01:13:40,500
of a graph to do your computation

731
01:13:40,500 --> 01:13:42,020
on all those individual pieces

732
01:13:42,020 --> 01:13:44,480
and then aggregate them together.

733
01:13:44,520 --> 01:13:48,800
And right now, I have seven partitions,

734
01:13:49,720 --> 01:13:53,160
some of you have six, it's gonna do those computations

735
01:13:53,160 --> 01:13:54,480
on each of those six or seven

736
01:13:54,480 --> 01:13:56,280
and then kind of pull them together.

737
01:13:58,360 --> 01:14:01,500
Okay, so everything you've run so far

738
01:14:01,500 --> 01:14:05,640
is running on your individual medium-sized node,

739
01:14:05,640 --> 01:14:09,480
and that has four cores and 16 GB.

740
01:14:09,480 --> 01:14:13,960
But this model can be extended to larger machines

741
01:14:13,960 --> 01:14:15,780
or larger clusters of machines.

742
01:14:15,780 --> 01:14:18,880
So you could have, the way it works is you have a scheduler,

743
01:14:18,880 --> 01:14:22,280
and the scheduler is holding your list of tasks

744
01:14:22,280 --> 01:14:24,400
and what needs to be done.

745
01:14:24,400 --> 01:14:27,440
And then you have workers, and if you're on a local laptop

746
01:14:27,440 --> 01:14:29,760
or local desktop, your workers are just threads

747
01:14:29,760 --> 01:14:32,140
or processes on your machine.

748
01:14:32,140 --> 01:14:35,280
But if you're on an HPC machine or on a cloud machine

749
01:14:35,280 --> 01:14:38,280
or something else, those workers could be distributed

750
01:14:38,280 --> 01:14:40,360
in a large cluster.

751
01:14:40,360 --> 01:14:42,920
And they all communicate back to the scheduler

752
01:14:42,920 --> 01:14:44,920
and they say, I'm free, give me tasks.

753
01:14:44,920 --> 01:14:47,000
And the scheduler parses the tasks out

754
01:14:47,000 --> 01:14:49,760
and then it collects them, and if something fails,

755
01:14:49,760 --> 01:14:52,680
it reschedules them, so it acts as the,

756
01:14:55,120 --> 01:14:57,400
kind of that piece.

757
01:14:57,400 --> 01:15:00,180
There are several ways to launch workers.

758
01:15:01,520 --> 01:15:05,440
So we need to basically say, hey, here's my scheduler,

759
01:15:05,440 --> 01:15:09,760
let's launch, I want this many workers, launch them,

760
01:15:09,760 --> 01:15:11,660
everything connect up, give me back something

761
01:15:11,660 --> 01:15:13,540
that can do the computation.

762
01:15:13,540 --> 01:15:16,960
In this particular example, we're gonna use Dask Gateway.

763
01:15:16,960 --> 01:15:20,660
There's also another commercial product from Coiled.

764
01:15:20,660 --> 01:15:25,180
There's also Dask Kubernetes, and there's a Dask version

765
01:15:25,180 --> 01:15:28,420
that works on Yarn called Dask Yarn.

766
01:15:28,420 --> 01:15:30,940
There's Dask, so there's several ways to launch

767
01:15:30,940 --> 01:15:34,220
these workers and the details are slightly different,

768
01:15:34,220 --> 01:15:36,380
but they're very, if you know one,

769
01:15:36,380 --> 01:15:38,140
you can switch out to another one.

770
01:15:38,140 --> 01:15:40,800
Essentially, we're using Dask Gateway,

771
01:15:40,800 --> 01:15:42,580
and Dask Gateway lets us run,

772
01:15:44,120 --> 01:15:45,600
some of the advantages of Dask Gateway

773
01:15:45,600 --> 01:15:49,960
is it lets you run clusters without giving end users

774
01:15:49,960 --> 01:15:52,320
elevated permissions on the cluster.

775
01:15:52,320 --> 01:15:53,920
For example, if you use Dask Kubernetes,

776
01:15:53,920 --> 01:15:56,720
anyone running Dask Kubernetes has to have

777
01:15:56,720 --> 01:15:59,560
high privileges on the Kubernetes cluster.

778
01:15:59,560 --> 01:16:03,100
Dask Gateway lets the sysadmin take care of that,

779
01:16:03,100 --> 01:16:05,600
and end users still be able to launch clusters.

780
01:16:05,600 --> 01:16:10,600
So this, and we're going to launch Gateway

781
01:16:12,320 --> 01:16:16,320
and ask for a Gateway link.

782
01:16:16,320 --> 01:16:19,720
And with our Gateway link, we're going to see

783
01:16:19,720 --> 01:16:21,880
what options we have to choose our cluster.

784
01:16:26,080 --> 01:16:27,720
And this matters because one of the things

785
01:16:27,720 --> 01:16:31,040
you're gonna find out is one of the difficult things

786
01:16:31,040 --> 01:16:33,200
when using clusters, any kind of clusters,

787
01:16:33,200 --> 01:16:38,200
not just this, is the software you have on your head node

788
01:16:38,280 --> 01:16:41,440
or main computer has to match the software you have

789
01:16:41,440 --> 01:16:42,680
on each of these workers.

790
01:16:43,560 --> 01:16:47,900
And getting those two to match is half the problem

791
01:16:47,900 --> 01:16:50,200
of using any tool like Dask.

792
01:16:50,200 --> 01:16:54,200
So, you know, one of the commercial companies

793
01:16:54,200 --> 01:16:58,200
around Dask coiled, they have a really good solution

794
01:16:58,200 --> 01:17:02,040
to let you distribute these environments.

795
01:17:02,040 --> 01:17:05,720
We have one built into Nabari, but you can also do it

796
01:17:05,720 --> 01:17:07,640
yourself, if you make a Docker image and you make sure

797
01:17:07,640 --> 01:17:12,080
the workers use that Docker image, you know, it's doable.

798
01:17:12,080 --> 01:17:15,320
But here, the way we set it up with Nabari is

799
01:17:15,320 --> 01:17:20,000
if you look at your, up here, we have all these

800
01:17:20,000 --> 01:17:21,220
environments available.

801
01:17:23,040 --> 01:17:25,040
You won't see the Quantsight ones because you're not

802
01:17:25,040 --> 01:17:26,840
in the Quantsight group.

803
01:17:26,840 --> 01:17:30,440
And here we have those same environments available,

804
01:17:30,440 --> 01:17:32,760
the ones which have Dask in them.

805
01:17:32,760 --> 01:17:37,760
And you need to choose the PyCon 2023 tutorial environment

806
01:17:38,720 --> 01:17:42,440
and we want to use the medium worker, okay?

807
01:17:45,520 --> 01:17:49,040
So that sets up what I'm gonna request.

808
01:17:49,040 --> 01:17:53,240
And then I'm gonna call gateway and say I want a cluster.

809
01:17:54,600 --> 01:17:55,440
Okay?

810
01:17:56,920 --> 01:17:58,220
What happened there?

811
01:18:00,740 --> 01:18:11,580
museum my

812
01:18:12,580 --> 01:18:13,480
It'll be pinging.

813
01:18:43,480 --> 01:19:00,400
I have a colleague watching Kubernetes on the back end, so I just pinged him.

814
01:19:00,400 --> 01:19:16,920
Yeah, no, we're going to, if this runs, I'll show you the auto scaling.

815
01:19:16,920 --> 01:19:21,360
You can choose to either auto scale or you can choose to manually scale.

816
01:19:21,360 --> 01:19:27,080
So if this doesn't work, we're going to be in trouble.

817
01:19:27,080 --> 01:19:35,080
Connection refused.

818
01:19:51,360 --> 01:19:58,080
You know what? I'm going to try one thing. I wonder if something is blocked on...

819
01:19:58,080 --> 01:20:05,080
Let me switch Wi-Fi to my hotspot and see if it's something to do with that.

820
01:20:28,080 --> 01:20:52,080
Was anyone able to get this cluster to show up?

821
01:21:52,080 --> 01:22:13,080
I don't think it's to do with Wi-Fi.

822
01:22:13,080 --> 01:22:42,080
Okay, let me try this again.

823
01:22:42,080 --> 01:22:55,080
Okay, let's do this. We think, my colleague thinks it might be all of us trying to connect

824
01:22:55,080 --> 01:23:02,800
to the gateway at the exact same time. So everyone hit restart on that kernel and I

825
01:23:02,800 --> 01:23:11,080
will connect first and see if we can connect and then maybe we'll do by table or something.

826
01:23:11,080 --> 01:23:40,080
Okay.

827
01:23:40,080 --> 01:23:47,080
So to give you a little background here, there is, so this is a mix of trying to do big data

828
01:23:47,080 --> 01:23:53,080
and visualization at the same time. I've helped teach DAS tutorials at SciPy and other conferences

829
01:23:53,080 --> 01:23:59,080
before. Typically, we don't give anyone, in those things, we tell everyone to run on their

830
01:23:59,080 --> 01:24:06,080
local computer, but we wanted to try doing it at scale this time. So it's a good stress

831
01:24:06,080 --> 01:24:14,080
for what we're doing. Okay, my colleague sees four people connected. So four of us got in.

832
01:24:14,080 --> 01:24:24,080
Oh, I'm, no, yeah, okay, let's see. Let's go here. And it also might be the case that

833
01:24:24,080 --> 01:24:33,080
one thing about Kubernetes is initially when it autoscales, it asks for stuff and it takes

834
01:24:33,080 --> 01:24:38,080
a few minutes for it to scale up. So we might be hitting this issue that it hasn't quite

835
01:24:38,080 --> 01:24:52,080
scaled up enough for all of us to hit it, but let's see. So he says he sees four people

836
01:24:52,080 --> 01:25:04,080
connected. Do you, anyone see that you're connected? Well, it's this next one that's

837
01:25:04,080 --> 01:25:11,080
getting stuck, this cluster one.

838
01:25:34,080 --> 01:26:02,080
Okay. So.

839
01:26:02,080 --> 01:26:06,080
So let's take a five minute break. Hopefully, I'll get this back. Once we get the cluster up

840
01:26:06,080 --> 01:26:10,080
and running, once everyone's connected to it, we'll leave it open for the rest of the

841
01:26:10,080 --> 01:26:16,080
tutorial, so we should be fine. But I will work on this in the meantime. If not, we'll switch

842
01:26:16,080 --> 01:26:44,080
to using local clusters, which will limit how big of the datasets we can go to.

843
01:26:44,080 --> 01:27:09,080
Okay. Does anyone who's got a cluster, would they be willing to swap laptops with me so

844
01:27:09,080 --> 01:27:21,080
I can do the rest of the presentation with their laptop? If not, we'll, okay. We'll see if this

845
01:27:21,080 --> 01:27:42,080
works. If it does, I'll switch back to you. Yep.

846
01:27:42,080 --> 01:27:59,080
Okay. Let's see. Okay. What's your name again? Thank you very much, Hannah. So, okay. So for those of

847
01:27:59,080 --> 01:28:05,080
you who were able to get a cluster, we are not going to shut it down anymore, but you should see

848
01:28:05,080 --> 01:28:10,080
something like this. And this has two options. It has a manual scaling and an adaptive scaling.

849
01:28:10,080 --> 01:28:17,080
And these are little widgets you can use, but you can also set these directly in the code. But here

850
01:28:17,080 --> 01:28:26,080
we're going to use adaptive scaling. And we're going to say a minimum of five and a maximum of

851
01:28:26,080 --> 01:28:32,080
ten. Okay? And that should be enough for all of us to do the rest of the exercises. So I'm going to

852
01:28:32,080 --> 01:28:43,080
click adapt. And that's now going to start requesting stuff on the back end. Oh, mine came up.

853
01:28:43,080 --> 01:28:53,080
Okay. We'll try this again here. So he restarted the cluster pod. So you guys might be able to get in

854
01:28:53,080 --> 01:29:16,080
now. What's a demo if something doesn't go wrong, right? Okay. So, you know, I'm going to, again, do

855
01:29:16,080 --> 01:29:22,080
the same thing. I'm going to say a minimum of five, a maximum of ten. Now, you could also use manual

856
01:29:22,080 --> 01:29:31,080
and say I want ten, and it will give you ten. Usually I use adaptive scaling. And the reason is

857
01:29:31,080 --> 01:29:38,080
Dask is fairly intelligent about how it does the compute. So if you use adaptive scaling, and it'll

858
01:29:38,080 --> 01:29:45,080
start with the minimum. And so often I put minimum of zero or one. And then when you throw a lot of

859
01:29:45,080 --> 01:29:52,080
tasks at it, it'll say, oh, all my nodes are full. I'm going to go grab some more. And it'll grab more

860
01:29:52,080 --> 01:29:59,080
until it hits the max. And then when it's done, it'll start dropping nodes. And so it's really more

861
01:29:59,080 --> 01:30:06,080
cost-effective to do it that way. But there is a, you'll see this in a second, requesting nodes from

862
01:30:06,080 --> 01:30:13,080
Kubernetes and getting them can take up to two or three minutes. Now it should be fast. It depends on

863
01:30:13,080 --> 01:30:18,080
the, Kubernetes has this weird thing where it's warm and cold. If no one's been using the cluster, a new

864
01:30:18,080 --> 01:30:23,080
node can take, you know, two, three minutes. But if you've been using it, they'll come up in a couple of

865
01:30:23,080 --> 01:30:29,080
seconds. So we'll see that. Now we're going to click on this link here. If you guys see this dashboard

866
01:30:29,080 --> 01:30:36,080
link, this is the Dask diagnostic dashboard. And first time you click on it, it'll take you back through

867
01:30:36,080 --> 01:30:42,080
the login thing. And just log in again. You only have to do this once. And now you'll see this particular

868
01:30:42,080 --> 01:30:50,080
thing. And right now we don't have any workers. It might take a couple of minutes for us to get some

869
01:30:50,080 --> 01:30:53,080
workers.

870
01:31:06,080 --> 01:31:14,080
Okay. So eventually you'll start seeing number of workers, number of threads, number of memory show up

871
01:31:14,080 --> 01:31:21,080
once I have workers. I'm going to now ask for clients. So now I've made a cluster. So this first step, the

872
01:31:21,080 --> 01:31:28,080
Dask Gateway new cluster, depending on what tool you use, like coiled or Dask Kubernetes or Dask Yarn,

873
01:31:28,080 --> 01:31:33,080
that command looks a little different, how you set the options and stuff. But then the next part, once you

874
01:31:33,080 --> 01:31:42,080
have a cluster, you get a client object. And the client object is connecting your local Jupyter Notebook

875
01:31:42,080 --> 01:31:48,080
to the scheduler on that cluster. So when you run stuff, it doesn't run locally, it will run on that

876
01:31:48,080 --> 01:32:16,080
cluster.

877
01:32:16,080 --> 01:32:35,080
I think we're learning that the pod Dask Gateway is running on is not big enough for 40 people.

878
01:32:35,080 --> 01:32:47,080
Yeah. Did you get the client? Okay. So you're the lucky person today. Once I get this client, we're basically

879
01:32:47,080 --> 01:33:02,080
done because we're not going to shut the cluster down for the rest of the tutorial. This is fun.

880
01:33:02,080 --> 01:33:31,080
I'll leave that there. Okay. Let's see. Oh, no, you don't have any workers yet either.

881
01:33:31,080 --> 01:33:41,080
Does anyone see any workers on the side of theirs? You see some? Okay. So let me walk you through the

882
01:33:41,080 --> 01:33:53,080
dashboard. So here we got the dashboard. So she still doesn't have any workers. Okay. So I'll walk you through this

883
01:33:53,080 --> 01:34:01,080
next step while we're waiting for the workers. But we're going to use this dash. So here you have another tab with

884
01:34:01,080 --> 01:34:07,080
all these nice graphs you can see when the computer's going on. But instead of that, we're going to use this lab

885
01:34:07,080 --> 01:34:15,080
extension. If you click this little red icon here, and we should, once you're, you have to wait until your client

886
01:34:15,080 --> 01:34:23,080
is available. Once you've run this client cell, you should be able to click the search button, and it'll bring you,

887
01:34:23,080 --> 01:34:36,080
you should see all these tabs pop up. And we are going to pick a few. I like cluster map. Okay. Cluster map is this

888
01:34:36,080 --> 01:34:48,080
one. This cluster map is kind of sad right now. It says we have a scheduler, but no workers. And then usually progress.

889
01:34:48,080 --> 01:34:54,080
And you can kind of just drag these around to make them fit on your screen in a reasonable way. And I'm going to pick on

890
01:34:54,080 --> 01:35:10,080
task stream. Let me move that here. And let's also do memory usage. Which one do I need? Which one do I want to use for memory?

891
01:35:10,080 --> 01:35:20,080
Cluster memory. I'm going to use total cluster memory. Okay. Okay.

892
01:35:40,080 --> 01:36:08,080
Okay.

893
01:36:08,080 --> 01:36:24,080
Yep. I guess it only works with your hands. Let's see. I have 16 cores.

894
01:36:24,080 --> 01:36:32,080
I think that's the problem. I think what's happening is the pod that the gateway is running on, because my colleague just texted me

895
01:36:32,080 --> 01:36:42,080
saying that pod keeps restarting. We're overloading the gateway pod. We should probably just, we're going to have to switch to more of a

896
01:36:42,080 --> 01:36:52,080
demo than a live thing. I'll write some code here that some of you can use that a lot of the examples we'll work on, we'll use local cluster.

897
01:36:52,080 --> 01:37:04,080
Local cluster will use the 16 cores that we have. Yeah, it'll use this. Okay. Let me actually do this.

898
01:37:22,080 --> 01:37:32,080
Okay.

899
01:37:32,080 --> 01:37:42,080
Okay. So I'm going to abandon task gateway so I can give this back to you. And I'm just going to use local cluster. I've also asked my colleague to enable

900
01:37:42,080 --> 01:37:52,080
a larger instance size on GCP. I'm not sure if you'll have that ready in time, but if we can, we can switch to like a 16 core or 64 core machine.

901
01:37:52,080 --> 01:38:02,080
But the rest of it we should still be able to do. Unfortunately, some of the bigger data sets, I'll change how much data we're reading in so we don't

902
01:38:02,080 --> 01:38:16,080
overwhelm things. But instead of using...

903
01:38:16,080 --> 01:38:23,080
Okay. So we're going to have to type this in. Let's restart the kernel.

904
01:38:23,080 --> 01:38:47,080
So we're going to type from distributed import local cluster. And then we're going to say cluster equals local cluster.

905
01:38:47,080 --> 01:38:57,080
Okay. Now if we run this... What did I do wrong? Oh, yeah. Okay.

906
01:38:57,080 --> 01:39:08,080
Okay. So this is going to use all the cores on your local machine, which here is the 16 thing. And if we go here, you should be able to click...

907
01:39:08,080 --> 01:39:36,080
I'll click delete this. Oh, yeah. You're going to have to do something slightly annoying.

908
01:39:36,080 --> 01:39:59,080
Okay. To get the dashboard, the URL local cluster is giving you is going to be a little wrong. And so we're actually going to use this URL.

909
01:39:59,080 --> 01:40:13,080
Let me make it where you need to replace your login. You can copy and paste this from your...

910
01:40:13,080 --> 01:40:28,080
Okay. So to get the dashboard link, copy your URL to the user and add slash proxy 787 status. This will give you the cluster.

911
01:40:28,080 --> 01:40:44,080
And so if you take this link and paste it into this field here and hit enter, don't hit search, now you'll get the same tabs.

912
01:40:58,080 --> 01:41:19,080
Okay. Was everyone able to get to this with local cluster? Okay. And in the meantime, we'll see how it goes. My colleague is trying to change the instance size for the DAS gateway to a bigger one.

913
01:41:19,080 --> 01:41:30,080
So if it comes back, we'll use it later. But let's move on.

914
01:41:30,080 --> 01:41:54,080
Okay.

915
01:41:54,080 --> 01:42:09,080
Yeah. So here, you know, see it has this dashboard link. That dashboard link is assuming you're running on a local computer, so it's wrong. What you need to do is if you look at the top in your browser bar, mine says slash user slash dark house on site.

916
01:42:09,080 --> 01:42:24,080
Yours will say slash user slash whatever your name is. Copy that down, add proxy 8 787 and status. And if you click this link, it should take you to the diagnostic dashboard.

917
01:42:24,080 --> 01:42:45,080
And here, if you click workers, yeah. So here you basically see I have four workers, you know, four cores, which is what I have on this instance size.

918
01:42:45,080 --> 01:43:01,080
Now if I come here, I'm going to open up this cluster map again. Oops.

919
01:43:01,080 --> 01:43:07,080
And I'm going to open the memory.

920
01:43:07,080 --> 01:43:26,080
And I'm going to do the progress. And you can choose any of these plots you want. I'm going to use these plots just to show you what's happening. But if you want to see other things. So I did task stream.

921
01:43:26,080 --> 01:43:54,080
Okay. Are we at this point, everyone?

922
01:43:54,080 --> 01:44:05,080
Okay, so I'm going to go back and.

923
01:44:05,080 --> 01:44:10,080
So I'm going to go back and re execute my desk data frame and my read.

924
01:44:10,080 --> 01:44:13,080
Because I need those.

925
01:44:13,080 --> 01:44:25,080
And now I'm going to.

926
01:44:25,080 --> 01:44:29,080
Calculate the maximum distance.

927
01:44:29,080 --> 01:44:31,080
Okay.

928
01:44:31,080 --> 01:44:46,080
And you basically should see how much memory is being stored. And I'm going to switch to here so you can see it a little better.

929
01:44:46,080 --> 01:44:52,080
Actually, let me run that again. I think it finished. Let me run this again.

930
01:44:52,080 --> 01:45:02,080
Move this down here so it's less. Okay. So when I run this, you can kind of see that I have four cores right now. So I have four streams of tasks.

931
01:45:02,080 --> 01:45:09,080
And if you look, it's doing it showing you all the things that's running while you're running it.

932
01:45:09,080 --> 01:45:13,080
And then when it finishes, that should go down.

933
01:45:13,080 --> 01:45:17,080
And yeah.

934
01:45:17,080 --> 01:45:23,080
Go ahead.

935
01:45:23,080 --> 01:45:33,080
Yeah, so if you're using local cluster, if you if you look you, you can if you're just using locally, it automatically uses local cluster on the back.

936
01:45:33,080 --> 01:45:41,080
And if you just do from Dask data frame import, that's a frame as DD. It automatically sets up a local cluster.

937
01:45:41,080 --> 01:45:47,080
The only reason I went through this way is it it falls the same format and you get the link to the dashboard.

938
01:45:47,080 --> 01:45:51,080
Which finding the link to the dashboard is a little more annoying if you don't have that. But yeah.

939
01:45:51,080 --> 01:46:00,080
But if our client was on the gateway, it's definitely no automatically use that on the gateway because there's no arguments that you can use the gateway for.

940
01:46:00,080 --> 01:46:11,080
Yeah, no. So basically it's basically this this command. This client equals get client automatically sets the default client to the gateway client.

941
01:46:11,080 --> 01:46:18,080
Now you can also do some stuff where you connect to different things, different clients for, you know, client one client two.

942
01:46:18,080 --> 01:46:24,080
And you could when you do the computer could say use this client. But usually people don't do that.

943
01:46:24,080 --> 01:46:29,080
So by default, if there's one client, it's going to connect to that one.

944
01:46:29,080 --> 01:46:36,080
OK, I'm going to go to the next.

945
01:46:36,080 --> 01:46:47,080
We're going to talk about storage formats.

946
01:46:47,080 --> 01:46:52,080
Yeah, you can. I mean, just hit restart on that one.

947
01:46:52,080 --> 01:47:02,080
Local cluster is going to connect to the same local cluster, so it shouldn't make much of a difference. But yeah, we can we can just hit restart on that.

948
01:47:02,080 --> 01:47:06,080
OK, I'm going to go into.

949
01:47:06,080 --> 01:47:17,080
Storage clusters and I'm going to go a bit fast. I'm going to skip some of the exercises, but.

950
01:47:17,080 --> 01:47:25,080
OK, so we have these CSV files which we you know I told you 70 gigabytes of CSV files.

951
01:47:25,080 --> 01:47:27,080
And.

952
01:47:27,080 --> 01:47:38,080
If we look at them, I'm just looking at the top one. These are all the fields and then that first line of the CSV file instead of this, we can kind of just avoid.

953
01:47:38,080 --> 01:47:41,080
All of this.

954
01:47:41,080 --> 01:47:48,080
Just skip this entire start, task, click gateway piece and let's just put.

955
01:47:48,080 --> 01:47:51,080
Import.

956
01:47:51,080 --> 01:47:53,080
Dask.

957
01:47:53,080 --> 01:47:59,080
Data frame.

958
01:47:59,080 --> 01:48:02,080
I think this should still be up. Let's see.

959
01:48:02,080 --> 01:48:10,080
No, we're going to have to let's copy that piece we put in the previous one.

960
01:48:10,080 --> 01:48:13,080
So let's use this instead of.

961
01:48:13,080 --> 01:48:22,080
Our Dask gateway piece.

962
01:48:22,080 --> 01:48:24,080
So I'm going to do the.

963
01:48:24,080 --> 01:48:26,080
Same thing.

964
01:48:26,080 --> 01:48:29,080
And now this should be working.

965
01:48:29,080 --> 01:48:34,080
Yeah, so the status is there. So now here.

966
01:48:34,080 --> 01:48:42,080
We're going to pick our.

967
01:48:42,080 --> 01:48:45,080
So again, we read all the data sets.

968
01:48:45,080 --> 01:48:54,080
And we are going to find the head and if you see every every time I run something is doing compute, you can see it's doing stuff over here.

969
01:48:54,080 --> 01:48:56,080
So it's going to run the head.

970
01:48:56,080 --> 01:49:00,080
Here and it takes a certain amount of time.

971
01:49:00,080 --> 01:49:06,080
And then completes took 8.2 seconds.

972
01:49:06,080 --> 01:49:10,080
And I'm just going to count the number of unique carriers per year.

973
01:49:10,080 --> 01:49:15,080
So.

974
01:49:15,080 --> 01:49:21,080
I'm not going to run this because this is going to take a lot more than four minutes on with with the number of nodes we have.

975
01:49:21,080 --> 01:49:27,080
But let's skip down to this one where we're talking about parquet data because this is one of the things we want to actually talk about.

976
01:49:27,080 --> 01:49:29,080
So right now we're using CSV.

977
01:49:29,080 --> 01:49:35,080
There are two actually lots of big problems with CSV.

978
01:49:35,080 --> 01:49:40,080
The two biggest one is the size of the CSV files.

979
01:49:40,080 --> 01:49:45,080
You know they're very wasteful.

980
01:49:45,080 --> 01:49:50,080
Storage format that nice because you can look at them and you can you know that's fine when they're small.

981
01:49:50,080 --> 01:49:53,080
But when you have lots and lots of them, they're very not good.

982
01:49:53,080 --> 01:49:57,080
The second thing is we don't know what the data types are.

983
01:49:57,080 --> 01:50:01,080
You have to guess them and that's.

984
01:50:01,080 --> 01:50:07,080
Usually fine if it's a small file or if you just have one or two files.

985
01:50:07,080 --> 01:50:14,080
But when you have lots it like you know when I was looking at this data set so we had data from 2003 to 2022.

986
01:50:14,080 --> 01:50:18,080
When I was looking at the first few CSV files from 2003.

987
01:50:18,080 --> 01:50:23,080
There's data columns that showed up in 2008 that don't exist in 2003.

988
01:50:23,080 --> 01:50:26,080
And so all of those are just listed as blank.

989
01:50:26,080 --> 01:50:31,080
So I'd have to look at a 2008 data set to work out what the format is.

990
01:50:31,080 --> 01:50:34,080
Is it a string? Is it a float? Is it a thing?

991
01:50:34,080 --> 01:50:44,080
So it's really annoying to work out like writing that data type.JSON file took me an hour of looking reading through the website.

992
01:50:44,080 --> 01:50:50,080
And trying to understand there what they were saying and then looking at the data and that's all a mess.

993
01:50:50,080 --> 01:50:54,080
So we are going to read.

994
01:50:59,080 --> 01:51:02,080
So now I've read the full data set.

995
01:51:02,080 --> 01:51:06,080
You know I with Dask I'm reading the full data set.

996
01:51:06,080 --> 01:51:07,080
This is just.

997
01:51:09,080 --> 01:51:10,080
The metadata.

998
01:51:10,080 --> 01:51:11,080
Nothing's happened.

999
01:51:11,080 --> 01:51:13,080
Now I'm going to do a head on that.

1000
01:51:14,080 --> 01:51:16,080
And it should be much faster.

1001
01:51:16,080 --> 01:51:18,080
So the head here took.

1002
01:51:20,080 --> 01:51:21,080
Two seconds.

1003
01:51:21,080 --> 01:51:22,080
Versus.

1004
01:51:24,080 --> 01:51:26,080
Over here I think we took eight seconds.

1005
01:51:26,080 --> 01:51:27,080
Okay.

1006
01:51:27,080 --> 01:51:28,080
For the same.

1007
01:51:28,080 --> 01:51:30,080
To get to pull out the same data.

1008
01:51:35,080 --> 01:51:36,080
The other thing we can do.

1009
01:51:36,080 --> 01:51:38,080
I'm skipping through a couple of these in interest of time.

1010
01:51:38,080 --> 01:51:40,080
So another thing we can do is.

1011
01:51:40,080 --> 01:51:47,080
Let's say who's you know how we had like a hundred sixty one columns of data in the data set.

1012
01:51:47,080 --> 01:51:52,080
What would you do if you wanted to read only the first ten columns in pandas.

1013
01:51:55,080 --> 01:51:59,080
Yeah you'd basically say use calls equals and you give it the list.

1014
01:52:00,080 --> 01:52:07,080
That actually reads the entire data set into memory and then drops those columns that you didn't want.

1015
01:52:08,080 --> 01:52:09,080
So.

1016
01:52:10,080 --> 01:52:11,080
That is slow.

1017
01:52:11,080 --> 01:52:15,080
The way the parquet format is designed it is a column of format.

1018
01:52:15,080 --> 01:52:18,080
So if I say I want three columns.

1019
01:52:18,080 --> 01:52:21,080
It will only read those three columns.

1020
01:52:21,080 --> 01:52:23,080
And it will not touch the rest.

1021
01:52:23,080 --> 01:52:29,080
So that does two things one is it improves your load time massively.

1022
01:52:29,080 --> 01:52:31,080
Second thing is it.

1023
01:52:31,080 --> 01:52:34,080
It makes your peak memory usage much lower.

1024
01:52:34,080 --> 01:52:39,080
Because if you think of a CSV file it has to load the entire data set into memory.

1025
01:52:39,080 --> 01:52:42,080
Which causes a huge peak in memory usage.

1026
01:52:42,080 --> 01:52:45,080
And then it's going to drop everything so it comes back down.

1027
01:52:48,080 --> 01:52:53,080
So let's let's do this we're going to read just the year and the unique carrier.

1028
01:52:55,080 --> 01:52:57,080
And I don't know how long this will take.

1029
01:52:57,080 --> 01:52:59,080
Oh wait that didn't take long.

1030
01:53:00,080 --> 01:53:03,080
Oh no I've got to compute it that didn't actually read it.

1031
01:53:03,080 --> 01:53:08,080
So now let's compute this took 12 seconds on the larger number of cores.

1032
01:53:09,080 --> 01:53:12,080
But it's a little chug longer right now.

1033
01:53:12,080 --> 01:53:13,080
So we'll see.

1034
01:53:13,080 --> 01:53:15,080
So we are reading in.

1035
01:53:26,080 --> 01:53:31,080
So you see this is chugging along this is reading in through the entire 20 years of data.

1036
01:53:31,080 --> 01:53:33,080
So we're reading through 70 gig of data.

1037
01:53:34,080 --> 01:53:37,080
Which actually when it's compressed is much smaller with parquet.

1038
01:53:38,080 --> 01:53:43,080
And this is something you couldn't have done with pandas and even with CSV it will work.

1039
01:53:43,080 --> 01:53:48,080
But it will use a lot more compute and a lot more memory because it's reading more stuff in.

1040
01:53:49,080 --> 01:53:52,080
So it's done and now that took.

1041
01:53:54,080 --> 01:53:55,080
40 seconds.

1042
01:53:55,080 --> 01:53:58,080
And I think we calculated here.

1043
01:53:59,080 --> 01:54:04,080
If you look at the notes here I think she put this takes about four minutes.

1044
01:54:05,080 --> 01:54:12,080
With CSV with more cores so it probably take about 15 minutes if we were doing the same thing with CSV.

1045
01:54:17,080 --> 01:54:19,080
So that's reading specific columns.

1046
01:54:19,080 --> 01:54:21,080
So that's reading specific columns.

1047
01:54:24,080 --> 01:54:28,080
OK so now I'm going to tell you something about partitioning.

1048
01:54:29,080 --> 01:54:30,080
OK.

1049
01:54:31,080 --> 01:54:36,080
With the original data we had one file per month.

1050
01:54:37,080 --> 01:54:39,080
That seems reasonably logical.

1051
01:54:41,080 --> 01:54:44,080
When we get to these larger data sets.

1052
01:54:44,080 --> 01:54:49,080
The way we access the data defines how we want to store the data.

1053
01:54:50,080 --> 01:54:52,080
And I'll give you an example.

1054
01:54:53,080 --> 01:54:58,080
Five probably 10 years ago now I was working with the UK Met Office.

1055
01:54:59,080 --> 01:55:08,080
They had an 80 year database of worldwide weather forecasts that they put in the cloud.

1056
01:55:08,080 --> 01:55:15,080
And you know I was working for the Army Corps of Engineers and we were looking at trying to analyze that data to find some long term weather trends and stuff like that.

1057
01:55:16,080 --> 01:55:18,080
And we also trying to experiment with cloud technologies.

1058
01:55:19,080 --> 01:55:28,080
And so I'm like oh wow you have an 80 gigabyte I mean not 80 gigabyte it was I think 80 terabytes of data for over like a hundred year 80 or 100 year period I forget what.

1059
01:55:29,080 --> 01:55:35,080
The data was unusable because the way they started in the cloud was every single day.

1060
01:55:35,080 --> 01:55:40,080
Every single day they stored a file that had the entire globe.

1061
01:55:42,080 --> 01:55:43,080
Great.

1062
01:55:44,080 --> 01:55:48,080
What I needed was North Carolina for 100 years.

1063
01:55:50,080 --> 01:56:01,080
And the only way to read 100 years of North Carolina data was to read the entire day the entire world for every day.

1064
01:56:01,080 --> 01:56:04,080
And so the fact that it was stored.

1065
01:56:06,080 --> 01:56:12,080
Temporarily and I needed spatially made it impossible to use.

1066
01:56:13,080 --> 01:56:15,080
So when you do these data sets.

1067
01:56:16,080 --> 01:56:20,080
The way you access them sort of defines the way you need to store them.

1068
01:56:21,080 --> 01:56:26,080
And often you have to come up with a compromise because the going back to this weather data set.

1069
01:56:26,080 --> 01:56:37,080
If I optimized it where I stored every like you know maybe I picked some lat long boxes and I stored everything or I stored everything at single lat long points.

1070
01:56:38,080 --> 01:56:44,080
Then if I wanted to get the entire world on a particular day it would be really hard because I have to read all these files.

1071
01:56:45,080 --> 01:56:50,080
So usually this is called chunking and you usually find a compromise in the way you chunk the data.

1072
01:56:50,080 --> 01:56:53,080
Like you pick something and I'm going to show this with this.

1073
01:56:54,080 --> 01:57:00,080
So we have divided this data by year and we have also divided it.

1074
01:57:01,080 --> 01:57:12,080
Yeah so we with when you save it when you when you read it in the CSV data with Dask you can say to parquet and will resave the data as parquet.

1075
01:57:13,080 --> 01:57:15,080
And you can also give it options on how to save it.

1076
01:57:15,080 --> 01:57:17,080
And you can also give it options on how to save it.

1077
01:57:18,080 --> 01:57:26,080
And here when I saved it I said when I save it I want to save it by year and sort of automatically split out each year's data.

1078
01:57:27,080 --> 01:57:30,080
And in each year it made a bunch of parquet files.

1079
01:57:31,080 --> 01:57:32,080
Go ahead.

1080
01:57:38,080 --> 01:57:42,080
There are places when databases are useful there are places when databases are not useful.

1081
01:57:42,080 --> 01:57:46,080
The databases are very slow.

1082
01:57:47,080 --> 01:57:58,080
This data is on blob storage on a job on GCS blob storage on cheap AWS or GCS or something is blazing fast and very cheap.

1083
01:57:59,080 --> 01:58:04,080
So if you put all this in a database and do an SQL statement it will chug.

1084
01:58:05,080 --> 01:58:08,080
You know so there are places when databases are useful.

1085
01:58:09,080 --> 01:58:16,080
Sometimes you can put the metadata in the database and point to data but usually large data sets putting it on blob storage is the fastest.

1086
01:58:17,080 --> 01:58:23,080
And this this blob storage is designed for extremely fast access by multiple people.

1087
01:58:24,080 --> 01:58:31,080
And so you know if you have the database if 40 people hit a database at the same time it would it would kind of fall over.

1088
01:58:31,080 --> 01:58:35,080
But you know S3 and equivalent are designed to be extremely fast.

1089
01:58:36,080 --> 01:58:45,080
So here I am going to do what's called predicate push down.

1090
01:58:46,080 --> 01:58:49,080
And so here I'm giving it this full data set.

1091
01:58:50,080 --> 01:58:54,080
You know so I've got two days that one is full data set where I've not done any partitioning.

1092
01:58:55,080 --> 01:58:59,080
It's just I've taken the CSP data and I've just dumped it into a parquet file.

1093
01:58:59,080 --> 01:59:04,080
And it's picked its own breaking it up. It usually you break up the parquet files to a certain size.

1094
01:59:05,080 --> 01:59:08,080
And then I've done another one where we've divided by year.

1095
01:59:12,080 --> 01:59:13,080
Okay.

1096
01:59:15,080 --> 01:59:20,080
I wonder if that was my colleague trying to give you guys a bigger instance type.

1097
01:59:29,080 --> 01:59:30,080
Okay.

1098
01:59:49,080 --> 01:59:50,080
Where was I.

1099
01:59:50,080 --> 01:59:55,080
I was down at.

1100
01:59:56,080 --> 02:00:13,080
Row wise filtering. Okay. So let's let's look at this here. I'm basically saying read all the files which are not 2020 because you know COVID was annoying when you're trying to analyze data.

1101
02:00:20,080 --> 02:00:21,080
Okay.

1102
02:00:39,080 --> 02:00:40,080
You just stop from the top.

1103
02:00:40,080 --> 02:01:03,080
So in interest of time maybe I should skip to the next section we're running late. So the big point I want to talk about here is if we will I'll leave this open the notebook open so you can guys can run it afterwards.

1104
02:01:03,080 --> 02:01:19,080
We'll fix the Dask gateway issue. But the basic point is the comparison we show here is the full data set partition reading the data by year is slower than reading the partition data by year.

1105
02:01:19,080 --> 02:01:30,080
So because the I got the data set which is partitioned by year. And so when I tell Dask actually this actually works with pandas as well when I tell pandas to

1106
02:01:34,080 --> 02:01:42,080
Read only 2022 it only reads the files which are marked 2022 and so it's reading less in because it's divided that way.

1107
02:01:43,080 --> 02:01:53,080
And then if I read the full data set it actually has to open the data set do the comparison and then subset it out so it takes longer computationally.

1108
02:01:54,080 --> 02:02:01,080
Now the flip side I want the other exercise which is in here which I'm going to skip is we've saved the data by year.

1109
02:02:01,080 --> 02:02:15,080
And then I ask for it by airline like you know I've got so the data split by year and I've asked for it by airline now it has to go into in the individual folders read all the files and then find the.

1110
02:02:18,080 --> 02:02:22,080
The list of carriers that we want so.

1111
02:02:22,080 --> 02:02:37,080
When you're reading data from the cloud again this is all stuff which when you're reading one file it doesn't matter but when you're reading 10,000 files these add up there is a cost to going connecting to a file opening it.

1112
02:02:38,080 --> 02:02:42,080
Once you open a file streaming the data is very fast.

1113
02:02:43,080 --> 02:02:44,080
Okay.

1114
02:02:44,080 --> 02:02:46,080
Streaming the data is very fast.

1115
02:02:47,080 --> 02:02:48,080
Okay.

1116
02:02:49,080 --> 02:02:52,080
So if you have lots and lots of small files.

1117
02:02:53,080 --> 02:02:58,080
The cost of connecting and opening can overwhelm the compute.

1118
02:03:00,080 --> 02:03:03,080
But if you if the files are too big.

1119
02:03:03,080 --> 02:03:17,080
The individual data won't fit on a note like we're with the cluster with setting up lots and lots of notes each node has a certain amount of memory like you know if we're doing medium each node has 16 GB.

1120
02:03:18,080 --> 02:03:26,080
So if your individual files are so big that when they are unzipped in memory they're more than 60 GB 16 GB you're not going to be able to do anything.

1121
02:03:26,080 --> 02:03:35,080
So again this is where I told you the automatic scale button doesn't work if you have files that are too big to fit on a note you won't be able to run if you have too many small files things will get really slow.

1122
02:03:46,080 --> 02:03:55,080
So we've done that before and it really depends on what matters to you like we were working for an ad tech company that was analyzing GPS data and they had two types of data.

1123
02:03:56,080 --> 02:04:04,080
So we had two types of algorithms one algorithm was hyper local it was by county and another algorithm was US wide and temporal.

1124
02:04:05,080 --> 02:04:19,080
So for them we had the main data set which was partitioned by day and then we had a second data set which was partitioned by county because then because otherwise so so you're paying twice the amount in storage.

1125
02:04:19,080 --> 02:04:25,080
But if it's a regular computation it's worth doing because you know it's like compute expense was a storage expense.

1126
02:04:28,080 --> 02:04:29,080
Okay.

1127
02:04:36,080 --> 02:04:37,080
Let me check something.

1128
02:04:37,080 --> 02:04:38,080
Let me check something.

1129
02:04:42,080 --> 02:04:44,080
Okay I am going to see I'm moving it.

1130
02:04:45,080 --> 02:04:52,080
Number five I'm going to see if we can get gateway running because I think they might have increased the size.

1131
02:04:53,080 --> 02:04:54,080
Okay.

1132
02:04:58,080 --> 02:04:59,080
Let's.

1133
02:05:01,080 --> 02:05:03,080
Actually I think it's up here.

1134
02:05:07,080 --> 02:05:22,080
I'm going to give it a second to see if we can get gateway running because the next notebook is going to be much better if we can get gateway running.

1135
02:05:29,080 --> 02:05:30,080
In the meantime any questions.

1136
02:05:37,080 --> 02:05:48,080
Yes.

1137
02:05:48,080 --> 02:05:51,080
So there's you can do this in pandas or Dask but if we look at that.

1138
02:05:53,080 --> 02:05:56,080
Essentially there's a particular way to write run conditions.

1139
02:05:58,080 --> 02:05:59,080
Let's see.

1140
02:06:00,080 --> 02:06:03,080
Yeah so you see here you can give these filters.

1141
02:06:03,080 --> 02:06:14,080
So here the filter is so I'm reading this file and I'm giving it that filter and you can change filters you can basically say you're greater than this year less than this.

1142
02:06:15,080 --> 02:06:22,080
So this is pushing those filters to the files so you've never actually read the whole thing in.

1143
02:06:23,080 --> 02:06:34,080
So this will like for example so exactly what's happening here if you did this with pandas you'd be reading the entire data set in you'd be taking that one column you'd be doing a greater than 222.

1144
02:06:34,080 --> 02:06:46,080
This is basically just looking at the one column in data just reading enough of that column to do the filtering and now it knows which rows to read and so it's only reading the rows that match the filter.

1145
02:06:46,080 --> 02:06:48,080
So it's really limiting them out your reading.

1146
02:06:52,080 --> 02:07:07,080
Oh I just read it with I did Dask reach CSV Dask to parquet you know so if you actually look in prep.

1147
02:07:10,080 --> 02:07:19,080
So this is the notebook you can't run this notebook because it uses our cloud credentials but if you look here I started I started Dask cluster.

1148
02:07:19,080 --> 02:07:32,080
I'm reading all the CSV files and I'm parsing the dates I think I did a sort I sorted the data by date and carrier and then I said to parquet.

1149
02:07:33,080 --> 02:07:35,080
So you know and.

1150
02:07:38,080 --> 02:07:40,080
So that's that's how that's how I did the conversion.

1151
02:07:40,080 --> 02:07:54,080
Yes I have a cluster OK so if you go back to 4 the top of 4 there's this started Dask gateway cluster and it's hard coded the profile and environment name run that cell.

1152
02:07:58,080 --> 02:08:02,080
And then come to 5 and 5 will reuse that cluster so.

1153
02:08:02,080 --> 02:08:05,080
Actually I probably shouldn't have told you to do that before I continued.

1154
02:08:05,080 --> 02:08:06,080
Got too excited.

1155
02:08:12,080 --> 02:08:13,080
Maybe I spoke too soon.

1156
02:08:13,080 --> 02:08:15,080
I think has been confirmed to.

1157
02:08:15,080 --> 02:08:38,620
Run these

1158
02:08:43,080 --> 02:08:45,080
I think I spoke too soon.

1159
02:08:52,380 --> 02:08:53,540
Okay, it's up.

1160
02:09:06,640 --> 02:09:08,740
Oh, I have five workers, okay.

1161
02:09:08,740 --> 02:09:11,740
This might actually be working then.

1162
02:09:13,080 --> 02:09:13,920
Okay.

1163
02:09:36,720 --> 02:09:40,800
Yeah, okay, let me see if I still have this one.

1164
02:09:43,720 --> 02:09:45,800
Okay, so this is actually interesting.

1165
02:09:45,800 --> 02:09:47,720
So I'm connected on this notebook,

1166
02:09:47,720 --> 02:09:49,400
but I'm not able to connect it from another notebook

1167
02:09:49,400 --> 02:09:51,640
because the API is failing.

1168
02:09:53,240 --> 02:09:55,200
So if I copy all the code to this notebook,

1169
02:09:55,200 --> 02:09:58,520
I should be able to do something.

1170
02:09:58,520 --> 02:10:00,020
Okay, let's do that.

1171
02:10:02,760 --> 02:10:03,720
This is gonna be live.

1172
02:10:03,720 --> 02:10:05,980
This should be interesting.

1173
02:10:07,920 --> 02:10:08,760
Okay, so.

1174
02:10:13,080 --> 02:10:13,920
Okay.

1175
02:10:29,520 --> 02:10:31,960
Okay, so now if you look at my cluster map,

1176
02:10:31,960 --> 02:10:34,960
you see I have five nodes, okay.

1177
02:10:34,960 --> 02:10:35,800
So,

1178
02:10:38,200 --> 02:10:39,620
and let me do the

1179
02:10:39,620 --> 02:10:40,460
copy,

1180
02:10:42,660 --> 02:10:43,500
progress,

1181
02:10:44,700 --> 02:10:45,700
task stream,

1182
02:10:46,820 --> 02:10:47,740
and memory.

1183
02:10:49,780 --> 02:10:50,620
Okay.

1184
02:10:50,620 --> 02:10:51,460
Okay.

1185
02:11:07,500 --> 02:11:09,460
Okay, I'm going to be copying stuff

1186
02:11:09,460 --> 02:11:11,020
from over here to over here,

1187
02:11:11,020 --> 02:11:12,860
so this should be nice.

1188
02:11:20,620 --> 02:11:21,460
Okay.

1189
02:11:50,620 --> 02:11:51,460
Okay.

1190
02:11:58,100 --> 02:12:00,380
So the point of what I'm gonna do right now,

1191
02:12:00,380 --> 02:12:01,260
let me just walk you through it,

1192
02:12:01,260 --> 02:12:04,220
I'm going to show you what happens

1193
02:12:04,220 --> 02:12:05,580
when you actually do a compute.

1194
02:12:05,580 --> 02:12:07,980
So I'm gonna do a plot with okay,

1195
02:12:09,560 --> 02:12:11,740
and I'm gonna do this,

1196
02:12:11,740 --> 02:12:14,760
I'm gonna look at all the council, you know,

1197
02:12:16,020 --> 02:12:18,120
sorry, the canceled things.

1198
02:12:21,620 --> 02:12:22,460
Okay.

1199
02:12:41,260 --> 02:12:42,580
I came up here, good.

1200
02:12:45,260 --> 02:12:46,100
Okay.

1201
02:12:50,620 --> 02:12:51,460
Okay.

1202
02:13:01,460 --> 02:13:05,180
Okay, so now we're up and running here on this notebook.

1203
02:13:05,180 --> 02:13:08,140
So I did a head, I have five nodes right now.

1204
02:13:09,340 --> 02:13:12,220
I'm going to import our plotting tool.

1205
02:13:20,620 --> 02:13:21,460
Okay.

1206
02:13:37,820 --> 02:13:39,300
Okay, now we're gonna try plotting this,

1207
02:13:39,300 --> 02:13:40,740
and you're gonna see,

1208
02:13:43,500 --> 02:13:45,240
see all those red lines?

1209
02:13:45,240 --> 02:13:46,900
That's communication between the nodes.

1210
02:13:46,900 --> 02:13:49,620
That's every time it's having to transform stuff

1211
02:13:49,620 --> 02:13:51,340
from one node to another node

1212
02:13:51,340 --> 02:13:54,740
because it needs to move things around.

1213
02:13:54,740 --> 02:13:57,000
Now, the more red you see,

1214
02:13:57,000 --> 02:13:59,740
the more inefficient your computation is gonna be.

1215
02:13:59,740 --> 02:14:02,540
So, and in a minute,

1216
02:14:02,540 --> 02:14:04,780
you should see my five nodes go to 10 nodes.

1217
02:14:04,780 --> 02:14:08,620
Like it's busy, so it's requesting more nodes,

1218
02:14:08,620 --> 02:14:09,900
and usually they come in a batch,

1219
02:14:09,900 --> 02:14:12,140
suddenly you'll get like five more coming in.

1220
02:14:17,620 --> 02:14:19,300
Yes, and that's part,

1221
02:14:19,300 --> 02:14:22,740
okay, so now I've got this plot,

1222
02:14:22,740 --> 02:14:26,380
and there's another reason why the plot looks terrible.

1223
02:14:26,380 --> 02:14:30,140
If I did the same plot in pandas, it would be fine,

1224
02:14:30,140 --> 02:14:33,740
but here it's doing the plot

1225
02:14:33,740 --> 02:14:38,340
for each pandas data frame individually,

1226
02:14:38,340 --> 02:14:41,020
and then it's kind of putting them together.

1227
02:14:41,020 --> 02:14:43,280
This data is not sorted by date.

1228
02:14:44,500 --> 02:14:46,980
Okay, and so the plot's a little weird

1229
02:14:46,980 --> 02:14:50,740
because it's got some data which is unsorted,

1230
02:14:50,740 --> 02:14:53,500
which if you did in pandas, it would sort it properly,

1231
02:14:53,500 --> 02:14:55,580
but now it doesn't know to do that,

1232
02:14:55,580 --> 02:14:57,340
and if it did it automatically, it'd be able to compute.

1233
02:14:57,340 --> 02:14:59,220
So this took a long time to compute,

1234
02:14:59,220 --> 02:15:01,980
you saw there was a lot of communication, okay?

1235
02:15:01,980 --> 02:15:03,260
We wanna minimize shuffling.

1236
02:15:03,260 --> 02:15:05,180
So one thing is I knew that this data

1237
02:15:05,180 --> 02:15:10,180
is something I would typically access by date

1238
02:15:10,420 --> 02:15:13,160
and by probably carrier.

1239
02:15:13,160 --> 02:15:17,240
So I have a second data set on GCP

1240
02:15:17,240 --> 02:15:22,040
where I sorted it by date.

1241
02:15:22,040 --> 02:15:24,680
So the first data set is just unsorted.

1242
02:15:24,680 --> 02:15:28,040
The second data set is sorted, okay?

1243
02:15:28,040 --> 02:15:32,620
And so when we run this on the second one,

1244
02:15:32,620 --> 02:15:34,520
you should see less communication.

1245
02:15:34,520 --> 02:15:36,600
Like you see there's less red in the top.

1246
02:15:36,600 --> 02:15:39,480
There's still some, but there's a lot less red

1247
02:15:39,480 --> 02:15:41,940
because I've already pre-sorted it.

1248
02:15:41,940 --> 02:15:46,940
So often with large data sets, you will read them in,

1249
02:15:48,300 --> 02:15:51,340
get it prepared for the kind of things you wanna do.

1250
02:15:51,340 --> 02:15:56,340
Like I expect to sort, I expect to do things in date order,

1251
02:15:56,980 --> 02:15:59,500
I expect to maybe do things by year,

1252
02:15:59,500 --> 02:16:01,580
so that defines how you wanna save it.

1253
02:16:05,220 --> 02:16:07,620
The other thing you can do is

1254
02:16:09,100 --> 02:16:10,540
what are the steps in the graph?

1255
02:16:10,540 --> 02:16:12,940
It's reading data from Parquet files.

1256
02:16:12,940 --> 02:16:16,460
Now I might sort it, I might do a group by,

1257
02:16:16,460 --> 02:16:18,660
I might do a mean, I might do several computations

1258
02:16:18,660 --> 02:16:21,740
and have my data ready for what I want to do.

1259
02:16:23,820 --> 02:16:28,020
So Dask has a special command called persist.

1260
02:16:29,020 --> 02:16:33,060
And what that does is

1261
02:16:36,060 --> 02:16:40,060
it takes all your compute till that point

1262
02:16:40,980 --> 02:16:43,100
so it's doing all this, it's doing the reading,

1263
02:16:43,100 --> 02:16:46,700
it's doing the group by, it can keep it stored in memory

1264
02:16:48,100 --> 02:16:50,080
and persisted in memory.

1265
02:16:50,080 --> 02:16:51,740
So even though you're done with the compute,

1266
02:16:51,740 --> 02:16:54,140
it knows that still exists.

1267
02:16:58,740 --> 02:17:00,860
Yep, see, you saw those new nodes pop up,

1268
02:17:00,860 --> 02:17:03,100
but they popped up just when my computation ended.

1269
02:17:03,100 --> 02:17:05,820
So this next time it should come up quicker.

1270
02:17:05,820 --> 02:17:06,660
But now

1271
02:17:09,900 --> 02:17:12,300
I'm going to run this compute.

1272
02:17:12,300 --> 02:17:16,100
I'm basically gonna say, I'm taking my ddf,

1273
02:17:16,100 --> 02:17:20,300
which is my data frame, I say I want these three variables

1274
02:17:20,300 --> 02:17:22,140
and I want to persist it.

1275
02:17:22,140 --> 02:17:24,380
And so what this is going to do

1276
02:17:24,380 --> 02:17:26,380
is it's reading all the data.

1277
02:17:26,380 --> 02:17:28,660
The yellow means read, get item.

1278
02:17:28,660 --> 02:17:31,500
There's another graph which is kind of useful.

1279
02:17:32,500 --> 02:17:36,260
Oh, this is not showing very well, but this,

1280
02:17:37,660 --> 02:17:40,460
oh yes, this shows you the graph of all the commands.

1281
02:17:40,460 --> 02:17:42,540
But essentially now this is reading.

1282
02:17:42,540 --> 02:17:43,980
If we look at cluster memory,

1283
02:17:45,380 --> 02:17:48,060
we've got five gig of data in memory now.

1284
02:17:48,060 --> 02:17:50,020
And it's gonna just keep it in memory.

1285
02:17:50,020 --> 02:17:52,500
So that read has already happened,

1286
02:17:52,500 --> 02:17:54,460
but I could also do a read and a group by

1287
02:17:54,460 --> 02:17:56,100
and then do .persist.

1288
02:17:56,100 --> 02:17:59,840
And so then it would do all of those things and stop there

1289
02:17:59,840 --> 02:18:02,040
and then you could do several different types of compute

1290
02:18:02,040 --> 02:18:04,200
on top of that data.

1291
02:18:05,560 --> 02:18:07,000
Now if we look at this,

1292
02:18:12,760 --> 02:18:14,960
so now if we use that persist,

1293
02:18:14,960 --> 02:18:16,880
I have my persisted data frame

1294
02:18:16,880 --> 02:18:18,560
and if we do the compute on this,

1295
02:18:18,560 --> 02:18:20,160
if we do the same group by,

1296
02:18:20,160 --> 02:18:22,440
it'll be a lot faster because it's not reading the data.

1297
02:18:22,440 --> 02:18:25,120
The data's already in memory, it's just doing the group by.

1298
02:18:25,120 --> 02:18:28,640
And if I do a group by a different thing,

1299
02:18:28,640 --> 02:18:30,560
again, that initial step doesn't exist

1300
02:18:30,560 --> 02:18:33,360
because we've just persisted the data in memory.

1301
02:18:33,360 --> 02:18:35,240
Now if you know that you're always going to do

1302
02:18:35,240 --> 02:18:36,080
these two things,

1303
02:18:36,080 --> 02:18:38,980
you could now save it to Parquet after doing the sort,

1304
02:18:38,980 --> 02:18:40,080
after doing the partition.

1305
02:18:40,080 --> 02:18:43,800
So a lot of this is often with these big data sets,

1306
02:18:43,800 --> 02:18:46,280
there's an initial, let me explore the data,

1307
02:18:46,280 --> 02:18:47,400
let me kind of find out

1308
02:18:47,400 --> 02:18:50,260
and then let me decide how I expect to use it in the future.

1309
02:18:50,260 --> 02:18:53,000
Now I'm gonna redo all that compute,

1310
02:18:53,000 --> 02:18:55,600
save it and the next time I can just start from there.

1311
02:18:58,760 --> 02:19:01,760
Okay, let's see, what time are we at?

1312
02:19:01,760 --> 02:19:03,520
I am going to go script right ahead,

1313
02:19:03,520 --> 02:19:06,520
the rest of this you can kind of go through on your own.

1314
02:19:11,840 --> 02:19:13,240
Let me see if I can.

1315
02:19:18,600 --> 02:19:20,600
So the cluster reduced IO time

1316
02:19:20,600 --> 02:19:23,600
because you have a longer partition file.

1317
02:19:23,600 --> 02:19:25,640
So I'm gonna go ahead and do that.

1318
02:19:25,640 --> 02:19:29,640
So you're reading a list of all the data files?

1319
02:19:29,640 --> 02:19:34,640
If you're reading a list of all the data files.

1320
02:19:34,680 --> 02:19:37,080
I always depend on where the data is usually.

1321
02:19:37,080 --> 02:19:39,640
If you put the, so one of the things which,

1322
02:19:39,640 --> 02:19:43,120
if you read from S3 it's gonna be really fast.

1323
02:19:43,120 --> 02:19:44,720
If you try and read from a local disk,

1324
02:19:44,720 --> 02:19:47,000
it'll thrash your disk, you know.

1325
02:19:47,000 --> 02:19:50,000
So the cluster would be a component too,

1326
02:19:50,000 --> 02:19:53,160
if you were reading S3?

1327
02:19:53,160 --> 02:19:55,440
Oh yeah, because it'd be reading everything in parallel.

1328
02:19:58,780 --> 02:20:01,320
Okay, so I may just go back here and do this here.

1329
02:20:01,320 --> 02:20:03,880
So we have DDF, right?

1330
02:20:03,880 --> 02:20:07,000
So this is our data frame.

1331
02:20:07,000 --> 02:20:08,920
Now let's say we want to plot.

1332
02:20:08,920 --> 02:20:10,020
How are we gonna plot?

1333
02:20:13,960 --> 02:20:15,040
We can actually just do this.

1334
02:20:15,040 --> 02:20:17,580
Let's do, let me just copy this from here.

1335
02:20:18,500 --> 02:20:23,500
Because I'm going to not worry

1336
02:20:24,300 --> 02:20:25,900
about whether that one comes up.

1337
02:20:32,120 --> 02:20:33,780
So I'm going to read the flight,

1338
02:20:33,780 --> 02:20:37,000
I'm gonna read a selected number of columns in.

1339
02:20:38,460 --> 02:20:42,120
And so now I have my flights, okay?

1340
02:20:42,120 --> 02:20:45,020
And you know, so the same data set we've been working with.

1341
02:20:45,020 --> 02:20:48,620
And I think we've already imported that.

1342
02:20:48,620 --> 02:20:53,620
So now I am going to actually plot the entire data set.

1343
02:20:55,740 --> 02:21:00,060
So this is the departure delay for the entire 20 years.

1344
02:21:00,060 --> 02:21:04,940
And if I plot that, you see it's kind of chugging along.

1345
02:21:04,940 --> 02:21:09,940
And this is a fun graph, because when this comes up

1346
02:21:09,940 --> 02:21:14,940
in a couple of seconds, that is, so here I've done a count.

1347
02:21:15,420 --> 02:21:18,100
So I've counted the total number of flights every day

1348
02:21:18,100 --> 02:21:20,700
since 2003.

1349
02:21:20,700 --> 02:21:23,100
Anyone notice anything interesting about this plot?

1350
02:21:25,100 --> 02:21:29,460
COVID, yeah, so actually if you look here,

1351
02:21:31,820 --> 02:21:33,340
if you look here,

1352
02:21:34,860 --> 02:21:37,060
you can see that the number of flights

1353
02:21:37,460 --> 02:21:42,460
if you look here, March 16th, shut down.

1354
02:21:45,140 --> 02:21:46,260
So that's the thing.

1355
02:21:46,260 --> 02:21:48,780
So when you can access the entire data,

1356
02:21:48,780 --> 02:21:50,340
you can see trends which you can't

1357
02:21:50,340 --> 02:21:52,380
when you're just getting little pieces.

1358
02:21:52,380 --> 02:21:54,900
This is 70 gigabytes of CSV files,

1359
02:21:54,900 --> 02:21:56,420
and it took us a couple of seconds.

1360
02:21:56,420 --> 02:21:59,380
So, you know, minus all the issues we've had today,

1361
02:21:59,380 --> 02:22:01,980
this is one of the things I want to take away is

1362
02:22:01,980 --> 02:22:03,740
this stuff becomes very powerful

1363
02:22:03,740 --> 02:22:05,620
once you can look at full data sets

1364
02:22:05,620 --> 02:22:09,380
instead of decimating it or doing anything else.

1365
02:22:11,420 --> 02:22:14,420
Now, there's one intricacy here.

1366
02:22:14,420 --> 02:22:17,340
If you look at what I did in this function,

1367
02:22:19,780 --> 02:22:22,620
I did a group by count HV plot, okay?

1368
02:22:24,500 --> 02:22:27,340
And if you see when I'm panning and zooming,

1369
02:22:28,180 --> 02:22:30,220
there's no additional compute happening.

1370
02:22:32,460 --> 02:22:34,700
And that's because this data set,

1371
02:22:34,700 --> 02:22:37,180
this aggregated data set fits in memory.

1372
02:22:40,180 --> 02:22:41,340
If you think,

1373
02:22:43,900 --> 02:22:47,020
it's roughly, the group by,

1374
02:22:47,020 --> 02:22:50,980
is roughly 20 years times 365 days, one point per day.

1375
02:22:50,980 --> 02:22:52,300
So it's not a very big data set.

1376
02:22:52,300 --> 02:22:53,820
If you look at this,

1377
02:22:55,140 --> 02:22:56,180
did this one come up?

1378
02:23:04,780 --> 02:23:05,620
Okay.

1379
02:23:06,420 --> 02:23:07,900
So the full data set,

1380
02:23:07,900 --> 02:23:10,460
let's look at the full data set size, okay?

1381
02:23:13,220 --> 02:23:16,460
The full data set, I'm just doing a length on the data.

1382
02:23:16,460 --> 02:23:18,420
It is 125 million rows.

1383
02:23:19,260 --> 02:23:24,260
If you try and plot 125 million dots on that plot,

1384
02:23:25,380 --> 02:23:27,020
your browser will crash.

1385
02:23:28,100 --> 02:23:31,700
You know, so I'm gonna tell you about a new tool,

1386
02:23:31,700 --> 02:23:35,220
which I saw this at a conference 10 years ago,

1387
02:23:35,220 --> 02:23:39,300
and I talked to the person who wrote it and said,

1388
02:23:39,300 --> 02:23:40,380
hey, if you can get this working

1389
02:23:40,380 --> 02:23:42,460
for triangular instruction meshes, I'll give you funding.

1390
02:23:42,460 --> 02:23:44,260
And you know, it's a long story.

1391
02:23:44,260 --> 02:23:46,340
There is another tool called Data Shader.

1392
02:23:49,620 --> 02:23:50,460
Wait, sorry.

1393
02:23:52,300 --> 02:23:53,140
So,

1394
02:23:56,900 --> 02:24:01,060
for, I'm going to make the data set slightly smaller.

1395
02:24:02,020 --> 02:24:05,420
Just so we don't have to wait a long time for these to run,

1396
02:24:05,420 --> 02:24:06,860
which is the one I was using.

1397
02:24:06,860 --> 02:24:08,260
I think I was using this one.

1398
02:24:09,660 --> 02:24:12,500
So here, I'm just pulling in five years of data,

1399
02:24:12,500 --> 02:24:14,780
everything greater than 2017.

1400
02:24:14,780 --> 02:24:16,460
And if we look at this data set,

1401
02:24:27,820 --> 02:24:29,700
so this has 30 million rows.

1402
02:24:29,700 --> 02:24:31,380
So I'm going, it's still probably more

1403
02:24:31,380 --> 02:24:33,060
than you can fit in a browser.

1404
02:24:33,060 --> 02:24:35,820
And so what we're going to do is,

1405
02:24:35,820 --> 02:24:38,260
we're going to use this tool called Data Shader.

1406
02:24:38,260 --> 02:24:40,700
And what Data Shader does is,

1407
02:24:41,660 --> 02:24:43,500
if you think about this,

1408
02:24:43,500 --> 02:24:45,820
when you look at any of these plots,

1409
02:24:45,820 --> 02:24:47,540
you're limited to what?

1410
02:24:49,980 --> 02:24:53,180
200 by 400 pixels or whatever.

1411
02:24:53,180 --> 02:24:57,340
Even if you plot two million dots on that plot,

1412
02:24:57,340 --> 02:25:00,620
you can only see 200 by 400 of them.

1413
02:25:00,620 --> 02:25:04,940
So there's no point sending all that data to the browser.

1414
02:25:04,940 --> 02:25:07,340
So what Data Shader does is,

1415
02:25:07,340 --> 02:25:12,220
it aggregates the data in a way

1416
02:25:12,220 --> 02:25:14,980
that keeps the distribution uniform

1417
02:25:14,980 --> 02:25:19,540
and only plots one pixel per whatever is there.

1418
02:25:21,500 --> 02:25:22,660
And so,

1419
02:25:24,180 --> 02:25:26,980
if we do this, and this is why,

1420
02:25:26,980 --> 02:25:28,700
so Data Shader is its own,

1421
02:25:30,660 --> 02:25:32,100
so I'm going to let this chug.

1422
02:25:32,980 --> 02:25:35,740
So if I, so I put data shade equals true.

1423
02:25:36,820 --> 02:25:40,980
And now it's chugging along, it's doing some calculations.

1424
02:25:40,980 --> 02:25:45,980
And on the back end, it's connected to Data Shader,

1425
02:25:46,340 --> 02:25:48,220
which if you, I think there's a link,

1426
02:25:48,220 --> 02:25:49,540
if you go to Data Shader,

1427
02:25:53,980 --> 02:25:56,060
Data Shader is its own software

1428
02:25:56,060 --> 02:25:57,340
that's integrated with all these.

1429
02:25:57,340 --> 02:26:00,940
HV plot is acting as a front end for Data Shader,

1430
02:26:00,940 --> 02:26:03,620
and it just, so you just add one line.

1431
02:26:03,620 --> 02:26:07,340
And here,

1432
02:26:08,380 --> 02:26:09,460
you know, I'm doing the entire,

1433
02:26:09,460 --> 02:26:14,180
so this is doing 30 million individual points

1434
02:26:14,180 --> 02:26:16,020
and calculating everything.

1435
02:26:16,020 --> 02:26:19,460
And then in a few minutes, I'll get a plot

1436
02:26:19,460 --> 02:26:20,860
that I can browse.

1437
02:26:21,460 --> 02:26:26,180
And as I change my zoom and pan level,

1438
02:26:26,180 --> 02:26:30,180
it will recalculate the pixels needed

1439
02:26:30,180 --> 02:26:32,460
for that zoom and pan level.

1440
02:26:32,460 --> 02:26:35,580
So this allows me to see a plot of the entire data set.

1441
02:26:36,580 --> 02:26:38,340
Actually, how many nodes do we have?

1442
02:26:40,020 --> 02:26:40,860
We have 10.

1443
02:26:44,860 --> 02:26:48,380
In interest of making life better for all of us, oops.

1444
02:26:48,740 --> 02:26:53,740
I am going to say scale from 10 to 100.

1445
02:26:58,660 --> 02:26:59,860
We'll see if I get them.

1446
02:27:08,660 --> 02:27:11,060
Okay, so here I have my plot.

1447
02:27:11,060 --> 02:27:14,220
That's every single flight for the last 20 years.

1448
02:27:14,220 --> 02:27:18,620
But I can now zoom in here,

1449
02:27:18,620 --> 02:27:20,980
and initially it's giving me this rasterized thing.

1450
02:27:20,980 --> 02:27:23,260
It's gonna start chugging again,

1451
02:27:23,260 --> 02:27:25,420
and then it'll give me a new plot,

1452
02:27:25,420 --> 02:27:29,060
which is re-rasterized for that date range,

1453
02:27:29,060 --> 02:27:30,340
and I can actually get down

1454
02:27:30,340 --> 02:27:33,220
to individual points seamlessly.

1455
02:27:33,220 --> 02:27:36,340
So this lets you look at the entire 20 years of data,

1456
02:27:36,340 --> 02:27:40,620
but still have access to the individual data point,

1457
02:27:40,620 --> 02:27:43,700
which is a lot better than calculating a mean

1458
02:27:44,180 --> 02:27:46,740
or median when you're looking at large data sets.

1459
02:27:48,180 --> 02:27:49,900
Let's see, oh, I got all my nodes.

1460
02:27:52,740 --> 02:27:56,700
So to be clear, I want to kind of mention one thing

1461
02:27:56,700 --> 02:27:59,300
because we've had a lot of technical problems today.

1462
02:27:59,300 --> 02:28:02,220
The technical problems were not the DAS clusters.

1463
02:28:02,220 --> 02:28:04,140
We tested the DAS cluster,

1464
02:28:04,140 --> 02:28:08,900
scaling up to 2,000 CPU cores on Friday,

1465
02:28:08,900 --> 02:28:11,700
actually on yesterday as well on Monday.

1466
02:28:11,700 --> 02:28:15,060
The issue we had here is our DAS gateway entrance,

1467
02:28:15,060 --> 02:28:18,220
which you ask for a cluster and get a cluster.

1468
02:28:18,220 --> 02:28:21,860
That is where we didn't have that sized appropriately

1469
02:28:21,860 --> 02:28:22,700
for the number of people,

1470
02:28:22,700 --> 02:28:24,660
because what we did on Monday

1471
02:28:24,660 --> 02:28:27,140
is we had 10 people each launch 100,

1472
02:28:27,140 --> 02:28:29,820
and now we have 40 people launching a smaller number.

1473
02:28:29,820 --> 02:28:32,140
So yeah, so now see I've zoomed in,

1474
02:28:32,140 --> 02:28:34,660
you start seeing the individual bits.

1475
02:28:34,660 --> 02:28:37,620
Now you can also do that with any plot,

1476
02:28:37,620 --> 02:28:39,620
like we have a scatter plot here,

1477
02:28:39,620 --> 02:28:43,780
and you see now it finished the compute,

1478
02:28:43,780 --> 02:28:44,980
so it dropped back down to 10,

1479
02:28:44,980 --> 02:28:46,580
because I put the minimum as 10.

1480
02:28:47,500 --> 02:28:49,220
And so if I do a scatter plot,

1481
02:28:50,880 --> 02:28:52,260
again, it'll start chugging up,

1482
02:28:52,260 --> 02:28:54,100
it'll start asking for more nodes.

1483
02:28:58,020 --> 02:28:58,860
We'll see how quick this is now,

1484
02:28:58,860 --> 02:29:00,060
because I have 100 nodes.

1485
02:29:02,620 --> 02:29:03,460
There they come.

1486
02:29:03,460 --> 02:29:04,300
There they come.

1487
02:29:08,900 --> 02:29:13,300
So I will leave this cluster on for the rest of PyCon.

1488
02:29:13,300 --> 02:29:14,780
So if you guys want to,

1489
02:29:14,780 --> 02:29:16,500
when we're not all hitting at the same time,

1490
02:29:16,500 --> 02:29:17,420
all these should work,

1491
02:29:17,420 --> 02:29:19,780
since we tested them four times before this.

1492
02:29:19,780 --> 02:29:20,780
So here's a scatter plot,

1493
02:29:20,780 --> 02:29:22,460
and actually it's kind of interesting.

1494
02:29:22,460 --> 02:29:24,260
You see that the arrival delay,

1495
02:29:25,680 --> 02:29:27,620
they can make up the arrival delay

1496
02:29:27,620 --> 02:29:29,220
when you're on longer flights.

1497
02:29:30,160 --> 02:29:31,340
But when you're on shorter flights,

1498
02:29:31,340 --> 02:29:33,540
they have more trouble making up the arrival delay.

1499
02:29:33,540 --> 02:29:35,380
So that's the kind of insight you can get

1500
02:29:35,380 --> 02:29:37,500
when you can look at the entire dataset.

1501
02:29:37,500 --> 02:29:41,180
And again, this is all the points, all 30 million.

1502
02:29:41,180 --> 02:29:43,020
Since I have 100 nodes,

1503
02:29:43,020 --> 02:29:44,980
I probably could have done the 125,

1504
02:29:44,980 --> 02:29:48,520
but we designed, I dropped it down to 30,

1505
02:29:49,900 --> 02:29:51,260
30 million rows of data,

1506
02:29:51,260 --> 02:29:54,540
because that worked pretty well with 10 nodes.

1507
02:29:56,660 --> 02:29:58,460
Okay, does folks need a quick break?

1508
02:29:59,460 --> 02:30:01,500
I'm kind of rushing through,

1509
02:30:01,500 --> 02:30:04,940
because we're about, yeah, that's stuff.

1510
02:30:06,860 --> 02:30:08,300
We good to keep going?

1511
02:30:08,300 --> 02:30:09,140
Okay.

1512
02:30:17,500 --> 02:30:18,980
Okay, so now,

1513
02:30:20,420 --> 02:30:21,740
this is,

1514
02:30:24,380 --> 02:30:26,640
since we have good luck with that notebook,

1515
02:30:26,640 --> 02:30:28,060
I'm gonna stay in that notebook.

1516
02:30:28,660 --> 02:30:30,420
I'm just gonna copy things over.

1517
02:30:30,420 --> 02:30:32,960
So I'm gonna read in this airport data.

1518
02:30:34,960 --> 02:30:36,140
Why am I, oh, okay.

1519
02:30:45,100 --> 02:30:47,260
So I'm reading in this airport data,

1520
02:30:47,260 --> 02:30:49,340
and it's from a different source from the earlier notebook,

1521
02:30:49,340 --> 02:30:53,980
because some airlines were missing in the other dataset.

1522
02:30:53,980 --> 02:30:55,060
Let me close this.

1523
02:30:55,620 --> 02:30:59,220
And I'm gonna do a quick plot of the data,

1524
02:30:59,220 --> 02:31:02,180
just to remind you guys what it looked like.

1525
02:31:09,220 --> 02:31:10,060
Ah, sorry.

1526
02:31:20,580 --> 02:31:21,420
Really?

1527
02:31:21,420 --> 02:31:22,940
Okay.

1528
02:31:22,940 --> 02:31:23,780
Let me see.

1529
02:31:25,060 --> 02:31:25,900
Okay.

1530
02:31:35,260 --> 02:31:37,060
It doesn't like me.

1531
02:31:37,060 --> 02:31:39,540
Anyway, I'll come back to this, see if it comes up.

1532
02:31:45,340 --> 02:31:47,700
Okay, so the error I got just now is,

1533
02:31:47,700 --> 02:31:50,460
there are two commands, import HP plot.pandas,

1534
02:31:50,460 --> 02:31:53,940
and import HP plot.dask.

1535
02:31:53,940 --> 02:31:57,020
And if you're using plots with Dask,

1536
02:31:57,020 --> 02:31:59,100
you need to do the other one.

1537
02:31:59,100 --> 02:32:02,020
And so this airline plot is using Pandas,

1538
02:32:02,020 --> 02:32:03,340
so I need that.

1539
02:32:06,020 --> 02:32:09,400
Okay, so this is a plot of all the airports in the world.

1540
02:32:09,400 --> 02:32:10,540
Not very interesting.

1541
02:32:14,400 --> 02:32:15,300
Oh, it came up.

1542
02:32:17,420 --> 02:32:18,580
Okay, so I can just,

1543
02:32:18,580 --> 02:32:20,860
let me make sure this is 10 and 100 still.

1544
02:32:24,780 --> 02:32:26,020
Okay.

1545
02:32:32,180 --> 02:32:33,580
Okay, so now I'm gonna do,

1546
02:32:33,580 --> 02:32:36,820
I'm gonna build a dashboard out of what we've done so far.

1547
02:32:36,820 --> 02:32:40,580
Okay, so till now we did individual plots, which is great,

1548
02:32:40,580 --> 02:32:42,180
but we're gonna build a panel dashboard.

1549
02:32:42,180 --> 02:32:47,180
And panel is a software that is similar to Streamlit

1550
02:32:48,280 --> 02:32:51,060
and Dash and some of the other things.

1551
02:32:51,060 --> 02:32:53,220
I actually have a blog post comparing all of them.

1552
02:32:53,220 --> 02:32:57,500
But panel specifically is very good at working

1553
02:32:57,500 --> 02:32:58,540
with really large data sets.

1554
02:32:58,540 --> 02:32:59,700
And all these tools which I've shown you

1555
02:32:59,700 --> 02:33:01,260
till now work great with it.

1556
02:33:02,180 --> 02:33:05,300
And there's three different ways to build

1557
02:33:05,300 --> 02:33:07,580
an interactive dashboard and panel

1558
02:33:07,580 --> 02:33:10,540
based on what you're trying to do.

1559
02:33:10,540 --> 02:33:11,700
I'm gonna use the simplest,

1560
02:33:11,700 --> 02:33:14,580
which is this concept of dot interactive.

1561
02:33:14,580 --> 02:33:17,140
Okay, and there's also,

1562
02:33:17,140 --> 02:33:19,620
if you're exploring, this is a good one,

1563
02:33:19,620 --> 02:33:21,380
if you're actually building an app,

1564
02:33:21,380 --> 02:33:23,140
there's another one where you write classes

1565
02:33:24,060 --> 02:33:25,940
and stuff like that that we can look at.

1566
02:33:25,940 --> 02:33:27,860
So here, this is how you'd write a pipeline.

1567
02:33:27,860 --> 02:33:31,660
If anyone's familiar with writing a pipeline in Pandas,

1568
02:33:31,660 --> 02:33:33,900
this is how you would write a pipeline in Pandas.

1569
02:33:33,900 --> 02:33:35,620
So this is a simple pipeline.

1570
02:33:40,100 --> 02:33:44,860
Okay, so the same kind of pipeline works with Dask.

1571
02:33:44,860 --> 02:33:46,620
So I'm basically saying I'm taking my flights.

1572
02:33:46,620 --> 02:33:50,900
I'm saying flights are greater than 2020, less than 2021.

1573
02:33:50,900 --> 02:33:52,200
I'm grouping by day of week.

1574
02:33:52,200 --> 02:33:55,080
I'm looking at the arrival array and I'm doing a mean,

1575
02:33:55,080 --> 02:33:56,880
and I'm just renaming the column

1576
02:33:56,880 --> 02:33:59,080
so it looks a little nicer.

1577
02:33:59,080 --> 02:34:00,640
Okay, so that's the static thing.

1578
02:34:00,640 --> 02:34:02,480
If I did pipeline.compute,

1579
02:34:03,520 --> 02:34:05,680
I would do pipeline.head,

1580
02:34:05,680 --> 02:34:08,580
it would generate that data.

1581
02:34:09,620 --> 02:34:11,240
But we actually want something interactive

1582
02:34:11,240 --> 02:34:13,620
where we change widgets, and as we would change widgets,

1583
02:34:13,620 --> 02:34:15,200
we want stuff to happen.

1584
02:34:15,200 --> 02:34:20,200
So we're gonna use HVPlot's dynamic interface

1585
02:34:20,400 --> 02:34:25,400
where what we are gonna do is,

1586
02:34:25,440 --> 02:34:29,240
if you look here, we have this date quantity,

1587
02:34:29,240 --> 02:34:31,360
we have this group by quantity,

1588
02:34:31,360 --> 02:34:34,620
we have this field, and we have this method.

1589
02:34:34,620 --> 02:34:38,160
So I want to make each of these,

1590
02:34:38,160 --> 02:34:40,960
date range, group by field, and method,

1591
02:34:40,960 --> 02:34:42,600
I want to make them widgets.

1592
02:34:42,600 --> 02:34:44,100
I don't want to hard code them in.

1593
02:34:44,100 --> 02:34:48,680
And so I am picking widgets from here.

1594
02:34:48,720 --> 02:34:52,560
These are all the different widgets available in Pandas.

1595
02:34:52,560 --> 02:34:54,280
Like for example, there's a date picker,

1596
02:34:54,280 --> 02:34:56,800
and so this is a date picker widget

1597
02:34:56,800 --> 02:34:59,920
that lets you pick dates.

1598
02:35:00,920 --> 02:35:04,960
And so I'm going to pick some widgets for them.

1599
02:35:04,960 --> 02:35:07,960
So here's a date time widget.

1600
02:35:07,960 --> 02:35:10,160
This is a date range slider widget.

1601
02:35:10,160 --> 02:35:12,260
And I'm giving it a start date and end date.

1602
02:35:12,260 --> 02:35:13,900
I'm giving it some default values,

1603
02:35:13,900 --> 02:35:16,040
and I'm giving it some how long I want it.

1604
02:35:16,040 --> 02:35:19,360
And if you look at it, now I get this little date range thing.

1605
02:35:19,360 --> 02:35:21,200
So I can kind of like play with this.

1606
02:35:21,200 --> 02:35:22,360
You know.

1607
02:35:22,360 --> 02:35:26,800
Now, when I change this,

1608
02:35:26,800 --> 02:35:29,800
the data from that is in date range.value.

1609
02:35:29,800 --> 02:35:31,680
You go to start date and end date.

1610
02:35:31,680 --> 02:35:36,680
If I move this, and I run it again, the dates have changed.

1611
02:35:37,040 --> 02:35:39,360
So fairly straightforward.

1612
02:35:39,360 --> 02:35:41,780
Now I'm gonna pick a radio button group

1613
02:35:41,780 --> 02:35:43,680
for the group by widget.

1614
02:35:43,680 --> 02:35:45,120
And so that looks like this.

1615
02:35:45,120 --> 02:35:49,140
So you pick whichever one you want.

1616
02:35:49,140 --> 02:35:50,240
I just gave it a name.

1617
02:35:50,240 --> 02:35:52,920
I gave it year, month, day.

1618
02:35:52,920 --> 02:35:56,520
And whatever I pick is now the value.

1619
02:35:59,720 --> 02:36:02,480
And I was similar for method.

1620
02:36:02,480 --> 02:36:03,960
You know, method I chose a different widget.

1621
02:36:03,960 --> 02:36:06,180
I chose a select widget for the method.

1622
02:36:06,180 --> 02:36:07,800
So we can do mean, max, min.

1623
02:36:09,480 --> 02:36:10,320
Okay.

1624
02:36:10,320 --> 02:36:15,320
And I think for field, I chose a radio button.

1625
02:36:16,520 --> 02:36:19,080
So you can, the departure delay or arrival delay.

1626
02:36:19,080 --> 02:36:19,920
So far so good.

1627
02:36:19,920 --> 02:36:21,460
These are fairly simple.

1628
02:36:21,460 --> 02:36:23,120
Now we wanna put everything together

1629
02:36:23,120 --> 02:36:24,760
and we wanna make it interactive.

1630
02:36:24,760 --> 02:36:28,800
So we're gonna take our flight pandas or Dask data frame.

1631
02:36:28,800 --> 02:36:30,560
And this also works with pandas by the way.

1632
02:36:30,560 --> 02:36:32,000
So if you wanna do this with pandas

1633
02:36:32,000 --> 02:36:34,880
with a smaller data set, exactly the same.

1634
02:36:34,880 --> 02:36:39,620
I'm going to basically make my data frame interactive

1635
02:36:39,620 --> 02:36:42,700
using HP plot by just calling dot interactive.

1636
02:36:42,700 --> 02:36:46,660
And now I'm calling the new interactive thing iFlights.

1637
02:36:46,660 --> 02:36:47,540
Okay.

1638
02:36:47,540 --> 02:36:52,540
For reference, this was our earlier pipeline.

1639
02:36:56,780 --> 02:36:59,020
I'm gonna use the same pipeline code.

1640
02:36:59,020 --> 02:37:02,820
I'm gonna call it iPipeline just because, you know, ting.

1641
02:37:02,820 --> 02:37:07,820
But wherever I had a number or a string,

1642
02:37:08,500 --> 02:37:12,560
like 2020, day of week, arrival delay, how,

1643
02:37:12,560 --> 02:37:14,980
I'm replacing it by my widget names.

1644
02:37:16,520 --> 02:37:17,940
Okay.

1645
02:37:17,940 --> 02:37:22,940
Now if I run that, now if you look at, oops.

1646
02:37:23,540 --> 02:37:25,700
I think I forgot to run this.

1647
02:37:25,700 --> 02:37:30,700
Now if I run this, now I have this nice widget thing.

1648
02:37:30,700 --> 02:37:32,700
If I change this and you basically,

1649
02:37:32,700 --> 02:37:34,700
you can interact with all these things,

1650
02:37:34,700 --> 02:37:36,700
but it hasn't done anything.

1651
02:37:36,700 --> 02:37:39,700
It just basically is this Dask data frame structure.

1652
02:37:39,700 --> 02:37:42,700
And it has a certain number of things.

1653
02:37:42,700 --> 02:37:45,300
You can use this as if it was a data frame.

1654
02:37:45,300 --> 02:37:46,700
So how did we Dask data frame?

1655
02:37:46,700 --> 02:37:49,100
How did we visualize a Dask data frame?

1656
02:37:49,100 --> 02:37:50,500
We did dot HP plot.

1657
02:37:50,500 --> 02:37:52,700
So I'm gonna do dot HP plot here.

1658
02:37:53,700 --> 02:37:54,900
And this is actually gonna chug

1659
02:37:54,900 --> 02:37:56,700
because we're gonna use the data frame.

1660
02:37:56,700 --> 02:37:58,700
So we're gonna use the data frame.

1661
02:37:59,200 --> 02:38:01,600
And this is actually gonna chug because what happens is,

1662
02:38:01,600 --> 02:38:02,800
and there's a way to get around this,

1663
02:38:02,800 --> 02:38:05,600
but the first time you run this,

1664
02:38:05,600 --> 02:38:08,000
it's going to use the default values of all the widgets

1665
02:38:08,000 --> 02:38:10,600
and calculate the graph based on the default values.

1666
02:38:10,600 --> 02:38:12,500
There is a way to avoid it from calculating

1667
02:38:12,500 --> 02:38:14,800
till you choose things, but I'm not gonna go into that.

1668
02:38:14,800 --> 02:38:19,800
So this is, you know, calculating, you know,

1669
02:38:19,800 --> 02:38:23,800
I guess this would be departure delay max by month.

1670
02:38:23,800 --> 02:38:28,800
And now you have this plot that's on the entire dataset

1671
02:38:30,500 --> 02:38:33,200
that you now can control with like,

1672
02:38:33,200 --> 02:38:35,100
okay, let me change it to day of month.

1673
02:38:35,100 --> 02:38:36,600
And it's gonna compute again.

1674
02:38:38,600 --> 02:38:41,000
And there are ways to make the compute more efficient.

1675
02:38:41,000 --> 02:38:42,700
Like right now I've got a general thing

1676
02:38:42,700 --> 02:38:45,600
where I change everything, it recomputes everything,

1677
02:38:45,600 --> 02:38:48,000
but you can actually make it more granular,

1678
02:38:48,000 --> 02:38:49,400
saying if I'm choosing a widget

1679
02:38:49,400 --> 02:38:50,700
that doesn't affect the quantity,

1680
02:38:50,700 --> 02:38:51,800
don't recompute that quantity.

1681
02:38:51,800 --> 02:38:53,600
There's some, if you use the other techniques,

1682
02:38:53,600 --> 02:38:55,200
you can make it more complicated.

1683
02:38:55,200 --> 02:38:58,500
But the simple one is simple.

1684
02:38:58,500 --> 02:39:00,100
They've got the new plot.

1685
02:39:02,000 --> 02:39:06,700
So I'm gonna do the same thing with the airlines.

1686
02:39:06,700 --> 02:39:07,800
So here, let's look.

1687
02:39:08,700 --> 02:39:12,800
I am grouping by origin, like the origin airport,

1688
02:39:12,800 --> 02:39:16,200
and I'm also joining the airport

1689
02:39:16,200 --> 02:39:18,900
so I have the latitude longitude, okay?

1690
02:39:18,900 --> 02:39:22,500
And if I ran this, you know,

1691
02:39:22,500 --> 02:39:24,100
so I'm making a new map plot.

1692
02:39:25,400 --> 02:39:27,700
And my map plot is hreplot.point,

1693
02:39:27,700 --> 02:39:29,600
slatitude longitude, geotrue.

1694
02:39:31,000 --> 02:39:32,500
And you know, if I uncomment,

1695
02:39:32,500 --> 02:39:34,500
actually I can uncomment this, I can run this.

1696
02:39:34,500 --> 02:39:35,800
Let's do this.

1697
02:39:35,800 --> 02:39:39,600
So this will make a new plot which is the map, okay?

1698
02:39:39,600 --> 02:39:40,900
And I'm making a couple of plots here

1699
02:39:40,900 --> 02:39:42,400
because I wanna show you the other thing.

1700
02:39:42,400 --> 02:39:44,300
This is one of the advantages Panel has

1701
02:39:44,300 --> 02:39:46,800
over something like Dash or Streamlit,

1702
02:39:46,800 --> 02:39:49,800
is a lot of the tools like Dash and stuff

1703
02:39:49,800 --> 02:39:51,400
require you to know some JavaScript

1704
02:39:51,400 --> 02:39:55,100
or some HTML or stuff to kind of lay things out.

1705
02:39:56,100 --> 02:39:57,900
Panel has a very simple,

1706
02:39:57,900 --> 02:40:00,100
I mean it has a more complicated templating system,

1707
02:40:00,100 --> 02:40:04,000
but it has a very simple row column grid system.

1708
02:40:04,000 --> 02:40:07,900
So here, I'm gonna say I want a column

1709
02:40:07,900 --> 02:40:11,400
and my first row in the column

1710
02:40:11,400 --> 02:40:13,800
is gonna be the date range widget.

1711
02:40:13,800 --> 02:40:18,200
The second row, I'm actually going to split it into two

1712
02:40:18,200 --> 02:40:21,100
and I'm gonna put the field at the beginning

1713
02:40:21,100 --> 02:40:22,400
and then I'm gonna put a column

1714
02:40:22,400 --> 02:40:24,900
with group by and method there

1715
02:40:24,900 --> 02:40:27,460
and the data and the map I want below them.

1716
02:40:28,500 --> 02:40:32,900
And so, yeah, I forgot this takes a long while to calculate.

1717
02:40:41,100 --> 02:40:43,300
Oh, that was just a data sort issue.

1718
02:40:43,300 --> 02:40:44,700
I think I was using the unsorted data.

1719
02:40:44,700 --> 02:40:47,800
So this is what the other plot looks like.

1720
02:40:51,700 --> 02:40:54,200
So now here, I'm gonna do the same thing.

1721
02:40:55,500 --> 02:40:56,500
Oh, I guess I should have waited

1722
02:40:56,500 --> 02:40:58,900
till the other one finished calculating, but okay.

1723
02:40:58,900 --> 02:41:00,400
This will show up in a couple of seconds

1724
02:41:00,400 --> 02:41:02,600
and it'll take what we did earlier

1725
02:41:02,600 --> 02:41:06,300
and rearrange them into a nicer interface.

1726
02:41:13,200 --> 02:41:15,100
Yeah, it's just column and row.

1727
02:41:15,100 --> 02:41:16,500
So anything you can do, column and row,

1728
02:41:16,500 --> 02:41:17,900
you can just very simply lay out

1729
02:41:17,900 --> 02:41:19,400
without having to worry about anything.

1730
02:41:19,500 --> 02:41:23,000
There is also a, I didn't do this,

1731
02:41:23,000 --> 02:41:25,200
but there's a template system

1732
02:41:25,200 --> 02:41:29,200
where you can basically have header, sidebar, main

1733
02:41:29,200 --> 02:41:33,200
and pop-ups and you can basically say what goes in each.

1734
02:41:33,200 --> 02:41:35,500
So you can build little websites pretty easily

1735
02:41:36,600 --> 02:41:40,000
without knowing any JavaScript or HTML or anything

1736
02:41:40,000 --> 02:41:44,100
and it's got some different template frameworks and stuff.

1737
02:41:44,100 --> 02:41:46,800
So you can build little applications like this.

1738
02:41:47,800 --> 02:41:51,200
There's actually a website called Awesome Panel

1739
02:41:55,600 --> 02:41:58,900
where they kind of show more complicated examples.

1740
02:42:00,100 --> 02:42:01,500
Again, this is really slow.

1741
02:42:09,300 --> 02:42:10,500
Come talk to my company.

1742
02:42:11,400 --> 02:42:13,800
No, there's ways you can do it on your own,

1743
02:42:13,800 --> 02:42:15,600
but there's no, like, you know,

1744
02:42:15,600 --> 02:42:18,900
Plotly has a enterprise option.

1745
02:42:18,900 --> 02:42:21,600
There's no enterprise option around Panel right now.

1746
02:42:22,600 --> 02:42:25,600
We've talked about maybe building one, but we, you know.

1747
02:42:26,800 --> 02:42:28,700
Okay, so if you look,

1748
02:42:28,700 --> 02:42:31,700
this is what it looks like with my layout.

1749
02:42:31,700 --> 02:42:33,800
I basically put the date range at the top.

1750
02:42:33,800 --> 02:42:35,800
I put the arrival departure here

1751
02:42:35,800 --> 02:42:38,600
and then I've got all these plots together

1752
02:42:38,600 --> 02:42:41,500
in one nice layout.

1753
02:42:44,600 --> 02:42:46,200
Now, the other thing you can do

1754
02:42:47,500 --> 02:42:52,500
is at the end of this, you can type .servable.

1755
02:42:53,700 --> 02:42:55,400
And if you type .servable,

1756
02:42:57,600 --> 02:42:59,100
Panel knows, go ahead.

1757
02:43:04,300 --> 02:43:05,500
Yeah, yeah.

1758
02:43:05,500 --> 02:43:07,500
So actually Panel is a layout tool

1759
02:43:08,500 --> 02:43:13,500
and you can actually mix HP plot,

1760
02:43:13,600 --> 02:43:17,900
Matplotlib, Plotly, all the plots

1761
02:43:17,900 --> 02:43:19,900
and just use Panel to lay them out.

1762
02:43:19,900 --> 02:43:22,200
It also has some ways you can,

1763
02:43:22,200 --> 02:43:25,000
really easy to connect the plots together.

1764
02:43:25,000 --> 02:43:26,400
So if you zoom on this plot

1765
02:43:26,400 --> 02:43:28,300
and you want this plot to update

1766
02:43:28,300 --> 02:43:29,900
or if you select something on this plot

1767
02:43:29,900 --> 02:43:31,500
and you want this plot to change,

1768
02:43:31,500 --> 02:43:32,800
Panel has a lot of tools for that,

1769
02:43:32,800 --> 02:43:36,900
but it's actually agnostic to which plotting engine you use.

1770
02:43:36,900 --> 02:43:41,400
So kind of like Dash only really works with Plotly,

1771
02:43:41,400 --> 02:43:44,300
but Panel, you can have a Plotly plot in here if you want.

1772
02:43:45,500 --> 02:43:48,800
And if you, actually.

1773
02:43:51,800 --> 02:43:56,000
Yeah, so basically you can look at the reference gallery.

1774
02:43:56,000 --> 02:43:58,200
It kind of gives you all these different widgets.

1775
02:44:02,200 --> 02:44:04,600
I will quickly do one last demo.

1776
02:44:07,600 --> 02:44:12,600
Actually let me, is this still really chugging?

1777
02:44:15,500 --> 02:44:16,600
No, it's done, yeah.

1778
02:44:19,600 --> 02:44:22,700
Okay, this is another thing which is sort of unique to Panel

1779
02:44:22,700 --> 02:44:25,700
which I like a lot because back when I was

1780
02:44:25,700 --> 02:44:28,700
in a different federal government agency, I helped fund it.

1781
02:44:28,700 --> 02:44:30,300
So it's one of my favorite tools.

1782
02:44:30,300 --> 02:44:31,900
I'm not gonna go through the code here,

1783
02:44:31,900 --> 02:44:34,700
but this is using Panel's pipeline feature.

1784
02:44:34,700 --> 02:44:36,100
So here, I'll just walk you through

1785
02:44:36,100 --> 02:44:37,400
what's kind of happening.

1786
02:44:38,900 --> 02:44:42,100
Here, I'm making, and this is using the class-based approach

1787
02:44:42,100 --> 02:44:43,700
I told you which I wasn't gonna go through.

1788
02:44:43,700 --> 02:44:46,900
This is making a class called LaunchDashCluster,

1789
02:44:46,900 --> 02:44:51,300
and it has some details in there about some widgets and stuff.

1790
02:44:51,300 --> 02:44:53,700
If you look at the top, there's a widget saying

1791
02:44:55,800 --> 02:44:59,200
number of workers, it's a range slider,

1792
02:44:59,200 --> 02:45:01,200
one to five, stuff like that.

1793
02:45:01,200 --> 02:45:04,600
And then here I have my main plot,

1794
02:45:04,600 --> 02:45:09,600
and then, actually, hopefully this will run,

1795
02:45:12,100 --> 02:45:17,100
but actually I didn't need to do this.

1796
02:45:18,400 --> 02:45:22,200
Okay, so this makes this little application for me,

1797
02:45:22,200 --> 02:45:24,000
and I'm actually gonna switch,

1798
02:45:24,000 --> 02:45:26,300
so this, you can prototype in the notebook.

1799
02:45:26,300 --> 02:45:29,000
I made three stages and I've run them here,

1800
02:45:29,000 --> 02:45:34,000
but I'm actually going to switch to a terminal.

1801
02:45:34,900 --> 02:45:35,900
Okay.

1802
02:45:49,900 --> 02:45:52,900
Okay, I'm gonna activate, let's see.

1803
02:45:54,500 --> 02:45:56,100
I'm gonna activate my environment.

1804
02:46:04,600 --> 02:46:09,600
Okay, so I'm gonna run, so I'm gonna take that notebook

1805
02:46:15,000 --> 02:46:18,400
which I had earlier, I think it's called 08 whatever,

1806
02:46:21,200 --> 02:46:24,200
and I'm going to panel serve it.

1807
02:46:24,200 --> 02:46:28,700
And this will launch the notebook,

1808
02:46:28,700 --> 02:46:30,700
and I should be able to get to it.

1809
02:46:30,800 --> 02:46:31,800
Let me see.

1810
02:46:40,800 --> 02:46:43,800
What was the port?

1811
02:46:43,800 --> 02:46:44,800
506.

1812
02:47:01,700 --> 02:47:04,700
I don't know what happened.

1813
02:47:09,700 --> 02:47:12,400
Okay, give me a second on that,

1814
02:47:12,400 --> 02:47:15,400
but in the meantime, if you guys go to here,

1815
02:47:15,400 --> 02:47:16,200
you should be able to do this.

1816
02:47:16,200 --> 02:47:20,600
Go to hub control panel in the file menu,

1817
02:47:20,600 --> 02:47:25,600
and click on dashboards, and you click on,

1818
02:47:27,000 --> 02:47:30,000
these are dashboards, we're using a JupyterHub plugin

1819
02:47:30,000 --> 02:47:32,700
called CDS dashboards, so you can share dashboards

1820
02:47:32,700 --> 02:47:34,900
like the one I just tried to create.

1821
02:47:34,900 --> 02:47:38,900
And if we click on this, Noah tides and currents,

1822
02:47:38,900 --> 02:47:41,300
this is one my colleague Kim made,

1823
02:47:41,300 --> 02:47:43,900
and this is the same three step process

1824
02:47:43,900 --> 02:47:46,400
where we have a, this is not using Dask,

1825
02:47:46,400 --> 02:47:50,300
but this lets you kind of zoom around

1826
02:47:52,500 --> 02:47:57,500
and pick a tide gauge, and then hit next,

1827
02:47:57,900 --> 02:48:02,100
and download the data from that tide gauge,

1828
02:48:02,100 --> 02:48:04,500
and so you can start building these little wizards

1829
02:48:04,500 --> 02:48:07,300
using these tools without knowing any kind of JavaScript

1830
02:48:07,300 --> 02:48:08,300
or anything else like that.

1831
02:48:08,300 --> 02:48:12,400
So I've got some data, I'm gonna generate a plot,

1832
02:48:12,400 --> 02:48:17,400
and then if I hit next, I should have the data set

1833
02:48:19,100 --> 02:48:22,900
and a download data button that will let me download the CSV.

1834
02:48:22,900 --> 02:48:27,500
So with that, this should also work,

1835
02:48:27,500 --> 02:48:30,800
let me see if I can get this, this should be the,

1836
02:48:35,500 --> 02:48:38,600
I see, that's the issue, I need to go here.

1837
02:48:52,900 --> 02:48:55,900
If this doesn't work, I'm going to stop and,

1838
02:49:22,900 --> 02:49:27,900
nope, I don't know why that's not working.

1839
02:49:36,300 --> 02:49:37,140
Do I?

1840
02:49:38,000 --> 02:49:39,200
Ah, okay.

1841
02:49:45,800 --> 02:49:46,800
Well, it's spinning.

1842
02:49:48,900 --> 02:49:52,000
Okay, well I'll show it to you in the notebook instead.

1843
02:49:52,000 --> 02:49:53,600
That should have come up on that page,

1844
02:49:53,600 --> 02:49:56,600
but essentially what it's gonna look like is,

1845
02:49:58,000 --> 02:49:59,900
if you look here, let me just make this a little bigger

1846
02:49:59,900 --> 02:50:01,200
so you can see it here.

1847
02:50:01,200 --> 02:50:06,200
So if you look here, I've basically created a little,

1848
02:50:06,200 --> 02:50:10,200
let's pick a number of nodes, let's pick a date range,

1849
02:50:10,200 --> 02:50:13,000
and I'm gonna launch Dask, and it found

1850
02:50:13,000 --> 02:50:15,600
that I had an existing cluster and connected to it,

1851
02:50:15,600 --> 02:50:17,300
and now I'm gonna hit next,

1852
02:50:17,300 --> 02:50:18,800
and it's gonna start computing.

1853
02:50:18,800 --> 02:50:19,640
Okay.

1854
02:50:21,600 --> 02:50:23,400
And so now I've built a little app

1855
02:50:23,400 --> 02:50:25,900
that lets you decide what cluster you want,

1856
02:50:25,900 --> 02:50:29,000
hit next, spins the cluster up, shows you your plot,

1857
02:50:29,000 --> 02:50:30,900
and then hit next and shut down your cluster.

1858
02:50:30,900 --> 02:50:34,200
And all of that can be done directly in Python,

1859
02:50:34,200 --> 02:50:36,400
in a Jupyter notebook or in a Python file,

1860
02:50:36,400 --> 02:50:38,700
without having to deal with what your company's

1861
02:50:38,700 --> 02:50:39,700
web development team.

1862
02:50:41,500 --> 02:50:43,200
Yep, so now I have my little thing.

1863
02:50:43,200 --> 02:50:45,900
I also embedded the notebook, the thing in here,

1864
02:50:45,900 --> 02:50:48,300
so if I change something like this,

1865
02:50:48,300 --> 02:50:50,300
you'll see it start to compute.

1866
02:50:52,600 --> 02:50:56,200
So this should have also come up on this,

1867
02:50:56,200 --> 02:51:01,200
but I'm just got the URL slightly messed up.

1868
02:51:03,500 --> 02:51:05,500
Oh, here it is, okay, yeah.

1869
02:51:05,500 --> 02:51:08,300
So now this is running on its own,

1870
02:51:08,300 --> 02:51:09,800
and I'm gonna click launch cluster.

1871
02:51:09,800 --> 02:51:12,000
It found an existing cluster.

1872
02:51:12,000 --> 02:51:13,000
I hit next.

1873
02:51:19,000 --> 02:51:21,400
And same thing I showed in the individual thing.

1874
02:51:21,400 --> 02:51:23,700
So this now is a little mini app

1875
02:51:23,700 --> 02:51:25,500
that's on the entire data set

1876
02:51:25,500 --> 02:51:28,000
that you can share with people.

1877
02:51:28,000 --> 02:51:29,600
And you can launch it locally,

1878
02:51:29,600 --> 02:51:32,300
or if you use a tool like CDS dashboards,

1879
02:51:32,300 --> 02:51:35,200
you can also share it via your Jupyter Hub.

1880
02:51:39,600 --> 02:51:42,100
Yeah, it is panel serve.

1881
02:51:43,400 --> 02:51:45,700
Yeah, this is, let me make it bigger.

1882
02:51:49,300 --> 02:51:52,100
So that particular application I made last night at midnight,

1883
02:51:52,100 --> 02:51:53,500
so I don't have the instructions fully there.

1884
02:51:53,500 --> 02:51:54,500
I'll improve them.

1885
02:51:54,500 --> 02:51:58,100
But basically, panel serve your notebook,

1886
02:51:58,100 --> 02:52:01,400
and since we're online, you have to say allow websocket

1887
02:52:01,400 --> 02:52:03,700
with our website name, so otherwise,

1888
02:52:03,700 --> 02:52:08,500
for security reasons, it won't let it do that.

1889
02:52:10,500 --> 02:52:13,900
So let's see, what was I gonna say?

1890
02:52:13,900 --> 02:52:17,100
Yeah, so the point with that is anything you make

1891
02:52:17,100 --> 02:52:21,300
with HVPlot or panel, if you add .servable,

1892
02:52:21,300 --> 02:52:26,300
then if you run that code with panel serve,

1893
02:52:26,300 --> 02:52:28,900
it will make it into a website.

1894
02:52:28,900 --> 02:52:31,600
So if you write this in a Python file or anything,

1895
02:52:31,600 --> 02:52:35,400
you put .servable next to the thing you want to display,

1896
02:52:35,400 --> 02:52:37,100
then you can use panel serve

1897
02:52:37,100 --> 02:52:39,400
to make it a little web application.

1898
02:52:39,400 --> 02:52:41,800
No, it's just one for the whole thing.

1899
02:52:41,800 --> 02:52:42,800
Yeah.

1900
02:52:44,800 --> 02:52:46,800
Yep, yep.

1901
02:52:48,800 --> 02:52:52,800
Again, this is, okay, so I have a blog post

1902
02:52:52,800 --> 02:52:54,800
about different ways to do this.

1903
02:52:55,800 --> 02:52:57,400
Well, so there are some ways to do it,

1904
02:52:57,400 --> 02:52:59,800
because actually, we're not doing it in a single day.

1905
02:52:59,800 --> 02:53:01,400
We're doing it in a single day.

1906
02:53:01,400 --> 02:53:02,800
So we're doing it in a single day.

1907
02:53:02,800 --> 02:53:04,200
So we're doing it in a single day.

1908
02:53:04,200 --> 02:53:05,800
So we're doing it in a single day.

1909
02:53:05,800 --> 02:53:07,400
So we're doing it in a single day.

1910
02:53:07,400 --> 02:53:08,800
Well, so there are some ways to do it,

1911
02:53:08,800 --> 02:53:10,200
because actually, I have a colleague

1912
02:53:10,200 --> 02:53:12,800
who has connected Bokeh to Dask,

1913
02:53:12,800 --> 02:53:15,300
so you can spin off threads and stuff like that.

1914
02:53:15,300 --> 02:53:20,500
So this kind of, this .servable panel.serve

1915
02:53:20,500 --> 02:53:25,000
is more like I want to share this visualization

1916
02:53:25,000 --> 02:53:27,500
with 10, 15 people kind of thing.

1917
02:53:27,500 --> 02:53:30,000
Or with Dask, probably, you know,

1918
02:53:30,000 --> 02:53:31,400
you were springing up Dask clusters.

1919
02:53:31,400 --> 02:53:33,300
If you actually, this is not what you want to use

1920
02:53:33,300 --> 02:53:35,800
when you're serving up your website on,

1921
02:53:35,800 --> 02:53:40,800
like if you're doing a dashboard that 1,000 people need.

1922
02:53:40,900 --> 02:53:42,600
So the use case is very different.

1923
02:53:42,600 --> 02:53:44,800
Like one of the reasons why, you know,

1924
02:53:44,800 --> 02:53:48,100
one of my previous federal research lab,

1925
02:53:48,100 --> 02:53:50,600
we funded some of this work with panel is

1926
02:53:50,600 --> 02:53:53,500
most large organizations have this disconnect

1927
02:53:53,500 --> 02:53:56,600
between the scientists, engineers, and folks,

1928
02:53:56,600 --> 02:53:58,200
and the web team.

1929
02:53:58,200 --> 02:54:01,900
And usually what happens is

1930
02:54:01,900 --> 02:54:03,500
you need to share something with colleagues,

1931
02:54:03,500 --> 02:54:05,100
and they're like, oh, you need to talk to the web team.

1932
02:54:05,100 --> 02:54:06,300
Oh, they're busy for six months.

1933
02:54:06,300 --> 02:54:07,600
Oh, yeah, now they're gonna rewrite

1934
02:54:07,600 --> 02:54:10,200
all your data science code in JavaScript.

1935
02:54:10,200 --> 02:54:12,200
And like a year later, you get back a dashboard,

1936
02:54:12,200 --> 02:54:15,800
and you're like, wait, that's not what I needed.

1937
02:54:15,800 --> 02:54:17,900
So this is about rapid prototyping

1938
02:54:17,900 --> 02:54:20,200
and sharing something quickly with folks.

1939
02:54:20,200 --> 02:54:23,200
Once you've worked out exactly what this needs to look like,

1940
02:54:23,200 --> 02:54:24,700
go ahead and give it to your web team and say,

1941
02:54:24,700 --> 02:54:26,700
hey, knock this out the park

1942
02:54:26,700 --> 02:54:30,700
and make me something with D3.js and everything for this.

1943
02:54:30,700 --> 02:54:33,300
But initial prototyping and exploration,

1944
02:54:33,300 --> 02:54:35,700
this is great.

1945
02:54:35,700 --> 02:54:38,500
I think the Streamlit and Dash website

1946
02:54:38,500 --> 02:54:40,500
can do a lot of things.

1947
02:54:40,500 --> 02:54:42,500
Yeah, you can.

1948
02:54:42,500 --> 02:54:44,300
Yeah, and I have a blog post.

1949
02:54:44,300 --> 02:54:45,400
If you go to the QuantSite website,

1950
02:54:45,400 --> 02:54:46,800
you can see a blog post I have about this.

1951
02:54:46,800 --> 02:54:50,000
But Streamlit and Dash are very good

1952
02:54:50,000 --> 02:54:52,900
for linear single dashboards.

1953
02:54:52,900 --> 02:54:54,100
When you have lots of things connected,

1954
02:54:54,100 --> 02:54:55,900
so when you have multi-page apps,

1955
02:54:55,900 --> 02:54:57,700
when you're building more of an application

1956
02:54:57,700 --> 02:55:00,200
versus a dashboard, panel wins.

1957
02:55:00,200 --> 02:55:02,200
When you're building more of just a single dashboard

1958
02:55:02,200 --> 02:55:04,400
on a single page,

1959
02:55:04,400 --> 02:55:05,700
Dash is probably the most scalable

1960
02:55:05,700 --> 02:55:06,700
for large numbers of people,

1961
02:55:06,700 --> 02:55:08,600
but you have to know JavaScript and HTML

1962
02:55:08,600 --> 02:55:10,500
and do callbacks and all that kind of stuff.

1963
02:55:10,500 --> 02:55:13,200
So there are pros and cons.

1964
02:55:13,200 --> 02:55:17,800
I will go to one last thing quickly,

1965
02:55:17,800 --> 02:55:22,400
just so as a concluding thing.

1966
02:55:26,100 --> 02:55:28,200
I don't know, I think it's in this one.

1967
02:55:30,800 --> 02:55:32,000
Okay, data science platforms.

1968
02:55:32,000 --> 02:55:33,500
I just want to leave you with this.

1969
02:55:33,500 --> 02:55:35,800
So the interface you've been using right now

1970
02:55:35,800 --> 02:55:37,900
is basically JupyterHub.

1971
02:55:37,900 --> 02:55:42,500
And with JupyterHub, you can install it yourself.

1972
02:55:42,500 --> 02:55:43,700
There's instructions online.

1973
02:55:43,700 --> 02:55:45,000
There's lots of different ways to install it.

1974
02:55:45,000 --> 02:55:47,900
It can work on everything from HPC to Kubernetes

1975
02:55:47,900 --> 02:55:50,700
to your local computer to a single server.

1976
02:55:50,700 --> 02:55:53,500
Dash can also be launched lots of different ways.

1977
02:55:53,500 --> 02:55:54,800
There's also Terraform scripts.

1978
02:55:54,800 --> 02:55:59,200
There's a GitHub repo called DaskHub.

1979
02:55:59,200 --> 02:56:00,400
So there's several different ways

1980
02:56:00,400 --> 02:56:04,500
to roll your own version of everything we've shown here.

1981
02:56:04,500 --> 02:56:07,800
We also have Nibari, which is the platform you use today.

1982
02:56:07,800 --> 02:56:12,300
And if you're interested, there's some things here.

1983
02:56:12,300 --> 02:56:16,100
It is a set of scripts to install everything for you

1984
02:56:16,100 --> 02:56:18,200
and to make it simple to manage.

1985
02:56:18,200 --> 02:56:19,600
It's also a reference architecture.

1986
02:56:19,600 --> 02:56:22,600
If you want to work out how to install JupyterHub,

1987
02:56:22,600 --> 02:56:26,400
Dask Gateway, Argo, look at our Terraform scripts,

1988
02:56:26,400 --> 02:56:29,000
customize them the way you want.

1989
02:56:29,100 --> 02:56:32,700
There's also some other companies like 2i2c

1990
02:56:32,700 --> 02:56:35,200
does something very similar,

1991
02:56:35,200 --> 02:56:37,000
but they're mainly focused on the educational space.

1992
02:56:37,000 --> 02:56:38,000
They work with universities,

1993
02:56:38,000 --> 02:56:41,500
get Dask and Jupyter installed for them.

1994
02:56:41,500 --> 02:56:45,800
And then Coiled is a company that very specifically

1995
02:56:45,800 --> 02:56:47,600
focused on launching Dask clusters.

1996
02:56:47,600 --> 02:56:50,300
Like we're using Dask Gateway, which is an open source product,

1997
02:56:50,300 --> 02:56:52,700
and as you see, it can fall down.

1998
02:56:52,700 --> 02:56:56,800
Coiled is designed, they don't do all the Jupyter stuff,

1999
02:56:56,800 --> 02:57:01,800
but they do Dask very well, and it's a commercial product.

2000
02:57:01,800 --> 02:57:05,200
And then Saturn Cloud and others are hosted things

2001
02:57:05,200 --> 02:57:06,600
where you just sign up with a credit card

2002
02:57:06,600 --> 02:57:09,100
and you can click launch Dask and they do it for you.

2003
02:57:09,100 --> 02:57:13,300
So while we did today's stuff on Nibari,

2004
02:57:13,300 --> 02:57:17,900
the Dask parts, deploying it can be complicated,

2005
02:57:17,900 --> 02:57:19,800
but there's lots of options.

2006
02:57:19,800 --> 02:57:23,900
The other tools which we use, like the HP plot and everything,

2007
02:57:23,900 --> 02:57:25,700
they can be installed with Pip or Condra.

2008
02:57:25,700 --> 02:57:28,700
And we install them with Condra because it works better.

2009
02:57:28,700 --> 02:57:33,300
But yeah, so overall, everything I've shown you today

2010
02:57:33,300 --> 02:57:36,000
is stuff you can use in your own organizations

2011
02:57:36,000 --> 02:57:37,500
and you can get set up pretty quickly.

2012
02:57:37,500 --> 02:57:42,500
The biggest difficulty is if you're using Dask in the cloud,

2013
02:57:42,500 --> 02:57:45,900
there are issues with how do you get your local environments

2014
02:57:45,900 --> 02:57:49,600
to the workers and stuff that solutions like Nibari or Coiled

2015
02:57:49,600 --> 02:57:51,200
handle that for you.

2016
02:57:51,200 --> 02:57:54,200
If you do it yourself, the simplest way

2017
02:57:54,200 --> 02:57:57,200
is to use a Docker image, but then you have to make a Docker image

2018
02:57:57,200 --> 02:57:58,300
every time you add a package.

2019
02:57:58,300 --> 02:58:00,800
So there are pros and cons there.

2020
02:58:04,700 --> 02:58:08,100
So we have about eight minutes left.

2021
02:58:08,100 --> 02:58:13,000
I will leave the access open the rest of PyCon

2022
02:58:13,000 --> 02:58:14,500
and maybe a day or two after.

2023
02:58:14,500 --> 02:58:17,600
So you should be able to run through all these notebooks

2024
02:58:17,600 --> 02:58:19,900
on your own and get the full feeling

2025
02:58:19,900 --> 02:58:23,600
since we didn't actually get to several of them.

2026
02:58:23,600 --> 02:58:28,400
And we shouldn't have that Dask Gateway problem.

2027
02:58:28,400 --> 02:58:30,200
Questions?

2028
02:58:30,200 --> 02:58:33,500
Thank you all for coming, by the way.

2029
02:58:33,500 --> 02:58:35,700
This was our first stress test with this many people.

2030
02:58:35,700 --> 02:58:36,700
Go ahead.

2031
02:58:37,200 --> 02:58:40,000
I think after the third question,

2032
02:58:40,000 --> 02:58:41,500
and I'm really here for my stop here,

2033
02:58:41,500 --> 02:58:45,500
but people that I've just been telling you

2034
02:58:45,500 --> 02:58:48,500
and you know I made this group, I want to mention

2035
02:58:48,500 --> 02:58:51,500
that there is going to be also a DMAX.

2036
02:58:51,500 --> 02:58:53,900
And you know, a lot of things that we made

2037
02:58:53,900 --> 02:58:57,500
in this project are not from this, not that.

2038
02:58:57,500 --> 02:58:59,500
So this is not that.

2039
02:58:59,500 --> 02:59:03,500
It's all based on the panel data data.

2040
02:59:03,500 --> 02:59:04,500
Nice.

2041
02:59:23,300 --> 02:59:24,300
Thank you.

2042
02:59:26,600 --> 02:59:27,600
Other questions?

2043
02:59:27,600 --> 02:59:28,600
Okay, go ahead.

2044
02:59:33,500 --> 02:59:41,500
No, we've been doing all the computations next to the data.

2045
02:59:41,500 --> 02:59:44,700
I skipped all this because this is where we have the problems.

2046
02:59:44,700 --> 02:59:46,300
I like to say data has gravity.

2047
02:59:46,300 --> 02:59:48,600
There's a reason why Nabari works on all the clouds

2048
02:59:48,600 --> 02:59:50,100
because we're a consulting company.

2049
02:59:50,100 --> 02:59:52,600
And when we go to a company, we can't tell them,

2050
02:59:52,600 --> 02:59:55,500
hey, take your 100 terabytes of data and move it to GCP.

2051
02:59:55,500 --> 02:59:57,400
It's like, no, your data's on AWS.

2052
02:59:57,400 --> 02:59:59,900
We have to run on AWS.

2053
02:59:59,900 --> 03:00:02,900
Today we worked with 70 gigabytes of data.

2054
03:00:03,400 --> 03:00:05,900
I've worked with five terabytes of data before.

2055
03:00:05,900 --> 03:00:09,300
If you are trying to download the five terabytes of data locally,

2056
03:00:09,300 --> 03:00:11,400
first, it's going to take forever.

2057
03:00:11,400 --> 03:00:17,500
Second, it's going to cost a lot because all the clouds

2058
03:00:17,500 --> 03:00:20,600
make it cheap to put data into the cloud,

2059
03:00:20,600 --> 03:00:24,000
but make it very, very expensive to take data out.

2060
03:00:24,000 --> 03:00:26,600
So even if, like right now, this platform,

2061
03:00:26,600 --> 03:00:29,700
this version of Nabari is running on GCP,

2062
03:00:29,700 --> 03:00:32,800
the data is also stored on GCS.

2063
03:00:32,900 --> 03:00:37,400
If the data was stored on AWS and we used it from GCP,

2064
03:00:37,400 --> 03:00:39,500
the performance would still be fine.

2065
03:00:39,500 --> 03:00:43,200
We would pay a lot of money because every transfer out of AWS,

2066
03:00:43,200 --> 03:00:45,400
they charge you a crazy amount.

2067
03:00:45,400 --> 03:00:48,400
Oh, they have what's called egress fees,

2068
03:00:48,400 --> 03:00:50,500
and egress fees are massive.

2069
03:00:52,200 --> 03:00:56,400
So, yeah, that's why data has gravity.

2070
03:00:56,400 --> 03:01:02,000
The old paradigm when Spark and Hive and all these first came out,

2071
03:01:02,000 --> 03:01:05,300
the idea was you move the data to your compute.

2072
03:01:05,300 --> 03:01:06,900
Now you move your compute to the data

2073
03:01:06,900 --> 03:01:08,400
because you do not want to move the data.

2074
03:01:08,400 --> 03:01:10,300
It's too expensive. It's too big.

2075
03:01:11,500 --> 03:01:16,700
I think I have a lot of colleagues or former people.

2076
03:01:16,700 --> 03:01:20,400
I'm from originally OceanMed geospatial type thing,

2077
03:01:20,400 --> 03:01:22,200
and a question came up once.

2078
03:01:22,200 --> 03:01:26,600
They had free credits on AWS and then they had free credits on GCP,

2079
03:01:26,600 --> 03:01:28,500
and it was like, well, we have this much data.

2080
03:01:28,500 --> 03:01:30,100
How much would it cost?

2081
03:01:30,600 --> 03:01:34,300
It was going to cost like 200,000 to move it from AWS to GCP

2082
03:01:34,300 --> 03:01:36,400
and would probably take four or five weeks.

2083
03:01:38,900 --> 03:01:43,600
The quickest way would be to AWS and GCP have this option

2084
03:01:43,600 --> 03:01:45,500
where they'll ship you hard drives.

2085
03:01:46,500 --> 03:01:49,900
Yeah, and then getting the hard drive from AWS

2086
03:01:49,900 --> 03:01:53,100
and then shipping it to GCP was going to be really the only way to do it.

2087
03:01:54,900 --> 03:01:56,300
So, that's a great question.

2088
03:02:00,100 --> 03:02:03,300
So, all this like, you talked about this thing on the right.

2089
03:02:03,300 --> 03:02:04,600
Could you talk about that?

2090
03:02:04,600 --> 03:02:06,800
Yeah, yeah, this is the Dask lab extension.

2091
03:02:08,200 --> 03:02:14,200
Yeah, so all the plots are part of Dask, the Dask thing,

2092
03:02:14,200 --> 03:02:17,600
and then lab extension lets you pull those plots into Jupyter.

2093
03:02:18,400 --> 03:02:19,700
And there's another set of plots.

2094
03:02:19,700 --> 03:02:21,500
Like, if you were using GPUs,

2095
03:02:21,500 --> 03:02:23,800
this would actually show you the GPU usage as well.

2096
03:02:24,700 --> 03:02:26,400
And actually, that's one of the things I like about Dask

2097
03:02:26,400 --> 03:02:28,500
because seeing the compute run

2098
03:02:28,500 --> 03:02:31,000
and seeing you can use that dashboard

2099
03:02:31,000 --> 03:02:32,700
to understand where your data is breaking.

2100
03:02:32,700 --> 03:02:35,800
Like, if you see a lot of red, that's a lot of memory transfer.

2101
03:02:35,800 --> 03:02:37,800
And so, it's very, and it also gives you,

2102
03:02:38,500 --> 03:02:41,100
when it auto scales up and you suddenly see like all these,

2103
03:02:41,100 --> 03:02:43,100
you feel good about it.

2104
03:02:43,100 --> 03:02:45,900
Like, you can feel the computer working for you, you know.

2105
03:02:49,900 --> 03:02:51,700
Oh, it's the same.

2106
03:02:51,700 --> 03:02:54,400
Like, you just choose GPUs and launch GPUs.

2107
03:02:54,400 --> 03:02:55,700
You can also use it with CUDA.

2108
03:02:55,700 --> 03:02:57,300
Like, there's a Dask CUDA thing,

2109
03:02:58,300 --> 03:03:02,500
which, so you can have a cluster of GPUs

2110
03:03:02,500 --> 03:03:05,300
and run parallel compute like XGBoost.

2111
03:03:05,300 --> 03:03:07,900
There's a Dask XGBoost, so you can,

2112
03:03:07,900 --> 03:03:11,600
I would say that most people use Dask

2113
03:03:11,600 --> 03:03:14,400
more on the data engineering ETL side.

2114
03:03:14,400 --> 03:03:17,000
There's another tool called Ray from AnyScale.

2115
03:03:17,900 --> 03:03:22,100
And it has some architectural differences with Dask,

2116
03:03:22,100 --> 03:03:23,800
but, you know, that kind of similar

2117
03:03:23,800 --> 03:03:26,600
distributed compute frameworks in Python and things.

2118
03:03:26,600 --> 03:03:31,600
Ray has better integration with machine learning libraries.

2119
03:03:32,300 --> 03:03:35,000
And especially reinforcement learning and stuff like that.

2120
03:03:36,100 --> 03:03:39,300
Ray's integrations to those machine learning libraries

2121
03:03:39,300 --> 03:03:41,100
is a bit better than Dask's, I would say.

2122
03:03:45,000 --> 03:03:46,800
And I'm around the rest of the conference

2123
03:03:46,800 --> 03:03:49,000
if anyone wants to chat about Dask or HVPlot

2124
03:03:49,000 --> 03:03:50,900
or Nabari or anything else.

2125
03:03:53,900 --> 03:03:54,900
Any last questions?

2126
03:03:58,600 --> 03:03:59,440
Pardon?

2127
03:04:04,400 --> 03:04:08,000
No, well, not exactly.

2128
03:04:08,000 --> 03:04:11,700
You'd have to change where you're writing the data to.

2129
03:04:11,700 --> 03:04:13,900
Because right now that notebook has some,

2130
03:04:13,900 --> 03:04:18,400
so the credentials, none of you guys used storage credentials

2131
03:04:18,400 --> 03:04:20,700
because we made it public read.

2132
03:04:20,700 --> 03:04:25,500
But that notebook also writes the data to our S3 bucket,

2133
03:04:25,500 --> 03:04:27,200
and you don't have credentials for those.

2134
03:04:27,200 --> 03:04:29,400
But if you change where it's writing to,

2135
03:04:30,700 --> 03:04:31,800
actually don't, because if you try

2136
03:04:31,800 --> 03:04:34,100
and write all that data locally, you'll, yeah.

2137
03:04:34,100 --> 03:04:36,400
As long as you don't write the data, you can do that.

2138
03:04:36,400 --> 03:04:40,300
Or if you provide your own credentials for your own bucket.

2139
03:04:47,300 --> 03:04:49,700
Yeah, there's quite a few, actually.

2140
03:04:51,700 --> 03:04:54,400
There's CDS dashboards, that's the sharing thing.

2141
03:04:56,800 --> 03:04:59,300
So there's the Git extension, the Dask extension,

2142
03:04:59,300 --> 03:05:00,700
and we dashboards.

2143
03:05:00,700 --> 03:05:03,000
We did enable, there's another one called execute time,

2144
03:05:03,000 --> 03:05:05,400
which shows you how long each cell took to run.

2145
03:05:05,400 --> 03:05:07,900
We disabled that because it was being a bit finicky.

2146
03:05:10,800 --> 03:05:14,500
And then with Nabari, we also have integrated,

2147
03:05:16,400 --> 03:05:18,000
so some of the things we've done is,

2148
03:05:18,000 --> 03:05:20,300
so Argo workflows is something like Prefect

2149
03:05:20,800 --> 03:05:24,300
or Airflow, it's like a workflow management tool.

2150
03:05:24,300 --> 03:05:25,700
We have environment managing.

2151
03:05:25,700 --> 03:05:29,100
There's also monitoring, which basically will just show you

2152
03:05:29,100 --> 03:05:31,500
what's happening on the backend with the graphs,

2153
03:05:31,500 --> 03:05:33,800
like as things spin up and spin down.

2154
03:05:35,900 --> 03:05:40,400
So this is, Nabari is more like a opinionated Jupiter hub

2155
03:05:40,400 --> 03:05:42,800
with integrations already done,

2156
03:05:43,700 --> 03:05:45,500
if you want to think of it that way.

2157
03:05:51,300 --> 03:05:52,900
Okay, last questions.

2158
03:05:55,900 --> 03:05:57,700
Okay, and also actually, if you do go,

2159
03:05:57,700 --> 03:06:00,900
one last thing is, on number 10,

2160
03:06:00,900 --> 03:06:03,000
there is a feedback form,

2161
03:06:03,000 --> 03:06:05,000
if you'd like to give some feedback on stuff,

2162
03:06:05,000 --> 03:06:08,000
and I know one of the feedbacks can be Dask Gateway,

2163
03:06:08,000 --> 03:06:09,100
it was annoying.

2164
03:06:09,100 --> 03:06:13,700
So, yeah, please do that.

2165
03:06:13,700 --> 03:06:15,700
This is the first time we've tried running,

2166
03:06:15,700 --> 03:06:19,300
we use these tools daily.

2167
03:06:20,500 --> 03:06:22,100
And we spin up to very large clusters,

2168
03:06:22,100 --> 03:06:23,800
and we do really big data sets.

2169
03:06:23,800 --> 03:06:28,200
I think probably we don't have more than 10 or 15 people

2170
03:06:28,200 --> 03:06:29,400
hitting it at the same time.

2171
03:06:29,400 --> 03:06:31,600
I think this is the first time we've had this many people

2172
03:06:31,600 --> 03:06:32,900
hit the Gateway node,

2173
03:06:32,900 --> 03:06:35,700
and so that's something we're gonna have to deal with

2174
03:06:35,700 --> 03:06:37,100
in an upcoming release.

2175
03:06:39,300 --> 03:06:40,300
Thank you.

2176
03:06:40,300 --> 03:06:41,300
Thank you.

