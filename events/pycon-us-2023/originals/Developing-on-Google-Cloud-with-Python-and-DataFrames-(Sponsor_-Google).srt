1
00:00:00,000 --> 00:00:04,160
Let's go ahead and get started as folks are coming in here.

2
00:00:04,160 --> 00:00:06,640
So thanks, everyone, for allowing me to present today.

3
00:00:06,640 --> 00:00:12,880
And a big thanks to the Python Software Foundation for everything that they do and everything

4
00:00:12,880 --> 00:00:19,600
that we can have the privilege of working with them on here at Google.

5
00:00:19,600 --> 00:00:21,280
My name is Jason Davenport.

6
00:00:21,280 --> 00:00:25,520
For those of you who I have not met, looking forward to some exciting Q&A here in a little

7
00:00:25,520 --> 00:00:26,520
bit.

8
00:00:26,520 --> 00:00:29,000
But just a little bit about me as we start to get in.

9
00:00:29,000 --> 00:00:32,680
I've been at Google Cloud for about two and a half years.

10
00:00:32,680 --> 00:00:37,000
I am currently the head of developer advocacy for our data analytics products.

11
00:00:37,000 --> 00:00:44,880
So that includes everything from Cloud Composer, which is a large Python project, to BigQuery,

12
00:00:44,880 --> 00:00:49,680
which is our serverless data warehouse that we'll get to talking about here in a couple

13
00:00:49,680 --> 00:00:51,120
minutes.

14
00:00:51,120 --> 00:00:56,600
And prior to that, I was a practice director for a global implementer focusing on software

15
00:00:56,640 --> 00:01:02,200
engineering and cloud data engineering and warehousing for that.

16
00:01:02,200 --> 00:01:08,360
In my free time, when I'm not chasing my children, I like to do things in the DevOps space.

17
00:01:08,360 --> 00:01:13,240
So as folks here are thinking about how do you do more with your data across environments,

18
00:01:13,240 --> 00:01:17,880
I would love to hear how you're using DevOps and DataOps in your processes when you have

19
00:01:17,880 --> 00:01:18,880
the chance.

20
00:01:18,880 --> 00:01:23,760
So what are we going to talk about today?

21
00:01:23,920 --> 00:01:28,080
So this thing will cover some of the innovations that we've launched over the past what I'll

22
00:01:28,080 --> 00:01:34,160
call six to 12 months focusing on how we bring more language interoperability to our data

23
00:01:34,160 --> 00:01:36,240
analytics stack at Google.

24
00:01:36,240 --> 00:01:41,560
We'll talk through some things with data frames, some different components of serverless, and

25
00:01:41,560 --> 00:01:45,720
then I will do a couple different demos of each of those things just to make it real

26
00:01:45,720 --> 00:01:47,400
for those in the room.

27
00:01:47,400 --> 00:01:51,280
And then last, open up for Q&A once it's all over.

28
00:01:51,400 --> 00:01:56,360
I'm planning just for those aware about a half hour of content demos and then questions

29
00:01:56,360 --> 00:01:59,920
from the room we can start fielding as we get into it.

30
00:01:59,920 --> 00:02:01,760
All right.

31
00:02:01,760 --> 00:02:04,400
How many folks here use Google today?

32
00:02:04,400 --> 00:02:08,120
It's about half the room.

33
00:02:08,120 --> 00:02:09,120
Always great.

34
00:02:09,120 --> 00:02:11,960
I would love to see everyone here using Google at some point.

35
00:02:11,960 --> 00:02:14,680
No shameful promotion in that.

36
00:02:14,680 --> 00:02:16,880
A couple things just to call it.

37
00:02:16,880 --> 00:02:24,160
So I think folks here are aware Google is really a company that has championed contributions

38
00:02:24,160 --> 00:02:28,840
to the open source community and then we do a lot of things with those that then become

39
00:02:28,840 --> 00:02:33,760
our products that we actually share with you in Google Cloud and across some of our different

40
00:02:33,760 --> 00:02:36,640
things like our ad stack.

41
00:02:36,640 --> 00:02:41,560
Folks may remember we actually wrote a paper about MapReduce technology.

42
00:02:41,560 --> 00:02:45,600
This is probably back in 2006 or 2007.

43
00:02:45,600 --> 00:02:50,000
This became what was then known as Apache Dremel which is actually the backbone of

44
00:02:50,000 --> 00:02:55,960
BigQuery which is our serverless elastic data warehouse, whichever word you choose to put

45
00:02:55,960 --> 00:02:58,360
in front of that.

46
00:02:58,360 --> 00:03:01,720
That notwithstanding, there's actually a number of other things that we have worked with the

47
00:03:01,720 --> 00:03:06,440
community on and then open source and continue to contribute on.

48
00:03:06,440 --> 00:03:08,480
Big table for those who may not be aware.

49
00:03:08,480 --> 00:03:13,240
Apache HBase is obviously the open source derivative of that.

50
00:03:13,240 --> 00:03:15,560
Who here doesn't know about Kubernetes?

51
00:03:15,600 --> 00:03:20,440
That's something where we donated essentially our code to the CNCF community.

52
00:03:20,440 --> 00:03:24,280
And then we've done a number of things along the way for things like data flow in the Apache

53
00:03:24,280 --> 00:03:30,040
Beam community and then obviously TensorFlow and the learnings that have come from that.

54
00:03:30,040 --> 00:03:36,160
So as we think through how do we continue to bring innovation to market with that, a

55
00:03:36,160 --> 00:03:41,000
lot of it is driven from the community like yourself helping us to understand what are

56
00:03:41,000 --> 00:03:45,160
the things that we can do with our products to make them better and then making sure that

57
00:03:45,160 --> 00:03:50,600
we enable access for everyone in order to actually have that technology available.

58
00:03:53,000 --> 00:03:57,960
All right, so let me do a quick overview of BigQuery because that's where we'll largely

59
00:03:57,960 --> 00:04:00,720
be spending our time today once we get into it.

60
00:04:00,720 --> 00:04:05,240
How many people here have heard of BigQuery and or use BigQuery?

61
00:04:07,240 --> 00:04:10,080
All right, so about a third of the room.

62
00:04:10,080 --> 00:04:12,200
How many folks here work with databases?

63
00:04:13,160 --> 00:04:17,160
All right, I should have started with that question.

64
00:04:17,160 --> 00:04:23,880
BigQuery is a large serverless database at its core and it's right off the notion that

65
00:04:23,880 --> 00:04:29,880
we fully separate our storage and our compute from the layers of the database itself.

66
00:04:29,880 --> 00:04:32,120
So what does that actually mean in implementation?

67
00:04:32,120 --> 00:04:38,320
It means first off, you never pay for BigQuery for the compute unless you're actually using it.

68
00:04:38,320 --> 00:04:42,440
So the only thing that you're ever actually being persistently charged for is the storage

69
00:04:42,440 --> 00:04:47,680
and that is something that we do behind the scenes using a file format called Capacitor,

70
00:04:47,680 --> 00:04:50,720
which is a columnar-based format for data.

71
00:04:50,720 --> 00:04:54,360
What does that mean using a columnar-based format versus a row-based format?

72
00:04:54,360 --> 00:04:58,400
Well, columnar is a lot faster for analytic workloads, so think of it as like getting

73
00:04:58,400 --> 00:05:02,000
the total of all of my sales over a range of time.

74
00:05:02,000 --> 00:05:03,660
Columnar is really great for that.

75
00:05:03,660 --> 00:05:05,800
Other databases use row-based.

76
00:05:05,800 --> 00:05:11,320
If you think of things like MySQL or Postgres, Postgres has a couple columnar things, but

77
00:05:11,320 --> 00:05:16,480
those are really focused on how do I pick a record, get a record, so what is Jason Davenport's

78
00:05:16,480 --> 00:05:20,360
user value that we may return in an application for that.

79
00:05:20,360 --> 00:05:24,520
These are all relational databases, not to be confused with NoSQL-based.

80
00:05:24,520 --> 00:05:29,480
We do have some ways that we can integrate with those, but we'll save that for another day.

81
00:05:29,480 --> 00:05:32,400
So with BigQuery, I have my storage, I get all my data in.

82
00:05:32,560 --> 00:05:36,080
What that means then is I can do all these different compute-based things in order to

83
00:05:36,080 --> 00:05:40,920
actually analyze that, make sense of it, and then share it with other downstream either

84
00:05:40,920 --> 00:05:45,760
applications or systems or people in that.

85
00:05:45,760 --> 00:05:50,600
We use essentially Google's backplane here, so it's a petabit network.

86
00:05:50,600 --> 00:05:53,560
You can do things at massive scale here.

87
00:05:53,560 --> 00:05:58,120
We have customers that are running, I think, the largest query.

88
00:05:58,120 --> 00:06:02,960
The single query that someone has executed is upwards of 250 petabytes.

89
00:06:02,960 --> 00:06:05,080
It's very, very large.

90
00:06:05,080 --> 00:06:08,520
What that essentially means is that we can scale up to that, but we can also do things

91
00:06:08,520 --> 00:06:11,040
on megabytes of data or kilobytes of data.

92
00:06:11,040 --> 00:06:14,840
It's really up to you as the developer how you want to bring your data to BigQuery and

93
00:06:14,840 --> 00:06:17,640
then how you want to manage it in there.

94
00:06:17,640 --> 00:06:23,720
The other thing with that is while we do support essentially a standard SQL dialect, we have

95
00:06:23,720 --> 00:06:28,280
a REST API, and then we have all of our great client libraries which we support.

96
00:06:28,280 --> 00:06:31,720
That's what we'll be getting into talking about here in a couple of minutes with our

97
00:06:31,720 --> 00:06:35,320
Python support and extensions that we've been launching in it.

98
00:06:35,320 --> 00:06:41,560
All right.

99
00:06:41,560 --> 00:06:46,160
One of the things that we typically see, and this is what I'll call the history of Google

100
00:06:46,160 --> 00:06:51,480
in the process, Google Cloud really started out as a way where you could use an API to

101
00:06:51,560 --> 00:06:55,840
do just about anything against a database or another platform or technology that we

102
00:06:55,840 --> 00:06:56,840
had.

103
00:06:56,840 --> 00:07:02,920
Write a REST API request, execute a Git, get data back.

104
00:07:02,920 --> 00:07:06,920
That was great if you were a software developer and you already understood things like how

105
00:07:06,920 --> 00:07:12,200
do I interact with a REST API, how do I catch errors, all those different things with it.

106
00:07:12,200 --> 00:07:16,520
What we realized is that how to start really democratizing access to data required us to

107
00:07:16,520 --> 00:07:19,960
also add things like SQL.

108
00:07:20,120 --> 00:07:25,000
We had a legacy version that essentially was a homegrown one we built.

109
00:07:25,000 --> 00:07:31,160
We decided let's implement a standard SQL version based on ANSI, so it's ANSI compliant.

110
00:07:31,160 --> 00:07:35,400
Then with that, we started really democratizing even more access to data because folks could

111
00:07:35,400 --> 00:07:41,160
do things like write simple create table statements, write selects, export that data, and then

112
00:07:41,160 --> 00:07:44,640
share all the goodness with that.

113
00:07:44,640 --> 00:07:49,940
If you think about the community and how things have changed, at the same time we were doing

114
00:07:49,940 --> 00:07:54,900
all this work, the other thing that started happening was that the Spark community started

115
00:07:54,900 --> 00:07:58,420
really getting active and we started seeing a lot of growth and momentum, in particular

116
00:07:58,420 --> 00:08:02,220
with things like data frames.

117
00:08:02,220 --> 00:08:07,940
For those who are following at home in terms of the very quick evolution of Spark in 30

118
00:08:07,940 --> 00:08:11,500
seconds or less, Spark started off with a concept called RDDs.

119
00:08:11,500 --> 00:08:12,500
They were super fast.

120
00:08:12,500 --> 00:08:14,500
They were very difficult to build.

121
00:08:14,500 --> 00:08:19,100
They moved to another thing called data sets, which were slightly easier to build but still

122
00:08:19,300 --> 00:08:21,300
somewhat difficult to work with.

123
00:08:21,300 --> 00:08:25,620
Then they actually ended up on data frames, which is essentially a scalable way where

124
00:08:25,620 --> 00:08:31,140
you can use programmatic languages to express what you want to do with data and data formats

125
00:08:31,140 --> 00:08:32,940
and that.

126
00:08:32,940 --> 00:08:35,260
Let's talk about what's great about data frames.

127
00:08:35,260 --> 00:08:39,340
First thing with data frames is they are very easy to test compared to SQL.

128
00:08:39,340 --> 00:08:44,140
If you've ever written a piece of SQL before, you know that, well, one, the code tends to

129
00:08:44,140 --> 00:08:48,220
be very wet, so you have to repeat yourself for very similar things that may happen.

130
00:08:48,620 --> 00:08:53,580
It's also very difficult to catch errors and to actually understand at a unit level what's

131
00:08:53,580 --> 00:08:59,180
happening and then how do I validate that that unit makes sense as a part of the system.

132
00:08:59,180 --> 00:09:03,380
Typically when I work with folks on pure SQL pipelines, we're focusing more on integration

133
00:09:03,380 --> 00:09:07,780
tests and quality tests than we are actually being able to get down and say, does this

134
00:09:07,780 --> 00:09:13,620
case statement work in the way that it's intended to for this very small unit of code in that?

135
00:09:13,620 --> 00:09:16,340
Data frames allow us to do a lot more.

136
00:09:16,340 --> 00:09:20,820
First we can express all of our code using the language syntax of our choice.

137
00:09:20,820 --> 00:09:24,060
The other thing with that is that we can actually do a lot more in terms of testing and then

138
00:09:24,060 --> 00:09:28,860
integration with that into our current application stacks.

139
00:09:28,860 --> 00:09:34,940
If we think about how does a data frame enable a web application, we can write our data frame

140
00:09:34,940 --> 00:09:39,460
in Python, it can be stored within our application framework, and then all that we need to do

141
00:09:39,460 --> 00:09:42,980
is have something that actually takes that data frame and then translates it into our

142
00:09:43,980 --> 00:09:46,900
database in order for execution.

143
00:09:46,900 --> 00:09:51,180
The other thing that's great about data frames is they work really great with Jupyter notebooks.

144
00:09:51,180 --> 00:09:55,820
For those here who are not familiar with notebooks, those are essentially interactive kernels

145
00:09:55,820 --> 00:09:59,620
which allow you to do things like data analysis in real time.

146
00:09:59,620 --> 00:10:06,220
I can do things like express different functions, maybe look at how I would calculate different

147
00:10:06,220 --> 00:10:11,180
fields, and all these things, at least until very recently, were things that I could just

148
00:10:11,180 --> 00:10:15,580
run locally and then do within my instance.

149
00:10:15,580 --> 00:10:19,900
With that, really the question is, if you think from our perspective, how do we make

150
00:10:19,900 --> 00:10:24,300
sure that if a user is interacting with a serverless data warehouse, that they can use

151
00:10:24,300 --> 00:10:28,180
the programming language and the programming tools of their choice in order to interact

152
00:10:28,180 --> 00:10:32,900
with it?

153
00:10:32,900 --> 00:10:34,140
Great leading, right?

154
00:10:34,140 --> 00:10:37,100
Here's where we start talking about data frame support.

155
00:10:37,260 --> 00:10:42,180
What this really is for us is how do we enable that programmatic access to BigQuery by using

156
00:10:42,180 --> 00:10:45,980
data frames and making sure that developers can pick the way that they want to interact

157
00:10:45,980 --> 00:10:48,460
with their data in the process.

158
00:10:48,460 --> 00:10:53,940
With that, one of the big things that we've been doing is working with the IBIS project,

159
00:10:53,940 --> 00:11:01,220
which is a large data frame project in the community, to make sure that the BigQuery

160
00:11:01,220 --> 00:11:04,940
support here is the best that it can be, and that we're also extending this even further

161
00:11:05,260 --> 00:11:12,980
You'll have in the future very similar Pandas capabilities, Pandas being a data frame library

162
00:11:12,980 --> 00:11:17,620
on top of IBIS or on top of BigQuery in that.

163
00:11:17,620 --> 00:11:21,660
Let's briefly talk about IBIS and why it's really useful here.

164
00:11:21,660 --> 00:11:26,580
First off, it allows us to express all of our different things as code, and not just

165
00:11:26,580 --> 00:11:32,660
SQL code, but actually data frames like syntax, so we can then test each component.

166
00:11:32,660 --> 00:11:36,900
It's very easy to see if we step through our code, what different things are happening

167
00:11:36,900 --> 00:11:42,820
and how am I mutating a table or a row or a column over time.

168
00:11:42,820 --> 00:11:50,140
Next thing, unlike Pandas, IBIS is what's called a lazy evaluator.

169
00:11:50,140 --> 00:11:53,540
IBIS will not actually execute our code until runtime.

170
00:11:53,540 --> 00:11:58,980
What's great about that is if you think about typically in Pandas how you may be constrained

171
00:11:58,980 --> 00:12:04,820
into a node or a set of nodes if you're maybe using an operator like Dask, what's great

172
00:12:04,820 --> 00:12:09,780
about using IBIS is that we can essentially hold execution until we get the final kind

173
00:12:09,780 --> 00:12:12,660
of query that's going to be executed.

174
00:12:12,660 --> 00:12:16,540
What that means is that we can push down all of that work to BigQuery, make BigQuery do

175
00:12:16,540 --> 00:12:20,860
all of our work and then just get the results set back for IBIS.

176
00:12:20,860 --> 00:12:24,760
If you think of this in a production setting, this is where we really start to harness the

177
00:12:24,760 --> 00:12:28,860
power of cloud compute because we can have something that's fully elastic, scales up

178
00:12:28,860 --> 00:12:32,980
when a user requests something but then shuts down at the end once the query is executed

179
00:12:32,980 --> 00:12:36,140
and I can get that result back.

180
00:12:36,140 --> 00:12:41,860
Last but not least, this is as of last week when I was talking to some of the contributors,

181
00:12:41,860 --> 00:12:45,620
IBIS actually supports 16 different backends.

182
00:12:45,620 --> 00:12:51,420
What's great about this, maybe you have BigQuery, you have Postgres, you can use IBIS to essentially

183
00:12:51,420 --> 00:12:58,140
do the same sorts of expressions across a number of different databases in that process.

184
00:12:58,140 --> 00:13:03,140
So I'll do a quick picture here and then we'll actually show this running here in a couple

185
00:13:03,140 --> 00:13:05,540
of minutes.

186
00:13:05,540 --> 00:13:10,380
But what you can see here, I pulled up just a very quick Python statement and then we'll

187
00:13:10,380 --> 00:13:12,180
run it here in a second.

188
00:13:12,180 --> 00:13:16,420
But what's great about this, there's some stuff up here in terms of the syntax, I'm

189
00:13:16,420 --> 00:13:18,940
doing my authentication, all that.

190
00:13:18,940 --> 00:13:23,800
But then I'm actually just using Python-based operators here, data frame operators to express

191
00:13:23,800 --> 00:13:26,740
how I want my code to actually be executed.

192
00:13:26,740 --> 00:13:32,120
So I'm doing things like casting some of my different fields, extracting different elements

193
00:13:32,120 --> 00:13:35,860
of the field, doing some group buys and aggregates.

194
00:13:35,860 --> 00:13:40,180
Then all of these things essentially I can just execute at the end with a simple execution

195
00:13:40,180 --> 00:13:43,500
command of the database for that.

196
00:13:43,500 --> 00:13:46,800
All right.

197
00:13:46,800 --> 00:13:51,660
So we talked a little bit about how do I start working with data frames but really with that,

198
00:13:52,220 --> 00:13:58,340
If a Python object is kind of a block that we need to execute, the question is, well,

199
00:13:58,340 --> 00:14:02,180
okay, that's great, but how do I actually execute my code on BigQuery?

200
00:14:02,180 --> 00:14:08,020
Because it's Python and you told me that BigQuery is a SQL-based interface for that.

201
00:14:08,020 --> 00:14:12,340
So the next thing that we've launched, and I believe this is in public preview now, is

202
00:14:12,340 --> 00:14:17,260
the ability to actually run PySparks stored procedures from within BigQuery itself.

203
00:14:17,260 --> 00:14:21,620
So what this means is that as I build my data frame, I can now just store that code in BigQuery

204
00:14:21,780 --> 00:14:26,980
from a single pane of glass, and then I can actually run that PySpark code or that data

205
00:14:26,980 --> 00:14:30,300
frame code on an arbitrary set of compute.

206
00:14:30,300 --> 00:14:35,860
And what I mean by that is that we can essentially take our data frame code that we have and

207
00:14:35,860 --> 00:14:41,780
we could either execute it on the BigQuery compute side, so using our forked version

208
00:14:41,780 --> 00:14:45,180
of Dremel, or we could actually run this on Spark.

209
00:14:45,180 --> 00:14:50,940
So I don't have to take my code, I don't have to reevaluate all of it for how BigQuery would

210
00:14:50,980 --> 00:14:54,940
execute, I can just use a serverless Spark instance to run that code.

211
00:14:54,940 --> 00:14:59,900
It'll execute, once it's done it'll spin back down, and then I get the benefits essentially

212
00:14:59,900 --> 00:15:05,060
of being able to use all these great technologies together in the same place.

213
00:15:05,060 --> 00:15:06,380
So what does this look like?

214
00:15:06,380 --> 00:15:09,700
Very similar to what we just looked at for the IBIS code.

215
00:15:09,700 --> 00:15:14,780
I'm just using essentially a decorator block up here where I'm creating a procedure, and

216
00:15:14,780 --> 00:15:18,900
then I'm just writing some Spark context code and I can put in here anything that I want,

217
00:15:18,940 --> 00:15:22,620
different libraries or things that are required.

218
00:15:22,620 --> 00:15:27,540
But what this allows us to do is to essentially run any code that I need against my data to

219
00:15:27,540 --> 00:15:31,740
make sure that I'm getting that analysis and that activation that I was hoping for.

220
00:15:34,540 --> 00:15:39,220
All right, so we've started going through, we've started building out some of the different

221
00:15:39,220 --> 00:15:42,260
ways we can build Python-based compute.

222
00:15:42,260 --> 00:15:47,420
But one of the next questions that a lot of our users get is, I'm afraid of lock-in, why

223
00:15:47,420 --> 00:15:51,420
do I have to put all of my data into the BigQuery-based format?

224
00:15:51,420 --> 00:15:55,500
And then how could I actually get access to that data but make sure it's in an open format

225
00:15:55,500 --> 00:16:00,140
or an open source format where it's something that's a community-driven thing that I can

226
00:16:00,140 --> 00:16:03,380
use and leverage.

227
00:16:03,380 --> 00:16:08,980
So next thing there, what we've been working on is a set of storage-based technologies

228
00:16:08,980 --> 00:16:11,100
which we call Big Lake.

229
00:16:11,100 --> 00:16:15,500
And what Big Lake allows you to do is to interact with things like Apache Iceberg format for

230
00:16:15,540 --> 00:16:16,540
your data.

231
00:16:16,540 --> 00:16:23,100
And for those who are curious, Iceberg is what I'll call kind of a variant of the Parquet

232
00:16:23,100 --> 00:16:24,100
format.

233
00:16:24,100 --> 00:16:27,500
It allows you to actually track things like state within the file.

234
00:16:27,500 --> 00:16:32,980
So as I have a table and it changes over time, Iceberg will essentially keep track of the

235
00:16:32,980 --> 00:16:37,420
metadata of what's changed and then will give me essentially the ability to do things like

236
00:16:37,420 --> 00:16:43,700
updates or inserts or deletes on the Parquet-based format.

237
00:16:43,700 --> 00:16:48,460
So what this really means for our users or for you following along is that you can pick

238
00:16:48,460 --> 00:16:51,020
whatever file format you want to store your data in.

239
00:16:51,020 --> 00:16:55,300
Obviously there will be some benefits for typically speed and performance if you use

240
00:16:55,300 --> 00:16:59,260
the Google format of Capacitor.

241
00:16:59,260 --> 00:17:05,580
But if there are reasons for your organization to bring your own kind of files or work maybe

242
00:17:05,580 --> 00:17:09,660
with Iceberg across a number of different compute engines, we're bringing that and making

243
00:17:09,660 --> 00:17:13,680
that easily accessible to you as the user.

244
00:17:13,680 --> 00:17:18,600
And this is just a quick overview of how the Big Lake API actually works.

245
00:17:18,600 --> 00:17:21,600
And we'll see it here in a demo just with the actual code execution.

246
00:17:21,600 --> 00:17:26,120
But what we essentially allow you to do is to either use our file format or bring your

247
00:17:26,120 --> 00:17:30,760
own, use streaming, and then we provide a unified API on top of all that for that data

248
00:17:30,760 --> 00:17:32,380
access.

249
00:17:32,380 --> 00:17:37,100
So here what the focus is really on is making sure that from your perspective as the end

250
00:17:37,100 --> 00:17:43,600
user, this is simple and it's open and it's easy to understand and execute in the app.

251
00:17:44,520 --> 00:17:48,160
All right.

252
00:17:48,160 --> 00:17:49,400
So we have all these formats.

253
00:17:49,400 --> 00:17:54,920
We've talked about how we start to transform the data, how we use data frames.

254
00:17:54,920 --> 00:17:58,200
The big thing now is, okay, how do I start enriching that data and then how do I actually

255
00:17:58,200 --> 00:18:02,960
get to execute my great Python code that I wrote on my database for that?

256
00:18:02,960 --> 00:18:07,240
And so next thing we'll talk about here is remote stored procedures.

257
00:18:07,240 --> 00:18:12,080
And so what remote procedures allow you to do is to essentially execute any set of code

258
00:18:12,640 --> 00:18:16,840
against BigQuery using something like a cloud function, which is essentially a serverless

259
00:18:16,840 --> 00:18:19,920
function that you can run in Google Cloud.

260
00:18:19,920 --> 00:18:22,600
And I can take whatever data I may have.

261
00:18:22,600 --> 00:18:26,960
In this case, I'm going to select some stuff from a comments table.

262
00:18:26,960 --> 00:18:31,520
And then I can execute my Python script against that data just using a remote function in

263
00:18:31,520 --> 00:18:32,700
that.

264
00:18:32,700 --> 00:18:38,440
So this is really, I think of it as kind of like a Swiss army knife of the ways you can

265
00:18:38,440 --> 00:18:40,440
work with code.

266
00:18:40,520 --> 00:18:42,320
I found this to be very valuable.

267
00:18:42,320 --> 00:18:48,280
In particular, many companies and users aren't thinking about terabytes or petabytes of data.

268
00:18:48,280 --> 00:18:53,080
You have 50,000 records that you need to augment with something else.

269
00:18:53,080 --> 00:18:57,640
Remote functions makes it ridiculously easy to do that because I can just write a function.

270
00:18:57,640 --> 00:18:58,920
I can select my data.

271
00:18:58,920 --> 00:19:01,080
I can add something else to that data.

272
00:19:01,080 --> 00:19:07,840
And then BigQuery will just materialize that as a new table or as the result of the select

273
00:19:07,840 --> 00:19:10,360
statement in that.

274
00:19:10,440 --> 00:19:15,160
So all this makes it really easy to make sure whatever Python you want to go write or to

275
00:19:15,160 --> 00:19:20,200
code that you can then bring your data to that with BigQuery and make it very easy and

276
00:19:20,200 --> 00:19:24,280
seamless from the execution perspective.

277
00:19:24,280 --> 00:19:30,180
And then last but not least, one of the most recent things we've actually been working

278
00:19:30,180 --> 00:19:35,360
on and launching is the ability to access what we call unstructured metadata as a part

279
00:19:35,360 --> 00:19:37,280
of BigQuery.

280
00:19:37,280 --> 00:19:44,560
So if you think about things like object analysis or image analysis, you'll end up with a bucket

281
00:19:44,560 --> 00:19:49,680
which is just kind of a logical grouping of objects where you may need to pass in what

282
00:19:49,680 --> 00:19:53,720
are the locator records for each of those.

283
00:19:53,720 --> 00:19:59,200
What you can now do with object tables is essentially store all that metadata in BigQuery

284
00:19:59,200 --> 00:20:04,220
and then allow all the different compute engines to access the metadata and then the binaries

285
00:20:04,220 --> 00:20:05,220
assist you with that.

286
00:20:05,220 --> 00:20:10,220
So in this case, the images to then perform additional analysis on top of that.

287
00:20:10,220 --> 00:20:14,580
And we'll see a demo here really quickly about how all these come to life by essentially

288
00:20:14,580 --> 00:20:20,180
using remote functions with object tables to be able to use Python to analyze all this

289
00:20:20,180 --> 00:20:24,700
data but keep the access management in BigQuery and make it very easy for end users to get

290
00:20:24,700 --> 00:20:29,060
to the data within it.

291
00:20:29,060 --> 00:20:30,340
Here's a quick thing here.

292
00:20:30,340 --> 00:20:34,180
You can see we're doing some house-based analysis.

293
00:20:35,180 --> 00:20:38,860
We have all those objects but the only thing we've had to do here is actually create an

294
00:20:38,860 --> 00:20:44,860
external table and an external table is just that pointer to all those files.

295
00:20:44,860 --> 00:20:48,860
I think in total it's like ten lines of code or less.

296
00:20:48,860 --> 00:20:52,380
So if you think about all the other processes that you may have to run, we try to make it

297
00:20:52,380 --> 00:20:58,540
really, really simple to do this stuff with BigQuery and then with different Python libraries.

298
00:20:58,540 --> 00:21:01,040
All right.

299
00:21:01,200 --> 00:21:05,480
So we'll go through the graph slides because those are fun enough but code's more fun.

300
00:21:05,480 --> 00:21:07,480
So let's go ahead and we'll walk through.

301
00:21:07,480 --> 00:21:11,000
First thing we'll start with is how some of the data frame stuff comes to life and then

302
00:21:11,000 --> 00:21:15,000
we'll step through each of these examples and build on that in the process.

303
00:21:15,000 --> 00:21:18,000
All right.

304
00:21:18,000 --> 00:21:23,000
Can folks in the back see okay?

305
00:21:23,000 --> 00:21:24,000
Awesome.

306
00:21:24,000 --> 00:21:26,000
All right.

307
00:21:26,000 --> 00:21:33,200
So the first thing that we're going to execute, you can see that I have some code here that

308
00:21:33,200 --> 00:21:34,200
I've created.

309
00:21:34,200 --> 00:21:38,600
I'm just using a notebook from Visual Studio Code but what I've done is I've essentially

310
00:21:38,600 --> 00:21:43,720
taken that IBIS script that we're going to execute and then we're going to actually go

311
00:21:43,720 --> 00:21:47,800
through and start to execute this code against some of the different data points here.

312
00:21:47,800 --> 00:21:54,800
So all that I need to do, we'll run this first one.

313
00:21:56,800 --> 00:22:01,800
My auth did not have to re-happen so that's great.

314
00:22:07,600 --> 00:22:12,600
Sorry, one second, let me pull up.

315
00:22:18,400 --> 00:22:19,400
All right.

316
00:22:19,400 --> 00:22:24,400
So you can see here if I come back, we can see that this query actually executed so I'm

317
00:22:24,600 --> 00:22:30,300
kind of using a dual screen method here just to show how is my code actually being formulated

318
00:22:30,300 --> 00:22:32,000
and run.

319
00:22:32,000 --> 00:22:36,880
What's cool about this is if you think about what we actually expressed using the IBIS

320
00:22:36,880 --> 00:22:41,640
syntax here for the data frame, you can see we've used things like mutate in order to

321
00:22:41,640 --> 00:22:45,000
actually add additional columns to that data.

322
00:22:45,000 --> 00:22:48,720
We've also done some order buys and group buys and aggregates.

323
00:22:48,720 --> 00:22:55,720
What this actually does if I come in and look at the query that was executed, as you can

324
00:22:59,400 --> 00:23:04,480
see that IBIS took all of that data frame logic and then it actually went and wrote

325
00:23:04,480 --> 00:23:07,920
the SQL that needs to be executed against BigQuery.

326
00:23:07,920 --> 00:23:11,320
And what's great about this is that if we think about all those different steps, because

327
00:23:11,320 --> 00:23:16,800
IBIS only evaluated and executed at the very end, it was able to optimize the statement

328
00:23:16,840 --> 00:23:21,280
so that I would actually only get that result and I would only incur the charge of running

329
00:23:21,280 --> 00:23:24,680
that query once against the database.

330
00:23:24,680 --> 00:23:28,480
So from the BigQuery perspective, we can see that we have a couple different records that

331
00:23:28,480 --> 00:23:30,760
were executed.

332
00:23:30,760 --> 00:23:34,200
One thing here just to call out because I was running this last night, BigQuery, we

333
00:23:34,200 --> 00:23:36,320
also store a result cache.

334
00:23:36,320 --> 00:23:41,240
So essentially anything up to I believe 24 hours that your user ID executes.

335
00:23:41,240 --> 00:23:45,480
If it's the exact same code and there's no things like current timestamp in that, we

336
00:23:45,480 --> 00:23:47,320
won't actually charge you again for running it.

337
00:23:47,320 --> 00:23:53,280
We just return that result set to you again because we're already storing it in the process.

338
00:23:53,280 --> 00:23:58,200
So we can see what's great about that is I've taken my code and then what I've been able

339
00:23:58,200 --> 00:24:01,040
to do is actually express it within the syntax.

340
00:24:01,040 --> 00:24:05,640
I could add maybe additional unit tests around this and maybe I would actually apply this

341
00:24:05,640 --> 00:24:11,880
and say, hey, I want to show my end users the sum of their sales over the past 30 days

342
00:24:11,880 --> 00:24:15,460
and just have that return to an ID or web interface.

343
00:24:15,460 --> 00:24:21,220
This easily integrates with my microservices and all those other things behind the scenes.

344
00:24:21,220 --> 00:24:22,820
All right.

345
00:24:22,820 --> 00:24:24,700
Different problem.

346
00:24:24,700 --> 00:24:25,700
Similar vein.

347
00:24:25,700 --> 00:24:30,500
One of the things that's always really difficult is building on top of data because as I work

348
00:24:30,500 --> 00:24:37,100
through these different steps, what we saw earlier was essentially the pre-built code.

349
00:24:37,100 --> 00:24:40,220
As I'm building, one of the things that I'm typically doing is thinking about, well, is

350
00:24:40,220 --> 00:24:44,580
what I did, so the group by that I did, does that actually make sense?

351
00:24:45,540 --> 00:24:50,880
Does the join that I wrote, did I accidentally create a Cartesian product which is essentially

352
00:24:50,880 --> 00:24:53,300
a many to many join?

353
00:24:53,300 --> 00:24:58,060
What IBIS also allows you to do in this case is to use what's called the interactive mode.

354
00:24:58,060 --> 00:25:03,860
An interactive mode based on wherever we actually go on this here will execute in real time

355
00:25:03,860 --> 00:25:06,420
as we're going through each of the steps.

356
00:25:06,420 --> 00:25:13,140
So the benefit here if you think about things like type mismatches are a very common one.

357
00:25:13,140 --> 00:25:17,540
I can essentially iterate in real time and look at these different results.

358
00:25:17,540 --> 00:25:24,940
So here what I could do, I'll just go ahead and start running this block.

359
00:25:24,940 --> 00:25:31,100
We can see here that I'm creating that hour column here, but what I'm doing is actually

360
00:25:31,100 --> 00:25:35,500
just looking at this in real time, so it's actually going and pushing down the execution.

361
00:25:35,500 --> 00:25:39,540
It's a big query for each of the different steps that are running through it.

362
00:25:39,540 --> 00:25:45,180
So what's really great about this is we're kind of solving for two worlds of things.

363
00:25:45,180 --> 00:25:47,460
One is how do I do interactive analysis?

364
00:25:47,460 --> 00:25:51,680
The other one is okay, I did that interactive analysis, how do I actually make this a production

365
00:25:51,680 --> 00:25:53,540
thing that's going to go run?

366
00:25:53,540 --> 00:25:57,980
And we can do it from a single library just using some flag changes in order to make the

367
00:25:57,980 --> 00:26:00,380
process work for what we want to do.

368
00:26:02,380 --> 00:26:04,980
All right, great stuff about IBIS.

369
00:26:04,980 --> 00:26:10,500
Let's talk then about what we're working on at Google in order to extend this even further.

370
00:26:10,500 --> 00:26:19,020
So while IBIS, if you notice, looks very similar to a data frame, it is not exact to the pandas

371
00:26:19,020 --> 00:26:21,660
functions and kind of the spec around that.

372
00:26:21,660 --> 00:26:26,260
So the next thing that we are working on here is how can we actually take that and then

373
00:26:26,260 --> 00:26:31,780
make what we're calling big frames, which is a pandas compatible API on top of all these

374
00:26:31,780 --> 00:26:33,580
things.

375
00:26:33,580 --> 00:26:39,980
And what I'm showing you here, I'm just in Google Colab, which is an almost serverless

376
00:26:39,980 --> 00:26:45,340
notebook that anyone here could go and open and then run against BigQuery pieces.

377
00:26:45,340 --> 00:26:49,460
What we're doing, I'm just installing the big frame wheel.

378
00:26:49,460 --> 00:26:54,220
But then you can see here that what I'm doing is I'm actually pulling in a data frame using

379
00:26:54,220 --> 00:26:56,140
my context here.

380
00:26:56,140 --> 00:27:00,420
Then interacting with this with the exact same syntax that I would in pandas.

381
00:27:00,420 --> 00:27:07,020
So again, really the intent here is meeting you as the developer where you are and making

382
00:27:07,020 --> 00:27:12,820
sure that whatever syntax we have available is similar to what you're using to make the

383
00:27:12,820 --> 00:27:16,360
product the most accessible in that.

384
00:27:16,360 --> 00:27:21,060
So here you can see once I pull that in, I'm just connecting to my context.

385
00:27:21,060 --> 00:27:23,140
I'm reading that table from BigQuery.

386
00:27:23,140 --> 00:27:25,580
In this case, it's a taxi trip table.

387
00:27:25,580 --> 00:27:29,660
I'm doing things like filtering down based on the number of columns.

388
00:27:29,700 --> 00:27:34,220
I can also do things like creating subtotals of my data, which I'm just displaying here

389
00:27:34,220 --> 00:27:37,380
as a part of the code.

390
00:27:37,380 --> 00:27:40,380
All right.

391
00:27:40,380 --> 00:27:44,380
More to come on the big frame stuff.

392
00:27:44,380 --> 00:27:48,700
Right now we are still actively developing this.

393
00:27:48,700 --> 00:27:53,140
But IBIS, if I didn't mention earlier, is already available for you to go and download

394
00:27:53,140 --> 00:27:56,180
today and to start using with these different pieces.

395
00:27:56,180 --> 00:27:57,820
All right.

396
00:27:57,820 --> 00:28:00,580
So we've done a lot of discussion here about data frames.

397
00:28:00,580 --> 00:28:04,100
Let's talk about how we can start to use some of the different execution things within this

398
00:28:04,100 --> 00:28:06,180
to make it happen.

399
00:28:06,180 --> 00:28:12,900
Next thing here that I'm showing, and apologies, let me make the screen a little bit easier

400
00:28:12,900 --> 00:28:20,140
to see, is essentially how we can take those data frames or our PySpark code or our other

401
00:28:20,140 --> 00:28:25,900
code and be able to execute what we call the Spark stored procedures on top of BigQuery.

402
00:28:26,100 --> 00:28:31,140
As you can see, what's great about this is I have a few things here where I'm just doing

403
00:28:31,140 --> 00:28:33,780
some specifications.

404
00:28:33,780 --> 00:28:37,540
This would be no different than if you were working with a Spark job, maybe providing

405
00:28:37,540 --> 00:28:41,500
some custom URIs for jars or something else.

406
00:28:41,500 --> 00:28:47,700
What I can do then is essentially just say, what is my PySpark code?

407
00:28:47,700 --> 00:28:48,940
I want to execute this.

408
00:28:48,940 --> 00:28:52,840
Then what it will do is manage everything behind the scenes, including the creation

409
00:28:52,920 --> 00:28:58,360
of instances to execute the code, the actual code execution, the logging and error handling,

410
00:28:58,360 --> 00:29:02,280
and then ultimately the spin down of those instances, and it's seamless to you as a

411
00:29:02,280 --> 00:29:06,160
user in that process.

412
00:29:06,160 --> 00:29:11,040
So once we have those executed, what's actually great about this, and for the sake of the

413
00:29:11,040 --> 00:29:15,440
demo, it takes about a minute to spin up the instances and then execute the code, so I

414
00:29:15,440 --> 00:29:17,800
won't go through that full piece.

415
00:29:17,800 --> 00:29:21,440
But what you can see is we actually create a table here.

416
00:29:21,560 --> 00:29:25,840
So we have our products, and then we could actually come and just query this table like

417
00:29:25,840 --> 00:29:28,640
anything else.

418
00:29:28,640 --> 00:29:32,520
What this is doing behind the scenes is this is actually an iceberg table that I have stood

419
00:29:32,520 --> 00:29:35,280
up in my data warehouse.

420
00:29:35,280 --> 00:29:48,280
If we come over to our bucket and click in, you can see here that I have my metadata.

421
00:29:48,440 --> 00:29:54,560
These are JSON objects that essentially tell that table, okay, what are the actual files

422
00:29:54,560 --> 00:29:56,720
to pull in?

423
00:29:56,720 --> 00:29:59,680
And I have my data itself.

424
00:29:59,680 --> 00:30:05,760
You can see here these are all of my different Parquet files that I have actually been creating

425
00:30:05,760 --> 00:30:10,160
over time in that process.

426
00:30:10,160 --> 00:30:15,440
So again, what we're essentially able to do in this, if you think of in your shoes as

427
00:30:15,520 --> 00:30:21,520
the Python developer, is we can bring what I'll call kind of an arbitrary file format

428
00:30:21,520 --> 00:30:22,520
of data.

429
00:30:22,520 --> 00:30:30,120
So we work with the newest things like iceberg, obviously Parquet, CSV, JSON, all those things.

430
00:30:30,120 --> 00:30:34,200
And then I'm also actually able to start bringing arbitrary compute to that data.

431
00:30:34,200 --> 00:30:38,840
So in this case, we could use our BigQuery compute or we could use our serverless Spark

432
00:30:38,840 --> 00:30:39,840
compute.

433
00:30:39,840 --> 00:30:43,400
And what that really just enables for you is the choice to pick where you want your

434
00:30:43,640 --> 00:30:48,640
code to execute and then to use the right platform for the right job in that.

435
00:30:48,640 --> 00:30:53,200
All right.

436
00:30:53,200 --> 00:30:55,800
So almost through here.

437
00:30:55,800 --> 00:31:00,720
Second to last thing we'll talk through is how we can then use our object tables.

438
00:31:00,720 --> 00:31:05,360
So here what I'm doing is, again, just using some simple SQL for the table creation.

439
00:31:05,360 --> 00:31:08,720
I'm creating a object table here.

440
00:31:08,720 --> 00:31:13,440
And then you can see here I've done a quick specification here that this is a directory

441
00:31:13,440 --> 00:31:14,560
of object metadata.

442
00:31:14,560 --> 00:31:17,960
So I want all of the metadata about the directory.

443
00:31:17,960 --> 00:31:21,480
I'm pulling in data from this cloud storage bucket.

444
00:31:21,480 --> 00:31:24,560
And then what we can actually do is start looking at some of the things that are pulled

445
00:31:24,560 --> 00:31:25,560
into that.

446
00:31:25,560 --> 00:31:29,960
So let me go ahead and run the statement here.

447
00:31:29,960 --> 00:31:34,520
Here you can see what we've essentially done is pulled in every URI that exists in that

448
00:31:34,520 --> 00:31:35,520
bucket.

449
00:31:35,600 --> 00:31:40,360
So in this case the URI is just the file location in the object storage.

450
00:31:40,360 --> 00:31:41,740
We have things like our hashes.

451
00:31:41,740 --> 00:31:44,440
We have things like when was the file last updated.

452
00:31:44,440 --> 00:31:48,840
And what we can actually do then is start to pass this into other things in order to

453
00:31:48,840 --> 00:31:50,880
do more analysis or work for that.

454
00:31:50,880 --> 00:31:54,600
And we're going to actually do just that here.

455
00:31:54,600 --> 00:31:58,880
What we're going to do is we're creating a table called a vision detection.

456
00:31:58,880 --> 00:32:03,880
And what this will do, and then we'll look at the code behind the scenes here in a minute.

457
00:32:04,880 --> 00:32:10,000
Is that we're going to actually execute one of those remote procedures.

458
00:32:10,000 --> 00:32:14,200
And what this is going to do is it's going to actually send our data or the components

459
00:32:14,200 --> 00:32:18,580
of data that we request in the function out to a cloud function.

460
00:32:18,580 --> 00:32:23,400
We're going to have Python essentially organize and map how that data should then be transformed.

461
00:32:23,400 --> 00:32:28,640
And then we're going to return that back to our essentially materialized results set here

462
00:32:28,640 --> 00:32:31,520
as a result of that.

463
00:32:31,520 --> 00:32:34,120
So while that's running, what we can actually do is flip over.

464
00:32:34,120 --> 00:32:38,200
And you can see here, I've just created a simple Python function.

465
00:32:38,200 --> 00:32:40,880
But what this does is take in our request.

466
00:32:40,880 --> 00:32:46,520
And in this case, the request, because it's a function within the select statement, is

467
00:32:46,520 --> 00:32:50,760
a set of values that go in.

468
00:32:50,760 --> 00:32:53,460
We're going and actually calling the vision API.

469
00:32:53,460 --> 00:32:57,360
And what this allows us to do then is to do things like annotate our images with additional

470
00:32:57,360 --> 00:32:58,360
information.

471
00:32:58,360 --> 00:33:03,640
So one of the things that may be popular is, hey, I have images.

472
00:33:03,640 --> 00:33:07,840
How do I actually know that there's a logo or something in that if there's maybe concerns

473
00:33:07,840 --> 00:33:09,380
around that?

474
00:33:09,380 --> 00:33:14,120
This allows us to take all those pieces and then return this back to the BigQuery results

475
00:33:14,120 --> 00:33:18,000
set, store it in BigQuery, and then allow other users actually to access that maybe

476
00:33:18,000 --> 00:33:20,920
in a dashboard or something else downstream.

477
00:33:20,920 --> 00:33:23,960
All right.

478
00:33:23,960 --> 00:33:25,800
So we can see our rows are there.

479
00:33:25,800 --> 00:33:35,840
What I'll go ahead and do is just go ahead and run a quick select off that.

480
00:33:35,840 --> 00:33:39,720
You can see here that what we've done is we've actually injected all of our metadata that

481
00:33:39,720 --> 00:33:41,480
was associated with an image.

482
00:33:41,480 --> 00:33:47,360
And you can see here that for one of the records, we actually identified that there may be personally

483
00:33:47,360 --> 00:33:49,400
identifiable information on this.

484
00:33:49,400 --> 00:33:52,720
So that's probably something we would want to look at and scrub depending on where this

485
00:33:52,720 --> 00:33:54,400
data was going.

486
00:33:54,400 --> 00:33:57,200
We also actually detected a face in this.

487
00:33:57,200 --> 00:33:59,480
And then we detected something with a car in that.

488
00:33:59,480 --> 00:34:05,280
So if you think about one of the big things now is obviously how do we keep data safe.

489
00:34:05,280 --> 00:34:08,540
There are a lot of ways here using different technologies that we can essentially make

490
00:34:08,540 --> 00:34:13,300
it very simple to identify where we may have potential issues in our data and really make

491
00:34:13,300 --> 00:34:17,440
sure that it's being governed in the right way for its use.

492
00:34:17,440 --> 00:34:20,160
And all of this is obviously accessible from BigQuery.

493
00:34:20,160 --> 00:34:26,680
So as you think about how do I share that data downstream, we're not passing around

494
00:34:26,680 --> 00:34:29,960
files right on a server or an object store.

495
00:34:29,960 --> 00:34:34,320
We're actually letting everything be managed via a set of role-based access controls in

496
00:34:34,320 --> 00:34:39,260
BigQuery, making it easy to share but also making it scalable in terms of the sharing

497
00:34:39,260 --> 00:34:41,200
capabilities on that.

498
00:34:41,200 --> 00:34:42,680
All right.

499
00:34:42,680 --> 00:34:44,120
So a quick summary here.

500
00:34:44,120 --> 00:34:49,120
So first thing we started off with was how do we build a Pythonic-based data frames and

501
00:34:49,160 --> 00:34:53,640
how do we actually execute those against BigQuery using IBIS?

502
00:34:53,640 --> 00:34:59,120
Can we also do the same thing using big frames and more of a pandas like syntax?

503
00:34:59,120 --> 00:35:03,320
And then what we got into is, OK, how do we take that code and then how do we actually

504
00:35:03,320 --> 00:35:08,720
execute that from BigQuery using something like PySpark or using remote functions where

505
00:35:08,720 --> 00:35:14,800
we could run any Python that we wanted against our data and then get that result set back?

506
00:35:14,800 --> 00:35:20,720
So with that, really appreciate everyone attending today.

507
00:35:20,720 --> 00:35:26,480
And with that, I will pass it over to questions and feel free to keep it open for anything

508
00:35:26,480 --> 00:35:31,480
Google Analytics or sorry, Google Data Analytics related.

509
00:35:31,480 --> 00:35:35,160
The only caveat is I cannot talk about genera vea, which I'm sure everyone here loves to

510
00:35:35,160 --> 00:35:36,640
talk about.

511
00:35:36,640 --> 00:35:40,640
So what questions help to answer?

512
00:35:45,640 --> 00:35:46,640
Yeah.

513
00:35:46,640 --> 00:35:51,640
So for the remote procedure call, is it shipping all the data to the function at once or is

514
00:35:51,640 --> 00:35:54,640
it making the call pretty slow?

515
00:35:54,640 --> 00:35:55,640
Yeah.

516
00:35:55,640 --> 00:36:00,040
So the question was how does the remote procedure actually handle record batching and other

517
00:36:00,040 --> 00:36:01,040
pieces in that?

518
00:36:01,040 --> 00:36:06,200
So are we shipping one really, really big thing or are we shipping one thing at a time

519
00:36:06,200 --> 00:36:07,560
in that?

520
00:36:07,560 --> 00:36:11,440
What happens behind the scenes is that we will manage the number of function instances

521
00:36:11,440 --> 00:36:14,560
that have to be spun up and then we will start to parse that out.

522
00:36:14,560 --> 00:36:19,400
So let's say you have 50,000 records.

523
00:36:19,400 --> 00:36:23,920
We may choose behind the scenes that you only need one function to actually execute on that

524
00:36:23,920 --> 00:36:27,480
or we may say, hey, we actually need to spin up five or 10 on that.

525
00:36:27,480 --> 00:36:30,560
We manage how those things have to be batched behind the scenes for you.

526
00:36:30,560 --> 00:36:35,160
So at the end of the day, what you experience is that the code returns the data that you

527
00:36:35,160 --> 00:36:39,760
expected and you're not spending the time troubleshooting like why does the network

528
00:36:39,760 --> 00:36:41,760
hate me in that process?

529
00:36:41,760 --> 00:36:44,480
Does that help to answer your question?

530
00:36:44,480 --> 00:36:45,480
Yeah.

531
00:36:45,480 --> 00:36:58,120
Is there a user case where you can use SAP HANA and make joins with another database?

532
00:36:58,120 --> 00:37:02,480
Yes the question may be, should I cut because I cut a little bit the door in the microphone.

533
00:37:02,480 --> 00:37:06,440
Arthur used cases where we can connect to and do joins based off data that sits in like

534
00:37:06,440 --> 00:37:09,760
SAP HANA and then you materialize that downstream.

535
00:37:09,760 --> 00:37:11,800
Is that correct?

536
00:37:11,800 --> 00:37:12,800
Yep.

537
00:37:13,720 --> 00:37:16,280
We have a couple things here.

538
00:37:16,280 --> 00:37:21,680
So one is that we have a actual framework called Cortex.

539
00:37:21,680 --> 00:37:25,280
What that allows you to do is to connect to a number of different sources with an SAP.

540
00:37:25,280 --> 00:37:28,460
I believe HANA is also one of them.

541
00:37:28,460 --> 00:37:34,800
With that you can then pull that data or replicate that data out of HANA into a tool like BigQuery

542
00:37:34,800 --> 00:37:40,480
and then do what I'll call kind of like a co-location of that and then do your joins,

543
00:37:40,480 --> 00:37:42,600
all of your other pieces with that.

544
00:37:42,600 --> 00:37:47,600
I believe it also ships back but that would just be something that I would probably want

545
00:37:47,600 --> 00:37:51,400
to check in the framework component for that.

546
00:37:51,400 --> 00:37:56,520
If you think for a lot of our customers like they'll use HANA and then something like Spanner

547
00:37:56,520 --> 00:38:03,040
or Postgres which are two of our transactional relational databases, we can query those in

548
00:38:03,040 --> 00:38:07,960
place because we control the location and how the data works in that.

549
00:38:07,960 --> 00:38:12,120
Once you do that, BigQuery can essentially analyze any of that data and then send your

550
00:38:12,120 --> 00:38:21,440
results set back for activation to the SAP platform.

551
00:38:21,440 --> 00:38:22,440
Other questions?

552
00:38:22,440 --> 00:38:23,440
Yeah.

553
00:38:23,440 --> 00:38:26,560
Yeah.

554
00:38:26,560 --> 00:38:29,240
Yeah.

555
00:38:29,240 --> 00:38:32,480
Yep.

556
00:38:32,480 --> 00:38:46,280
So, as it stands right now, remote function essentially runs in Cloud Functions or another

557
00:38:47,280 --> 00:38:48,600
Cloud Run.

558
00:38:48,600 --> 00:38:55,080
And Cloud Run for the technically interested actually powers remote functions.

559
00:38:55,080 --> 00:38:59,900
It's our fully serverless stack for running containers.

560
00:38:59,900 --> 00:39:03,620
As it works right now, what happens is that you'll deploy the function and then it would

561
00:39:03,620 --> 00:39:05,840
execute all of those stacks.

562
00:39:05,840 --> 00:39:10,940
Now there's exploratory things where we're looking at how can we make that even simpler.

563
00:39:10,940 --> 00:39:16,040
So it's literally like you just tell BigQuery here's the Python and we figure out even the

564
00:39:16,080 --> 00:39:17,560
underlying components.

565
00:39:17,560 --> 00:39:23,880
But right now you would deploy that and then the function or Cloud Run would execute that

566
00:39:23,880 --> 00:39:24,880
job.

567
00:39:24,880 --> 00:39:26,320
It's still relatively easy to set up.

568
00:39:26,320 --> 00:39:30,520
I do recognize in the deployment it's one additional step because you've got to ship

569
00:39:30,520 --> 00:39:40,680
that code to like Buildpack or something for that.

570
00:39:40,680 --> 00:39:41,680
Any other questions?

571
00:39:41,680 --> 00:39:47,800
All right.

572
00:39:47,800 --> 00:39:48,800
Well thanks everyone.

573
00:39:48,800 --> 00:39:52,760
I hope this was useful for folks learning more about BigQuery.

574
00:39:52,760 --> 00:39:57,080
I'll stick around here until obviously the end of the session if you have other questions

575
00:39:57,080 --> 00:39:59,160
that you'd like to ask.

576
00:39:59,160 --> 00:40:00,440
But really appreciate your time.

577
00:40:00,440 --> 00:40:03,360
Hope everyone here has a great day at the conference.

578
00:40:03,360 --> 00:40:05,360
And best wishes.

