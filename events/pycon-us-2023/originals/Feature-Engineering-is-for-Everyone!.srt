1
00:00:00,000 --> 00:00:02,000
Alright, hello everyone.

2
00:00:03,240 --> 00:00:09,180
I'll let everyone get settled in here. My name is Ray McLinden and this is Leah Berg, my colleague.

3
00:00:09,240 --> 00:00:16,040
We're with Data Science Rebalanced and we both happen to be two data scientists at the Kansas City Federal Reserve.

4
00:00:17,660 --> 00:00:20,460
When we first started working at the Federal Reserve together,

5
00:00:20,460 --> 00:00:26,000
we kind of bonded over the fact that we felt like our sort of schooling, our education,

6
00:00:26,040 --> 00:00:29,080
education didn't fully prepare us for our data science careers.

7
00:00:29,080 --> 00:00:32,240
And so what we decided to do was put something together,

8
00:00:32,680 --> 00:00:40,180
education-wise and data science rebalance to help bridge that gap from going from an academic setting into the industry setting from a data science perspective.

9
00:00:41,640 --> 00:00:43,640
That being said,

10
00:00:43,720 --> 00:00:47,040
we are Federal Reserve employees. Hopefully this will

11
00:00:47,880 --> 00:00:49,880
move for me. Maybe not. Oh,

12
00:00:50,760 --> 00:00:52,760
need to move back up.

13
00:00:53,560 --> 00:00:55,560
Yeah.

14
00:00:56,320 --> 00:00:59,120
So whenever we talk we have to come with this lovely disclaimer

15
00:01:00,160 --> 00:01:07,840
stating that everything we say is our opinion and doesn't represent the Federal Reserve, the system, the Kansas City or the Board of Governors.

16
00:01:07,840 --> 00:01:10,120
You know, don't want to freak out the stock market.

17
00:01:10,760 --> 00:01:12,000
So,

18
00:01:12,000 --> 00:01:17,280
with that said, we'll dive into our actual workshop, which is Feature Engineering is for Everyone.

19
00:01:17,840 --> 00:01:19,160
And

20
00:01:19,160 --> 00:01:21,540
we'll cover a couple of things here real quick.

21
00:01:21,540 --> 00:01:26,960
So a quick intro to features and then we'll kind of break the world down into two different spaces,

22
00:01:27,440 --> 00:01:29,880
discrete data and continuous data sets.

23
00:01:31,120 --> 00:01:35,640
And from there, we'll basically talk about exploring those data sets versus creating them.

24
00:01:37,240 --> 00:01:42,360
Here are the tools that we'll be using along the way, some Colab, Pandas and so forth.

25
00:01:42,360 --> 00:01:44,360
So,

26
00:01:45,840 --> 00:01:48,480
let's dig in. Introduction. What in the world is a feature?

27
00:01:49,160 --> 00:01:56,240
Some of you probably have a good feel for this already. A numerical representation of some form of raw data is how I guess Webster

28
00:01:56,240 --> 00:01:57,720
might put it.

29
00:01:57,720 --> 00:02:02,720
For us being data scientists, we often use it as an input into our machine learning models,

30
00:02:02,960 --> 00:02:05,760
but it can also be used for data analysis or

31
00:02:06,480 --> 00:02:09,840
data analytics, automation, quality and such.

32
00:02:10,040 --> 00:02:12,760
What we're really going to aim for is creating features.

33
00:02:12,760 --> 00:02:17,840
So the feature engineering process, which includes creating them and selecting them. I'll be frank,

34
00:02:18,320 --> 00:02:24,800
we don't have enough time today to really get to the selection part. We're mostly just going to cover creating them for today.

35
00:02:27,640 --> 00:02:30,200
So, talk over some quick use cases.

36
00:02:31,000 --> 00:02:33,000
Data analytics, I mentioned earlier.

37
00:02:33,000 --> 00:02:38,160
So I think if you were to take a take a course in like BI and data analytics,

38
00:02:38,400 --> 00:02:43,680
they would probably never use the word feature in the entire time that you're in that course.

39
00:02:43,840 --> 00:02:51,160
But a number of the items that you're using are technically features. They are numerical representations, and then you're just making

40
00:02:51,560 --> 00:02:53,560
visualizations with them.

41
00:02:54,000 --> 00:03:00,960
Another example, I used to work in the renewable energy space with wind energy and internet of things. You have sensors out in the field.

42
00:03:00,960 --> 00:03:06,400
Sometimes they go bad. You can use features to help you determine when your sensors are going bad.

43
00:03:06,400 --> 00:03:10,400
We work in the Federal Reserve Banking System.

44
00:03:10,400 --> 00:03:16,400
And so whenever we want to use machine learning, we have to have interpretable models.

45
00:03:16,400 --> 00:03:19,400
Now, there's some really cool deep learning stuff that's available.

46
00:03:19,400 --> 00:03:29,400
But generally speaking, that's not allowed as much in the banking space because you have to be able to interpret why you're, let's for instance, denying someone credit.

47
00:03:30,400 --> 00:03:33,400
So simpler models, we're back to using features.

48
00:03:33,400 --> 00:03:43,400
So for the, I apologize, break up. I'll switch over to Leah for the next part.

49
00:03:43,400 --> 00:03:48,400
No, you're good. All right, guys. So we're going to head over to Colab.

50
00:03:48,400 --> 00:03:55,400
Hopefully you guys got our message ahead of time and were able to access the Colab notebook.

51
00:03:55,400 --> 00:03:59,400
You should be seeing something that looks like this.

52
00:03:59,400 --> 00:04:02,400
Yes. Yeah, we can zoom.

53
00:04:02,400 --> 00:04:04,400
Actually, you know what?

54
00:04:04,400 --> 00:04:07,400
Maybe let me. Here we go.

55
00:04:07,400 --> 00:04:11,400
Okay. Does anyone not have the notebook?

56
00:04:11,400 --> 00:04:14,400
If not, not a big deal.

57
00:04:14,400 --> 00:04:18,400
I do have the link for it here in case anyone does need it.

58
00:04:18,400 --> 00:04:22,400
So I'll flash that here for a second in case anyone needs that.

59
00:04:22,400 --> 00:04:26,400
Okay. I'm going to move this out of the way.

60
00:04:26,400 --> 00:04:32,400
Okay. And so first off, we're going to just do a quick intro to Google Colab.

61
00:04:32,400 --> 00:04:36,400
If you haven't used this tool before, it's a really nice tool.

62
00:04:36,400 --> 00:04:44,400
It's really great for presenters like us because what it allows us to do is just focus on the content and the content itself.

63
00:04:44,400 --> 00:04:47,400
If you haven't used this tool before, it's a really nice tool.

64
00:04:47,400 --> 00:04:58,400
It's really great for presenters like us because what it allows us to do is just focus on the content and not worrying about getting Python installed on your machine or setting up an ID or anything like that.

65
00:04:58,400 --> 00:05:01,400
So this all runs in the cloud, which is really awesome.

66
00:05:01,400 --> 00:05:04,400
And this is where we're going to be doing our coding today.

67
00:05:04,400 --> 00:05:06,400
So I'll just give a quick little tour here.

68
00:05:06,400 --> 00:05:12,400
Over on the left-hand side, we've got this little hamburger-looking menu that gives us this table of content.

69
00:05:12,400 --> 00:05:17,400
What we've tried to do throughout this workbook is make it so that it's really easy to navigate.

70
00:05:17,400 --> 00:05:20,400
We've got these headers so that you can jump around in there.

71
00:05:20,400 --> 00:05:31,400
We've also included a lot of extra detail that Ray is going to be covering in the slides, hopefully so that you can use this really as a standalone notebook moving forward and not have to bounce between slides and code.

72
00:05:31,400 --> 00:05:33,400
So we've got our table of contents.

73
00:05:33,400 --> 00:05:36,400
If you need to find and replace, that's available.

74
00:05:36,400 --> 00:05:38,400
We're not going to really be doing that today.

75
00:05:38,400 --> 00:05:41,400
Over here, we're going to have some variables.

76
00:05:41,400 --> 00:05:50,400
If you've ever coded in an IDE before or a development environment, you normally have a section where you can see all your variables and what the type of them are and all that good stuff.

77
00:05:50,400 --> 00:05:52,400
So that's there in case we need to look at that.

78
00:05:52,400 --> 00:05:56,400
And then last thing I'll touch on over here is this folder icon.

79
00:05:56,400 --> 00:06:00,400
So what you should be seeing right now is this sample data.

80
00:06:00,400 --> 00:06:04,400
And maybe if you've stepped ahead a couple of steps, you might be seeing your own Google Drive.

81
00:06:04,400 --> 00:06:12,400
So that's where we're going to get to is connecting up your Google Drive to this instance of Colab so that it can reference the data set we're going to work with today.

82
00:06:12,400 --> 00:06:15,400
Okay.

83
00:06:15,400 --> 00:06:19,400
So I think that wraps up our Colab intro.

84
00:06:19,400 --> 00:06:29,400
I'm going to toss it back to Ray so that he can talk to us about exploring discrete data.

85
00:06:29,400 --> 00:06:31,400
Okay.

86
00:06:31,400 --> 00:06:38,400
So exploring discrete data, this probably isn't going to take a crazy amount of time per se.

87
00:06:38,400 --> 00:06:42,400
So first off, what in the world is discrete data?

88
00:06:42,400 --> 00:06:53,400
Our simplistic way of putting it is it's data that can be represented by the natural numbers or whole numbers, one, two, three through N, however you like.

89
00:06:53,400 --> 00:07:00,400
And some examples are number of employees in your department or number of customers acquired this month.

90
00:07:00,400 --> 00:07:08,400
Product categories, sentiment analysis, positive, negative, neutral, that can be represented as just three, discrete.

91
00:07:08,400 --> 00:07:12,400
And in our case, with a lot of the work that we do, unstructured text.

92
00:07:12,400 --> 00:07:18,400
So we'll have a fair amount of examples around that because that's what we use on a regular basis.

93
00:07:18,400 --> 00:07:22,400
So how might we explore this type of data set?

94
00:07:22,400 --> 00:07:29,400
There's basic summary statistics, the kind of stuff you would have learned back in school, mean, median, mode, yada yada.

95
00:07:29,400 --> 00:07:34,400
There's also bar charts, which we're going to definitely do some of.

96
00:07:34,400 --> 00:07:38,400
Line charts, especially if you're wanting to see how your discrete data is changing over time.

97
00:07:38,400 --> 00:07:40,400
And then there's comparisons.

98
00:07:40,400 --> 00:07:49,400
So if we have a couple of columns of discrete data, we might want to compare the behavior between two sets.

99
00:07:49,400 --> 00:07:55,400
Again, we deal with a lot of text, so I figured I'd just have fun and throw up an example real quick.

100
00:07:55,400 --> 00:08:01,400
In this example of doing a comparison, it's between two documents, document one and document two.

101
00:08:01,400 --> 00:08:04,400
The combined documents we refer to as our corpus.

102
00:08:04,400 --> 00:08:06,400
It's all of our documents together.

103
00:08:06,400 --> 00:08:17,400
And the vocabulary down below is the unique set of words, or if you want to be picky, tokens, that exist in the entire corpus.

104
00:08:17,400 --> 00:08:23,400
And so what we can do here is we can actually represent each document as kind of like this word count

105
00:08:23,400 --> 00:08:27,400
of how often each word in the vocabulary appears.

106
00:08:27,400 --> 00:08:34,400
And that becomes a way in which we can compare just these kind of discrete pieces of data in terms of documents.

107
00:08:34,400 --> 00:08:38,400
So we'll dive into more of that later.

108
00:08:38,400 --> 00:08:42,400
So with that, I'll switch back over.

109
00:08:42,400 --> 00:08:45,400
Sounds good.

110
00:08:45,400 --> 00:08:49,400
Okay, so we are going to get into our Google Colab notebook.

111
00:08:49,400 --> 00:08:52,400
Hopefully you all got that up and running.

112
00:08:52,400 --> 00:08:58,400
What we need to do first is make sure you're connected to a runtime up in this top right corner.

113
00:08:58,400 --> 00:09:04,400
Just make sure you see basically what I'm showing here, the screen check mark, and you're good for that.

114
00:09:04,400 --> 00:09:06,400
I should also say a couple logistics things.

115
00:09:06,400 --> 00:09:11,400
Any time you guys have questions, feel free to stop us and we'll answer those.

116
00:09:11,400 --> 00:09:17,400
And we've got some, what we're going to do in these is first Ray's going to talk through all the slides for us,

117
00:09:17,400 --> 00:09:19,400
give us the conceptual understanding.

118
00:09:19,400 --> 00:09:22,400
Then we're going to do an example through it in Colab.

119
00:09:22,400 --> 00:09:25,400
And then we have several exercises for you guys to actually do on your own.

120
00:09:25,400 --> 00:09:29,400
So that'll be just kind of the way that we run down on things here.

121
00:09:29,400 --> 00:09:33,400
Okay, so with that, let's get started.

122
00:09:33,400 --> 00:09:36,400
So first off, we just have to do a tiny bit of setup.

123
00:09:36,400 --> 00:09:42,400
What you'll do in your notebook is if you haven't already, click on that this link button.

124
00:09:42,400 --> 00:09:45,400
And I can even zoom in a little bit more here.

125
00:09:45,400 --> 00:09:47,400
So you'll want to click on that.

126
00:09:47,400 --> 00:09:52,400
What we need to do here is get the data set that we're going to work with for this first section.

127
00:09:52,400 --> 00:10:00,400
And what we're going to do here is instead of having us all on the conference Wi-Fi download a 200-meg data set each,

128
00:10:00,400 --> 00:10:07,400
we're going to just point to where it's stored on our Google Drive, ours being mine and Ray's Google Drive.

129
00:10:07,400 --> 00:10:13,400
So what you'll do if you haven't done this already is just click on 2023 and you're going to do add shortcut to drive.

130
00:10:13,400 --> 00:10:21,400
So that is going to make a copy of that over into, well, not a copy, but just a link out to it inside of your Google Drive.

131
00:10:21,400 --> 00:10:26,400
So once you do that, and there's a picture there, if you need those instructions again,

132
00:10:26,400 --> 00:10:30,400
you should be able to go ahead and click the play button on that first cell here.

133
00:10:30,400 --> 00:10:36,400
It might ask you just to give permission and make sure that it's okay that it's connecting to the drive.

134
00:10:36,400 --> 00:10:40,400
And then you might have to select your account over here as well.

135
00:10:40,400 --> 00:10:43,400
So I'm going to select the one that I'm using today.

136
00:10:45,400 --> 00:10:50,400
And then it's just asking to make sure that it's okay that it connects. And I'm going to say allow.

137
00:10:50,400 --> 00:10:57,400
And once we do that, it should give us a little green check here saying that we're all good to go.

138
00:10:57,400 --> 00:11:00,400
So I'll wait for that to show up.

139
00:11:00,400 --> 00:11:04,400
Okay, we've got our check. And to double check that this actually works,

140
00:11:04,400 --> 00:11:07,400
what we can do is click on that folder icon on the left-hand side.

141
00:11:07,400 --> 00:11:10,400
And you should now see a drive option.

142
00:11:10,400 --> 00:11:13,400
And if you click into that, you should see a my drive.

143
00:11:13,400 --> 00:11:17,400
And then under that, that's actually all of your Google Drive.

144
00:11:17,400 --> 00:11:22,400
But what we've done is create that shortcut to our 2023 folder that has the data set in it.

145
00:11:22,400 --> 00:11:25,400
So everyone good on that step so far?

146
00:11:29,400 --> 00:11:32,400
Okay, no problem. We can wait just a second.

147
00:11:34,400 --> 00:11:36,400
Okay.

148
00:11:39,400 --> 00:11:41,400
Good. Awesome.

149
00:11:41,400 --> 00:11:46,400
Okay. So I'm going to minimize that so it doesn't take up half of my screen.

150
00:11:46,400 --> 00:11:50,400
So we're going to actually get into exploring discrete data.

151
00:11:50,400 --> 00:11:55,400
If you guys want to go ahead and run that next cell there under the exploring discrete data header,

152
00:11:55,400 --> 00:11:58,400
what we're going to use is use a library called Pandas.

153
00:11:58,400 --> 00:12:02,400
If you haven't used this one before, very popular library for data science.

154
00:12:02,400 --> 00:12:06,400
It allows you to manipulate data and do a lot of analysis.

155
00:12:06,400 --> 00:12:08,400
A really nice library.

156
00:12:08,400 --> 00:12:12,400
So we're going to use Pandas to load in our CSV data set.

157
00:12:12,400 --> 00:12:16,400
And this data set that we're working with is all about technology product reviews.

158
00:12:16,400 --> 00:12:22,400
So you can imagine anytime someone buys a product online, maybe that's a new laptop,

159
00:12:22,400 --> 00:12:29,400
maybe that's a new camera, anything like that, they might write a review for it in publications like CNET

160
00:12:29,400 --> 00:12:33,400
or I think another one from this data set is TechRadar.

161
00:12:33,400 --> 00:12:35,400
Lots of publications out there.

162
00:12:35,400 --> 00:12:39,400
But they'll give their opinion on what their thoughts on the product is.

163
00:12:39,400 --> 00:12:41,400
And that's the data set we're going to be using.

164
00:12:41,400 --> 00:12:45,400
So mainly unstructured text is what we're going to be working with today.

165
00:12:45,400 --> 00:12:52,400
Now to explore this data, a really nice tool that we like using is a library called YDataProfiling.

166
00:12:52,400 --> 00:12:56,400
And if you haven't used that before, you might know of it under another name.

167
00:12:56,400 --> 00:12:58,400
It used to be called PandasProfiling.

168
00:12:58,400 --> 00:13:06,400
But this is a really handy library that allows you to generate a bunch of charts and a bunch of insights on your data set.

169
00:13:06,400 --> 00:13:15,400
So if you want to go ahead and run that next cell there, what we're going to do is install the YDataProfiling library into our environment.

170
00:13:15,400 --> 00:13:20,400
I should have mentioned earlier that in Colab, it comes with a lot of built-in libraries for us.

171
00:13:20,400 --> 00:13:22,400
But this is one that does not come with it.

172
00:13:22,400 --> 00:13:25,400
So we just have to pip install that.

173
00:13:25,400 --> 00:13:33,400
And similar syntax, so like if you're using a Jupyter Notebook, that exclamation point is just saying let's run it in the terminal.

174
00:13:33,400 --> 00:13:36,400
So we'll let that install.

175
00:13:36,400 --> 00:13:42,400
And once we get that installed, the next thing we'll do is actually create the report.

176
00:13:42,400 --> 00:13:44,400
And I'm going to go ahead and run that.

177
00:13:44,400 --> 00:13:45,400
You guys can go ahead and do that too.

178
00:13:45,400 --> 00:13:46,400
It'll take it a second.

179
00:13:46,400 --> 00:13:52,400
What we've done actually is just take a sample of the data set so that it runs a little bit faster.

180
00:13:52,400 --> 00:14:01,400
We're just picking up the 500 random rows in our data set to this random state parameter is just saying let's initialize on a certain point

181
00:14:01,400 --> 00:14:03,400
so that we all get the same data points to work with.

182
00:14:03,400 --> 00:14:10,400
If you don't want to see the same data points that I'm seeing, you can change that number if you like.

183
00:14:10,400 --> 00:14:13,400
Yeah.

184
00:14:13,400 --> 00:14:15,400
Oh, sometimes it might.

185
00:14:15,400 --> 00:14:17,400
Oh, you're talking about here.

186
00:14:17,400 --> 00:14:19,400
Don't worry about that.

187
00:14:19,400 --> 00:14:27,400
It's talking about like Matplotlib and some other stuff, so we should be good to not restart it.

188
00:14:27,400 --> 00:14:28,400
Good catch, though.

189
00:14:28,400 --> 00:14:30,400
Thank you.

190
00:14:30,400 --> 00:14:31,400
Okay.

191
00:14:31,400 --> 00:14:40,400
So what this does is we've actually saved this report off into an HTML file, so we should be able to see like somewhere over in, yes,

192
00:14:40,400 --> 00:14:42,400
our documents over here that we have this.

193
00:14:42,400 --> 00:14:45,400
So if you wanted to, you could open it up in the browser.

194
00:14:45,400 --> 00:14:53,400
I'm just displaying it inside of our cell here, but what's really nice about this is it gives us a whole bunch of charts and insights into our data

195
00:14:53,400 --> 00:15:01,400
like I mentioned earlier, and I'm actually not going to go through it too much because that's going to be an exercise for you guys to explore the report itself,

196
00:15:01,400 --> 00:15:08,400
but you can see at the top here we've got just a bunch of headers for giving you an intro to what all it has to offer,

197
00:15:08,400 --> 00:15:14,400
but a lot there to explore for every variable that's in our data set and an easy way to do that.

198
00:15:14,400 --> 00:15:20,400
I would say before we knew about this library, Ray and I wrote a whole bunch of custom code to basically do all of this,

199
00:15:20,400 --> 00:15:24,400
so we were really happy when we found this library, really useful one.

200
00:15:24,400 --> 00:15:34,400
So I'm going to, again, let you guys explore that here in a second, but before we do that, we're going to talk through histograms,

201
00:15:34,400 --> 00:15:37,400
which are a type of bar chart that Ray mentioned.

202
00:15:37,400 --> 00:15:46,400
So for this next section here, what we're going to do is explore the titles of our reviews.

203
00:15:46,400 --> 00:15:51,400
So these are the title of those articles that people have written on the technology product reviews.

204
00:15:51,400 --> 00:15:59,400
What we're going to do actually here is see how many tokens or words are in a title on average.

205
00:15:59,400 --> 00:16:06,400
If you want to go ahead and run that cell, we will get a histogram that we can talk through.

206
00:16:06,400 --> 00:16:10,400
First, I'll talk through the code here just so we understand what's going on here.

207
00:16:10,400 --> 00:16:15,400
And again, stop me if anything's not making sense or you've got questions or anything, happy to answer those.

208
00:16:15,400 --> 00:16:21,400
So a couple things here. Don't worry too much about this warning stuff and the pandas options.

209
00:16:21,400 --> 00:16:25,400
That's me just setting some things to make sure the workshop goes well.

210
00:16:25,400 --> 00:16:28,400
But to do this plot, we're going to use a library called matplotlib.

211
00:16:28,400 --> 00:16:35,400
So I'm importing that, and then what we're going to plot, like I mentioned, is the number of words or tokens,

212
00:16:35,400 --> 00:16:39,400
which are called in natural language processing, that are in the title of our articles.

213
00:16:39,400 --> 00:16:45,400
So when we do this plot down here, on our x-axis, we get number of tokens that are in a title.

214
00:16:45,400 --> 00:16:51,400
So we're ranging all the way from like two tokens up to 26 tokens or words in a title.

215
00:16:51,400 --> 00:16:57,400
And then these are the number of articles that had that amount in them.

216
00:16:57,400 --> 00:17:00,400
So this is another way to explore a data set.

217
00:17:00,400 --> 00:17:04,400
The YData profiling library doesn't give us this out of the box.

218
00:17:04,400 --> 00:17:08,400
It's a really great starting point for some analysis, but as you kind of start exploring data,

219
00:17:08,400 --> 00:17:11,400
you'll naturally have more questions that you might want to answer.

220
00:17:11,400 --> 00:17:15,400
So this was one of those for us, and that's how we did that there.

221
00:17:15,400 --> 00:17:20,400
And with that, I think we're ready for you guys to actually code.

222
00:17:20,400 --> 00:17:30,400
So what we're going to do here is we're going to have about 10 minutes to explore that YData profiling library and the results of that.

223
00:17:30,400 --> 00:17:38,400
So we will have you guys create a new column in the data set that counts the number of words that are in the article.

224
00:17:38,400 --> 00:17:40,400
So in our example, we were looking at the title.

225
00:17:40,400 --> 00:17:44,400
We just want to do the same thing, but on the article itself.

226
00:17:44,400 --> 00:17:49,400
So we've got, I think, some helper text in there to help you guys out on this.

227
00:17:49,400 --> 00:17:58,400
I will say what we're going to do here is I'm just going to stop talking, and you guys are going to go for 10 minutes and explore, do all the exercise there.

228
00:17:58,400 --> 00:18:00,400
We would like to really encourage pair programming.

229
00:18:00,400 --> 00:18:06,400
So if you've got a neighbor next to you that you know or one that you don't know, good opportunity to meet people.

230
00:18:06,400 --> 00:18:10,400
And if you're having any challenges or anything, feel free to reach out to them.

231
00:18:10,400 --> 00:18:15,400
Ray and I will also be circling the room at that time as well to help out there, too.

232
00:18:15,400 --> 00:18:19,400
Any questions before we get going?

233
00:18:19,400 --> 00:18:21,400
Okay.

234
00:18:21,400 --> 00:18:25,400
So with that, I'm going to start a 10-minute timer.

235
00:18:25,400 --> 00:18:30,400
So let me just get that ready.

236
00:18:30,400 --> 00:18:32,400
Okay. And I'm going to go ahead and start.

237
00:18:32,400 --> 00:18:37,400
So you've got 10 minutes to work on that.

238
00:18:37,400 --> 00:18:41,400
Okay. So we are back from our 10-minute exercise.

239
00:18:41,400 --> 00:18:46,400
A quick thumbs up, thumbs down on how that went for you guys.

240
00:18:46,400 --> 00:18:47,400
Okay. Cool.

241
00:18:47,400 --> 00:18:50,400
Seen mostly thumbs up. That's awesome.

242
00:18:50,400 --> 00:18:55,400
So I'm just going to go through our way that we did this.

243
00:18:55,400 --> 00:19:01,400
Most of this is just copying the code from earlier, so shouldn't be too much to explain here.

244
00:19:01,400 --> 00:19:10,400
So first off, what we wanted you guys to do was create a new column in the data frame that calculates the number of words that are in the article, so the text of the article.

245
00:19:10,400 --> 00:19:11,400
So we did this earlier.

246
00:19:11,400 --> 00:19:19,400
We just had it on a different column, so it should have just copied that code there and changed that column in that you're referencing to article.

247
00:19:19,400 --> 00:19:20,400
Not too tricky there.

248
00:19:20,400 --> 00:19:23,400
I'll go ahead and run that on mine.

249
00:19:23,400 --> 00:19:25,400
And I will run this one as well.

250
00:19:25,400 --> 00:19:30,400
So then what we wanted you to do was just explore the profile report and all that it has to offer.

251
00:19:30,400 --> 00:19:35,400
And I heard a lot of you saying that you liked using this library maybe a first time for a lot of you.

252
00:19:35,400 --> 00:19:39,400
It's fairly new to us as well, and we found a lot of benefit out of it.

253
00:19:39,400 --> 00:19:42,400
So I really recommend that library.

254
00:19:42,400 --> 00:19:49,400
But while that is loading, actually let me go up to the sample version and I'll just talk through it a little bit.

255
00:19:49,400 --> 00:19:54,400
So at the top here, we just have some navigation for all of the things that the library offers.

256
00:19:54,400 --> 00:20:05,400
You start off with an overview of your data set, which is really nice, seeing how many rows are in your data set, if there are any duplicates in your data, what's missing, all that good stuff.

257
00:20:05,400 --> 00:20:08,400
You also get some nice alerts.

258
00:20:08,400 --> 00:20:17,400
So all of these ones that have high cardinality, that's just saying that there's a lot of distinct values or unique values in that column of your data set.

259
00:20:17,400 --> 00:20:21,400
So this is a nice way to just get a quick overview of that data set.

260
00:20:21,400 --> 00:20:33,400
And then if you ever need to reproduce this analysis, it does have a nice section here that tells you what version it was on, as well as just this JSON file with all the settings that were used to produce this report.

261
00:20:33,400 --> 00:20:37,400
Which in the data science space, reproducibility is super, super critical.

262
00:20:37,400 --> 00:20:40,400
So nice that it has that.

263
00:20:40,400 --> 00:20:44,400
And I won't go in too much because hopefully you guys had enough time to explore on your own.

264
00:20:44,400 --> 00:20:54,400
But the nice thing about this library is that for each column in your data set, what it does is it gives you some more information about that column, as well as a quick bar chart.

265
00:20:54,400 --> 00:20:57,400
You can also use this library with time series data as well.

266
00:20:57,400 --> 00:20:59,400
We will do that later.

267
00:20:59,400 --> 00:21:01,400
So it can even do that.

268
00:21:01,400 --> 00:21:05,400
You can click on more details and it has a lot of stuff for text as well.

269
00:21:05,400 --> 00:21:09,400
So what are the most popular values in the data set?

270
00:21:09,400 --> 00:21:11,400
What are the most popular characters?

271
00:21:11,400 --> 00:21:15,400
I use this a lot to check for data quality issues myself.

272
00:21:15,400 --> 00:21:19,400
Just making sure that the field is showing what I would expect it to.

273
00:21:19,400 --> 00:21:21,400
So really powerful there.

274
00:21:21,400 --> 00:21:25,400
There's also a section on interactions.

275
00:21:25,400 --> 00:21:28,400
So kind of similar to correlations.

276
00:21:28,400 --> 00:21:34,400
This doesn't really make sense for this data set because we only have one field that it can do the interactions on.

277
00:21:34,400 --> 00:21:37,400
We'll get a better understanding later in our data set.

278
00:21:37,400 --> 00:21:43,400
We've got correlations, which not really relevant for our data set.

279
00:21:43,400 --> 00:21:45,400
It could be.

280
00:21:45,400 --> 00:21:53,400
Seeing what the correlation is between the publication that the article was published in versus the score that it was given might be useful for us in our analysis.

281
00:21:53,400 --> 00:21:55,400
Might not.

282
00:21:55,400 --> 00:22:00,400
And then there's a section down here that gives you where you're missing data.

283
00:22:00,400 --> 00:22:03,400
I don't think we're missing any data in this data set.

284
00:22:03,400 --> 00:22:05,400
And finally you get a sample of the data set.

285
00:22:05,400 --> 00:22:10,400
So this would be like if you printed out like a DF.head on your data set or tail.

286
00:22:10,400 --> 00:22:16,400
It just shows you what the data looks like, which is probably useful since you guys haven't seen this before, but I've seen it several times.

287
00:22:16,400 --> 00:22:17,400
Yeah.

288
00:22:17,400 --> 00:22:29,400
Yeah, so the question was, would it run a lot slower if we did not do just the 500 sample?

289
00:22:29,400 --> 00:22:31,400
And yes, it would.

290
00:22:31,400 --> 00:22:34,400
That was why I actually scrolled up to this one because mine was still running.

291
00:22:34,400 --> 00:22:36,400
I did mine on the entire data set.

292
00:22:36,400 --> 00:22:40,400
It does take a little while and it also takes a lot of RAM.

293
00:22:40,400 --> 00:22:47,400
We run into some issues where, especially in this Colab notebook, we'll run out of memory when we're trying to work with these things.

294
00:22:47,400 --> 00:22:50,400
So bigger your data set, yeah, unfortunately the slower it does run.

295
00:22:50,400 --> 00:22:56,400
But there are a lot of settings you can set on it to limit some of the things that it's doing in the background.

296
00:22:56,400 --> 00:23:04,400
Like you can cut out the correlations and other things like that so that it's not having to compare every single column against every other single column.

297
00:23:04,400 --> 00:23:06,400
So yeah, there's ways you can limit it.

298
00:23:06,400 --> 00:23:09,400
But in general, yeah, it does run slower.

299
00:23:09,400 --> 00:23:16,400
Okay, so I am going to pass it back to Ray and he's going to talk us through how to create features.

300
00:23:16,400 --> 00:23:20,400
We just went through exploring and he's going to go through creating now.

301
00:23:26,400 --> 00:23:28,400
All righty.

302
00:23:28,400 --> 00:23:31,400
So creating features for your discrete data set.

303
00:23:31,400 --> 00:23:38,400
I think I warned ahead of time there will be a very strong NLP bent to it because that's what we do on a regular basis.

304
00:23:38,400 --> 00:23:46,400
So the first one that we'll talk about is one-hot encoding, as it's referred to.

305
00:23:46,400 --> 00:23:52,400
It ends up being really useful as an input to machine learning models.

306
00:23:52,400 --> 00:24:03,400
So if you're going to build out a machine learning model and like you have a sort of a set of discrete publications as an example this time around,

307
00:24:03,400 --> 00:24:08,400
and you want that to be an input into your machine learning model, one-hot encoding is the way to go.

308
00:24:08,400 --> 00:24:14,400
What you'll see here is that we have essentially three distinct publications listed here.

309
00:24:14,400 --> 00:24:16,400
I think your data set actually has four.

310
00:24:16,400 --> 00:24:25,400
But because we're showing three here, the length of our vector or array, however you like it, is a length of three.

311
00:24:25,400 --> 00:24:31,400
And then each position is associated with a specific publication and it's purely binary, yes or no.

312
00:24:31,400 --> 00:24:34,400
So tech radar is our first one.

313
00:24:34,400 --> 00:24:37,400
It shows up as a one in the one position.

314
00:24:37,400 --> 00:24:39,400
PC mag is in the second position.

315
00:24:39,400 --> 00:24:42,400
And then CNET is in the third one.

316
00:24:42,400 --> 00:24:47,400
And as you can see in the fourth example, it's CNET again, so it's in that third spot.

317
00:24:47,400 --> 00:24:52,400
If you were doing a one-hot encoding for the data set, which we will be doing in a little bit,

318
00:24:52,400 --> 00:24:56,400
you would see that your array would actually have four instead of three.

319
00:24:56,400 --> 00:25:05,400
It's just however many unique discrete points you have in your column of data.

320
00:25:05,400 --> 00:25:08,400
So that's one-hot encoding.

321
00:25:08,400 --> 00:25:13,400
So on the flip side, if we're looking at the output of a machine learning model

322
00:25:13,400 --> 00:25:19,400
and we're expecting it to be a discrete form of data, we would classically call that classification.

323
00:25:19,400 --> 00:25:25,400
That output, we would actually encode that using what we call as a label encoder.

324
00:25:25,400 --> 00:25:34,400
And so if we have, in this case, let's see, four distinct product categories, car, computer, audio, and mobile,

325
00:25:34,400 --> 00:25:42,400
then it's simply going to just produce a one for the first one, two for the second one, three for audio,

326
00:25:42,400 --> 00:25:44,400
and then mobile gets set to four.

327
00:25:44,400 --> 00:25:46,400
So one through N basically.

328
00:25:46,400 --> 00:25:52,400
It's just doing that quick little switch so that we have a numerical representation of our data set.

329
00:25:52,400 --> 00:25:59,400
And so label encoding, the big key point is that we generally use it for what we call the target value,

330
00:25:59,400 --> 00:26:03,400
the output of a machine learning model, whereas the one-hot encoding is for the input.

331
00:26:03,400 --> 00:26:09,400
That's a great question.

332
00:26:09,400 --> 00:26:13,400
I'm not sure that I know why it starts at one versus zero.

333
00:26:13,400 --> 00:26:18,400
I've just always seen it start at one, but I don't know that there's a reason it has to.

334
00:26:18,400 --> 00:26:20,400
Yeah.

335
00:26:20,400 --> 00:26:24,400
Okay.

336
00:26:24,400 --> 00:26:26,400
So I mentioned the text bent.

337
00:26:26,400 --> 00:26:27,400
Here we go.

338
00:26:27,400 --> 00:26:35,400
So in order to lay this out, I'll start off by sharing this basic concept of text similarity.

339
00:26:35,400 --> 00:26:39,400
So earlier we were talking about documents and how we might compare them.

340
00:26:39,400 --> 00:26:43,400
Similarity is the way to go about it.

341
00:26:43,400 --> 00:26:49,400
There are two sort of basic ways of thinking about it, lexical similarity versus semantic similarity.

342
00:26:49,400 --> 00:26:53,400
In lexical similarity, you're looking at the literal words.

343
00:26:53,400 --> 00:26:58,400
So two documents, if we say they're very similar to each other from a lexical perspective,

344
00:26:58,400 --> 00:27:05,400
it means that they share the exact same words, whereas semantic similarity is a meaning that they share the same meaning.

345
00:27:05,400 --> 00:27:10,400
So even though the words may not be the same, they're still talking about the exact same thing.

346
00:27:10,400 --> 00:27:13,400
So two different types of similarity.

347
00:27:13,400 --> 00:27:20,400
And you'll see that we will work with different features that handle each type.

348
00:27:20,400 --> 00:27:24,400
Oh, pointing the wrong way.

349
00:27:24,400 --> 00:27:26,400
Oh, sorry.

350
00:27:26,400 --> 00:27:42,400
No, you're good.

351
00:27:42,400 --> 00:27:44,400
Okay.

352
00:27:44,400 --> 00:27:46,400
So some quick use cases.

353
00:27:46,400 --> 00:27:49,400
One would be for removing duplicates.

354
00:27:49,400 --> 00:27:54,400
I've worked in a number of places where databases may have duplications in them that shouldn't be there.

355
00:27:54,400 --> 00:27:57,400
You can use similarity to root some of that out.

356
00:27:57,400 --> 00:28:02,400
Recommending stuff to people, especially when you're in the world of text.

357
00:28:02,400 --> 00:28:06,400
Also, if you're dealing with students, plagiarism, perhaps.

358
00:28:06,400 --> 00:28:12,400
And then finally, there's a kind of connection going on between the recommendation engine, search engine thing.

359
00:28:12,400 --> 00:28:21,400
There's a lot of times where there's like lexical search and then there's semantic search.

360
00:28:21,400 --> 00:28:23,400
So back to this example.

361
00:28:23,400 --> 00:28:29,400
Hasn't really changed, except we're going to talk about a couple different ways of doing this.

362
00:28:29,400 --> 00:28:33,400
So the first one, which sets our foundation, I spoke through earlier,

363
00:28:33,400 --> 00:28:43,400
is where this list in this column here is the unique set of words amongst all our documents, what we call our vocabulary.

364
00:28:43,400 --> 00:28:49,400
And then each column is for us each one of the documents that's in our corpus.

365
00:28:49,400 --> 00:28:52,400
And it's just a word count, simple word count.

366
00:28:52,400 --> 00:28:59,400
So what this column represents is basically a vector and that numerical representation for document one.

367
00:28:59,400 --> 00:29:03,400
And the next column is the numerical representation of document two.

368
00:29:03,400 --> 00:29:09,400
So similar to kind of like the one hot encoding, you have a vector that has a certain length.

369
00:29:09,400 --> 00:29:14,400
These vectors would be the length of your vocabulary, the size of your vocabulary.

370
00:29:14,400 --> 00:29:21,400
And then it's just counting up how often each word appears.

371
00:29:21,400 --> 00:29:25,400
So we're going to get to doing some fun bar chart stuff.

372
00:29:25,400 --> 00:29:27,400
And you'll see things exactly like this.

373
00:29:27,400 --> 00:29:31,400
In fact, I think you will see exactly this one when you get there.

374
00:29:31,400 --> 00:29:42,400
And that's just actioning off of those word count vectors and allows us to have a be able to paint a picture of which words are most common in our data set.

375
00:29:42,400 --> 00:29:49,400
So actually, I'm going to bounce back really fast.

376
00:29:49,400 --> 00:29:52,400
There's an interesting issue that happens in this.

377
00:29:52,400 --> 00:29:57,400
So this is coming from our product data set, product reviews.

378
00:29:57,400 --> 00:30:04,400
It's not too crazy that the word that appears most often is use because they're talking about their use of the product.

379
00:30:04,400 --> 00:30:15,400
That seems quite natural, which also means that if I were to stick these word vectors into a machine learning model, it might actually overemphasize that word.

380
00:30:15,400 --> 00:30:17,400
So we want to clean that up.

381
00:30:17,400 --> 00:30:21,400
And that's essentially what the next technique gives us.

382
00:30:21,400 --> 00:30:25,400
TF-IDF, Term Frequency Inverse Document Frequency.

383
00:30:25,400 --> 00:30:27,400
There's two core parts to it.

384
00:30:27,400 --> 00:30:31,400
There's think fractions, numerator, denominator.

385
00:30:31,400 --> 00:30:38,400
In the top is how many times did this word show up in the specific document I'm looking at.

386
00:30:38,400 --> 00:30:46,400
And in the denominator, what we're essentially looking at is how often did that same word appear in the entire corpus.

387
00:30:46,400 --> 00:30:54,400
So essentially, if the word shows up too often across all the documents, the denominator is going to grow big and squash the signal.

388
00:30:54,400 --> 00:30:59,400
It's going to compress it and say, we don't want to pay attention to this word because it shows up too much.

389
00:30:59,400 --> 00:31:01,400
It's not meaningful.

390
00:31:01,400 --> 00:31:02,400
So.

391
00:31:02,400 --> 00:31:04,400
But this might fill a word, right?

392
00:31:04,400 --> 00:31:06,400
But you may want to use the word.

393
00:31:06,400 --> 00:31:07,400
That's right.

394
00:31:07,400 --> 00:31:13,400
And there's actually a whole set of words that are like that called stop words in NLP.

395
00:31:13,400 --> 00:31:17,400
So the, it, of, if.

396
00:31:17,400 --> 00:31:20,400
These are just words that we just say, remove it altogether.

397
00:31:20,400 --> 00:31:24,400
And we're going to use a library that will automatically remove those stop words.

398
00:31:24,400 --> 00:31:28,400
But use is not a stop word in most dictionaries.

399
00:31:28,400 --> 00:31:34,400
So whenever you're working in a specific industry, there will be certain words that industry overuses.

400
00:31:34,400 --> 00:31:41,400
And TF-IDF helps squash that out.

401
00:31:41,400 --> 00:31:46,400
Cutting over the first two that we talked about are in the lexical similarity space.

402
00:31:46,400 --> 00:31:49,400
And now we're touching on the semantic very, very briefly.

403
00:31:49,400 --> 00:31:53,400
So in this space, we have embeddings.

404
00:31:53,400 --> 00:31:57,400
And they're all the rage these days.

405
00:31:57,400 --> 00:32:06,400
We have word embeddings, sentence embeddings, document embeddings, all types of embeddings that handle different sizes of text, basically.

406
00:32:06,400 --> 00:32:13,400
So certain models, even if you were looking at GPT models, they can only handle so much stuff being shoved into them at a time.

407
00:32:13,400 --> 00:32:21,400
So this is just, when we say word embedding, it's meant to consume one word at a time versus something that's relatively the length of a sentence, basically.

408
00:32:21,400 --> 00:32:29,400
But what this does for us is it captures the meaning behind the words that are in this case, we're going to use a sentence embedding.

409
00:32:30,400 --> 00:32:41,400
As opposed to, and we'll show an example where we have two strings that they do not have any words in common, but they're talking about the same thing and it matches them as being similar, somewhat similar to each other.

410
00:32:41,400 --> 00:32:46,400
So that's the nice part about this technique.

411
00:32:46,400 --> 00:32:52,400
With that, I will cut it back to Leah.

412
00:32:53,400 --> 00:32:59,400
Hey, how the heck do we do this in Python? That's what we're going to learn next.

413
00:32:59,400 --> 00:33:05,400
Okay, let me get past all my exercises.

414
00:33:05,400 --> 00:33:12,400
Okay, creating features for discrete data. So everything that Ray just talked about in the slides, we're going to do it in Python.

415
00:33:12,400 --> 00:33:16,400
And similar to last time, we're going to have some exercises for you guys to try it out yourself.

416
00:33:16,400 --> 00:33:22,400
I will say just another logistics thing. I'm going to run through the examples of how to do this stuff.

417
00:33:22,400 --> 00:33:27,400
Then we're going to go for a 15-minute break, so bio break, whatever you need to do then.

418
00:33:27,400 --> 00:33:30,400
And then we'll come back and you'll have time to do the exercise.

419
00:33:30,400 --> 00:33:35,400
Or I guess if you don't want to take a break, you can have extra time for the exercises.

420
00:33:35,400 --> 00:33:42,400
Okay, so what we're going to do here is first talk about how to do a one-hot encoding.

421
00:33:42,400 --> 00:33:48,400
You can imagine you probably could write this up yourself if you wanted to, but there's nice libraries that do this for us.

422
00:33:48,400 --> 00:33:52,400
One of the libraries that can do this for us is pandas.

423
00:33:52,400 --> 00:33:57,400
It uses a function called get dummies, kind of a funny name.

424
00:33:57,400 --> 00:34:02,400
It's creating a one-hot encoding for us. I don't know why they didn't call it one-hot encoding.

425
00:34:02,400 --> 00:34:08,400
Question for another day. So to create a one-hot encoding, you guys can go ahead and run that cell if you like.

426
00:34:08,400 --> 00:34:13,400
What we're doing is we have to pass in what data we want it to create a one-hot encoding on.

427
00:34:13,400 --> 00:34:18,400
In this example, we're passing in the publication, which I believe is what Ray had showed in the slides.

428
00:34:18,400 --> 00:34:27,400
So we're matching up there. We're saving that off as its own data frame and then doing a merge to merge that back on to our original data frame.

429
00:34:27,400 --> 00:34:31,400
If we didn't do that, we'd just have a data frame with like ones and zeros and that's it.

430
00:34:31,400 --> 00:34:36,400
But we kind of want to know what were the articles that that was actually for.

431
00:34:36,400 --> 00:34:39,400
So that's why we're joining that back on to the original data set.

432
00:34:39,400 --> 00:34:49,400
And so we can see here if we scroll over in our data set and scroll up as well, what that has done for us is create four new columns for us.

433
00:34:49,400 --> 00:34:52,400
One for each of the publications that we're working with.

434
00:34:52,400 --> 00:34:56,400
CNET, PCMag, TechRadar, and Trusted Reviews.

435
00:34:56,400 --> 00:35:03,400
And if we look over in our publication column, for example, this first row here, the publication was TechRadar.

436
00:35:03,400 --> 00:35:09,400
So what we do to create the one hot encoding is that we put a one in that column.

437
00:35:09,400 --> 00:35:14,400
And likewise, I think if we scroll down to the bottom of the data set, we can just make sure that it's working.

438
00:35:14,400 --> 00:35:21,400
We've got some ones that are in that Trusted Review publication and we get a one in that last column there.

439
00:35:21,400 --> 00:35:25,400
Scroll bars are messed up.

440
00:35:25,400 --> 00:35:28,400
OK, so that's how you can create a one hot encoding really easily.

441
00:35:28,400 --> 00:35:32,400
Any questions on that?

442
00:35:32,400 --> 00:35:35,400
Oh, yeah. So the question was, what's the left index, right index?

443
00:35:35,400 --> 00:35:38,400
That's me telling it how I want it to join.

444
00:35:38,400 --> 00:35:45,400
So behind the scenes in Pandas, actually, I think right here for us, this first column here, that is our index.

445
00:35:45,400 --> 00:35:47,400
So it's basically like a row number.

446
00:35:47,400 --> 00:35:50,400
So we have the same in both of the data sets.

447
00:35:50,400 --> 00:35:53,400
So I'm saying that's the key that I want to join on between the two.

448
00:35:53,400 --> 00:35:59,400
So it's saying from the left data frame, use the index, from the right data frame, use the index.

449
00:35:59,400 --> 00:36:01,400
And that's what you join on.

450
00:36:01,400 --> 00:36:06,400
Good question. Any other questions?

451
00:36:06,400 --> 00:36:09,400
OK, on to the next.

452
00:36:09,400 --> 00:36:14,400
So Ray mentioned that one hot encodings, that's usually used for inputs into machine learning models.

453
00:36:14,400 --> 00:36:22,400
And on the flip side of that, we have label encodings, which are used for the target value or what we try to predict in machine learning models.

454
00:36:22,400 --> 00:36:25,400
And that's where we use a label encoding.

455
00:36:25,400 --> 00:36:28,400
So you could also do this by hand if you wanted to.

456
00:36:28,400 --> 00:36:37,400
But there's a really nice library called Scikit-learn, which is a very popular machine learning library that has a built-in label encoder for us.

457
00:36:37,400 --> 00:36:39,400
You guys can go ahead and run that cell.

458
00:36:39,400 --> 00:36:43,400
And then I'm going to talk through what we're doing here.

459
00:36:43,400 --> 00:36:49,400
So first up, this line here, what I'm doing is importing the label encoder from Scikit-learn.

460
00:36:49,400 --> 00:36:54,400
And then something I'm doing here is not necessary for making the label encoding.

461
00:36:54,400 --> 00:36:59,400
So what I'm doing is just filtering down to the tech radar publication.

462
00:36:59,400 --> 00:37:02,400
That's the syntax of how you do that in pandas.

463
00:37:02,400 --> 00:37:07,400
If you haven't seen that too much before, I'm just saying where publication equals tech radar.

464
00:37:07,400 --> 00:37:11,400
And I'm saving that off as a new data frame called tech radar DF.

465
00:37:11,400 --> 00:37:14,400
And then from there, I'm taking a sample.

466
00:37:14,400 --> 00:37:18,400
Again, not necessary for label encoding, but I need this later on in the workshop.

467
00:37:18,400 --> 00:37:20,400
So we're doing it now.

468
00:37:21,400 --> 00:37:25,400
So save that off as its own data frame called tech radar sample.

469
00:37:25,400 --> 00:37:29,400
And then to actually do the label encoding, we have to initialize our label encoder.

470
00:37:29,400 --> 00:37:31,400
So that's what we're doing here.

471
00:37:31,400 --> 00:37:34,400
And I'm just calling it Ellie for shorthand.

472
00:37:34,400 --> 00:37:42,400
And from there to do the label encoding, finally, what we do is call this fit transform function off of the label encoder.

473
00:37:42,400 --> 00:37:46,400
So what that's going to do is look at our data that we're passing into it.

474
00:37:46,400 --> 00:37:50,400
In this case, we're passing in that category column from our sample data frame.

475
00:37:50,400 --> 00:37:57,400
And it takes a look at all of those and then assigns that one through end value, depending on how many categories we have in our data set.

476
00:37:57,400 --> 00:38:01,400
I'm saving that off as a new column called encoded category.

477
00:38:01,400 --> 00:38:03,400
So we can take a look at that here.

478
00:38:03,400 --> 00:38:06,400
And that's what we're going to do with this tech radar sample dot head.

479
00:38:06,400 --> 00:38:09,400
That's going to show the first five rows in our data set.

480
00:38:09,400 --> 00:38:17,400
And what we should see is we've got after our one hot encodings that we did, we have a new column called encoded category.

481
00:38:17,400 --> 00:38:19,400
And that gives us a number.

482
00:38:19,400 --> 00:38:24,400
So one through end, however many categories that we had, it assigns a value to those.

483
00:38:24,400 --> 00:38:29,400
So in this case, this category was mobile phones and it gets an eight.

484
00:38:29,400 --> 00:38:35,400
Likewise, these next two were in the computing category and those two get assigned to five.

485
00:38:35,400 --> 00:38:38,400
So it looks like that's working correctly for us.

486
00:38:38,400 --> 00:38:41,400
Any questions on labeling coding before I move on?

487
00:38:49,400 --> 00:38:55,400
Yeah, so the question was, why are we using one hot encodings for inputs and labeling codeings for outputs?

488
00:38:55,400 --> 00:38:58,400
Ray, you want to take a stab at that?

489
00:38:58,400 --> 00:39:03,400
Yeah, I don't know that I have an amazing answer for that one.

490
00:39:03,400 --> 00:39:13,400
To be fair, it's essentially what I was taught is the way to go about it, doing it for machine learning purposes to get the best result out of it.

491
00:39:13,400 --> 00:39:19,400
So I'm not sure that I have a good breakdown as to why one is better than the other.

492
00:39:19,400 --> 00:39:30,400
Most, yeah, I guess most of the classification models that I've seen use like a labeling coder on the end of it.

493
00:39:30,400 --> 00:39:37,400
And that usually works really well as opposed to trying to have it spit out an entire vector.

494
00:39:37,400 --> 00:39:39,400
I mean, it's already technically doing that.

495
00:39:39,400 --> 00:39:43,400
It's spitting out a vector with all the scores across every class.

496
00:39:43,400 --> 00:39:46,400
But yeah, I don't know that there's I have a perfect answer.

497
00:39:46,400 --> 00:39:47,400
Yes.

498
00:39:47,400 --> 00:40:02,400
So, I think it is a foreshadowing, but the fact that you're bothering to do a label encoding of the publication, are you trying to predict what the publication is based on something?

499
00:40:02,400 --> 00:40:05,400
Why would you label something that you already have?

500
00:40:05,400 --> 00:40:15,400
Yeah, so we're just messing around to give an example and also not to do the assignment part.

501
00:40:15,400 --> 00:40:23,400
In a lot of, in a use case that's been done with this data set before, you're actually predicting the product category based off of the review.

502
00:40:23,400 --> 00:40:25,400
And I have two hands.

503
00:40:25,400 --> 00:40:33,400
I don't know if this is a question, but if you want to control something, there's no implied higher than.

504
00:40:33,400 --> 00:40:35,400
Ah, thank you.

505
00:40:35,400 --> 00:40:42,400
If you want something to be a sequence, one, two, three, four, it applies to the relationship between two and one.

506
00:40:42,400 --> 00:40:44,400
That's true. Yeah, that's right.

507
00:40:44,400 --> 00:40:50,400
And so that's why a lot of times, I'm thinking a lot of people have just one box to go.

508
00:40:50,400 --> 00:41:00,400
And then for categories, it's kind of more for me, you know, one has the whole vector, keep it track and then something that's happened.

509
00:41:00,400 --> 00:41:02,400
Thank you so much.

510
00:41:02,400 --> 00:41:03,400
No, that's a fantastic.

511
00:41:03,400 --> 00:41:05,400
Great explanation.

512
00:41:05,400 --> 00:41:08,400
You want to paraphrase for the, since they're not on the mic.

513
00:41:08,400 --> 00:41:09,400
You're right. Yes.

514
00:41:09,400 --> 00:41:10,400
Thank you.

515
00:41:10,400 --> 00:41:12,400
Yeah, exactly.

516
00:41:12,400 --> 00:41:23,400
Wonderful audience member called out the fact that if you use labeling coding one through in, it gives an association of hierarchy ranking, so to speak,

517
00:41:23,400 --> 00:41:28,400
and that a one hot encoding actually prevents that from happening.

518
00:41:28,400 --> 00:41:32,400
That's my summarization, not as eloquently stated as you did.

519
00:41:32,400 --> 00:41:37,400
But yeah, that sounds dead straight, straight in line.

520
00:41:37,400 --> 00:41:38,400
Question.

521
00:41:38,400 --> 00:41:39,400
Yes.

522
00:41:39,400 --> 00:41:48,400
I missed the very end.

523
00:41:48,400 --> 00:42:00,400
Gotcha.

524
00:42:00,400 --> 00:42:01,400
Any others?

525
00:42:01,400 --> 00:42:02,400
Oh, yes.

526
00:42:02,400 --> 00:42:09,400
Yes.

527
00:42:09,400 --> 00:42:14,400
So during the so it says fit transform stepping on yours.

528
00:42:14,400 --> 00:42:21,400
The fitting part that it's doing is basically going through and identifying how many unique items are in the field.

529
00:42:21,400 --> 00:42:24,400
So that establishes what the end is.

530
00:42:24,400 --> 00:42:34,400
So actually, you can.

531
00:42:34,400 --> 00:42:37,400
Yes, because I've done that before.

532
00:42:37,400 --> 00:42:47,400
Except the situation where I did that, to be fair, was I wanted to my data set always assigned a one through in.

533
00:42:47,400 --> 00:42:52,400
But I also want to account for the situation where we didn't know what the category was.

534
00:42:52,400 --> 00:42:56,400
So you can append extra labels onto it.

535
00:42:56,400 --> 00:42:57,400
That is possible.

536
00:42:57,400 --> 00:42:58,400
I have done that before.

537
00:42:58,400 --> 00:42:59,400
We don't have an example here.

538
00:42:59,400 --> 00:43:00,400
I'm sorry.

539
00:43:00,400 --> 00:43:04,400
But yeah.

540
00:43:04,400 --> 00:43:07,400
Awesome.

541
00:43:07,400 --> 00:43:08,400
All right.

542
00:43:08,400 --> 00:43:10,400
So that was labeling coding.

543
00:43:10,400 --> 00:43:14,400
Yes, the psychic learn makes that really easy for us and does all that code in the background for it.

544
00:43:14,400 --> 00:43:26,400
And we just have to call the two lines of code basically, which is really nice.

545
00:43:26,400 --> 00:43:27,400
Right.

546
00:43:27,400 --> 00:43:31,400
Yeah, that was what Ray was mentioning is that you you do have an opportunity.

547
00:43:31,400 --> 00:43:37,400
So maybe your data set that you're working with doesn't have all the possibility of the categories that you're trying to encode.

548
00:43:37,400 --> 00:43:43,400
So that is a way you can do that is to add those on and tell it about that data as well.

549
00:43:43,400 --> 00:43:54,400
And I guess the thing that I'd add is is that this fit transform and then you encode there is a function inside kit learned to like reverse the back out of it.

550
00:43:54,400 --> 00:43:55,400
Right.

551
00:43:55,400 --> 00:43:57,400
To say, OK, here's the number you gave me.

552
00:43:57,400 --> 00:43:59,400
Give me back the actual category.

553
00:43:59,400 --> 00:44:01,400
And then the library to.

554
00:44:01,400 --> 00:44:02,400
Yep.

555
00:44:02,400 --> 00:44:03,400
Yep.

556
00:44:03,400 --> 00:44:04,400
All right.

557
00:44:04,400 --> 00:44:07,400
So moving into text.

558
00:44:07,400 --> 00:44:10,400
Next up, what we're going to do is start simple.

559
00:44:10,400 --> 00:44:16,400
We're going to start with word count and then work our way up to the sentence embeddings that Ray was talking about.

560
00:44:16,400 --> 00:44:20,400
So for word counts, I've got like a whole big paragraph of text here.

561
00:44:20,400 --> 00:44:24,400
I'm going to try to summarize it for you so you don't have to read all of that.

562
00:44:24,400 --> 00:44:32,400
One of the nice things that I can learn also provides for us is something called count vectorizer, which basically does the word count for us.

563
00:44:32,400 --> 00:44:36,400
Like I've mentioned a couple of times, you could definitely write this yourself if you wanted to.

564
00:44:36,400 --> 00:44:39,400
But why not reinvent the wheel?

565
00:44:39,400 --> 00:44:42,400
So let's use what we know and use our resources.

566
00:44:42,400 --> 00:44:45,400
So count vectorizer.

567
00:44:45,400 --> 00:44:47,400
We're going to be using that to do our word counts here.

568
00:44:47,400 --> 00:45:00,400
And then we're going to be using a library called Yellow Brick, which sits on top of a sidekit learn and does some nice charts for us so that we don't have to write hundreds and hundreds of Matplotlib code to do our plots.

569
00:45:00,400 --> 00:45:02,400
So we'll be using that one.

570
00:45:02,400 --> 00:45:04,400
It's called their frequency distribution visualizer.

571
00:45:04,400 --> 00:45:06,400
So what we'll be using.

572
00:45:06,400 --> 00:45:10,400
So for this example, you guys can go ahead and run that cell if you want.

573
00:45:10,400 --> 00:45:15,400
If you haven't already, that's going to give us our plots and I'll talk through the code here.

574
00:45:15,400 --> 00:45:20,400
So what we're doing here is we have to tell count vectorizer what do we want to count.

575
00:45:20,400 --> 00:45:30,400
In this case, we're going to use our sample data frame that we created earlier just so it doesn't take as long to run and so that the chart isn't exploded.

576
00:45:30,400 --> 00:45:39,400
But we're going to plot the top words that are in the title of our articles and our reviews that people have written about these technology products.

577
00:45:39,400 --> 00:45:45,400
So to do that, we have to define what is our corpus, which Ray was talking about earlier back in the slides.

578
00:45:45,400 --> 00:45:50,400
So in this case, we're saying the title column of that data set, that's going to be our corpus.

579
00:45:50,400 --> 00:45:52,400
I'm calling it variable corpus.

580
00:45:52,400 --> 00:45:55,400
And then from there, we're going to call count vectorizer.

581
00:45:55,400 --> 00:45:57,400
Well, initialize it here.

582
00:45:57,400 --> 00:46:00,400
So count vectorizer has a ton of parameters on it.

583
00:46:00,400 --> 00:46:04,400
I think I linked up here to all of the things you can customize on it.

584
00:46:04,400 --> 00:46:10,400
We could go into all of those, but that's probably like its own natural language processing course.

585
00:46:10,400 --> 00:46:13,400
So I'm going to just talk about two main ones that are really useful.

586
00:46:13,400 --> 00:46:17,400
First one being stop words, which Ray got to earlier.

587
00:46:17,400 --> 00:46:19,400
Steal a little bit of my thunder here.

588
00:46:19,400 --> 00:46:21,400
That's okay.

589
00:46:21,400 --> 00:46:30,400
So stop words, like he was mentioning earlier, things that just aren't really important in the English language or they get used so much like they don't really add any value if you're trying to analyze text.

590
00:46:30,400 --> 00:46:36,400
Those are things like I, but, for, not, usually short words that get used a lot.

591
00:46:36,400 --> 00:46:43,400
So what we can do in Scikit-learn, count vectorizer, which is really nice, is they have a built-in list of stop words for us.

592
00:46:43,400 --> 00:46:48,400
Every different library that you use, there's a lot out there that can do the same sort of thing.

593
00:46:48,400 --> 00:46:51,400
They each have their own list of stop words.

594
00:46:51,400 --> 00:46:58,400
It can be sometimes useful to combine those lists together or sometimes, like Ray was mentioning earlier, even create your own custom list.

595
00:46:59,400 --> 00:47:10,400
Like, for example, we work in the banking space, so words like bank, finance, those get used a lot and they don't really add anything to our analysis, but they're not by definition just a regular stop word.

596
00:47:10,400 --> 00:47:17,400
So what we do a lot of times is create a custom list that we add on to the list of words that come out of count vectorizer.

597
00:47:17,400 --> 00:47:20,400
So that is one technique you can use.

598
00:47:20,400 --> 00:47:26,400
They also have this for many different languages that you'll see work here calling out the English language.

599
00:47:26,400 --> 00:47:30,400
So those are the list of stop words that I'm using, but you could do Spanish.

600
00:47:30,400 --> 00:47:33,400
I don't even know what other languages they have support for.

601
00:47:33,400 --> 00:47:35,400
A ton out there.

602
00:47:35,400 --> 00:47:37,400
So that's that first parameter.

603
00:47:37,400 --> 00:47:40,400
Second one here is called n-gram range.

604
00:47:40,400 --> 00:47:46,400
And this gets into a concept in natural language processing around something called n-grams.

605
00:47:46,400 --> 00:47:53,400
And it's basically like how many words at a time or chunks of text at a time do you want to analyze.

606
00:47:53,400 --> 00:47:58,400
So something that's called a one-gram or a unigram would just be one word at a time.

607
00:47:58,400 --> 00:48:02,400
So we're just going to take one single word at a time, which is actually what we're doing here.

608
00:48:02,400 --> 00:48:04,400
And that's what's in our chart.

609
00:48:04,400 --> 00:48:07,400
When we print that out, you'll see we just have one word here.

610
00:48:07,400 --> 00:48:10,400
So we are printing out unigrams.

611
00:48:10,400 --> 00:48:19,400
What we could do if we wanted to change this is we could do something like bigrams, which would be taking two words at a time.

612
00:48:19,400 --> 00:48:26,400
So you could, if you want, even just change those ones to twos, rerun that cell.

613
00:48:26,400 --> 00:48:31,400
And then we should get a chart that shows us two words at a time.

614
00:48:31,400 --> 00:48:32,400
So this is really nice.

615
00:48:32,400 --> 00:48:35,400
You can start to see what phrases are people talking about in our data set.

616
00:48:35,400 --> 00:48:43,400
This is a powerful technique for exploring the data, but also for creating features for the data, which we're on the creating features section,

617
00:48:43,400 --> 00:48:46,400
but just wanted to highlight it is good for exploring as well.

618
00:48:46,400 --> 00:48:49,400
So you can go up to any number that you want there.

619
00:48:49,400 --> 00:48:53,400
It would likely decrease the chart the higher numbers you go,

620
00:48:53,400 --> 00:48:59,400
because there's probably less common phrases that people are using across all of the articles in our data set.

621
00:48:59,400 --> 00:49:02,400
But I'm going to flip this back to just unigrams.

622
00:49:02,400 --> 00:49:09,400
I should also mention that if you wanted to show unigrams and bigrams, so one word and two word at the same time,

623
00:49:09,400 --> 00:49:11,400
that's how you would do that here. It's a range.

624
00:49:11,400 --> 00:49:14,400
So you could put any numbers you want there.

625
00:49:14,400 --> 00:49:17,400
I'm going to flip it back to unigrams.

626
00:49:17,400 --> 00:49:23,400
And that kind of goes through the two important parameters for count vectorizer that we want to talk through.

627
00:49:23,400 --> 00:49:25,400
So that's us just initializing it.

628
00:49:25,400 --> 00:49:32,400
Then to actually apply it to our data set, we're going to use that fit transform function that we learned about earlier.

629
00:49:32,400 --> 00:49:38,400
So it takes a look at all of our data and then does the transformation of the counts based on what we're passing in.

630
00:49:38,400 --> 00:49:43,400
So we're passing in all the titles of our articles, so saving that off as this docs variable.

631
00:49:43,400 --> 00:49:50,400
And then for the plot to get the words to show up here, we just have to use this function get feature names out.

632
00:49:50,400 --> 00:49:55,400
That pulls out that list of words so that we can actually interpret this chart correctly.

633
00:49:55,400 --> 00:49:57,400
So we have all that.

634
00:49:57,400 --> 00:50:02,400
And then from there, we pass that into our frequency distribution visualizer from Yellowbrick.

635
00:50:02,400 --> 00:50:05,400
Setting a few parameters on there.

636
00:50:05,400 --> 00:50:09,400
You can print how many tokens or words you want to show at a time.

637
00:50:09,400 --> 00:50:13,400
I'm just doing 20. You can change that if you want to see more than that.

638
00:50:13,400 --> 00:50:17,400
And then we just have to fit that on our data set and then do a show there.

639
00:50:17,400 --> 00:50:22,400
So all that to say, we get this nice chart here of our word counts.

640
00:50:22,400 --> 00:50:29,400
And Ray kind of went into this a little bit earlier, but he mentioned that sometimes some of these words that get used,

641
00:50:29,400 --> 00:50:36,400
like in this example, review, is in almost every title, which makes sense because it's people reviewing technology products.

642
00:50:36,400 --> 00:50:42,400
So if we were going deeper in this course, probably what we would want to do is remove that from our data set,

643
00:50:42,400 --> 00:50:45,400
because that doesn't really add any value. We know all of our data set is reviews.

644
00:50:45,400 --> 00:50:48,400
So just get rid of that from the data set.

645
00:50:48,400 --> 00:50:56,400
But we're not going to actually do that today, but just wanted to highlight in case further exploration.

646
00:50:56,400 --> 00:51:01,400
OK, any questions on word count so far? Yeah.

647
00:51:01,400 --> 00:51:07,400
Correct. Yes, they are raw frequencies.

648
00:51:19,400 --> 00:51:25,400
Yeah, well, so what it's doing behind the scenes is literally going through every piece of our text here and counting up.

649
00:51:25,400 --> 00:51:30,400
How many times do I see this word? So that's what we get there.

650
00:51:30,400 --> 00:51:35,400
And you were asking about could you do that with a group by, I think.

651
00:51:35,400 --> 00:51:39,400
I think you could. We would have to finagle the data a little bit more.

652
00:51:39,400 --> 00:51:44,400
So we in this example just passed in that straight, the title itself.

653
00:51:44,400 --> 00:51:54,400
If we wanted to do a group by, we would probably need to like split that out into a list or get our data like prepped in a certain way for us to do that.

654
00:51:54,400 --> 00:51:58,400
So this just makes it a little bit easier for us to do.

655
00:52:00,400 --> 00:52:05,400
Any other questions on word count before we go to TF IDF?

656
00:52:05,400 --> 00:52:08,400
OK.

657
00:52:08,400 --> 00:52:13,400
OK. So on to TF IDF.

658
00:52:13,400 --> 00:52:20,400
So we remember word counts, those like this gentleman in the front mentioned, are just those raw counts.

659
00:52:20,400 --> 00:52:24,400
So what we get is like how we saw earlier.

660
00:52:24,400 --> 00:52:30,400
Review shows up a lot, but if we were to pass like just these word counts into a machine learning model,

661
00:52:30,400 --> 00:52:35,400
it might look at review and try to make decisions off of that, which we don't really want that to happen.

662
00:52:35,400 --> 00:52:43,400
So one technique to improve on that is TF IDF or Term Frequency Inverse Document Frequency that Ray talked us through earlier.

663
00:52:43,400 --> 00:52:47,400
So we're going to just go through how to do that on our data set.

664
00:52:47,400 --> 00:52:53,400
So scikit-learn to the rescue again has a built in method for us called TF IDF vectorizer.

665
00:52:53,400 --> 00:52:55,400
So we're just importing that.

666
00:52:55,400 --> 00:52:59,400
We have to, again, define what we're going to pass into TF IDF.

667
00:52:59,400 --> 00:53:02,400
So what do we want it to count up for us and normalize?

668
00:53:02,400 --> 00:53:05,400
In this case, we're going to do the title of our articles again.

669
00:53:05,400 --> 00:53:10,400
And one quirk about TF IDF is it expects a list of those values.

670
00:53:10,400 --> 00:53:14,400
So I'm just formatting that data set as a list there.

671
00:53:14,400 --> 00:53:18,400
So we get that. And then from there, we have to initialize TF IDF.

672
00:53:18,400 --> 00:53:22,400
We're using those same two parameters. It has a lot more.

673
00:53:22,400 --> 00:53:24,400
Those are the only two we're going to cover today.

674
00:53:24,400 --> 00:53:30,400
So again, getting rid of those English stop words, things like of, but, for stuff like that.

675
00:53:30,400 --> 00:53:34,400
And then we are, for this example, only going to go with unigrams.

676
00:53:34,400 --> 00:53:39,400
But you could change those values if you want to, like two, three, whatever number you like.

677
00:53:39,400 --> 00:53:41,400
Yes, there's a question.

678
00:53:41,400 --> 00:53:44,400
You created a corpus of your corpus.

679
00:53:44,400 --> 00:53:47,400
But I don't see where you use it.

680
00:53:47,400 --> 00:53:51,400
Oh, you are so right. That is a mistake on our part.

681
00:53:51,400 --> 00:53:55,400
I guess it doesn't have to have a list. Thank you for pointing that out.

682
00:53:55,400 --> 00:54:00,400
He was saying that we define corpus list here, but we don't actually use it.

683
00:54:00,400 --> 00:54:04,400
Maybe it might be later on potentially that we use it.

684
00:54:04,400 --> 00:54:08,400
You are correct. It's later. Sorry. It's my fault.

685
00:54:08,400 --> 00:54:12,400
Sorry about that. Thank you for pointing that out, though.

686
00:54:12,400 --> 00:54:20,400
So we define TFI IDF vectorizer, initialize it, and then we do that fit transform that we've learned about before to actually apply it to the data.

687
00:54:20,400 --> 00:54:23,400
And you might have noticed that my runtime just got disconnected.

688
00:54:23,400 --> 00:54:25,400
Don't know if that happened for anyone else.

689
00:54:25,400 --> 00:54:30,400
Not a big deal. You should just be able to click this a few times and it will come back.

690
00:54:30,400 --> 00:54:34,400
Sometimes it just kind of clears out the resources.

691
00:54:34,400 --> 00:54:36,400
It will keep all of our variables and stuff in there.

692
00:54:36,400 --> 00:54:39,400
So we just have to wait for that to come back here.

693
00:54:39,400 --> 00:54:41,400
But that should come quickly.

694
00:54:41,400 --> 00:54:45,400
Maybe.

695
00:54:45,400 --> 00:54:48,400
Always when you need it, right?

696
00:54:48,400 --> 00:54:52,400
It doesn't show up.

697
00:54:52,400 --> 00:54:54,400
Sure. Yeah.

698
00:54:54,400 --> 00:54:59,400
It doesn't know any knowledge about English phrases.

699
00:54:59,400 --> 00:55:02,400
It doesn't know any knowledge about English phrases.

700
00:55:02,400 --> 00:55:11,400
It doesn't know the correct way to talk.

701
00:55:11,400 --> 00:55:14,400
Yeah. You want to dive into that, Ray?

702
00:55:14,400 --> 00:55:21,400
Yeah. So this is kind of getting into the tokenization, like how you choose to break it up.

703
00:55:21,400 --> 00:55:23,400
There's a pre-built one.

704
00:55:23,400 --> 00:55:32,400
You can actually also define your own tokenizer if you want to, to have it choose how to break things up.

705
00:55:32,400 --> 00:55:38,400
So if I were to talk about the default one that's in here, it's basically stripping.

706
00:55:38,400 --> 00:55:40,400
It lower cases everything.

707
00:55:40,400 --> 00:55:44,400
If I recall, I think it also strips out some punctuation.

708
00:55:44,400 --> 00:55:47,400
And then we told it to strip out stop words.

709
00:55:47,400 --> 00:55:50,400
You can also tell it not to do that if you like.

710
00:55:50,400 --> 00:55:56,400
And then it's just looking at whatever two words are next to each other and kind of going across them.

711
00:55:56,400 --> 00:56:01,400
So I always do this thing of like the cat in the hat.

712
00:56:01,400 --> 00:56:03,400
So I try to not mess it up.

713
00:56:03,400 --> 00:56:08,400
The cat, cat in, in the, the hat is like your chunks.

714
00:56:08,400 --> 00:56:11,400
So it's the cat is a bigram.

715
00:56:11,400 --> 00:56:13,400
Cat in is a bigram.

716
00:56:13,400 --> 00:56:18,400
In the. So that's the way that it works across it, if that makes sense.

717
00:56:18,400 --> 00:56:22,400
And then it counts up all of those bigrams and whichever ones have the highest count.

718
00:56:22,400 --> 00:56:24,400
That's what you get in your chart.

719
00:56:24,400 --> 00:56:26,400
Does it make sense?

720
00:56:26,400 --> 00:56:27,400
Yes.

721
00:56:39,400 --> 00:56:40,400
Okay.

722
00:56:40,400 --> 00:56:41,400
Okay.

723
00:56:41,400 --> 00:56:43,400
Good call on that.

724
00:56:43,400 --> 00:56:46,400
It will give slightly results, slightly different results.

725
00:56:46,400 --> 00:56:49,400
If you don't have the exact same thing that we do.

726
00:56:49,400 --> 00:56:56,400
So just with that, if that's just me and it's using the defaults, which I think it uses stop words.

727
00:56:56,400 --> 00:56:58,400
And I think those might actually be defaults.

728
00:56:58,400 --> 00:57:00,400
I think they are technically the defaults.

729
00:57:00,400 --> 00:57:01,400
So I think you're okay.

730
00:57:01,400 --> 00:57:02,400
But yeah.

731
00:57:02,400 --> 00:57:07,400
So if you wanted to change it to not do stop words, I think that would be a problem.

732
00:57:07,400 --> 00:57:12,400
But it defaults to n-gram of one, one, a unigram.

733
00:57:12,400 --> 00:57:13,400
I know that for sure.

734
00:57:13,400 --> 00:57:16,400
And then, yeah, I think it does English stop words by default.

735
00:57:16,400 --> 00:57:17,400
Yeah.

736
00:57:17,400 --> 00:57:18,400
Thanks.

737
00:57:18,400 --> 00:57:20,400
Sorry about that, guys.

738
00:57:20,400 --> 00:57:24,400
Keeping track of differences between two notebooks is not fun.

739
00:57:24,400 --> 00:57:26,400
Awesome.

740
00:57:26,400 --> 00:57:27,400
Okay.

741
00:57:27,400 --> 00:57:30,400
Any other questions on TF IDF?

742
00:57:30,400 --> 00:57:31,400
Cool.

743
00:57:31,400 --> 00:57:32,400
Okay.

744
00:57:32,400 --> 00:57:40,400
So what we're going to do now is take TF IDF, what we learned there, and go through a text similarity example.

745
00:57:41,400 --> 00:57:47,400
So Ray talked about semantic and lexical similarity.

746
00:57:47,400 --> 00:57:51,400
What we're going to start with here is a lexical similarity technique.

747
00:57:51,400 --> 00:58:00,400
So what we're going to do here in this example is find in our data set what are the most two similar reviews to each other based on the text.

748
00:58:00,400 --> 00:58:05,400
And since this is a lexical method, this means we're literally having to look at the words themselves.

749
00:58:05,400 --> 00:58:11,400
Like, are they literally the same words between the two documents that we're looking at rather than the meanings of them?

750
00:58:11,400 --> 00:58:16,400
And so to do that, there's a bunch of different ways you can do text similarity.

751
00:58:16,400 --> 00:58:19,400
Like, tons and tons of different ways could probably be its own course.

752
00:58:19,400 --> 00:58:24,400
We actually did teach an NLP course last year, if anyone was in that.

753
00:58:24,400 --> 00:58:26,400
But we went through a lot more than this.

754
00:58:26,400 --> 00:58:29,400
But what we're going to use today is cosine similarity.

755
00:58:29,400 --> 00:58:33,400
And I'm not going to go through, like, the math behind it.

756
00:58:33,400 --> 00:58:37,400
I don't think we really have to know how that works to be able to do this example.

757
00:58:37,400 --> 00:58:46,400
But the general gist of it is we're going to take the cosine angle between two text vectors based on our TFIDF counts that we got.

758
00:58:46,400 --> 00:58:51,400
So to do that, you guys can go ahead and run that cell if you like.

759
00:58:51,400 --> 00:58:57,400
If you haven't already, that's going to start printing out a bunch of numbers here that we'll explain here in a second.

760
00:58:57,400 --> 00:59:01,400
So Scikit-learn has a lot of different ways that you can do text similarity.

761
00:59:01,400 --> 00:59:03,400
We're going to import cosine similarity there.

762
00:59:03,400 --> 00:59:09,400
And we're going to run that on our TFIDF vector that we created earlier.

763
00:59:09,400 --> 00:59:15,400
So to do that, we're just going to call that cosine similarity function, pass in our TFIDF that we created.

764
00:59:15,400 --> 00:59:18,400
And then we're going to save that off as this cosine similarity matrix.

765
00:59:18,400 --> 00:59:23,400
And what I'm doing here is just printing out the first 10 rows in our matrix.

766
00:59:23,400 --> 00:59:29,400
So what this is doing is taking every single data point in our data set.

767
00:59:29,400 --> 00:59:32,400
And we're working on the titles.

768
00:59:32,400 --> 00:59:34,400
So we're working on the title of our article.

769
00:59:34,400 --> 00:59:40,400
So it's going to compare every single title of each article that's in our data set to each other.

770
00:59:40,400 --> 00:59:44,400
And that's what this matrix that we're printing out is showing.

771
00:59:44,400 --> 00:59:50,400
You'll notice here across this diagonal, maybe it's a little bit hard to see, I suppose.

772
00:59:50,400 --> 00:59:56,400
But you should be able to see on your screen there starts to be this diagonal of all ones here.

773
00:59:56,400 --> 01:00:01,400
Before I explain that, let me back up and say cosine similarity, it goes between 0 and 1.

774
01:00:01,400 --> 01:00:06,400
1 being two things are identical to each other and 0 meaning they are not at all similar.

775
01:00:06,400 --> 01:00:15,400
So this diagonal of ones here, what that's showing us is that it's actually the review or the title of our article compared with itself.

776
01:00:15,400 --> 01:00:18,400
So if you're comparing something to itself, it's going to be exactly identical.

777
01:00:18,400 --> 01:00:22,400
So we get that value of 1 and we get that across the diagonal.

778
01:00:22,400 --> 01:00:29,400
But otherwise here what we get are all the individual cosine similarity scores between each of our other data points.

779
01:00:29,400 --> 01:00:32,400
So that's what our matrix is showing here.

780
01:00:32,400 --> 01:00:35,400
Yeah, question.

781
01:00:35,400 --> 01:00:39,400
Is it unusual that the fourth row has all zeros except for the diagonal?

782
01:00:39,400 --> 01:00:43,400
Yeah, it's not similar to anything else.

783
01:00:43,400 --> 01:00:44,400
Yeah.

784
01:00:44,400 --> 01:00:47,400
And remember this is lexical similarity.

785
01:00:47,400 --> 01:00:51,400
So we're talking about they share words in common.

786
01:00:52,400 --> 01:00:56,400
So nothing else has similar words in it basically.

787
01:00:56,400 --> 01:00:58,400
Yeah, at least so far in this printout.

788
01:00:58,400 --> 01:01:02,400
It's definitely possible maybe somewhere across here.

789
01:01:02,400 --> 01:01:05,400
Yeah.

790
01:01:05,400 --> 01:01:08,400
Could point to a data quality issue potentially in our data set.

791
01:01:08,400 --> 01:01:12,400
Maybe something that we need to investigate and clean up.

792
01:01:12,400 --> 01:01:15,400
It's a good call out there.

793
01:01:15,400 --> 01:01:19,400
So we got this matrix, but what the heck does that matter to us?

794
01:01:19,400 --> 01:01:21,400
Why would we even use this?

795
01:01:21,400 --> 01:01:31,400
Well, what we can do there is actually take our matrix and with a little bit of matrix math, we can get the two most similar article titles to each other.

796
01:01:31,400 --> 01:01:33,400
So that's what we're going to do here.

797
01:01:33,400 --> 01:01:34,400
You can go ahead and run this cell.

798
01:01:34,400 --> 01:01:37,400
I'm going to explain here what we're doing.

799
01:01:37,400 --> 01:01:42,400
So to do this matrix manipulation that we're going to be doing, nothing too crazy.

800
01:01:42,400 --> 01:01:45,400
So we're going to be like you don't need linear algebra or anything to do this.

801
01:01:45,400 --> 01:01:50,400
So don't stress if you are trying to rack your brain on matrix manipulation.

802
01:01:50,400 --> 01:01:54,400
We're going to use a library called NumPy, which is also very popular.

803
01:01:54,400 --> 01:01:56,400
You guys probably use this library.

804
01:01:56,400 --> 01:01:58,400
I don't need to explain it.

805
01:01:58,400 --> 01:02:03,400
But what we're going to do is find where is the highest value in our data set.

806
01:02:03,400 --> 01:02:06,400
So we'd be looking for a one, presumably.

807
01:02:06,400 --> 01:02:12,400
But you'll notice we've got this whole diagonal of ones that I just explained is a review or the sorry, I keep saying review.

808
01:02:12,400 --> 01:02:15,400
It's the article title compared with itself.

809
01:02:15,400 --> 01:02:21,400
So we don't want that to show up as our most similar item because it's the thing compared with itself.

810
01:02:21,400 --> 01:02:28,400
So what we're going to do is use NumPy fill diagonal and then pass in our matrix and tell it what we want to fill it with.

811
01:02:28,400 --> 01:02:34,400
So I'm going to fill that with zeros so that it doesn't pick up on those when we do our max here in a second.

812
01:02:34,400 --> 01:02:36,400
So we filled the diagonal.

813
01:02:36,400 --> 01:02:41,400
And then after that, what we're doing is using this argware function from NumPy.

814
01:02:41,400 --> 01:02:46,400
And that just allows us to search through the matrix and say what we want to get out from it.

815
01:02:46,400 --> 01:02:51,400
So in this case, I'm searching my matrix and saying where is the max value.

816
01:02:51,400 --> 01:02:53,400
So that gives me the indices of where that is.

817
01:02:53,400 --> 01:03:03,400
And then down here, I am printing out the variable or sorry, the title of the review that were most similar to each other.

818
01:03:03,400 --> 01:03:06,400
So we print the maximum cosine similarity.

819
01:03:06,400 --> 01:03:09,400
We did have a one in this case, so they're perfectly similar.

820
01:03:09,400 --> 01:03:13,400
And then we print out that value as well for the title.

821
01:03:13,400 --> 01:03:18,400
So we get Xiaomi, Mi, three review, and then the same thing but with a nine.

822
01:03:18,400 --> 01:03:22,400
So Ray, remind me, what did we say on these numbers here?

823
01:03:22,400 --> 01:03:24,400
So they're not technically identical.

824
01:03:24,400 --> 01:03:26,400
They're two different numbers.

825
01:03:26,400 --> 01:03:31,400
Yeah, but essentially there's a couple things that could be happening here.

826
01:03:31,400 --> 01:03:34,400
We haven't explored it deeply.

827
01:03:34,400 --> 01:03:39,400
Remind me, this one we're running off of TF-IDF.

828
01:03:39,400 --> 01:03:46,400
So one of the things that can happen is that your numbers are getting squashed down with TF-IDF.

829
01:03:46,400 --> 01:03:49,400
This data set is a bunch of product reviews.

830
01:03:49,400 --> 01:03:55,400
And if you think about it, a lot of companies use numbers to name the version of iPhone 11, iPhone whatever.

831
01:03:55,400 --> 01:03:58,400
So numbers kind of get squashed out a bit.

832
01:03:58,400 --> 01:04:02,400
And so those vectors end up being really, really close to each other.

833
01:04:02,400 --> 01:04:10,400
It's probably not technically perfectly one, but it's close enough and it just kind of rounds up to it, if that makes sense.

834
01:04:10,400 --> 01:04:13,400
Thanks.

835
01:04:13,400 --> 01:04:18,400
That wraps us up on lexical similarity.

836
01:04:18,400 --> 01:04:24,400
We're going to go to semantic similarity, but I'll pause for questions before that.

837
01:04:24,400 --> 01:04:26,400
Yes?

838
01:04:30,400 --> 01:04:33,400
Sorry, I did not catch that all the way.

839
01:04:38,400 --> 01:04:39,400
Right.

840
01:04:39,400 --> 01:04:41,400
So the question was what we're analyzing it on.

841
01:04:41,400 --> 01:04:49,400
And yes, so what we did our TF-IDF on earlier was just the title of the article, so not the actual content of them themselves.

842
01:04:49,400 --> 01:04:53,400
I think in our next example, we are still using title.

843
01:04:53,400 --> 01:05:02,400
But you could definitely, if you wanted to play around, go back in there and just switch out that column name that you're talking about rather than the title, use the article text itself.

844
01:05:02,400 --> 01:05:05,400
Good question.

845
01:05:05,400 --> 01:05:06,400
Okay.

846
01:05:06,400 --> 01:05:09,400
We're going to go on to sentence transformers.

847
01:05:09,400 --> 01:05:16,400
So like Ray mentioned, there's a lot of, or sentence embeddings rather, transformers embeddings kind of same thing.

848
01:05:16,400 --> 01:05:24,400
So there's a lot of different ways you can do this, like at the word level, at the sentence level, at the document level, probably even larger than that if you like.

849
01:05:24,400 --> 01:05:29,400
And there's popular ways to do that, or popular methods for it.

850
01:05:29,400 --> 01:05:33,400
Two popular ones are Word2vec and BERT.

851
01:05:33,400 --> 01:05:34,400
So just letting you guys know about those.

852
01:05:34,400 --> 01:05:38,400
And I've got links to those in case you want to check those out on your own time.

853
01:05:38,400 --> 01:05:45,400
But what we're going to do, if you want to go ahead and run that cell there, is use a library called sentence transformers that makes this pretty easy for us.

854
01:05:45,400 --> 01:05:48,400
So we're going to install that into our environment.

855
01:05:48,400 --> 01:05:52,400
And while that is running, I'm going to talk through this next section here.

856
01:05:52,400 --> 01:05:57,400
So what we're doing with this library is we have to tell it what model we want to use.

857
01:05:57,400 --> 01:05:59,400
Don't worry about that too much.

858
01:05:59,400 --> 01:06:03,400
This is just the name of the model for the sentence transformers library.

859
01:06:03,400 --> 01:06:08,400
Something called all mini LM and some version number there.

860
01:06:08,400 --> 01:06:12,400
So that's the model we're going to be using to create our sentence embeddings.

861
01:06:12,400 --> 01:06:18,400
And what we're going to do is just pass in what we want it to encode or transform in our data set.

862
01:06:18,400 --> 01:06:24,400
So this doesn't print out a billion different things and it runs pretty quickly.

863
01:06:24,400 --> 01:06:28,400
We're going to limit this to only the first five titles in our data set.

864
01:06:28,400 --> 01:06:32,400
Or I guess maybe six if we're indexing at zero.

865
01:06:32,400 --> 01:06:38,400
Is this a Word2vec or a BERT transform?

866
01:06:38,400 --> 01:06:39,400
Oh, that's a great question.

867
01:06:39,400 --> 01:06:42,400
It's actually BERT.

868
01:06:42,400 --> 01:06:49,400
So the sentence transformer site, if you go look at their documentation, they mainly work off of BERT based models.

869
01:06:49,400 --> 01:06:53,400
Good question.

870
01:06:53,400 --> 01:06:55,400
So we will define what we're going to do.

871
01:06:55,400 --> 01:06:57,400
We're going to do that first five titles in our data set.

872
01:06:57,400 --> 01:07:04,400
And then here what we do to actually create the embedding is we call this model and then we do dot encode on it.

873
01:07:04,400 --> 01:07:08,400
So if you guys want to go ahead and run that, it should print out a bunch of stuff.

874
01:07:08,400 --> 01:07:11,400
We're just going to show you what an embedding looks like.

875
01:07:11,400 --> 01:07:13,400
It probably doesn't really make sense to most of us.

876
01:07:13,400 --> 01:07:15,400
We can't really interpret it.

877
01:07:15,400 --> 01:07:24,400
But this is just to show what all it's doing to create it basically into numbers for a computer to understand from the text.

878
01:07:24,400 --> 01:07:26,400
So I can scroll back up here.

879
01:07:26,400 --> 01:07:29,400
What we do is just print out what title are we looking at.

880
01:07:29,400 --> 01:07:32,400
It gives you the dimensions or the size.

881
01:07:32,400 --> 01:07:34,400
And then here is the actual embedding.

882
01:07:34,400 --> 01:07:36,400
So can't really interpret that at all.

883
01:07:36,400 --> 01:07:42,400
So not really useful for us, but just wanted to show what an embedding ends up looking like.

884
01:07:42,400 --> 01:07:44,400
And so same thing here.

885
01:07:44,400 --> 01:07:52,400
What we can do actually is get two titles and see which ones are most similar based on the semantic way.

886
01:07:52,400 --> 01:07:58,400
So remember semantic is they don't literally have to share the same words, but rather the same meaning behind the words.

887
01:07:58,400 --> 01:08:04,400
So here what we're going to do is just take two indices since we only had six in our data set.

888
01:08:04,400 --> 01:08:07,400
I'm just defining the two that I want to compare to each other.

889
01:08:07,400 --> 01:08:09,400
We're going to print those out.

890
01:08:09,400 --> 01:08:13,400
We're going to encode them individually each.

891
01:08:13,400 --> 01:08:17,400
And then we're going to run a cosine similarity against those.

892
01:08:17,400 --> 01:08:19,400
That's the technique we learned about earlier.

893
01:08:19,400 --> 01:08:24,400
But rather instead of passing in tf.idf, this time we're going to pass in those sentence embeddings.

894
01:08:24,400 --> 01:08:28,400
Don't worry too much about this reshaping stuff.

895
01:08:28,400 --> 01:08:31,400
To use cosine similarity, we have to get it in a certain format.

896
01:08:31,400 --> 01:08:33,400
So that's what we're doing there.

897
01:08:33,400 --> 01:08:39,400
So if you want to run that, what that will do is print out the two title of our articles that we're comparing against each other.

898
01:08:39,400 --> 01:08:46,400
So we've got one that's a B-link, T4, Windows thin client review, and then one that's a HP Star Wars special edition notebook review.

899
01:08:46,400 --> 01:08:55,400
So you'll see that they I guess only have that review word in common, but everything else seems to be pretty different.

900
01:08:55,400 --> 01:09:00,400
But we get this cosine similarity score of 0.21 if you round up.

901
01:09:00,400 --> 01:09:08,400
So not a super high score, but what this is telling us is that even though they don't really share a ton of the same words, they mean the same thing.

902
01:09:08,400 --> 01:09:13,400
So a thin client and a special edition notebook are probably both laptops.

903
01:09:13,400 --> 01:09:21,400
So this is recognizing that those are both talking about the same thing and being able to score those as similar to each other.

904
01:09:21,400 --> 01:09:25,400
Any questions there before we pause for a break?

905
01:09:25,400 --> 01:09:26,400
Yes.

906
01:09:26,400 --> 01:09:32,400
The cosine similarity, that's of the 384 original vectors.

907
01:09:32,400 --> 01:09:35,400
Yep. Yep. Question up here.

908
01:09:35,400 --> 01:09:39,400
Does the model have to be trained using cosine similarity?

909
01:09:39,400 --> 01:09:45,400
So the question was, does the model have to be trained using cosine similarity?

910
01:09:45,400 --> 01:09:51,400
Not to my knowledge. It doesn't have to have cosine similarity in its training.

911
01:09:51,400 --> 01:09:59,400
So the word embeddings is one piece of technology that allows you to represent some text semantically.

912
01:09:59,400 --> 01:10:08,400
And then you can take the cosine similarity between any two vectors and see how similar the vectors are to each other,

913
01:10:08,400 --> 01:10:12,400
which maps back to how similar the original text was, if that makes sense.

914
01:10:12,400 --> 01:10:15,400
But they don't have to be trained with any cosine similarity.

915
01:10:21,400 --> 01:10:24,400
So where is the connection between the two vectors?

916
01:10:24,400 --> 01:10:27,400
Do they have to define a distance function?

917
01:10:27,400 --> 01:10:30,400
I understand. Yes. So that's true.

918
01:10:30,400 --> 01:10:39,400
So there is a form of what you might, well, not all embeddings actually function quite that way, actually.

919
01:10:39,400 --> 01:10:43,400
But I get what you're saying by defining a distance function.

920
01:10:43,400 --> 01:10:46,400
There is a distance function.

921
01:10:46,400 --> 01:10:54,400
I'm just trying to say it doesn't have to be the exact same distance function is, I guess, what I'm trying to express.

922
01:10:54,400 --> 01:11:01,400
Because there's a lot of models that you can plug the embeddings into that function off of different distance functions.

923
01:11:01,400 --> 01:11:07,400
And yet the vectors are still close to each other, relatively speaking, in space semantically,

924
01:11:07,400 --> 01:11:13,400
even though the way you're calculating the distance may be slightly different, if that makes sense.

925
01:11:13,400 --> 01:11:17,400
So it's not required that it uses cosine similarity.

926
01:11:17,400 --> 01:11:20,400
Yeah. Yeah.

927
01:11:20,400 --> 01:11:27,400
So you could do as you did above, compute the cosine similarity over the whole n by n of the titles,

928
01:11:27,400 --> 01:11:31,400
find the largest, you could find the most common titles.

929
01:11:31,400 --> 01:11:32,400
Absolutely.

930
01:11:32,400 --> 01:11:38,400
The problem is that this Google CoLab is free and it doesn't have enough RAM to do that.

931
01:11:38,400 --> 01:11:40,400
But yes, absolutely.

932
01:11:40,400 --> 01:11:42,400
Yes.

933
01:11:42,400 --> 01:11:44,400
Yes.

934
01:11:44,400 --> 01:11:49,400
That's right.

935
01:11:49,400 --> 01:11:51,400
Yeah.

936
01:11:56,400 --> 01:11:57,400
Great question.

937
01:11:57,400 --> 01:12:05,400
So the library, we're not limiting the encodings, but we limited the list again because we're running on a free CoLab that doesn't have enough RAM.

938
01:12:05,400 --> 01:12:10,400
I actually ran it with the whole thing originally and it died because it did not have enough RAM.

939
01:12:10,400 --> 01:12:14,400
And then I backed it off so that it could run successfully here.

940
01:12:14,400 --> 01:12:18,400
We were just looking at the parts and they're like, they're nothing alike, you know?

941
01:12:18,400 --> 01:12:21,400
Well, they're both actually referring to compute.

942
01:12:21,400 --> 01:12:25,400
So semantically, they actually have some similarity to each other.

943
01:12:25,400 --> 01:12:27,400
But the similarity score you'll note is point two.

944
01:12:27,400 --> 01:12:29,400
A perfect score is one.

945
01:12:29,400 --> 01:12:31,400
So it's not a high score by any means.

946
01:12:31,400 --> 01:12:34,400
But they are both in reference to compute.

947
01:12:34,400 --> 01:12:38,400
So there's a little bit of relationship, which is what it's picking out.

948
01:12:38,400 --> 01:12:45,400
If we ran a different type of vectorization technique, it would be zero probably all day, every day.

949
01:12:45,400 --> 01:12:49,400
Because there's like words wise, they're not similar other than review.

950
01:12:49,400 --> 01:12:51,400
Yeah.

951
01:12:51,400 --> 01:12:53,400
Yes.

952
01:12:55,400 --> 01:13:02,400
How is embedding different from the word count?

953
01:13:02,400 --> 01:13:05,400
An embedding is different versus what?

954
01:13:05,400 --> 01:13:08,400
Have I tried to do a company that has a word count?

955
01:13:08,400 --> 01:13:15,400
So like word counts and TF-IDF, how are they different from each other is what you're asking.

956
01:13:15,400 --> 01:13:16,400
Yes.

957
01:13:16,400 --> 01:13:18,400
How it's different from an embedding.

958
01:13:18,400 --> 01:13:19,400
Yeah.

959
01:13:19,400 --> 01:13:20,400
Yeah.

960
01:13:20,400 --> 01:13:26,400
So I'll try to express it as simply as I can.

961
01:13:26,400 --> 01:13:32,400
In the case of word counts and TF-IDF, that is basically a number.

962
01:13:32,400 --> 01:13:40,400
The vector that's being produced is just a direct calculation of which words appear in the document.

963
01:13:40,400 --> 01:13:45,400
Whereas the word embedding, what it attempts to do is to construct a vector in space

964
01:13:45,400 --> 01:13:50,400
so that if two words are similar to each other, they're close to each other.

965
01:13:50,400 --> 01:13:56,400
So they don't have to be the exact same word for you to actually see that they're similar to each other.

966
01:13:56,400 --> 01:14:02,400
So a classic example is with analogies where they say like king and queen.

967
01:14:02,400 --> 01:14:07,400
Those would be the vectors for king and queen in a word embedding would be close to each other because they're similar.

968
01:14:07,400 --> 01:14:10,400
They're not the same thing, but they're close to each other in space.

969
01:14:11,400 --> 01:14:17,400
And if you ran a cosine similarity on king and queen, you would see that the similarity score is pretty high.

970
01:14:17,400 --> 01:14:24,400
Whereas if you did TF-IDF or word vector, the similarity is zero because they aren't the same word.

971
01:14:24,400 --> 01:14:27,400
So that's the difference, if that makes sense.

972
01:14:31,400 --> 01:14:34,400
Awesome. Great questions, everyone.

973
01:14:34,400 --> 01:14:39,400
So I think that leads us up to our exercises.

974
01:14:39,400 --> 01:14:45,400
But before we do that, we're going to actually take a 15-minute bio break for everyone.

975
01:14:45,400 --> 01:14:48,400
So it's probably very well needed at this point.

976
01:14:48,400 --> 01:14:51,400
So go ahead and get on for your break.

977
01:14:51,400 --> 01:14:54,400
If you want to get a head start, you can read through the exercises.

978
01:14:54,400 --> 01:14:56,400
But otherwise, I'll come back and talk about those.

979
01:14:57,400 --> 01:15:04,400
I think we are ready to come back here.

980
01:15:04,400 --> 01:15:06,400
Hope everyone enjoyed their break.

981
01:15:06,400 --> 01:15:12,400
I'll give you guys just a second to wrap up your last combo.

982
01:15:12,400 --> 01:15:15,400
Cool. Okay. So we are back.

983
01:15:15,400 --> 01:15:21,400
What we're going to actually dive into now is some exercises for you guys to have some fun with.

984
01:15:21,400 --> 01:15:25,400
So if you haven't already gotten a head start on these during the break, no worries.

985
01:15:25,400 --> 01:15:31,400
We're going to give you guys about 15 minutes to go through two exercises here.

986
01:15:31,400 --> 01:15:38,400
And so what we're going to do in this first one is we're going to do, or it's all about really just stuff that we've done already,

987
01:15:38,400 --> 01:15:41,400
just kind of further expanding on it a bit.

988
01:15:41,400 --> 01:15:48,400
So what we're going to do here, this is talking about similarity between product reviews or between a certain product category.

989
01:15:48,400 --> 01:15:53,400
So in our data set earlier, we were looking at, at one point we were looking at the entire data set,

990
01:15:53,400 --> 01:15:57,400
and then another point we scaled it down to just like five examples.

991
01:15:57,400 --> 01:16:02,400
In this example, what we want to do is filter down the data set to a specific product category.

992
01:16:02,400 --> 01:16:05,400
Doesn't matter which one you choose and choose whatever one you like.

993
01:16:05,400 --> 01:16:14,400
You'll do that, and then you'll use the techniques we learned earlier to figure out which two reviews within that category are most similar to each other.

994
01:16:14,400 --> 01:16:18,400
So I should be able to use some of the code that we learned earlier.

995
01:16:18,400 --> 01:16:26,400
And I don't think I specify like what to how or how to like create the data behind the scenes for it.

996
01:16:26,400 --> 01:16:29,400
So feel free to use your creativity there.

997
01:16:29,400 --> 01:16:31,400
So that's activity one.

998
01:16:31,400 --> 01:16:38,400
And then activity two is getting to a little bit of machine learning and using a technique called nearest neighbors.

999
01:16:38,400 --> 01:16:41,400
If you don't know what that is, it's no big deal at all.

1000
01:16:41,400 --> 01:16:45,400
We've actually, I think, got most of the code in there for you guys to use.

1001
01:16:45,400 --> 01:16:52,400
But one of the things we have to do a lot is find similar documents to the ones that we're currently looking at.

1002
01:16:52,400 --> 01:16:58,400
And we talked about how that can be used for like records matching, duplicate records, plagiarism, all that stuff.

1003
01:16:58,400 --> 01:17:03,400
But one way to do that is through like a nearest neighbors unsupervised machine learning model.

1004
01:17:03,400 --> 01:17:09,400
So what we'll have you guys do is use that machine learning model to find the most similar articles.

1005
01:17:09,400 --> 01:17:11,400
And we've got some helper text in there.

1006
01:17:11,400 --> 01:17:14,400
Again, use your neighbors pair programming.

1007
01:17:14,400 --> 01:17:17,400
And then Ray and I will be walking around as well if you've got questions in the meantime.

1008
01:17:17,400 --> 01:17:21,400
So I'm going to set a timer for 15 minutes.

1009
01:17:21,400 --> 01:17:24,400
And you guys can get going on that.

1010
01:17:42,400 --> 01:17:46,400
All right, guys. We've had our 15 minutes.

1011
01:17:46,400 --> 01:17:49,400
That actually went by really fast. I don't know if it did for everyone else.

1012
01:17:49,400 --> 01:17:55,400
So quick like thumbs up, thumbs down on how this one went or maybe.

1013
01:17:55,400 --> 01:18:01,400
OK, yeah, this one was a little bit more challenging than the last one, for sure.

1014
01:18:01,400 --> 01:18:05,400
So I think, yeah, let's just start going through the code.

1015
01:18:05,400 --> 01:18:09,400
This was how we chose to solve it.

1016
01:18:09,400 --> 01:18:13,400
The beauty of coding is that you don't have to have the exact same answer as us.

1017
01:18:13,400 --> 01:18:15,400
This is just one way to do it.

1018
01:18:15,400 --> 01:18:19,400
But first thing we had you guys do is pick your own category that you wanted to work on.

1019
01:18:19,400 --> 01:18:22,400
So for us, we chose the cameras category.

1020
01:18:22,400 --> 01:18:29,400
So using that syntax here for filtering down a data frame so that we're only working with the cameras category.

1021
01:18:29,400 --> 01:18:31,400
I'm just calling that subset.

1022
01:18:31,400 --> 01:18:33,400
So I'll run that.

1023
01:18:33,400 --> 01:18:35,400
And then what we chose to do.

1024
01:18:35,400 --> 01:18:39,400
So you guys could have chosen to do TFIDF or another method.

1025
01:18:39,400 --> 01:18:45,400
We chose to do sentence embeddings here, because that would be the most complicated thing that anyone would be doing here.

1026
01:18:45,400 --> 01:18:49,400
So this required a little bit of reshaping to do that.

1027
01:18:49,400 --> 01:18:51,400
So a little bit tricky there.

1028
01:18:51,400 --> 01:18:55,400
But I'm going to go ahead and run that and we'll see what that prints out for us.

1029
01:18:55,400 --> 01:18:58,400
So we chose to do sentence embeddings, like I said.

1030
01:18:58,400 --> 01:19:03,400
So what we did was calculate those using the code that we learned earlier.

1031
01:19:03,400 --> 01:19:10,400
And we just have to have two for loops here to loop through all of our data to create that cosine similarity matrix.

1032
01:19:10,400 --> 01:19:13,400
Calculate that cosine similarity there.

1033
01:19:13,400 --> 01:19:18,400
And then we are just creating our matrix basically with this append here.

1034
01:19:18,400 --> 01:19:25,400
So when we do all that, it's going to keep printing for quite a while.

1035
01:19:25,400 --> 01:19:28,400
But we get that nice diagonal of ones.

1036
01:19:28,400 --> 01:19:30,400
So that's the review compared with itself.

1037
01:19:30,400 --> 01:19:36,400
And then the rest of our scores are those cosine similarity scores for our sentence embeddings.

1038
01:19:36,400 --> 01:19:40,400
Once that finishes, I think that it's getting close to run.

1039
01:19:40,400 --> 01:19:43,400
Maybe.

1040
01:19:43,400 --> 01:19:48,400
So then the last part of that was a bonus, like if you had extra time to do that.

1041
01:19:48,400 --> 01:19:52,400
So this was finding the two articles with the highest similarity.

1042
01:19:52,400 --> 01:19:56,400
And so we did that similar to what we did earlier.

1043
01:19:56,400 --> 01:20:05,400
Since we did these sentence embeddings, what we had to do was reshape our matrix so that we would be able to fill the diagonal on it.

1044
01:20:05,400 --> 01:20:13,400
So that's what we're doing here is just reshaping it based on how many titles that we had in our data set that we were working with.

1045
01:20:13,400 --> 01:20:15,400
So that's what we did there.

1046
01:20:15,400 --> 01:20:19,400
Create that similarity matrix and then fill the diagonal like we did earlier.

1047
01:20:19,400 --> 01:20:21,400
So set that with zeros.

1048
01:20:21,400 --> 01:20:23,400
Get the max.

1049
01:20:23,400 --> 01:20:25,400
And then print out those values.

1050
01:20:25,400 --> 01:20:33,400
The two most similar that we found were, had a cosine similarity based on the sentence embeddings of like .77.

1051
01:20:33,400 --> 01:20:35,400
And these were our two reviews.

1052
01:20:35,400 --> 01:20:39,400
So looks like two like Panasonic cameras, I think.

1053
01:20:39,400 --> 01:20:41,400
Is Panasonic a camera?

1054
01:20:41,400 --> 01:20:43,400
I'm not sure.

1055
01:20:43,400 --> 01:20:45,400
But sounds like two similar products to each other.

1056
01:20:45,400 --> 01:20:51,400
Yeah.

1057
01:20:51,400 --> 01:20:57,400
Yeah.

1058
01:20:57,400 --> 01:21:03,400
That was the one we created earlier, the BERT sentence transformer from that sentence transformer library.

1059
01:21:03,400 --> 01:21:07,400
Yep, yep.

1060
01:21:07,400 --> 01:21:11,400
And I should mention we're going to send out this instructor's notebook afterwards.

1061
01:21:11,400 --> 01:21:13,400
You guys have the answers to everything.

1062
01:21:13,400 --> 01:21:15,400
So no stress if we're going pretty fast.

1063
01:21:15,400 --> 01:21:21,400
I know we're moving fast and we're actually a little bit short on time, too, unfortunately.

1064
01:21:21,400 --> 01:21:25,400
So trying to keep moving through.

1065
01:21:25,400 --> 01:21:27,400
So that was that first exercise.

1066
01:21:27,400 --> 01:21:31,400
The next one was using nearest neighbors.

1067
01:21:31,400 --> 01:21:33,400
And a lot of this code we already had in there for you guys.

1068
01:21:33,400 --> 01:21:35,400
This was actually something we did at our work.

1069
01:21:35,400 --> 01:21:39,400
So we decided to include it in here as well.

1070
01:21:39,400 --> 01:21:41,400
Just as a real life use case.

1071
01:21:41,400 --> 01:21:45,400
So what we had was this first cell here I think that was already given for you guys.

1072
01:21:45,400 --> 01:21:49,400
This is running a K nearest neighbors model from scikit-learn.

1073
01:21:49,400 --> 01:21:57,400
And I think using TFIDF, yep, to transform our text for us.

1074
01:21:57,400 --> 01:22:05,400
All of this junk down here is just getting some extra information to return back the results appropriately so that we can interpret them.

1075
01:22:05,400 --> 01:22:09,400
So don't stress too much about that.

1076
01:22:09,400 --> 01:22:11,400
This was also I think given to you guys.

1077
01:22:11,400 --> 01:22:13,400
So we had to reset the index on our data frame.

1078
01:22:13,400 --> 01:22:15,400
So that's what we're doing there.

1079
01:22:15,400 --> 01:22:19,400
Then we go into where you guys came in.

1080
01:22:19,400 --> 01:22:25,400
So this was creating a feature using one of the techniques that we learned is to be the input for the model.

1081
01:22:25,400 --> 01:22:29,400
So for this one we chose to do TFIDF.

1082
01:22:29,400 --> 01:22:31,400
So we created our list of our titles.

1083
01:22:31,400 --> 01:22:33,400
Called that our corpus.

1084
01:22:33,400 --> 01:22:35,400
Set up TFIDF like we learned before.

1085
01:22:35,400 --> 01:22:37,400
Did our fit transform.

1086
01:22:37,400 --> 01:22:41,400
And then, excuse me, pass that into our model in this next row here.

1087
01:22:41,400 --> 01:22:43,400
Or this next cell, sorry, rather.

1088
01:22:43,400 --> 01:22:45,400
Did anyone try out sentence embeddings?

1089
01:22:45,400 --> 01:22:47,400
Just curious.

1090
01:22:47,400 --> 01:22:49,400
That was probably a little bit challenging.

1091
01:22:49,400 --> 01:22:51,400
So TFIDF was probably the way to go on that.

1092
01:22:51,400 --> 01:22:53,400
Yeah.

1093
01:22:53,400 --> 01:22:55,400
Yeah.

1094
01:22:55,400 --> 01:23:01,400
I have that except that I also tried to put the TFIDF into the data frame.

1095
01:23:01,400 --> 01:23:03,400
And I complained about that.

1096
01:23:03,400 --> 01:23:05,400
Oh, gotcha.

1097
01:23:05,400 --> 01:23:07,400
So it doesn't go into the data frame.

1098
01:23:07,400 --> 01:23:09,400
Right.

1099
01:23:09,400 --> 01:23:11,400
Right.

1100
01:23:11,400 --> 01:23:15,400
Yeah, so the question was about putting TFIDF back in the data frame.

1101
01:23:15,400 --> 01:23:17,400
I can see why you might think that.

1102
01:23:17,400 --> 01:23:19,400
Because I think that's what we did with one-hot encodings and things.

1103
01:23:19,400 --> 01:23:21,400
But yeah, for TFIDF we don't add it onto the data frame.

1104
01:23:21,400 --> 01:23:23,400
We just have that.

1105
01:23:23,400 --> 01:23:25,400
We create that on its own.

1106
01:23:25,400 --> 01:23:27,400
And that's its whole set of counts and things that it's done.

1107
01:23:27,400 --> 01:23:29,400
And just pass that into the model itself.

1108
01:23:29,400 --> 01:23:31,400
Yeah, for this example, there's another technique.

1109
01:23:31,400 --> 01:23:33,400
And I know we're on time as well.

1110
01:23:33,400 --> 01:23:35,400
But in Scikit-Learn there's like a column transformer.

1111
01:23:35,400 --> 01:23:37,400
And so you can actually define the vectorizer that you want to use.

1112
01:23:37,400 --> 01:23:39,400
And you can actually define the vectorizer that you want to use.

1113
01:23:39,400 --> 01:23:41,400
So that's the way to do it.

1114
01:23:41,400 --> 01:23:43,400
So that's the way to do it.

1115
01:23:43,400 --> 01:23:45,400
And so the way to do it is actually to add the vectorizer.

1116
01:23:45,400 --> 01:23:47,400
So that's the way to do it.

1117
01:23:47,400 --> 01:23:49,400
And so you can actually define the vectorizer that you want to use.

1118
01:23:49,400 --> 01:23:51,400
Or whatever feature creation.

1119
01:23:51,400 --> 01:23:53,400
And apply it across a data frame.

1120
01:23:53,400 --> 01:23:55,400
Using what's called column transformer in Scikit-Learn.

1121
01:23:55,400 --> 01:23:57,400
So I know that wasn't represented here.

1122
01:23:57,400 --> 01:23:59,400
But as far as adding things onto a data frame and such.

1123
01:23:59,400 --> 01:24:01,400
Column transformer I think is the way to go for that.

1124
01:24:01,400 --> 01:24:03,400
Yeah.

1125
01:24:03,400 --> 01:24:05,400
And then these next couple of cells I believe you guys had.

1126
01:24:05,400 --> 01:24:07,400
So really it was just creating the feature there.

1127
01:24:07,400 --> 01:24:09,400
That's the input for the model.

1128
01:24:09,400 --> 01:24:11,400
So we'll go ahead and run that.

1129
01:24:11,400 --> 01:24:13,400
And then we'll go ahead and run that.

1130
01:24:13,400 --> 01:24:15,400
And then we'll go ahead and run that.

1131
01:24:15,400 --> 01:24:17,400
And then this was showing our results I believe in these last few.

1132
01:24:17,400 --> 01:24:19,400
So this was giving you those results.

1133
01:24:19,400 --> 01:24:21,400
And I'm not going to explain this too much.

1134
01:24:21,400 --> 01:24:23,400
I'm sorry.

1135
01:24:23,400 --> 01:24:25,400
We're a little bit short on time.

1136
01:24:25,400 --> 01:24:27,400
So we're going to pass it back to Ray actually.

1137
01:24:27,400 --> 01:24:29,400
And he's going to go through exploring continuous data.

1138
01:24:29,400 --> 01:24:31,400
So all of this we've been working with discrete data up until this point.

1139
01:24:31,400 --> 01:24:33,400
But there's a whole other kind of data set that we haven't even touched on yet.

1140
01:24:33,400 --> 01:24:35,400
So we will go back to Ray for that.

1141
01:24:35,400 --> 01:24:37,400
And sorry let me get the...

1142
01:24:37,400 --> 01:24:39,400
I'm sorry.

1143
01:24:39,400 --> 01:24:41,400
I'm sorry.

1144
01:24:41,400 --> 01:24:43,400
So we'll go ahead and run that.

1145
01:24:43,400 --> 01:24:45,400
And sorry let me get the slides back up to the right spot.

1146
01:24:49,400 --> 01:24:51,400
Oh wait.

1147
01:24:51,400 --> 01:24:53,400
Yes. We're on that.

1148
01:24:53,400 --> 01:24:55,400
Okay. Good.

1149
01:24:55,400 --> 01:24:57,400
Thank you.

1150
01:24:57,400 --> 01:24:59,400
Okay. So yes. Exploring continuous data.

1151
01:24:59,400 --> 01:25:01,400
Our other big data type that we're going to mess around with today.

1152
01:25:03,400 --> 01:25:05,400
So continuous data examples.

1153
01:25:05,400 --> 01:25:07,400
Things that can range from basically negative infinity in theory to positive infinity.

1154
01:25:07,400 --> 01:25:09,400
Like the whole real number line.

1155
01:25:09,400 --> 01:25:11,400
Obviously probably don't have negative height.

1156
01:25:11,400 --> 01:25:13,400
But height, age, weight, temperature, distance.

1157
01:25:13,400 --> 01:25:15,400
These are all forms of generally speaking continuous data sets.

1158
01:25:15,400 --> 01:25:17,400
We'll keep it moving.

1159
01:25:17,400 --> 01:25:19,400
Ways of exploring continuous data sets.

1160
01:25:19,400 --> 01:25:21,400
Yes. You are seeing some closing stock prices listed here.

1161
01:25:21,400 --> 01:25:23,400
That's the data set we're going to be playing around with.

1162
01:25:23,400 --> 01:25:25,400
So yes.

1163
01:25:25,400 --> 01:25:27,400
So yes.

1164
01:25:27,400 --> 01:25:29,400
So yes.

1165
01:25:29,400 --> 01:25:31,400
So yes.

1166
01:25:31,400 --> 01:25:33,400
So yes.

1167
01:25:33,400 --> 01:25:35,400
So yes.

1168
01:25:35,400 --> 01:25:37,400
So yes.

1169
01:25:37,400 --> 01:25:39,400
So yes.

1170
01:25:39,400 --> 01:25:41,400
And one way to do that is I like to do an hugs and kisses mix that we're playing around with.

1171
01:25:41,400 --> 01:25:43,400
So it's technically time series based data.

1172
01:25:43,400 --> 01:25:45,400
But line charts are a classic way of exploring it.

1173
01:25:45,400 --> 01:25:47,400
Box plots are another fun one.

1174
01:25:47,400 --> 01:25:49,400
That was a blast from the past for me.

1175
01:25:49,400 --> 01:25:51,400
As far as

1176
01:25:51,400 --> 01:25:53,400
doing like the old box,

1177
01:25:53,400 --> 01:25:55,400
and whisker plots.

1178
01:25:55,400 --> 01:25:57,400
Yeah whiskers.

1179
01:25:57,400 --> 01:25:59,400
They don't use that terminology anymore.

1180
01:25:59,400 --> 01:26:01,400
But we're going to show that off.

1181
01:26:01,400 --> 01:26:06,000
And then heat maps are another way to go about it.

1182
01:26:06,000 --> 01:26:07,360
I've always been fond of these.

1183
01:26:07,360 --> 01:26:09,520
I've used them in a number of situations

1184
01:26:09,520 --> 01:26:13,100
because your standard X, Y, you have two dimensions,

1185
01:26:13,100 --> 01:26:15,860
but adding in color gives you this third way

1186
01:26:15,860 --> 01:26:19,240
of visualizing a third dimension of data.

1187
01:26:19,240 --> 01:26:21,040
So I'm a big fan of heat maps.

1188
01:26:22,040 --> 01:26:23,920
And then scatter plots.

1189
01:26:23,920 --> 01:26:25,960
Can't get away from those.

1190
01:26:25,960 --> 01:26:28,440
I'm gonna assume most everyone's seen a scatter plot before.

1191
01:26:28,440 --> 01:26:29,440
You got your X and your Y,

1192
01:26:29,440 --> 01:26:33,280
and you're able to do a comparison across two pieces of data.

1193
01:26:33,280 --> 01:26:36,480
In this case, two continuous pieces of data.

1194
01:26:36,480 --> 01:26:37,320
Hypothetically.

1195
01:26:38,280 --> 01:26:39,120
Okay.

1196
01:26:41,360 --> 01:26:43,280
That's gonna be as fast as I can breeze through that

1197
01:26:43,280 --> 01:26:45,080
so we can get to some actual coding.

1198
01:26:46,840 --> 01:26:48,120
Cool, thanks Ray.

1199
01:26:49,440 --> 01:26:50,920
All right, back to the CoLabs.

1200
01:26:50,920 --> 01:26:54,960
We're gonna talk about how we do this in Python.

1201
01:26:54,960 --> 01:26:57,600
So, like Ray mentioned in the slides,

1202
01:26:57,600 --> 01:26:59,080
we're gonna be using a different data set

1203
01:26:59,080 --> 01:27:00,080
for this section.

1204
01:27:00,080 --> 01:27:02,740
We're actually gonna be using some stock data.

1205
01:27:02,740 --> 01:27:04,620
Hopefully something that a lot of us are familiar with,

1206
01:27:04,620 --> 01:27:06,520
but if you're not, don't worry about it.

1207
01:27:06,520 --> 01:27:07,860
It's not a big deal.

1208
01:27:07,860 --> 01:27:09,960
So first off, we can run that first cell there.

1209
01:27:09,960 --> 01:27:12,640
We're gonna use a library called Yfinance.

1210
01:27:12,640 --> 01:27:13,880
If you haven't used that before,

1211
01:27:13,880 --> 01:27:17,600
really handy library to pull down stock data

1212
01:27:17,600 --> 01:27:19,520
so you don't have to like go out and download a CSV

1213
01:27:19,520 --> 01:27:20,360
and all that stuff.

1214
01:27:20,360 --> 01:27:22,060
A really nice library there.

1215
01:27:22,060 --> 01:27:24,120
And we'll also be using pandas here to explore

1216
01:27:24,120 --> 01:27:25,920
as we have been before.

1217
01:27:26,200 --> 01:27:29,240
So we can run that next cell.

1218
01:27:29,240 --> 01:27:32,200
What we're doing there is using the Yfinance library

1219
01:27:32,200 --> 01:27:34,920
to download specific stock tickers.

1220
01:27:34,920 --> 01:27:38,640
In this case, we're looking at indexes or index funds.

1221
01:27:38,640 --> 01:27:42,280
We're looking at SPI, QQQ, and VIX.

1222
01:27:42,280 --> 01:27:45,920
And then we also pass in a start and end date range.

1223
01:27:45,920 --> 01:27:48,720
If you want to, you can pass in your own tickers

1224
01:27:48,720 --> 01:27:51,040
if you like, if you have a favorite stock,

1225
01:27:51,040 --> 01:27:53,360
and also do a separate date range if you want to.

1226
01:27:53,360 --> 01:27:55,840
But if you wanna match up to what I'm doing here

1227
01:27:56,720 --> 01:27:57,920
then just leave that as is.

1228
01:27:57,920 --> 01:28:01,480
So we are just gonna analyze data for those three indices

1229
01:28:01,480 --> 01:28:04,040
from all of 2022.

1230
01:28:05,840 --> 01:28:08,480
Run that next cell and let's talk about the format

1231
01:28:08,480 --> 01:28:09,900
of this data.

1232
01:28:09,900 --> 01:28:11,600
This is printing out just the first five rows

1233
01:28:11,600 --> 01:28:12,520
of our data set.

1234
01:28:13,560 --> 01:28:15,760
You'll notice here that we've got like some hierarchy

1235
01:28:15,760 --> 01:28:16,960
going on in our data.

1236
01:28:16,960 --> 01:28:20,360
So you'll see like we have an adjusted close column.

1237
01:28:20,360 --> 01:28:22,440
So these are all about like the price of the stock

1238
01:28:22,440 --> 01:28:23,520
at certain points.

1239
01:28:23,520 --> 01:28:25,840
So this is what it closed out on that day,

1240
01:28:25,840 --> 01:28:27,960
the high price for the day, the low price,

1241
01:28:27,960 --> 01:28:31,120
we get the open and the volume as well.

1242
01:28:31,120 --> 01:28:34,200
But actually before this workshop,

1243
01:28:34,200 --> 01:28:36,920
I had never used this kind of a data frame before,

1244
01:28:36,920 --> 01:28:40,840
but it's really nice and I'm glad I learned about it.

1245
01:28:40,840 --> 01:28:42,800
It's called multi-index or multi-label

1246
01:28:42,800 --> 01:28:44,280
is another way to call it.

1247
01:28:44,280 --> 01:28:46,680
But what it allows you to do is just have a hierarchy

1248
01:28:46,680 --> 01:28:48,000
in your data set.

1249
01:28:48,000 --> 01:28:49,640
So what's nice, we'll see later,

1250
01:28:49,640 --> 01:28:52,600
is that we can just reference like this top level column

1251
01:28:52,600 --> 01:28:56,680
and it will give us the three columns underneath of it.

1252
01:28:56,680 --> 01:28:58,600
So that makes it really nice for plotting.

1253
01:28:58,600 --> 01:29:00,000
We'll see that in a second.

1254
01:29:00,960 --> 01:29:03,400
But just wanted to highlight the format of our data here,

1255
01:29:03,400 --> 01:29:05,320
a little bit different.

1256
01:29:05,320 --> 01:29:07,680
So you'll also notice that each row here

1257
01:29:07,680 --> 01:29:09,400
is an individual day.

1258
01:29:09,400 --> 01:29:12,320
And then we've got all that information about the price

1259
01:29:12,320 --> 01:29:14,920
as well as the volume of those stocks on those days.

1260
01:29:16,640 --> 01:29:19,600
So we're gonna use our friend from the past,

1261
01:29:19,600 --> 01:29:21,320
our YDataProfiler library

1262
01:29:21,320 --> 01:29:23,720
to be able to explore this data.

1263
01:29:23,720 --> 01:29:26,880
So if you want to go ahead and run those two cells there,

1264
01:29:28,040 --> 01:29:31,560
what that's gonna do is first of all, sorry,

1265
01:29:32,880 --> 01:29:34,880
unfortunately that library cannot handle

1266
01:29:34,880 --> 01:29:36,040
multi-index data frames.

1267
01:29:36,040 --> 01:29:38,120
So we just have to, if we wanna explore it

1268
01:29:38,120 --> 01:29:40,000
to download one stock at a time,

1269
01:29:40,000 --> 01:29:41,120
so that's what I'm doing here

1270
01:29:41,120 --> 01:29:44,600
is just only downloading SPI for that same year

1271
01:29:44,600 --> 01:29:46,600
and then saving that off as its own data frame

1272
01:29:46,600 --> 01:29:49,440
so that we can run the profile report against it.

1273
01:29:49,440 --> 01:29:53,920
So that is the one downside of that sort of data frame style

1274
01:29:53,920 --> 01:29:55,520
is that not a ton of libraries

1275
01:29:55,520 --> 01:29:57,800
will work with it unfortunately.

1276
01:29:57,800 --> 01:30:01,120
But we can work around it, it's not too big of a deal.

1277
01:30:01,120 --> 01:30:02,840
So like before, you guys should be familiar

1278
01:30:02,840 --> 01:30:04,480
with this kind of report now.

1279
01:30:04,480 --> 01:30:07,120
We're getting the same thing, but for time series data.

1280
01:30:07,120 --> 01:30:10,720
And I mentioned earlier that there is a parameter

1281
01:30:10,720 --> 01:30:12,720
you can set on the profile report

1282
01:30:12,720 --> 01:30:15,720
so that it makes line charts for you out of the box,

1283
01:30:15,720 --> 01:30:16,800
which is nice.

1284
01:30:16,800 --> 01:30:20,200
So we're using this extra parameter, tsMode equals true,

1285
01:30:20,200 --> 01:30:21,600
to tell it that it's a time series

1286
01:30:21,600 --> 01:30:23,160
and then I'm just saying that we wanna sort

1287
01:30:23,160 --> 01:30:26,480
by the date column so that our line chart makes sense

1288
01:30:26,480 --> 01:30:27,520
like over time.

1289
01:30:28,480 --> 01:30:29,920
I'm not gonna go through this

1290
01:30:29,920 --> 01:30:31,520
because that is gonna be again,

1291
01:30:31,520 --> 01:30:33,440
an exercise for you guys to go through.

1292
01:30:33,440 --> 01:30:35,880
So we will skip a little bit through that

1293
01:30:35,880 --> 01:30:37,880
and then now we're gonna talk through

1294
01:30:37,880 --> 01:30:41,200
how to make all those charts that Ray was talking about.

1295
01:30:41,200 --> 01:30:43,720
So first up, we're gonna go through line charts.

1296
01:30:44,720 --> 01:30:48,800
What we're gonna do is just plot each ticker's close price.

1297
01:30:48,800 --> 01:30:50,960
So you can go ahead and run that cell there

1298
01:30:50,960 --> 01:30:52,040
if you haven't already.

1299
01:30:52,040 --> 01:30:55,440
And that gets us this chart from the slides

1300
01:30:55,440 --> 01:30:56,560
that we saw earlier.

1301
01:30:56,560 --> 01:30:59,600
So we get over time, what was the close price

1302
01:30:59,600 --> 01:31:01,920
of each of those tickers and their color coded.

1303
01:31:01,920 --> 01:31:03,880
And I'm just setting those colors here.

1304
01:31:05,600 --> 01:31:07,800
You can pass in your own colors if you like.

1305
01:31:07,800 --> 01:31:11,520
That's just a thing that's built into this plotting.

1306
01:31:11,520 --> 01:31:12,800
You could also do this with a library

1307
01:31:12,840 --> 01:31:14,920
like Matplotlib or Seaborn.

1308
01:31:14,920 --> 01:31:17,560
But lucky for us, Pandas actually has,

1309
01:31:17,560 --> 01:31:19,760
they've done some work to like wrap Matplotlib

1310
01:31:19,760 --> 01:31:22,680
into the library so that you can just call this .plot

1311
01:31:22,680 --> 01:31:25,840
instead of having to write a whole bunch of code.

1312
01:31:25,840 --> 01:31:27,600
So that gives us a line chart.

1313
01:31:27,600 --> 01:31:31,360
What we'll notice here is actually that this VIX index

1314
01:31:31,360 --> 01:31:34,920
appears to be on a different scale than SPI and QQQ.

1315
01:31:34,920 --> 01:31:36,760
So it can be a little difficult to see

1316
01:31:36,760 --> 01:31:38,880
what is the actual trend going on here.

1317
01:31:38,880 --> 01:31:41,160
So one thing we might wanna do is split that off

1318
01:31:41,160 --> 01:31:44,000
into its own charts and that's what we're gonna do next.

1319
01:31:45,000 --> 01:31:47,800
So to do that, I'm gonna run that next cell.

1320
01:31:47,800 --> 01:31:52,600
And how you like access this in a multi-index data frame

1321
01:31:52,600 --> 01:31:55,920
is by using these parentheses on the inside.

1322
01:31:55,920 --> 01:31:58,360
And so you first call what high level column

1323
01:31:58,360 --> 01:31:59,240
do you wanna look at?

1324
01:31:59,240 --> 01:32:00,920
In our case, it's the close column

1325
01:32:00,920 --> 01:32:03,400
and then what specific column underneath that.

1326
01:32:03,400 --> 01:32:06,400
So I'm just saying only give me VIX.

1327
01:32:06,400 --> 01:32:10,800
And what that does for us is give us a chart like that.

1328
01:32:11,440 --> 01:32:13,880
So we can better see what trend is going on.

1329
01:32:14,880 --> 01:32:17,480
You could also manipulate this scaling if you want.

1330
01:32:17,480 --> 01:32:19,280
I'm just leaving it at the default,

1331
01:32:19,280 --> 01:32:22,120
which is maybe not a best practice for data visit,

1332
01:32:22,120 --> 01:32:25,320
but for ease of the workshop, that's what we're gonna do.

1333
01:32:26,320 --> 01:32:28,480
And we can also plot those other two together

1334
01:32:28,480 --> 01:32:29,640
if we wanted to.

1335
01:32:29,640 --> 01:32:32,080
And to do that, what we can do is use that same

1336
01:32:32,080 --> 01:32:34,640
parentheses notation, but then just put a comma

1337
01:32:34,640 --> 01:32:35,480
in between them.

1338
01:32:35,480 --> 01:32:37,840
So we're saying give us the close for SPI,

1339
01:32:37,840 --> 01:32:41,280
give us the close for QQQ and plot those together.

1340
01:32:41,280 --> 01:32:43,800
So if you can run that in Excel and then you get that.

1341
01:32:43,800 --> 01:32:45,080
So you can see a little bit better

1342
01:32:45,080 --> 01:32:47,320
of the trend that's happening when they're on their own

1343
01:32:47,320 --> 01:32:49,960
versus when we've got that other one that's unscaled.

1344
01:32:51,040 --> 01:32:52,440
Any questions on line charts?

1345
01:32:54,760 --> 01:32:56,640
Pretty straightforward, we probably all have done

1346
01:32:56,640 --> 01:32:57,720
line charts before.

1347
01:32:57,720 --> 01:32:59,800
So let's get into stuff maybe we haven't done before

1348
01:32:59,800 --> 01:33:02,240
or in maybe like mine and Ray's case,

1349
01:33:02,240 --> 01:33:05,160
haven't done since like middle school or elementary school.

1350
01:33:05,160 --> 01:33:06,840
So we're gonna go with box plots,

1351
01:33:06,840 --> 01:33:09,320
which is a really great way to visualize distribution

1352
01:33:09,320 --> 01:33:11,240
of your data and also see outliers.

1353
01:33:12,960 --> 01:33:15,840
To do this, we're gonna use a library called Seaborn,

1354
01:33:15,840 --> 01:33:18,400
which is another really powerful plotting library.

1355
01:33:19,760 --> 01:33:21,720
But to get it ready for Seaborn,

1356
01:33:21,720 --> 01:33:25,880
we have to format our data a little bit.

1357
01:33:25,880 --> 01:33:28,160
So we gotta do a little bit of data engineering here.

1358
01:33:28,160 --> 01:33:31,720
So I'm in these cells or in this cell right here,

1359
01:33:31,720 --> 01:33:34,400
I am formatting the data the way that I need it to.

1360
01:33:34,400 --> 01:33:36,040
I'm gonna go ahead and run this

1361
01:33:36,040 --> 01:33:38,680
so that we can see the format that I wanted in

1362
01:33:38,680 --> 01:33:40,920
and I can talk through that too.

1363
01:33:40,920 --> 01:33:44,400
So what we're doing here is getting rid of basically

1364
01:33:44,400 --> 01:33:46,480
that like hierarchical structure

1365
01:33:46,480 --> 01:33:47,960
that I was talking about is really nice.

1366
01:33:47,960 --> 01:33:49,360
So that was nice for the line plots,

1367
01:33:49,360 --> 01:33:52,800
but not for the box plots that we're gonna want to do.

1368
01:33:52,800 --> 01:33:57,800
So we are using this stack level zero to get rid of that

1369
01:33:58,120 --> 01:34:00,240
and then also format it this way.

1370
01:34:00,240 --> 01:34:02,800
So instead of also having a column for each ticker,

1371
01:34:02,800 --> 01:34:05,280
we now have one column called ticker

1372
01:34:05,280 --> 01:34:06,560
and a row for each one.

1373
01:34:06,560 --> 01:34:08,640
So instead of being a wide data frame,

1374
01:34:08,640 --> 01:34:10,040
we're taking it to a tall.

1375
01:34:11,200 --> 01:34:13,600
So then we get our close value.

1376
01:34:13,600 --> 01:34:16,000
One thing we're gonna want to do in our box plot

1377
01:34:16,000 --> 01:34:17,960
is plot everything by month,

1378
01:34:17,960 --> 01:34:20,080
but that means what we have to do is we have to get

1379
01:34:20,080 --> 01:34:22,000
from the date what is the month.

1380
01:34:22,000 --> 01:34:23,720
And this gets a little bit actually

1381
01:34:23,720 --> 01:34:25,360
into our feature engineering section

1382
01:34:25,360 --> 01:34:27,200
that we're gonna talk about later.

1383
01:34:27,200 --> 01:34:29,400
So a little bit of a precursor here,

1384
01:34:29,400 --> 01:34:32,080
but to do that, what I'm doing is using this

1385
01:34:32,080 --> 01:34:35,440
string F time, string formatter time function.

1386
01:34:35,440 --> 01:34:37,200
And that allows you to take any date

1387
01:34:37,200 --> 01:34:40,440
and then format it as a string based on the formatter

1388
01:34:40,440 --> 01:34:42,280
that you pass into it.

1389
01:34:42,280 --> 01:34:46,640
I am doing the three letter abbreviation for US months

1390
01:34:46,640 --> 01:34:49,480
and our US version of those.

1391
01:34:49,480 --> 01:34:52,600
And the way we get that is with this percent B.

1392
01:34:52,600 --> 01:34:54,080
So that's what I'm doing there.

1393
01:34:54,080 --> 01:34:56,280
And now our data has been pivoted correctly

1394
01:34:56,280 --> 01:34:58,160
so that we can do our box plot on it.

1395
01:34:59,560 --> 01:35:02,000
So let's actually create the box plot.

1396
01:35:02,920 --> 01:35:07,680
So run that next cell and we get some nice box plots,

1397
01:35:07,680 --> 01:35:10,400
but it's a little bit hard to see what's going on.

1398
01:35:10,400 --> 01:35:13,760
So this is a way we can get box plots for each month,

1399
01:35:13,760 --> 01:35:17,000
for each ticker, for the close price of those.

1400
01:35:17,000 --> 01:35:21,080
So it gives us a good view of the distribution of those,

1401
01:35:21,080 --> 01:35:22,800
but a little bit hard again to see

1402
01:35:22,800 --> 01:35:24,840
what's going on with fix down here.

1403
01:35:24,840 --> 01:35:27,120
So there are some other ways we can do this

1404
01:35:27,120 --> 01:35:27,960
and split it out.

1405
01:35:27,960 --> 01:35:30,880
So one way we can do that is the next cell,

1406
01:35:30,880 --> 01:35:35,120
which is using something called cat plot in Seaborn

1407
01:35:35,120 --> 01:35:38,200
to actually put them each on their own plot.

1408
01:35:38,200 --> 01:35:40,160
That helps us a little bit here,

1409
01:35:40,160 --> 01:35:42,640
but still hard to see what's going on with VIX.

1410
01:35:43,600 --> 01:35:46,780
So I should have selected VIX down here,

1411
01:35:46,780 --> 01:35:49,760
but I'm just showing you that you can just plot one

1412
01:35:49,760 --> 01:35:50,680
at a time.

1413
01:35:50,680 --> 01:35:52,440
So if you wanted to see VIX on its own,

1414
01:35:52,440 --> 01:35:55,160
just replace spy with caret VIX.

1415
01:35:56,640 --> 01:35:59,120
And then what that does is give you a nice chart

1416
01:35:59,120 --> 01:36:01,040
that you can actually see what's going on

1417
01:36:01,040 --> 01:36:03,040
and identify some outliers as well.

1418
01:36:04,200 --> 01:36:07,600
So really handy being able to do those charts.

1419
01:36:07,600 --> 01:36:09,240
Any questions on box plots?

1420
01:36:10,180 --> 01:36:11,020
Yeah.

1421
01:36:12,600 --> 01:36:15,920
No, they don't, which is probably another

1422
01:36:15,920 --> 01:36:18,240
bad data vis practice of me to do.

1423
01:36:18,240 --> 01:36:19,080
So.

1424
01:36:22,660 --> 01:36:23,800
Very true, yeah.

1425
01:36:23,800 --> 01:36:25,920
I should have done that too.

1426
01:36:25,920 --> 01:36:27,440
Sorry I did not.

1427
01:36:27,440 --> 01:36:28,280
Good question.

1428
01:36:29,880 --> 01:36:30,720
Okay.

1429
01:36:32,240 --> 01:36:34,360
And next up, we're gonna do heat maps.

1430
01:36:34,360 --> 01:36:35,920
Ray was talking about these are really nice

1431
01:36:35,920 --> 01:36:38,120
that you can see like three dimensions of data

1432
01:36:38,120 --> 01:36:38,960
in one chart.

1433
01:36:38,960 --> 01:36:41,200
So here color will actually matter.

1434
01:36:42,080 --> 01:36:45,480
So what we're gonna do is we need our data formatted

1435
01:36:45,480 --> 01:36:48,460
also a little bit differently to work with a heat map.

1436
01:36:49,480 --> 01:36:51,920
But first what we wanna do is when we're making

1437
01:36:51,920 --> 01:36:54,280
our heat map, we're gonna actually do what was in the slides,

1438
01:36:54,280 --> 01:36:59,080
which is four every month on the X axis.

1439
01:37:00,000 --> 01:37:02,540
We're gonna have our days and then for color,

1440
01:37:02,540 --> 01:37:04,240
it's gonna be that close price.

1441
01:37:04,240 --> 01:37:08,160
So we need to extract the day out of our date that we have.

1442
01:37:08,160 --> 01:37:10,520
And to do that, there's a nice built in function

1443
01:37:10,520 --> 01:37:13,320
just called day that we can call on a date

1444
01:37:13,320 --> 01:37:14,800
and get that pulled out.

1445
01:37:14,800 --> 01:37:18,500
So if we run that cell there, what we'll see is we added

1446
01:37:18,500 --> 01:37:20,880
on to our data frame, what is the day?

1447
01:37:20,880 --> 01:37:23,320
And so the heat map needs that information.

1448
01:37:23,320 --> 01:37:24,760
Unfortunately, not to my knowledge,

1449
01:37:24,760 --> 01:37:27,280
can't like parse out from this date, unfortunately.

1450
01:37:27,280 --> 01:37:28,700
So we just have to do a little bit of prep

1451
01:37:28,700 --> 01:37:30,100
to get that ready.

1452
01:37:31,500 --> 01:37:34,740
So then what we're gonna do is just filter down

1453
01:37:34,740 --> 01:37:36,400
to one ticker at a time.

1454
01:37:36,400 --> 01:37:37,980
So we're gonna filter to spy.

1455
01:37:39,300 --> 01:37:40,800
And you guys can go ahead and run that cell too

1456
01:37:40,800 --> 01:37:41,900
if you haven't already.

1457
01:37:42,820 --> 01:37:44,660
We have to pivot the data again,

1458
01:37:44,660 --> 01:37:46,100
because it just needs it in a little bit

1459
01:37:46,100 --> 01:37:46,980
of a different format.

1460
01:37:46,980 --> 01:37:48,420
So that's what we're doing there.

1461
01:37:48,420 --> 01:37:52,180
And then pandas annoyingly orders columns alphabetically,

1462
01:37:52,180 --> 01:37:54,340
but if we're dealing with like time series data,

1463
01:37:54,340 --> 01:37:56,260
alphabetical order doesn't make sense.

1464
01:37:56,260 --> 01:37:58,540
So I am just re, I'm telling it the order

1465
01:37:58,540 --> 01:38:00,620
that I want them in, which is, you know,

1466
01:38:00,620 --> 01:38:03,180
what we would normally expect for a time series.

1467
01:38:03,180 --> 01:38:06,020
So doing that and then re like ordering the months

1468
01:38:06,020 --> 01:38:07,900
so that that works out.

1469
01:38:07,900 --> 01:38:09,620
And when we run that last cell there,

1470
01:38:09,620 --> 01:38:12,140
this actually creates the heat map for us.

1471
01:38:12,140 --> 01:38:14,620
And I might zoom out a little bit here

1472
01:38:14,620 --> 01:38:17,380
so we can see it all on one screen.

1473
01:38:17,380 --> 01:38:20,180
So we'll see on our X axis, we have the months,

1474
01:38:20,180 --> 01:38:22,380
Y axis, we have the day of the month.

1475
01:38:22,380 --> 01:38:25,700
And then that color chart is showing us the close price.

1476
01:38:25,860 --> 01:38:28,780
So higher close prices being that darker color

1477
01:38:28,780 --> 01:38:31,300
and lower prices being the lighter color.

1478
01:38:31,300 --> 01:38:34,460
So this is a nice way to get us to see three dimensions

1479
01:38:34,460 --> 01:38:35,580
of data at once.

1480
01:38:36,900 --> 01:38:39,660
Can be a little bit tricky here, I guess,

1481
01:38:39,660 --> 01:38:41,380
interpreting this, cause you might be wondering

1482
01:38:41,380 --> 01:38:43,580
why the heck do we have all these blank spaces in it

1483
01:38:43,580 --> 01:38:45,860
or white, white's not in our color chart.

1484
01:38:46,860 --> 01:38:49,060
Well, that actually happens because the data

1485
01:38:49,060 --> 01:38:51,460
that we're dealing with doesn't have weekends in it

1486
01:38:51,460 --> 01:38:54,420
because stocks, stock market isn't open on the weekends.

1487
01:38:54,420 --> 01:38:56,020
So just doesn't have those.

1488
01:38:56,020 --> 01:38:58,140
So that's showing we have no data there.

1489
01:38:59,500 --> 01:39:02,700
But some months, the weekends are on different days

1490
01:39:02,700 --> 01:39:05,300
of the month, so that's why we still show all the days

1491
01:39:05,300 --> 01:39:08,460
on that Y axis, but just not showing all on the chart.

1492
01:39:10,980 --> 01:39:13,300
Any questions on heat map before I move on?

1493
01:39:14,980 --> 01:39:16,140
Okay.

1494
01:39:16,140 --> 01:39:18,900
And last one in this section is scatter plots,

1495
01:39:18,900 --> 01:39:22,020
which like Ray said, probably a lot of us have seen before.

1496
01:39:22,060 --> 01:39:24,460
You can go ahead and run that cell there.

1497
01:39:24,460 --> 01:39:27,020
What we're gonna do here is plot to,

1498
01:39:27,900 --> 01:39:30,380
on the X axis, we're gonna plot the close price,

1499
01:39:30,380 --> 01:39:33,140
and then on the Y axis, we're gonna plot the volume.

1500
01:39:33,140 --> 01:39:35,460
And so what we're looking for here,

1501
01:39:35,460 --> 01:39:37,900
scatter plots are good for seeing if two variables

1502
01:39:37,900 --> 01:39:39,340
are correlated with each other,

1503
01:39:39,340 --> 01:39:40,580
and that's what we're trying to see here,

1504
01:39:40,580 --> 01:39:43,460
basically is the close price correlated with the volume

1505
01:39:43,460 --> 01:39:44,620
or vice versa.

1506
01:39:45,500 --> 01:39:47,060
How we interpret a scatter plot,

1507
01:39:47,060 --> 01:39:51,060
to have it written out here, if you haven't seen one before,

1508
01:39:51,060 --> 01:39:53,600
positive correlation would be if our data was moving

1509
01:39:53,600 --> 01:39:56,400
from this bottom left corner all the way to the right,

1510
01:39:56,400 --> 01:39:59,420
top right, to kind of in a line like that way,

1511
01:39:59,420 --> 01:40:02,820
that'd be a positive, so both data points,

1512
01:40:02,820 --> 01:40:05,380
as one increases, the other increases.

1513
01:40:05,380 --> 01:40:07,580
Otherwise, you've got negative correlation

1514
01:40:07,580 --> 01:40:10,460
from the top left down to the bottom right,

1515
01:40:10,460 --> 01:40:14,060
or in this case, it looks like we've got no correlation,

1516
01:40:14,060 --> 01:40:15,620
appearing at random, basically.

1517
01:40:17,540 --> 01:40:20,740
Okay, so I think we're ready for exercises.

1518
01:40:21,500 --> 01:40:23,780
We're gonna give you guys 10 minutes for this section.

1519
01:40:23,780 --> 01:40:25,420
One of it's gonna be just exploring the data

1520
01:40:25,420 --> 01:40:27,700
using the profile report that we learned about earlier,

1521
01:40:27,700 --> 01:40:29,860
so hopefully that should feel pretty familiar.

1522
01:40:29,860 --> 01:40:32,540
And then the other section will be visualizing

1523
01:40:32,540 --> 01:40:33,940
the stock data.

1524
01:40:33,940 --> 01:40:36,620
Using the techniques that we described above,

1525
01:40:36,620 --> 01:40:39,500
we only, remember, looked at the close price, basically,

1526
01:40:39,500 --> 01:40:41,860
but there's four or five other columns of data

1527
01:40:41,860 --> 01:40:45,020
you can explore, so that's one thing to do there.

1528
01:40:45,020 --> 01:40:48,140
And then also, if you are a fan of some kind of chart

1529
01:40:48,140 --> 01:40:49,980
that we didn't talk about or know of something,

1530
01:40:49,980 --> 01:40:52,100
feel free to use that for bonus points.

1531
01:40:53,260 --> 01:40:58,260
So with that, I'm gonna put 10 minutes on the clock,

1532
01:40:58,900 --> 01:41:00,700
and we'll let you guys work on that.

1533
01:41:03,540 --> 01:41:04,820
All right, there you go.

1534
01:41:10,020 --> 01:41:14,260
All righty, I think we're wrapped up on this section here.

1535
01:41:14,980 --> 01:41:19,980
Again, quick thumbs up, thumbs down, thumbs sideways,

1536
01:41:19,980 --> 01:41:21,740
and how we did?

1537
01:41:21,740 --> 01:41:24,460
Okay, seeing lots of thumbs up, good deal.

1538
01:41:24,460 --> 01:41:26,380
Was hoping for that, because a lot of it is what we did

1539
01:41:26,380 --> 01:41:29,100
in the previous section, so nothing too new here.

1540
01:41:30,620 --> 01:41:33,020
Okay, so I'm gonna explain these here,

1541
01:41:33,020 --> 01:41:34,700
and then I realize you guys are due for another break,

1542
01:41:34,700 --> 01:41:36,180
so right after I get done explaining this,

1543
01:41:36,180 --> 01:41:39,660
we'll do another 15 minute break, good, and then come back.

1544
01:41:39,660 --> 01:41:43,740
Okay, so for exploring, what we did here

1545
01:41:43,740 --> 01:41:45,860
was just use the code that we had earlier

1546
01:41:45,860 --> 01:41:48,860
from the Y Finance Library to download the specific tickers

1547
01:41:48,860 --> 01:41:50,340
that we're interested in.

1548
01:41:50,340 --> 01:41:53,260
In this case, I'm downloading QQQ here,

1549
01:41:53,260 --> 01:41:57,140
and then doing that for 2022, same like we did earlier.

1550
01:41:57,140 --> 01:41:58,620
So mainly just a copy paste,

1551
01:41:58,620 --> 01:42:01,340
but switching out to the ticker there.

1552
01:42:01,340 --> 01:42:03,580
Then we're using our code from above

1553
01:42:03,580 --> 01:42:05,580
to create that profile report like we did,

1554
01:42:05,580 --> 01:42:06,940
and so we'll do that.

1555
01:42:08,780 --> 01:42:09,860
And then that creates that,

1556
01:42:09,860 --> 01:42:11,660
and you guys had plenty of time to explore,

1557
01:42:11,660 --> 01:42:12,700
see what that's all about.

1558
01:42:12,700 --> 01:42:16,740
So that's old news by now, so I won't go into that.

1559
01:42:16,740 --> 01:42:18,980
And then same thing really here for this section.

1560
01:42:18,980 --> 01:42:22,620
Second section was just switching out that ticker for fix,

1561
01:42:22,620 --> 01:42:24,620
and then doing that same exploration.

1562
01:42:25,460 --> 01:42:28,380
So we'll run all those, and okay.

1563
01:42:28,380 --> 01:42:30,740
Then the next section was all about

1564
01:42:30,740 --> 01:42:31,900
visualizing the stock data,

1565
01:42:31,900 --> 01:42:34,460
so using some techniques that we talked about,

1566
01:42:34,460 --> 01:42:37,900
or for bonus points, go through one that we did not discuss.

1567
01:42:37,900 --> 01:42:40,980
Anyone do one that we didn't talk about?

1568
01:42:41,060 --> 01:42:44,700
Oh, okay.

1569
01:42:44,700 --> 01:42:45,620
Very cool.

1570
01:42:47,220 --> 01:42:48,060
Awesome.

1571
01:42:48,060 --> 01:42:51,700
So again, beauty of this section is your code

1572
01:42:51,700 --> 01:42:53,580
should not really even match mine,

1573
01:42:53,580 --> 01:42:56,100
this was just what we chose to do for this example.

1574
01:42:56,100 --> 01:43:01,100
So we decided to do a heat map on the QQQ close price.

1575
01:43:01,340 --> 01:43:03,140
So mostly just copying the code from earlier,

1576
01:43:03,140 --> 01:43:06,100
but swapping out that ticker for QQQ.

1577
01:43:06,100 --> 01:43:09,140
So doing that, and seeing what that looked like.

1578
01:43:09,980 --> 01:43:12,940
And then our second example here was looking

1579
01:43:12,940 --> 01:43:15,260
at the volume column for QQQ.

1580
01:43:15,260 --> 01:43:16,740
So most of our examples before,

1581
01:43:16,740 --> 01:43:18,500
we looked at the close price column,

1582
01:43:18,500 --> 01:43:20,380
so this is just looking at a different column.

1583
01:43:20,380 --> 01:43:21,620
So mainly copying that code,

1584
01:43:21,620 --> 01:43:24,620
and then swapping out to whichever column you're interested,

1585
01:43:24,620 --> 01:43:26,820
as well as this was on QQQ,

1586
01:43:26,820 --> 01:43:29,700
rather than SPI that we did in the examples earlier.

1587
01:43:29,700 --> 01:43:31,220
So I won't go through that too much,

1588
01:43:31,220 --> 01:43:33,140
but we get that nice plot here,

1589
01:43:33,140 --> 01:43:36,420
which same sort of setup as the other one,

1590
01:43:36,420 --> 01:43:38,260
but you can tell values are a lot different.

1591
01:43:38,260 --> 01:43:40,340
So nice to explore that too.

1592
01:43:41,460 --> 01:43:44,180
And unless anyone has questions,

1593
01:43:44,180 --> 01:43:46,940
I'll pause for a sec, questions on either of those,

1594
01:43:46,940 --> 01:43:48,540
hopefully pretty straightforward.

1595
01:43:49,620 --> 01:43:52,100
Okay, so you guys are due for a break.

1596
01:43:52,100 --> 01:43:54,580
We're gonna give you 15 minutes to go run around,

1597
01:43:54,580 --> 01:43:56,540
stretch your legs, get some more water,

1598
01:43:56,540 --> 01:43:59,140
go to the bathroom, all that fun stuff.

1599
01:43:59,140 --> 01:43:59,980
Oops.

1600
01:44:08,260 --> 01:44:12,220
All righty, welcome back.

1601
01:44:14,980 --> 01:44:17,180
Appreciate you coming back from your break.

1602
01:44:18,700 --> 01:44:21,260
This will be the last fill or last section

1603
01:44:21,260 --> 01:44:22,620
you'll have of me,

1604
01:44:22,620 --> 01:44:25,100
and then we'll pass it back to Leah

1605
01:44:25,100 --> 01:44:27,180
to walk you through the actual code

1606
01:44:27,180 --> 01:44:29,580
and get you going on the final exercises.

1607
01:44:30,460 --> 01:44:34,700
So we've talked a little bit about exploring features

1608
01:44:34,700 --> 01:44:35,780
for continuous data,

1609
01:44:35,780 --> 01:44:37,980
and now we're gonna talk about creating some,

1610
01:44:37,980 --> 01:44:40,700
actually exploring the continuous data,

1611
01:44:40,700 --> 01:44:43,860
and now creating features for said continuous data.

1612
01:44:46,300 --> 01:44:49,540
A lot of it might be pretty darn familiar, to be frank.

1613
01:44:49,540 --> 01:44:51,860
Continuous data is something that we do work with.

1614
01:44:51,860 --> 01:44:55,060
Again, we primarily work with text data,

1615
01:44:55,060 --> 01:44:58,340
but this is also a good thing to have in the tool belt.

1616
01:44:58,340 --> 01:45:01,940
Most of our continuous data is coming in over time anyways.

1617
01:45:01,940 --> 01:45:03,940
So let's dive in.

1618
01:45:04,860 --> 01:45:08,500
So one of the first techniques is normalization.

1619
01:45:08,500 --> 01:45:12,900
There's a number of different ways to normalize a data set.

1620
01:45:12,900 --> 01:45:14,900
Min-max normalization is one of the ones

1621
01:45:14,900 --> 01:45:16,980
that we tend to lean into.

1622
01:45:17,980 --> 01:45:20,860
Here we have some X value

1623
01:45:20,860 --> 01:45:24,260
that we're wanting to scale to normalize.

1624
01:45:24,260 --> 01:45:28,340
We can take the maximum value that exists for this,

1625
01:45:28,340 --> 01:45:29,740
in let's say a field of data,

1626
01:45:29,740 --> 01:45:31,620
and I'll show an example in a sec,

1627
01:45:31,620 --> 01:45:33,820
subtract it from the minimum value,

1628
01:45:34,660 --> 01:45:36,780
and then for each value in,

1629
01:45:36,780 --> 01:45:39,100
each individual value in the data set,

1630
01:45:39,100 --> 01:45:42,300
we can subtract the minimum from it

1631
01:45:42,300 --> 01:45:44,260
in this little fraction, and that'll scale it.

1632
01:45:44,260 --> 01:45:46,300
So a nice example will help,

1633
01:45:47,260 --> 01:45:48,740
if you haven't seen this before.

1634
01:45:48,740 --> 01:45:52,740
So here we start off with, for example,

1635
01:45:52,740 --> 01:45:56,460
views on a set of 10 hypothetical videos,

1636
01:45:58,100 --> 01:46:02,140
and we'll notice that the eighth video

1637
01:46:02,140 --> 01:46:03,660
has the largest number of views

1638
01:46:04,500 --> 01:46:05,500
and the 10th video has the smallest.

1639
01:46:05,500 --> 01:46:07,380
So those are like the two core ones

1640
01:46:07,380 --> 01:46:09,740
that latch into immediately.

1641
01:46:09,740 --> 01:46:12,740
And so from our chart that we talked about earlier,

1642
01:46:14,900 --> 01:46:16,940
the denominator will be the maximum value

1643
01:46:16,940 --> 01:46:18,620
minus the minimum value.

1644
01:46:19,660 --> 01:46:21,620
And then in the numerator,

1645
01:46:21,620 --> 01:46:24,740
we have the specific value for that row

1646
01:46:24,740 --> 01:46:29,180
subtracted from the minimum value, which is 32.

1647
01:46:29,580 --> 01:46:33,780
Just as a sanity check, when we look across all of this,

1648
01:46:33,780 --> 01:46:36,620
we'll see that yes, our maximum value

1649
01:46:36,620 --> 01:46:38,620
has been normalized to one

1650
01:46:39,580 --> 01:46:41,900
since we get maximum minus minimum

1651
01:46:41,900 --> 01:46:43,700
divided by maximum minus minimum.

1652
01:46:43,700 --> 01:46:47,260
So two identical values over each other,

1653
01:46:47,260 --> 01:46:51,460
that gives us one, and then the 32 minus 32

1654
01:46:51,460 --> 01:46:53,660
makes this thing zero at the end of the day.

1655
01:46:53,660 --> 01:46:56,300
And then everything else is pretty much spread out,

1656
01:46:57,300 --> 01:47:02,300
again, normalized across the span of those values.

1657
01:47:03,100 --> 01:47:08,100
So that's one technique, normalizing the data set.

1658
01:47:09,860 --> 01:47:12,540
Another fun one is log transformations.

1659
01:47:13,380 --> 01:47:16,940
Leo'll touch on this in a bit about the type of data

1660
01:47:16,940 --> 01:47:20,140
that you'll wanna have when you actually do this one.

1661
01:47:20,140 --> 01:47:22,420
But there are some specific time periods,

1662
01:47:22,420 --> 01:47:24,980
and stock market data actually is one of them,

1663
01:47:24,980 --> 01:47:28,620
where you'll actually want to do a log transformation

1664
01:47:28,620 --> 01:47:31,580
on your data set, in this case, your X value.

1665
01:47:32,700 --> 01:47:34,460
Classically speaking,

1666
01:47:34,460 --> 01:47:37,420
if you go back to your old algebra school days,

1667
01:47:37,420 --> 01:47:41,380
your base of your log would be probably either two, 10,

1668
01:47:41,380 --> 01:47:43,420
or E, the natural number,

1669
01:47:43,420 --> 01:47:47,060
and LN is the natural log there for that situation.

1670
01:47:49,780 --> 01:47:51,180
Yeah, there's a little visual.

1671
01:47:51,180 --> 01:47:53,500
If I have a data set X,

1672
01:47:54,540 --> 01:47:58,020
what applying that natural log would look like at least.

1673
01:47:59,460 --> 01:48:01,500
Okay, ah, this is fun.

1674
01:48:01,500 --> 01:48:03,060
Yeah, we have this in here too.

1675
01:48:05,700 --> 01:48:07,340
So table example,

1676
01:48:07,340 --> 01:48:10,220
we're looking at the same views from before.

1677
01:48:10,220 --> 01:48:11,580
So nothing's really changed,

1678
01:48:11,580 --> 01:48:13,820
we're just applying the natural log to it.

1679
01:48:13,820 --> 01:48:15,340
And then we can see,

1680
01:48:15,340 --> 01:48:17,660
unlike with the normalization from before,

1681
01:48:17,660 --> 01:48:19,380
that was from zero to one,

1682
01:48:19,380 --> 01:48:24,380
this time it's 3.5, all the way up to a whopping 13.8.

1683
01:48:24,620 --> 01:48:27,260
But that natural log curve should make sense

1684
01:48:27,260 --> 01:48:28,940
as to why it's like squashing things

1685
01:48:28,940 --> 01:48:31,940
in a non-linear fashion, basically.

1686
01:48:31,940 --> 01:48:36,940
Even 500,000, which is roughly halfway, is at 13.1,

1687
01:48:37,020 --> 01:48:40,820
as opposed to being on the previous one at approximately 0.5.

1688
01:48:40,820 --> 01:48:44,460
So normalizing the way we would expect it to for natural log.

1689
01:48:45,220 --> 01:48:49,580
Another technique, ratios.

1690
01:48:49,580 --> 01:48:52,220
This one I have used a crazy amount,

1691
01:48:52,220 --> 01:48:53,780
as goofy as that sounds.

1692
01:48:55,380 --> 01:48:57,540
I think at the very, very beginning,

1693
01:48:57,540 --> 01:48:59,180
hours ago it feels like,

1694
01:49:00,580 --> 01:49:03,100
I talked about like just even something as simple

1695
01:49:03,100 --> 01:49:08,100
as quality controlling sensors that exist.

1696
01:49:08,980 --> 01:49:10,500
One of the things that we would do often

1697
01:49:10,500 --> 01:49:13,620
is we'd put two sensors out in the field at the same time

1698
01:49:13,620 --> 01:49:16,020
and then just take a ratio against the two.

1699
01:49:16,020 --> 01:49:18,060
And if the ratio went way off whack,

1700
01:49:18,060 --> 01:49:20,060
we knew one of the sensors was going bad.

1701
01:49:22,580 --> 01:49:23,940
Yes.

1702
01:49:23,940 --> 01:49:25,780
If they start diverging from each other,

1703
01:49:25,780 --> 01:49:27,100
a ratio would point that out

1704
01:49:27,100 --> 01:49:28,780
and we know one of our sensors has gone bad,

1705
01:49:28,780 --> 01:49:30,420
we need to replace it in the field.

1706
01:49:30,420 --> 01:49:31,260
Exactly.

1707
01:49:33,940 --> 01:49:34,780
Yep.

1708
01:49:35,940 --> 01:49:39,140
So ratio, I feel bad going too crazy into this.

1709
01:49:39,140 --> 01:49:40,500
It's a division.

1710
01:49:40,500 --> 01:49:42,740
So you got impressions, you got clicks.

1711
01:49:42,740 --> 01:49:46,020
Divide one, clicks divided by impressions,

1712
01:49:46,020 --> 01:49:48,500
we get our CTR, voila.

1713
01:49:48,500 --> 01:49:51,660
So again, the previous ones we talked about,

1714
01:49:51,660 --> 01:49:56,660
it's a transformation on one piece of data, one field,

1715
01:49:56,780 --> 01:49:58,940
and the ratio gives you a transformation,

1716
01:49:58,940 --> 01:50:03,940
giving you a feature with a relationship between two fields.

1717
01:50:03,980 --> 01:50:05,500
So bit of comparison there.

1718
01:50:07,740 --> 01:50:10,620
All right, some of our fun ones.

1719
01:50:10,620 --> 01:50:11,860
Rolling windows.

1720
01:50:13,180 --> 01:50:15,420
Let's imagine that each one of these is a month,

1721
01:50:15,420 --> 01:50:17,540
even though we're stopping at 10,

1722
01:50:17,540 --> 01:50:21,860
and we want to actually take a rolling calculation.

1723
01:50:21,860 --> 01:50:26,300
So we would, for example, take three months worth of data

1724
01:50:26,300 --> 01:50:29,620
and we'd perform some form of calculation on it,

1725
01:50:29,620 --> 01:50:32,740
let's say an average, or a min or a max.

1726
01:50:32,740 --> 01:50:35,260
It literally can be any kind of calculation you want.

1727
01:50:35,260 --> 01:50:36,940
But we've defined a window,

1728
01:50:36,940 --> 01:50:39,620
in this case the window is a length of three,

1729
01:50:39,620 --> 01:50:42,940
and we roll it across with each one,

1730
01:50:42,940 --> 01:50:46,420
and we do that calculation across each window

1731
01:50:46,420 --> 01:50:48,180
as we're sliding it across.

1732
01:50:48,180 --> 01:50:50,260
So as a rolling windows approach,

1733
01:50:52,060 --> 01:50:53,420
it does something nice,

1734
01:50:53,420 --> 01:50:56,000
it actually smooths out your data set for you,

1735
01:50:56,000 --> 01:50:58,380
and can be really nice for actually capturing

1736
01:50:58,380 --> 01:51:00,340
what the general trend is,

1737
01:51:00,340 --> 01:51:03,420
and kind of denoising some of the choppiness that's in there.

1738
01:51:03,420 --> 01:51:08,020
So very popular technique, and yeah, I'll leave it at that.

1739
01:51:08,020 --> 01:51:09,460
There are some folks that like to play with that

1740
01:51:09,460 --> 01:51:12,800
in the stock market data, but leave it there.

1741
01:51:15,460 --> 01:51:18,260
Here's a table example to go through.

1742
01:51:18,260 --> 01:51:21,220
The big thing to call out here, we are doing average,

1743
01:51:21,220 --> 01:51:24,200
but because we're doing this rolling,

1744
01:51:24,200 --> 01:51:25,860
and we have chosen three,

1745
01:51:27,420 --> 01:51:29,580
whatever length of window you choose,

1746
01:51:29,580 --> 01:51:32,540
you're gonna lose that amount of data off the top,

1747
01:51:32,540 --> 01:51:33,380
essentially.

1748
01:51:34,340 --> 01:51:35,660
It's because we've chosen three,

1749
01:51:35,660 --> 01:51:39,500
we can't have an average for the first two items.

1750
01:51:39,500 --> 01:51:41,900
We have to go down to the third one

1751
01:51:41,900 --> 01:51:44,860
in order to produce an average of three numbers.

1752
01:51:44,860 --> 01:51:47,860
So whatever n is, the length of your window,

1753
01:51:47,860 --> 01:51:51,300
n minus one data points are kinda gonna get washed out.

1754
01:51:52,500 --> 01:51:53,900
Okay?

1755
01:51:53,900 --> 01:51:55,220
And so that's all this is doing,

1756
01:51:55,220 --> 01:51:57,940
is taking an average of the top three months,

1757
01:51:57,940 --> 01:52:01,660
and storing it here under March 2020 at 2.5.

1758
01:52:01,660 --> 01:52:04,200
The next three months, February, March, April,

1759
01:52:04,200 --> 01:52:05,580
get stored here.

1760
01:52:06,460 --> 01:52:07,780
So that's 3.1, so on and so forth,

1761
01:52:07,780 --> 01:52:10,080
as we're sliding that window across time.

1762
01:52:12,620 --> 01:52:16,020
I'm moving, but shout at me if you have questions.

1763
01:52:20,220 --> 01:52:23,020
Okay, something somewhat kinda related,

1764
01:52:23,020 --> 01:52:25,260
but not exactly the same as lag.

1765
01:52:25,260 --> 01:52:27,540
So especially if you're dealing with

1766
01:52:27,540 --> 01:52:29,860
time series based stuff, lag is really popular,

1767
01:52:29,860 --> 01:52:33,640
but anytime you have a data point,

1768
01:52:33,640 --> 01:52:35,280
and you believe there's a relationship

1769
01:52:35,280 --> 01:52:37,440
between that data point, and again,

1770
01:52:37,440 --> 01:52:39,580
it's usually time or sequence based,

1771
01:52:39,580 --> 01:52:43,360
you think there's a relationship to the next data point,

1772
01:52:43,360 --> 01:52:45,160
lag is something that you wanna look at.

1773
01:52:45,160 --> 01:52:48,400
I should also note for fun that you can think of

1774
01:52:48,400 --> 01:52:50,280
human language as a sequence.

1775
01:52:50,280 --> 01:52:52,760
It has to be ordered for it to make any sense.

1776
01:52:52,760 --> 01:52:54,440
You can't just jumble it in any order.

1777
01:52:54,440 --> 01:52:57,720
And so there's kind of a relationship that occurs there.

1778
01:52:57,720 --> 01:52:58,560
Yes?

1779
01:52:58,560 --> 01:52:59,400
Yeah?

1780
01:53:05,400 --> 01:53:06,240
Yes.

1781
01:53:07,580 --> 01:53:08,960
Absolutely, yep.

1782
01:53:09,920 --> 01:53:11,160
Yeah, absolutely.

1783
01:53:12,520 --> 01:53:15,480
So all we're doing is taking that data set

1784
01:53:15,480 --> 01:53:19,860
and just shifting it, and so now we can look across time,

1785
01:53:20,760 --> 01:53:25,040
and run kinda like this correlation for March

1786
01:53:25,040 --> 01:53:28,220
on the previous timestamp, which was February.

1787
01:53:28,220 --> 01:53:31,260
So that's lag for you.

1788
01:53:34,820 --> 01:53:36,340
Okay, date time.

1789
01:53:36,340 --> 01:53:38,060
This is one of my favorites,

1790
01:53:39,460 --> 01:53:43,260
cause a date seems like not a lot going on there,

1791
01:53:43,260 --> 01:53:45,740
but there's a lot of interesting information

1792
01:53:45,740 --> 01:53:48,380
that's embedded in a date, technically speaking.

1793
01:53:49,860 --> 01:53:53,140
So you can have yes, the month, the day of the month,

1794
01:53:53,140 --> 01:53:54,500
the year, so on and so forth,

1795
01:53:54,500 --> 01:53:56,380
but you can have things like,

1796
01:53:56,380 --> 01:53:57,700
is it the weekend, yes or no,

1797
01:53:57,820 --> 01:53:58,780
or is it a weekday?

1798
01:53:58,780 --> 01:54:01,260
That can be a feature that actually gets used.

1799
01:54:01,260 --> 01:54:04,860
In fact, that's actually a feature that gets used,

1800
01:54:05,980 --> 01:54:09,740
I guess, in the cash department at the Fed.

1801
01:54:09,740 --> 01:54:12,400
So when they're looking at how much cash is flowing in

1802
01:54:12,400 --> 01:54:15,040
versus not, they really care about whether it's a weekend

1803
01:54:15,040 --> 01:54:18,240
or a weekday, and we can extract that from a date

1804
01:54:18,240 --> 01:54:19,080
as a feature.

1805
01:54:21,020 --> 01:54:23,740
Likewise with holidays, you get spikes and behaviors

1806
01:54:23,740 --> 01:54:26,180
in the holiday section.

1807
01:54:26,180 --> 01:54:27,620
So there's all this information

1808
01:54:27,620 --> 01:54:29,220
that's actually kind of technically wrapped up

1809
01:54:29,220 --> 01:54:30,700
in just something as simple as a date

1810
01:54:30,700 --> 01:54:33,060
that you can extract out and use

1811
01:54:33,060 --> 01:54:35,860
to make a clear signal for whatever process

1812
01:54:35,860 --> 01:54:37,980
you're looking at.

1813
01:54:38,860 --> 01:54:42,380
So with that, that pretty much sums up date time

1814
01:54:42,380 --> 01:54:45,640
and at least a quick overview of features.

1815
01:54:45,640 --> 01:54:48,460
Like I said, this is kind of the end for me on this part

1816
01:54:48,460 --> 01:54:51,020
before Leah dives you into all the coding piece.

1817
01:54:52,060 --> 01:54:53,820
I did mention at the very beginning

1818
01:54:53,820 --> 01:54:57,780
that we are a part of data science rebalanced,

1819
01:54:57,780 --> 01:54:59,100
helping to try to bridge that gap

1820
01:54:59,100 --> 01:55:01,540
between academia and industry.

1821
01:55:01,540 --> 01:55:03,540
The core way that we do that is by offering

1822
01:55:03,540 --> 01:55:06,220
a data science workshop where we actually teach

1823
01:55:06,220 --> 01:55:09,700
about our own sort of data science life cycle

1824
01:55:09,700 --> 01:55:12,720
from acquiring the customer's story and understanding

1825
01:55:12,720 --> 01:55:15,580
to collecting the data, challenging the data,

1826
01:55:15,580 --> 01:55:17,220
and even creating a demonstration

1827
01:55:17,220 --> 01:55:20,520
that will move your customers to implement your solution.

1828
01:55:21,480 --> 01:55:26,480
So the way our workshop works is we do video lessons.

1829
01:55:26,720 --> 01:55:29,280
You can paste them yourself so you don't feel rushed

1830
01:55:29,280 --> 01:55:30,880
like you might right this moment.

1831
01:55:33,240 --> 01:55:36,320
There's actually a weekly live coaching call with us

1832
01:55:36,320 --> 01:55:40,120
to actually help out with triaging personal feedback,

1833
01:55:40,120 --> 01:55:43,000
career advice, anything in that kind of space.

1834
01:55:43,000 --> 01:55:46,000
We also provide assignments and general broad

1835
01:55:46,000 --> 01:55:48,560
community discussion around data science topics

1836
01:55:48,600 --> 01:55:50,700
in our product.

1837
01:55:52,040 --> 01:55:54,560
I wanted to give a shout out that our next workshop

1838
01:55:54,560 --> 01:55:56,120
is coming up May 7th.

1839
01:55:56,120 --> 01:56:00,480
If you have any interest, everybody at PyCon, 20% off,

1840
01:56:00,480 --> 01:56:03,040
coupon code PyCon 2023.

1841
01:56:03,040 --> 01:56:06,320
So with that, I appreciate your time.

1842
01:56:08,220 --> 01:56:10,120
We got a few seats left in the spot,

1843
01:56:10,120 --> 01:56:12,920
so if you have any interest, I would say definitely

1844
01:56:12,920 --> 01:56:14,420
hop on it before it goes away.

1845
01:56:16,640 --> 01:56:17,820
All right.

1846
01:56:17,820 --> 01:56:18,660
All you, Leah.

1847
01:56:19,860 --> 01:56:21,520
Thanks, Ray.

1848
01:56:21,520 --> 01:56:22,980
Yeah, I would love to have you guys in the workshop

1849
01:56:22,980 --> 01:56:24,820
if that interests you at all.

1850
01:56:24,820 --> 01:56:26,300
Hopefully you've enjoyed what you've had today.

1851
01:56:26,300 --> 01:56:28,260
We've got a little bit more left.

1852
01:56:28,260 --> 01:56:30,700
We're on the very home stretch,

1853
01:56:30,700 --> 01:56:33,100
so not too much more here to go.

1854
01:56:33,100 --> 01:56:34,780
Okay, so we're gonna dive in again.

1855
01:56:34,780 --> 01:56:37,460
How do we do all this stuff in Python?

1856
01:56:37,460 --> 01:56:40,180
We're gonna start off with our min-max normalization,

1857
01:56:40,180 --> 01:56:42,860
and with that, we're gonna use scikit-learn,

1858
01:56:42,860 --> 01:56:44,620
which we've used before throughout the workshop,

1859
01:56:44,860 --> 01:56:48,260
it has a built-in function to our rescue once again,

1860
01:56:48,260 --> 01:56:50,380
called min-max-scaler.

1861
01:56:50,380 --> 01:56:52,760
To apply it to our dataset, what we have to do

1862
01:56:52,760 --> 01:56:55,540
is just initialize it like we've done several times before

1863
01:56:55,540 --> 01:56:58,260
in the workshop, and then we apply the fit-transform

1864
01:56:58,260 --> 01:56:59,880
to the column we wanna transform.

1865
01:56:59,880 --> 01:57:02,300
And in this case, we're gonna do that on the close price

1866
01:57:02,300 --> 01:57:05,540
of our stocks, and this whole mess here

1867
01:57:05,540 --> 01:57:08,300
is me just assigning that back to the appropriate columns,

1868
01:57:08,300 --> 01:57:12,380
and I'm calling it close underscore MMS for min-max-scaling,

1869
01:57:12,380 --> 01:57:13,940
so that's what I'm doing there.

1870
01:57:13,980 --> 01:57:18,700
So if we run that, what we can see is in the dataset,

1871
01:57:18,700 --> 01:57:20,980
our data has been scaled between zero and one,

1872
01:57:20,980 --> 01:57:22,740
just like we would expect.

1873
01:57:22,740 --> 01:57:25,520
So it looks like on the first day of the year

1874
01:57:25,520 --> 01:57:28,260
for all of these, or two of them, I guess,

1875
01:57:28,260 --> 01:57:29,760
it was one that had max value,

1876
01:57:29,760 --> 01:57:31,640
and then one had its min value that day.

1877
01:57:31,640 --> 01:57:33,420
So that's what that tells us.

1878
01:57:33,420 --> 01:57:36,460
So not really great at interpreting numbers

1879
01:57:36,460 --> 01:57:38,560
just on their own, so let's plot them.

1880
01:57:38,560 --> 01:57:41,000
We're gonna use some plotting that we learned earlier.

1881
01:57:41,000 --> 01:57:42,900
We're gonna plot first the unscaled version,

1882
01:57:42,900 --> 01:57:46,140
which we've seen this chart before, if you run that cell.

1883
01:57:46,140 --> 01:57:47,320
Same thing we showed earlier,

1884
01:57:47,320 --> 01:57:48,860
those are just the raw close prices

1885
01:57:48,860 --> 01:57:50,620
for each of those indices,

1886
01:57:50,620 --> 01:57:55,100
and then we can compare that to our scaled values.

1887
01:57:55,100 --> 01:57:56,660
So if you remember, actually, from last time

1888
01:57:56,660 --> 01:57:59,300
when we looked at this, that VIX was on a different scale,

1889
01:57:59,300 --> 01:58:01,260
so we ended up plotting that on its own,

1890
01:58:01,260 --> 01:58:03,740
but one way to avoid that is just by scaling the data,

1891
01:58:03,740 --> 01:58:06,140
so that's what we've done here using min-max-scaling,

1892
01:58:06,140 --> 01:58:08,140
and so we can see they're all on the same scale now

1893
01:58:08,140 --> 01:58:10,700
between zero and one, and we can see that trend,

1894
01:58:10,700 --> 01:58:12,100
what's going on there better.

1895
01:58:13,900 --> 01:58:15,460
Questions on min-max-scaling?

1896
01:58:18,380 --> 01:58:20,980
Purpose, yeah, so a lot of times

1897
01:58:20,980 --> 01:58:23,760
for when you're trying to create inputs

1898
01:58:23,760 --> 01:58:25,700
for a machine learning model,

1899
01:58:25,700 --> 01:58:28,180
what you wanna do is scale your data

1900
01:58:28,180 --> 01:58:32,260
so that the model doesn't handle outliers,

1901
01:58:32,260 --> 01:58:34,900
or it handles outliers better.

1902
01:58:34,900 --> 01:58:37,640
Ray, you got anything else you wanna add onto that?

1903
01:58:38,600 --> 01:58:43,600
Well, maybe not, oh, there we go.

1904
01:58:43,880 --> 01:58:45,280
The only other thing that I'd note

1905
01:58:45,280 --> 01:58:47,120
is that there are situations where

1906
01:58:48,120 --> 01:58:53,000
you may wanna compare different columns of data

1907
01:58:53,000 --> 01:58:55,680
that are on drastically different scales,

1908
01:58:55,680 --> 01:58:57,120
and so you can actually normalize them

1909
01:58:57,120 --> 01:58:59,640
and get a little bit closer, like how are they moving

1910
01:58:59,640 --> 01:59:02,440
or behaving relative to each other in the chart visually,

1911
01:59:02,440 --> 01:59:04,240
so that they're not so spread out

1912
01:59:04,240 --> 01:59:07,960
if they're too much of a difference in scale between them.

1913
01:59:07,960 --> 01:59:10,360
So that's another technique we've used for that.

1914
01:59:12,560 --> 01:59:13,400
Yeah?

1915
01:59:13,400 --> 01:59:16,120
Example of giving a column from the same length as P

1916
01:59:16,120 --> 01:59:17,880
on that same graph.

1917
01:59:19,280 --> 01:59:20,440
Sorry, say that one more time.

1918
01:59:20,440 --> 01:59:22,040
Oh, that's right there, okay.

1919
01:59:22,040 --> 01:59:22,880
Yes.

1920
01:59:22,880 --> 01:59:25,560
Columns are the same length as P.

1921
01:59:27,040 --> 01:59:29,080
Oh, shoot, do I have an error in there?

1922
01:59:29,080 --> 01:59:31,080
From the neumatic box, okay, I'll look at the escale.

1923
01:59:31,080 --> 01:59:33,640
Okay, sorry.

1924
01:59:34,440 --> 01:59:36,520
Probably another mistake on my part, sorry about that.

1925
01:59:36,520 --> 01:59:39,240
What's the scale over relative to the scale

1926
01:59:39,240 --> 01:59:40,200
that didn't relate to it?

1927
01:59:40,200 --> 01:59:43,080
Gotcha, gotcha, okay, yep, no problem.

1928
01:59:44,680 --> 01:59:45,520
Oh, yes?

1929
01:59:46,520 --> 01:59:50,000
So in this case, each dot would mean scale

1930
01:59:50,960 --> 01:59:53,320
being normalized to its own scale

1931
01:59:53,320 --> 01:59:56,800
not to the scale across all three, right?

1932
01:59:56,800 --> 01:59:59,640
I mean, so you could argue this is confusing now,

1933
01:59:59,640 --> 02:00:02,760
but is that all fluctuation? Yeah, it's true.

1934
02:00:02,760 --> 02:00:04,080
Yeah, no, you're right.

1935
02:00:04,120 --> 02:00:07,320
If I were to, so for this example,

1936
02:00:07,320 --> 02:00:09,960
we just ran it across each one separately,

1937
02:00:09,960 --> 02:00:13,200
but yes, if you're looking, depending on the situation,

1938
02:00:15,160 --> 02:00:16,920
there's a number of times where you would want

1939
02:00:16,920 --> 02:00:19,520
all of them to be scaled across the same sort

1940
02:00:19,520 --> 02:00:23,280
of general range, I guess is what we're getting at, yeah.

1941
02:00:24,400 --> 02:00:26,760
But this isn't, I guess it's more of an example

1942
02:00:26,760 --> 02:00:29,360
of how to do it in code as opposed to exactly

1943
02:00:29,360 --> 02:00:31,440
how you would do it for your use case.

1944
02:00:34,080 --> 02:00:35,600
I'm just trying to think, how would you,

1945
02:00:35,600 --> 02:00:38,760
how do you run the scalers on this,

1946
02:00:38,760 --> 02:00:42,080
or I don't know if you would want to do that.

1947
02:00:43,520 --> 02:00:45,200
Great question.

1948
02:00:45,200 --> 02:00:47,960
Couple different ways to go about it, technically.

1949
02:00:50,760 --> 02:00:52,960
If I were trying to force it.

1950
02:00:54,920 --> 02:00:58,520
Well, one way that I could force that is to actually

1951
02:00:58,520 --> 02:01:01,080
technically split this up as a fit

1952
02:01:01,080 --> 02:01:02,680
and then to transform separately.

1953
02:01:02,680 --> 02:01:04,800
So for these, we've just been running fit transform

1954
02:01:04,800 --> 02:01:06,920
all in one, but they actually are provided

1955
02:01:06,920 --> 02:01:10,520
as separate calls as well, and you could do a fit

1956
02:01:10,520 --> 02:01:12,920
across all the data and then it would transform

1957
02:01:12,920 --> 02:01:14,960
to your lowest and highest across all of it.

1958
02:01:14,960 --> 02:01:17,600
That's the way you could, that's off the top of my head,

1959
02:01:17,600 --> 02:01:20,000
the quick way you could pull that off.

1960
02:01:20,000 --> 02:01:21,880
Yeah, great question.

1961
02:01:21,880 --> 02:01:22,840
Good call out.

1962
02:01:26,080 --> 02:01:30,080
All right, next section is on how to do a log transformation.

1963
02:01:30,120 --> 02:01:34,080
So Ray alluded to this earlier,

1964
02:01:34,080 --> 02:01:36,560
but there are times when you should not do

1965
02:01:36,560 --> 02:01:39,520
a log transformation and there are times when you can.

1966
02:01:39,520 --> 02:01:41,480
One of the base assumptions of when you're doing

1967
02:01:41,480 --> 02:01:44,480
a log transformation is your data needs to follow

1968
02:01:44,480 --> 02:01:47,920
a log normal distribution, and if your data doesn't follow

1969
02:01:47,920 --> 02:01:51,640
that, you shouldn't really be using a log transformation.

1970
02:01:51,640 --> 02:01:56,000
So, lucky for us, we picked this data set especially

1971
02:01:56,000 --> 02:01:59,160
because stock prices are assumed to be log normal,

1972
02:01:59,160 --> 02:02:02,680
but if you're not sure if your data does follow

1973
02:02:02,680 --> 02:02:05,560
a log normal distribution, one of the techniques

1974
02:02:05,560 --> 02:02:08,920
you can do to figure that out is use what's called

1975
02:02:08,920 --> 02:02:11,680
the Kolmogorov-Smirnov test.

1976
02:02:11,680 --> 02:02:14,160
That allows you to compare one distribution to another,

1977
02:02:14,160 --> 02:02:16,480
so test if your distribution is similar

1978
02:02:16,480 --> 02:02:19,440
to the log normal distribution, and you can pair that

1979
02:02:19,440 --> 02:02:21,520
with a hypothesis test, so like back when you learned

1980
02:02:21,520 --> 02:02:24,880
in your basic stats class about hypothesis testing

1981
02:02:24,880 --> 02:02:27,200
to be able to interpret those results.

1982
02:02:27,240 --> 02:02:30,080
So, got links to those, I'm not gonna go any further

1983
02:02:30,080 --> 02:02:32,220
really than that since our data, we're assuming,

1984
02:02:32,220 --> 02:02:34,320
to be log normal, but that's what you would do

1985
02:02:34,320 --> 02:02:37,320
if you weren't sure if your data set was log normal or not.

1986
02:02:38,800 --> 02:02:42,000
So, to do a log transformation, NumPy actually has

1987
02:02:42,000 --> 02:02:45,360
this built into it, and we're doing a natural log

1988
02:02:45,360 --> 02:02:46,840
transformation, and that's wrapped up

1989
02:02:46,840 --> 02:02:49,360
in this NumPy log function.

1990
02:02:49,360 --> 02:02:52,600
So, we're passing in the column that we wanna do,

1991
02:02:52,600 --> 02:02:54,640
and same thing as before, I'm just setting

1992
02:02:54,640 --> 02:02:56,700
the new column name that I wanna call it as,

1993
02:02:56,700 --> 02:03:00,460
which is close natural log, so if we run that,

1994
02:03:00,460 --> 02:03:03,600
we should get those values kicked out to us there.

1995
02:03:04,680 --> 02:03:06,620
Like we did earlier, let's go ahead and plot those

1996
02:03:06,620 --> 02:03:10,220
so we can see how that compares to the original data.

1997
02:03:10,220 --> 02:03:13,300
So, here's our unscaled data, and then that Excel there

1998
02:03:14,420 --> 02:03:16,040
prints us out the scaled data.

1999
02:03:18,100 --> 02:03:21,060
Any questions on log transformations?

2000
02:03:21,420 --> 02:03:22,260
Okay.

2001
02:03:25,940 --> 02:03:30,460
Next up, ratios, so should be a pretty intuitive concept,

2002
02:03:30,460 --> 02:03:33,580
so we're just dividing two data points against each other.

2003
02:03:33,580 --> 02:03:37,380
In stock market data, and like stock analysis specifically,

2004
02:03:37,380 --> 02:03:39,820
this is a concept called relative strength,

2005
02:03:39,820 --> 02:03:42,540
where people will take two stock tickers

2006
02:03:42,540 --> 02:03:45,000
and divide them out by each other to see how they're

2007
02:03:45,000 --> 02:03:47,340
comparing to each other and then make purchasing decisions

2008
02:03:47,340 --> 02:03:49,700
off of that, whether you want it like by or sell,

2009
02:03:49,700 --> 02:03:51,260
based on how that is doing.

2010
02:03:51,260 --> 02:03:53,180
More info on that in that link there

2011
02:03:53,180 --> 02:03:55,960
if you wanna check it out, but what we're doing here

2012
02:03:55,960 --> 02:03:59,020
is just using that division operator between the columns,

2013
02:03:59,020 --> 02:04:02,020
and so I'm taking the close price for SPI,

2014
02:04:02,020 --> 02:04:06,060
dividing it by VIX, and then doing similarly,

2015
02:04:06,060 --> 02:04:10,340
SPI divided by QQQ, and then QQQ divided by VIX.

2016
02:04:10,340 --> 02:04:14,440
So when we do that, we get our ratios between those two,

2017
02:04:14,440 --> 02:04:16,540
and that gives us what's called our relative strength

2018
02:04:16,540 --> 02:04:17,840
in the stock market world.

2019
02:04:18,680 --> 02:04:22,520
So we can plot those like we've done several times now.

2020
02:04:22,520 --> 02:04:24,680
So we get our ratios there,

2021
02:04:25,800 --> 02:04:29,400
and then kind of hard to see what's going on

2022
02:04:29,400 --> 02:04:33,900
with the SPI QQQ, so we can plot those out on their own.

2023
02:04:33,900 --> 02:04:37,000
So first I'll plot the other two on their own,

2024
02:04:37,000 --> 02:04:38,740
just so they're on their own scale,

2025
02:04:38,740 --> 02:04:41,280
and then likewise, we can do that

2026
02:04:41,280 --> 02:04:42,840
with that last one there too.

2027
02:04:43,600 --> 02:04:44,440
Yeah.

2028
02:04:44,440 --> 02:04:45,280
Yeah.

2029
02:04:45,280 --> 02:04:46,120
Yeah.

2030
02:04:46,120 --> 02:04:46,960
Yeah.

2031
02:04:50,960 --> 02:04:52,460
Any questions on ratios?

2032
02:04:54,480 --> 02:04:57,800
Okay, we'll keep rolling into rolling windows.

2033
02:04:59,280 --> 02:05:01,120
So what we can do with rolling windows,

2034
02:05:01,120 --> 02:05:02,560
there's a couple ways we can do this,

2035
02:05:02,560 --> 02:05:04,260
and we'll talk about two here.

2036
02:05:04,260 --> 02:05:07,440
So one is using the rolling function from pandas,

2037
02:05:07,440 --> 02:05:09,080
and then there's another function called pandas

2038
02:05:09,080 --> 02:05:11,460
called resample that we'll talk about here.

2039
02:05:11,460 --> 02:05:13,940
So what we're gonna do here is calculate

2040
02:05:13,940 --> 02:05:17,220
a three day rolling average for our close prices

2041
02:05:17,220 --> 02:05:19,820
for each of our individual stock tickers.

2042
02:05:19,820 --> 02:05:22,100
And how we're doing that here is all the way

2043
02:05:22,100 --> 02:05:23,780
to the right hand side over here.

2044
02:05:23,780 --> 02:05:25,460
We're calling the column we wanna do that on,

2045
02:05:25,460 --> 02:05:26,940
which is the close price.

2046
02:05:26,940 --> 02:05:29,760
We're calling rolling, we're passing in this number here,

2047
02:05:29,760 --> 02:05:31,960
and that is telling us like what window

2048
02:05:31,960 --> 02:05:33,220
we wanna be looking at.

2049
02:05:33,220 --> 02:05:35,420
In this case, our data is daily data,

2050
02:05:35,420 --> 02:05:39,340
so it ends up being a three day window that we're looking at.

2051
02:05:39,340 --> 02:05:40,780
And then finally at the end here,

2052
02:05:40,780 --> 02:05:43,820
we chain on what calculation do we wanna do.

2053
02:05:43,820 --> 02:05:45,100
Like Ray mentioned in the slides,

2054
02:05:45,100 --> 02:05:47,420
you can do any calculation you want.

2055
02:05:47,420 --> 02:05:50,020
If you wanna do median, standard deviation,

2056
02:05:50,020 --> 02:05:51,820
something way more complex than that,

2057
02:05:51,820 --> 02:05:53,980
we're just gonna keep it at average.

2058
02:05:53,980 --> 02:05:54,820
Yes.

2059
02:05:54,820 --> 02:05:59,820
Does the rolling need to have a date, time, index to work?

2060
02:06:00,820 --> 02:06:02,300
The question was does the rolling need

2061
02:06:02,300 --> 02:06:03,780
to have a date, time, index to work,

2062
02:06:03,780 --> 02:06:06,460
and I believe I think so.

2063
02:06:07,460 --> 02:06:09,820
But it might not, it might just be able to take

2064
02:06:09,820 --> 02:06:11,420
like position in the data frame.

2065
02:06:11,420 --> 02:06:13,620
I honestly, I don't know the answer to that.

2066
02:06:15,220 --> 02:06:17,580
I've always used it with date.

2067
02:06:17,580 --> 02:06:20,180
Yeah, same, always use it with dates.

2068
02:06:20,180 --> 02:06:21,580
Might not require it.

2069
02:06:21,580 --> 02:06:22,800
Yeah, I don't know.

2070
02:06:22,800 --> 02:06:23,640
Good question.

2071
02:06:25,260 --> 02:06:29,180
So we can run this cell and we will get those values.

2072
02:06:29,180 --> 02:06:31,340
And just like we showed in the slides,

2073
02:06:31,340 --> 02:06:33,580
we end up losing a couple of days

2074
02:06:33,580 --> 02:06:35,000
because we just don't have three days

2075
02:06:35,000 --> 02:06:36,600
to calculate that average.

2076
02:06:36,600 --> 02:06:38,800
But that's all right, not a big deal there.

2077
02:06:39,760 --> 02:06:41,060
So another way we can do this

2078
02:06:41,060 --> 02:06:44,200
is through the resample function in pandas.

2079
02:06:44,200 --> 02:06:47,040
And what this does is a little bit different.

2080
02:06:47,040 --> 02:06:49,240
So you'll notice here in our last example,

2081
02:06:49,240 --> 02:06:54,240
we still have a row for every date in our data frame.

2082
02:06:54,700 --> 02:06:57,920
What we're gonna see here when we run this resample

2083
02:06:57,920 --> 02:07:00,960
is that we actually like change the structure

2084
02:07:00,960 --> 02:07:04,320
or like the frequency of our data set

2085
02:07:04,320 --> 02:07:05,560
that we're working with.

2086
02:07:05,560 --> 02:07:08,600
So instead of having a row for every day

2087
02:07:09,440 --> 02:07:11,040
and maybe like putting the average for every row

2088
02:07:11,040 --> 02:07:12,640
and repeating it, what we're doing

2089
02:07:12,640 --> 02:07:14,400
is just like aggregating it up

2090
02:07:14,400 --> 02:07:16,560
so that we have the last day of the month

2091
02:07:16,560 --> 02:07:18,480
and that's what we're showing is the average

2092
02:07:18,480 --> 02:07:19,320
for that month.

2093
02:07:20,320 --> 02:07:22,040
And so we do that with resample.

2094
02:07:22,040 --> 02:07:25,220
You can pass in whatever value you want here.

2095
02:07:25,220 --> 02:07:27,000
I've linked to the list where it shows you

2096
02:07:27,000 --> 02:07:29,160
all of those different possible options.

2097
02:07:29,160 --> 02:07:31,100
We're just doing month for illustrating that

2098
02:07:31,100 --> 02:07:32,560
and then doing the average again here.

2099
02:07:32,560 --> 02:07:34,360
But you could do the standard deviation,

2100
02:07:34,360 --> 02:07:37,360
the median, whatever you want there for your calculations.

2101
02:07:39,560 --> 02:07:41,920
And last but not least in this section,

2102
02:07:41,920 --> 02:07:44,760
let's go ahead and plot those values

2103
02:07:44,760 --> 02:07:46,780
so we can see the trend line here.

2104
02:07:46,780 --> 02:07:50,060
So not like an easy way to do this out of the box.

2105
02:07:50,060 --> 02:07:53,100
So I had to do a little bit of Matplotlib plotting code here.

2106
02:07:53,100 --> 02:07:55,500
So setting up the three subplots here,

2107
02:07:55,500 --> 02:07:57,280
setting the size on them,

2108
02:07:57,280 --> 02:08:00,880
setting the format of the axis that I want it to show as.

2109
02:08:00,880 --> 02:08:02,500
And then finally over here,

2110
02:08:02,500 --> 02:08:05,000
we're adding in what do we want to plot?

2111
02:08:05,000 --> 02:08:07,080
So the first plot is gonna be the daily data,

2112
02:08:07,080 --> 02:08:08,720
which that's how our data came to us.

2113
02:08:08,720 --> 02:08:11,120
So we don't have to do any transformation to that.

2114
02:08:11,120 --> 02:08:13,520
Next one, we're gonna resample at the weekly level

2115
02:08:13,520 --> 02:08:14,780
to do a weekly average.

2116
02:08:14,780 --> 02:08:16,700
Then we're gonna resample at the month level

2117
02:08:16,700 --> 02:08:18,400
to do a monthly average.

2118
02:08:18,400 --> 02:08:21,320
Lastly there, I'm just setting the title

2119
02:08:21,320 --> 02:08:25,140
and making sure we can read the font hopefully at this size.

2120
02:08:26,440 --> 02:08:29,200
And so that gives us this chart that was in the slides

2121
02:08:29,200 --> 02:08:31,000
actually if you were paying close attention.

2122
02:08:31,000 --> 02:08:33,000
So we get that daily data first,

2123
02:08:33,000 --> 02:08:36,720
which is the data that are the way our data came to us.

2124
02:08:37,120 --> 02:08:38,440
Then we get weekly so that trend

2125
02:08:38,440 --> 02:08:39,920
gets a little bit more smoothed out.

2126
02:08:39,920 --> 02:08:41,700
And then monthly we can even see the trend

2127
02:08:41,700 --> 02:08:42,700
more clearly there.

2128
02:08:45,460 --> 02:08:48,460
Okay, and then I'm gonna go through this last section here.

2129
02:08:48,460 --> 02:08:51,000
Oh wait, sorry, two more sections.

2130
02:08:51,000 --> 02:08:52,060
One is lag.

2131
02:08:52,060 --> 02:08:55,840
So lag is very similar to that resampling kind of concept,

2132
02:08:55,840 --> 02:08:58,240
except instead of changing the format of our data,

2133
02:08:58,240 --> 02:09:01,800
we're gonna just shift it down by however many we want to.

2134
02:09:01,800 --> 02:09:03,120
So in this case,

2135
02:09:03,120 --> 02:09:05,440
pandas actually has a function called shift.

2136
02:09:05,440 --> 02:09:07,200
So that's what I'm using over here.

2137
02:09:07,200 --> 02:09:10,000
And then you pass in what you want to shift by.

2138
02:09:10,000 --> 02:09:12,800
So in this case, I am shifting by one day

2139
02:09:12,800 --> 02:09:14,800
because I'm doing this on our original data set

2140
02:09:14,800 --> 02:09:17,040
that we were working with, which was daily data.

2141
02:09:17,040 --> 02:09:18,920
But you could do,

2142
02:09:18,920 --> 02:09:21,560
if you wanted to take like the monthly resampling

2143
02:09:21,560 --> 02:09:23,320
that we did and you wanna shift by one month,

2144
02:09:23,320 --> 02:09:24,960
so have a one month lag,

2145
02:09:24,960 --> 02:09:26,760
you could do that to,

2146
02:09:26,760 --> 02:09:28,820
you could like the possibilities are endless here.

2147
02:09:28,820 --> 02:09:31,680
So this is just one example of how to do that.

2148
02:09:31,680 --> 02:09:34,840
We'll run that cell and then run the next one after that

2149
02:09:34,840 --> 02:09:38,120
and that shows us our results.

2150
02:09:38,120 --> 02:09:40,560
So we get the original close prices here

2151
02:09:40,560 --> 02:09:41,400
and then all we've done,

2152
02:09:41,400 --> 02:09:44,360
you'll notice is just shifted down by one day.

2153
02:09:44,360 --> 02:09:46,040
So this allows us on the same row

2154
02:09:46,040 --> 02:09:48,440
to see what was the close price on that day.

2155
02:09:48,440 --> 02:09:50,120
So January 4th in this case.

2156
02:09:50,120 --> 02:09:51,980
And then we also get the previous day,

2157
02:09:51,980 --> 02:09:53,840
which was January 3rd in that case.

2158
02:09:53,840 --> 02:09:56,680
So it's a really nice way,

2159
02:09:56,680 --> 02:10:00,120
lots of times used for like time series modeling,

2160
02:10:00,120 --> 02:10:01,580
especially like ARIMA models,

2161
02:10:01,580 --> 02:10:02,960
if you're familiar with those,

2162
02:10:02,960 --> 02:10:04,200
they have like a base assumption

2163
02:10:05,160 --> 02:10:07,960
the thing before it is correlated with what's coming next.

2164
02:10:07,960 --> 02:10:11,040
So that's why we use lags

2165
02:10:11,040 --> 02:10:14,200
to be able to tell the model about what happened in the past.

2166
02:10:15,400 --> 02:10:19,240
Okay, and then actual last section here is on date time.

2167
02:10:19,240 --> 02:10:22,560
So Ray mentioned there's a lot you can do with date times.

2168
02:10:22,560 --> 02:10:25,160
We're gonna just barely scratch the surface on it.

2169
02:10:26,480 --> 02:10:27,940
So much you can do here,

2170
02:10:27,940 --> 02:10:32,080
but what we can do is pull out actually a lot of information.

2171
02:10:32,080 --> 02:10:33,360
Where did my mouse go?

2172
02:10:33,360 --> 02:10:34,440
There it is.

2173
02:10:34,440 --> 02:10:37,480
Okay, so I've linked out here.

2174
02:10:37,480 --> 02:10:39,680
This is a really helpful resource that I found.

2175
02:10:39,680 --> 02:10:43,080
It uses the string format time function

2176
02:10:43,080 --> 02:10:43,920
that we found earlier,

2177
02:10:43,920 --> 02:10:45,240
but it's a nice cheat sheet for it

2178
02:10:45,240 --> 02:10:48,520
because this documentation is surprisingly hard to find,

2179
02:10:48,520 --> 02:10:49,440
at least it was for me,

2180
02:10:49,440 --> 02:10:51,660
like in the actual library itself.

2181
02:10:51,660 --> 02:10:53,040
So someone made a website for it.

2182
02:10:53,040 --> 02:10:56,560
So it must've been a problem for a lot of other people.

2183
02:10:56,560 --> 02:10:58,020
But it gives you just a cheat sheet

2184
02:10:58,020 --> 02:10:59,600
of all those different formatter options

2185
02:10:59,600 --> 02:11:00,800
that you can pass in.

2186
02:11:00,800 --> 02:11:02,240
Sorry, I know this is probably small.

2187
02:11:02,240 --> 02:11:03,840
Let me zoom in.

2188
02:11:03,840 --> 02:11:06,800
So you can get a variety of stuff just out of the box,

2189
02:11:06,800 --> 02:11:08,280
which is really nice.

2190
02:11:08,280 --> 02:11:10,320
I will say it does not do everything

2191
02:11:10,320 --> 02:11:11,480
that's possible with dates.

2192
02:11:11,480 --> 02:11:12,980
So that's where you kind of have to come in

2193
02:11:12,980 --> 02:11:15,400
and do a little bit of future engineering yourself.

2194
02:11:15,400 --> 02:11:18,200
And I think that's a few exercises we have.

2195
02:11:18,200 --> 02:11:20,400
This doesn't handle holidays or anything.

2196
02:11:20,400 --> 02:11:22,160
So there's some custom things you have to write

2197
02:11:22,160 --> 02:11:23,160
to handle holidays.

2198
02:11:23,160 --> 02:11:25,480
But this is really nice for just out of the box

2199
02:11:25,480 --> 02:11:26,960
if you need to get a whole bunch

2200
02:11:26,960 --> 02:11:28,360
of these different formats.

2201
02:11:29,200 --> 02:11:30,480
There's libraries for holidays,

2202
02:11:30,480 --> 02:11:33,240
but can we use certain holidays?

2203
02:11:33,240 --> 02:11:34,080
Right.

2204
02:11:34,080 --> 02:11:35,440
That's a good idea.

2205
02:11:35,440 --> 02:11:38,280
Yeah, yeah, question was about the libraries for holidays.

2206
02:11:38,280 --> 02:11:39,840
Yeah, there are holidays.

2207
02:11:39,840 --> 02:11:41,240
So you can do like US holidays

2208
02:11:41,240 --> 02:11:42,720
and any holiday in the world basically.

2209
02:11:42,720 --> 02:11:44,000
And they've already got the dates

2210
02:11:44,000 --> 02:11:45,720
of all those prepackaged for you.

2211
02:11:48,080 --> 02:11:51,440
We have separate holidays with the bank.

2212
02:11:51,440 --> 02:11:52,720
Oh, yeah.

2213
02:11:52,720 --> 02:11:55,880
Ray was talking about there's some bank holidays too

2214
02:11:55,880 --> 02:11:58,240
that are additional in there.

2215
02:11:59,080 --> 02:11:59,920
Yeah, yeah.

2216
02:11:59,920 --> 02:12:00,760
Yeah, that's why there's a lot of it.

2217
02:12:00,760 --> 02:12:01,800
That they don't always capture.

2218
02:12:01,800 --> 02:12:02,640
Yeah.

2219
02:12:03,680 --> 02:12:05,920
Okay, so we'll talk through this here real quick.

2220
02:12:05,920 --> 02:12:09,040
So what we're doing here is taking our date,

2221
02:12:09,040 --> 02:12:11,760
that date column that we worked with,

2222
02:12:11,760 --> 02:12:15,120
to use the string format or time function on it,

2223
02:12:15,120 --> 02:12:18,280
we have to do a little bit of manipulation here

2224
02:12:18,280 --> 02:12:19,960
to get it in the series format.

2225
02:12:19,960 --> 02:12:21,120
And then I'm using an apply,

2226
02:12:21,120 --> 02:12:22,600
but there's probably other ways to do it.

2227
02:12:22,600 --> 02:12:24,640
I think you were showing us some as well.

2228
02:12:24,640 --> 02:12:27,560
So this is just the way I chose to do it.

2229
02:12:27,560 --> 02:12:30,680
But I'm just passing in those different formatter options

2230
02:12:30,680 --> 02:12:34,000
from that reference document that we looked at earlier

2231
02:12:34,000 --> 02:12:36,760
to get to, for example, the year out of it,

2232
02:12:36,760 --> 02:12:38,120
so the month number.

2233
02:12:38,120 --> 02:12:38,960
I'll go ahead and run this

2234
02:12:38,960 --> 02:12:41,120
and we can see the output of it.

2235
02:12:41,120 --> 02:12:42,520
We get pulling from the date,

2236
02:12:42,520 --> 02:12:45,800
we're getting that year as a column, the month number.

2237
02:12:45,800 --> 02:12:48,880
We can get the long version of the text of the month.

2238
02:12:48,880 --> 02:12:51,720
We can get the short version of the text of the month.

2239
02:12:51,720 --> 02:12:53,680
So many different possibilities.

2240
02:12:53,720 --> 02:12:57,600
So that wraps us up there.

2241
02:12:57,600 --> 02:12:59,960
Since we were short on time,

2242
02:12:59,960 --> 02:13:02,320
we won't have time to go through all of the exercise,

2243
02:13:02,320 --> 02:13:03,840
but I think for the remaining amount of time,

2244
02:13:03,840 --> 02:13:05,680
we'll give you the time to do the exercise

2245
02:13:05,680 --> 02:13:07,720
if you choose to do that.

2246
02:13:07,720 --> 02:13:09,200
Otherwise, you pretty much have heard

2247
02:13:09,200 --> 02:13:10,280
all we had to say today.

2248
02:13:10,280 --> 02:13:12,840
So if you choose not to stick around, totally understand,

2249
02:13:12,840 --> 02:13:14,760
wanna go get dinner or something,

2250
02:13:14,760 --> 02:13:17,640
but really thank you guys for coming to our workshop.

2251
02:13:17,640 --> 02:13:20,000
Hope you enjoyed, hope you got something out of it.

2252
02:13:20,000 --> 02:13:22,160
And again, just a plug for our other workshop.

2253
02:13:22,160 --> 02:13:24,760
If you enjoyed hearing from us today,

2254
02:13:24,760 --> 02:13:26,880
we do similar stuff on our free time

2255
02:13:26,880 --> 02:13:29,000
and get to have live coaching calls with us

2256
02:13:29,000 --> 02:13:30,320
if that's any incentive.

2257
02:13:30,320 --> 02:13:32,600
So thank you again guys for everything.

2258
02:13:33,560 --> 02:13:34,400
Yeah.

2259
02:13:34,400 --> 02:13:35,240
Yeah.

2260
02:13:35,240 --> 02:13:36,080
Yeah.

2261
02:13:36,080 --> 02:13:36,920
Yeah.

2262
02:13:36,920 --> 02:13:37,760
Yeah.

2263
02:13:37,760 --> 02:13:38,600
Yeah.

2264
02:13:38,600 --> 02:13:39,440
Yeah.

2265
02:13:39,440 --> 02:13:40,280
Yeah.

2266
02:13:40,280 --> 02:13:41,120
Yeah.

2267
02:13:41,120 --> 02:13:41,940
Yeah.

2268
02:13:41,940 --> 02:13:42,780
Yeah.

2269
02:13:42,780 --> 02:13:43,620
Yeah.

2270
02:13:43,620 --> 02:13:44,440
Yeah.

2271
02:13:44,440 --> 02:13:45,280
Yeah.

2272
02:13:45,280 --> 02:13:46,120
Yeah.

2273
02:13:46,120 --> 02:13:46,940
Yeah.

2274
02:13:46,940 --> 02:13:47,780
Yeah.

2275
02:13:47,780 --> 02:13:48,620
Yeah.

2276
02:13:48,620 --> 02:13:49,440
Yeah.

2277
02:13:49,440 --> 02:13:50,280
Yeah.

2278
02:13:50,280 --> 02:13:51,120
Yeah.

2279
02:13:51,120 --> 02:13:52,640
And I'll send that out through the,

2280
02:13:52,640 --> 02:13:55,280
there's like a message attendees option.

2281
02:13:55,280 --> 02:13:56,120
Yep.

2282
02:13:56,120 --> 02:13:56,940
Yep.

2283
02:13:56,940 --> 02:13:57,780
So the one you got to set up the course,

2284
02:13:57,780 --> 02:13:59,920
it'll be similar email there.

2285
02:13:59,920 --> 02:14:00,840
Yeah, I can send that today.

2286
02:14:00,840 --> 02:14:02,040
No problem.

2287
02:14:02,040 --> 02:14:03,320
Yeah.

2288
02:14:03,320 --> 02:14:05,160
And we'll be hanging around for questions too,

2289
02:14:05,160 --> 02:14:07,760
as well, if you wanna come up and chat individually.

2290
02:14:09,760 --> 02:14:11,760
The question that I'm having is,

2291
02:14:11,760 --> 02:14:14,520
there are a lot of libraries that you wanna work.

2292
02:14:14,520 --> 02:14:18,400
I'm wondering, like, as a company,

2293
02:14:18,400 --> 02:14:22,560
how did you find libraries that are useful

2294
02:14:22,560 --> 02:14:25,060
and how did you go from that?

2295
02:14:26,520 --> 02:14:28,080
Yeah, yeah, great question.

2296
02:14:28,080 --> 02:14:31,160
The question was about, we used a lot of libraries today.

2297
02:14:31,160 --> 02:14:32,680
How did we find those libraries?

2298
02:14:32,680 --> 02:14:34,440
How do we know what libraries to use?

2299
02:14:34,440 --> 02:14:36,000
All that good stuff.

2300
02:14:36,000 --> 02:14:36,840
So I'll take a crack at it,

2301
02:14:36,840 --> 02:14:38,820
and then you can add on to Ray.

2302
02:14:39,840 --> 02:14:42,180
I'd say for general, like in data science,

2303
02:14:42,180 --> 02:14:44,560
there's pretty typical ones that you get used to.

2304
02:14:44,560 --> 02:14:47,760
Like, it's kind of like a tech stack for data science,

2305
02:14:47,760 --> 02:14:49,240
I would almost say.

2306
02:14:49,240 --> 02:14:51,520
So like Pandas, NumPy, Scikit-learn,

2307
02:14:51,520 --> 02:14:53,120
those are like really popular ones

2308
02:14:53,120 --> 02:14:55,600
that we probably use on a day-to-day basis.

2309
02:14:55,600 --> 02:14:57,960
So just going through projects over time,

2310
02:14:57,960 --> 02:14:59,460
we've learned that that's the tech stack,

2311
02:14:59,460 --> 02:15:00,840
but that seems to be community-wide,

2312
02:15:00,840 --> 02:15:03,240
that's what the tech stack is.

2313
02:15:03,240 --> 02:15:04,920
Other way, I would say, is Google.

2314
02:15:04,920 --> 02:15:08,580
I usually am Googling every single day for my job,

2315
02:15:08,580 --> 02:15:11,000
and then I'll find libraries that way,

2316
02:15:11,000 --> 02:15:14,920
of Stack Overflow, like someone using a certain technique.

2317
02:15:14,920 --> 02:15:16,760
That's where I pick up on new stuff a lot of times,

2318
02:15:16,760 --> 02:15:19,080
like the Y Data Profiling Library was a new one

2319
02:15:19,080 --> 02:15:21,920
that we picked up just from searching the internet.

2320
02:15:21,920 --> 02:15:24,160
So that's one way for me.

2321
02:15:24,160 --> 02:15:26,080
There's a lot of online resources as well,

2322
02:15:26,080 --> 02:15:30,720
like Towards Data Science is a popular medium publication

2323
02:15:30,720 --> 02:15:33,260
that usually people are on the cutting edge

2324
02:15:33,260 --> 02:15:34,560
of new packages that come out,

2325
02:15:34,560 --> 02:15:37,280
and they'll write up an article about how they use them.

2326
02:15:37,280 --> 02:15:40,020
So we like using that resource as well.

2327
02:15:40,020 --> 02:15:41,280
Ray, you got anything you wanna?

2328
02:15:41,280 --> 02:15:43,020
No, you pretty much nailed it for me.

2329
02:15:43,020 --> 02:15:46,400
I was gonna say, as a part of most data science projects,

2330
02:15:46,920 --> 02:15:49,200
even if I've solved a problem before,

2331
02:15:50,120 --> 02:15:53,160
the reality is, is a lot of times, stuff is updated

2332
02:15:53,160 --> 02:15:55,640
since the last time I worked on that type of problem.

2333
02:15:55,640 --> 02:15:57,660
And so I have the tendency to just always insert

2334
02:15:57,660 --> 02:16:01,360
a little bit of a literature review right into my workflow,

2335
02:16:01,360 --> 02:16:04,520
and that's where I'll hit up stuff on media.

2336
02:16:04,520 --> 02:16:05,920
I'm like, hey, I'm trying to do this thing,

2337
02:16:05,920 --> 02:16:08,680
and the next thing I know, oh, pops out an article,

2338
02:16:08,680 --> 02:16:11,000
and someone used the library I never heard of

2339
02:16:11,000 --> 02:16:15,080
to do it way faster than the way I had done it previously.

2340
02:16:15,760 --> 02:16:18,560
Like I hadn't used that Y Finance library

2341
02:16:18,560 --> 02:16:20,400
because I don't really work with stock data.

2342
02:16:20,400 --> 02:16:22,160
We just chose it for this workshop,

2343
02:16:22,160 --> 02:16:26,240
and I had Googled, how do you get stock market data easily

2344
02:16:26,240 --> 02:16:28,560
or something like in Python, and that library popped up.

2345
02:16:28,560 --> 02:16:30,980
So usually I'm trying to do something,

2346
02:16:30,980 --> 02:16:33,180
and then that's how I learn about the library.

