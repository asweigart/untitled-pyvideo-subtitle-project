1
00:00:00,000 --> 00:00:15,000
So welcome everyone to the Getting Started with Polars class. I'm Matt Harrison. So if you go to this link, this will take you to the GitHub project here for the course.

2
00:00:15,000 --> 00:00:20,000
And one thing I found out is that, let me...

3
00:00:20,000 --> 00:00:29,000
So the link is Matt Harrison in GitHub and then it's 2023-PyCon-Polars.

4
00:00:29,000 --> 00:00:41,000
If you hit period when you're in here, it will launch this thing called Codespaces and you can actually run this all from Codespaces too if you want to.

5
00:00:41,000 --> 00:00:50,000
So I don't know if anyone needs help with that, but otherwise if you download the note...if you clone the project or you can download the notebook, either way will work.

6
00:00:50,000 --> 00:01:05,000
And then pip install Polars. You should be mostly good. You probably want to install, I believe, PyArrow as well if you want some Pandas capability.

7
00:01:05,000 --> 00:01:19,000
Okay, so I just want to maybe go around the class really quickly and understand our backgrounds here. So who here is working sort of in a data science-like capacity?

8
00:01:19,000 --> 00:01:42,000
Okay.

9
00:01:42,000 --> 00:01:50,000
Okay. So we have a lot of data science-y people in here. Who here uses Pandas? Okay.

10
00:01:50,000 --> 00:02:04,000
So I imagine you're here because you're either looking for something different or you're just curious. I mean, I'd be interested in knowing what your motivation is for being here.

11
00:02:04,000 --> 00:02:18,000
One thing to note is we are kind of small because this was just put together Monday. The PyCon reached out to me and said, do you want to do a tutorial? We have someone drop out and make sure, okay, you're doing it, and that was the end of it.

12
00:02:18,000 --> 00:02:28,000
So there's not been a lot of marketing or whatnot, but I guess that's fine for you all. What's the motivation for Polars? Anyone want to share?

13
00:02:34,000 --> 00:02:51,000
Okay.

14
00:02:51,000 --> 00:03:14,000
So the comment for those who didn't hear, speed, memory, and ancillary interest in Rust as well. Okay.

15
00:03:14,000 --> 00:03:24,000
So whether Polars is the end all be all to all of our Pandas woes sort of thing. Okay. Anyone else?

16
00:03:24,000 --> 00:03:26,000
Curious.

17
00:03:26,000 --> 00:03:44,000
Curious. Okay. Okay. So let me give my background just so we're like on the same page as to like what's going on here. I mean, I wrote this book, Effective Pandas, and my job is corporate trainer.

18
00:03:44,000 --> 00:04:01,000
So I like to say that I sell snake oil and teach people to tell lies with data, but I spend a good deal of my time in front of larger groups than this helping them level up in Python and learn things like Pandas and machine learning.

19
00:04:01,000 --> 00:04:12,000
And so I've been using Pandas almost since it came out in both professional capacity and then more recently in consulting and training capacity.

20
00:04:12,000 --> 00:04:23,000
But I use it often on my own data as well, in addition to like consulting and whatnot. So I feel relatively comfortable with Pandas.

21
00:04:23,000 --> 00:04:32,000
And, you know, Polars has been out for a while. And at the end of last year, I'm like, okay, maybe I should I should like see what this buzz is about.

22
00:04:32,000 --> 00:04:48,000
And so this this tutorial is a result of me basically taking, I would say, like a lot of content that I do in Pandas and sort of porting that to Polars and seeing like where the give and take is.

23
00:04:48,000 --> 00:04:58,000
So am I a Polars expert? No, not by any shot of the imagination. Am I a Rust expert? No, I'm not a Rust expert either.

24
00:04:58,000 --> 00:05:07,000
So what we're what we're going to get here is I'm biased, but a biased look at Polars.

25
00:05:07,000 --> 00:05:22,000
But hopefully we can sort of see like where the pros and cons are. And I think we can come to some preliminary suggestions on when it might make sense to use and explore a little bit more.

26
00:05:22,000 --> 00:05:37,000
So again, you can go to that URL. Just download the notebook, throw it into Jupiter and make sure that you pip installed. You'll want to have installed Polars and I believe Pi Arrow as well.

27
00:05:37,000 --> 00:05:44,000
If you pip install both of those, you should be good. So I'm going to restart my kernel here.

28
00:05:44,000 --> 00:05:46,000
And I'm going to clear my output as well.

29
00:05:48,000 --> 00:05:49,000
OK.

30
00:05:51,000 --> 00:05:53,000
So and I'm.

31
00:05:54,000 --> 00:06:01,000
I think I think the schedule as we go till 1030 then we have a break. I'm not quite sure how long the break is.

32
00:06:01,000 --> 00:06:18,000
OK, I've heard anywhere from 10 to 30 minutes. So OK, 10 minute break at that time. And then basically I've got this notebook here. It's got a bunch of examples, but also has lab work in it as well.

33
00:06:18,000 --> 00:06:27,000
So what we'll do is all talk for a little bit. You can ask questions or whatnot. Nice thing about a notebook is you can try things out and play around in it.

34
00:06:27,000 --> 00:06:38,000
And then we do have some labs in there. So I'll give you some time to work on the labs. Try out Polars on your own and we'll just sort of rinse and repeat that for for our time here.

35
00:06:39,000 --> 00:06:50,000
But yeah, like I said, I'm I'm pretty flexible here. So if you have questions, feel free to just raise your hand, shout them out and let's answer those.

36
00:06:50,000 --> 00:07:01,000
So I am using Polars 017.5. So I assume by the time this makes it out to like YouTube or whatever, this will be an outdated version of Polars.

37
00:07:01,000 --> 00:07:13,000
Sort of only half kidding there, but like they release Polars pretty quickly. And so that is something to note is that actually did a version of this talk at the end of last year.

38
00:07:13,000 --> 00:07:16,000
And and I would say.

39
00:07:16,000 --> 00:07:25,000
A good not a lot. Well, not a lot. Portions of it broke from from doing it last year, right? So.

40
00:07:25,000 --> 00:07:34,000
What's my point there? My point is that Polars is is moving and evolving and it's not the API isn't stable per se, right?

41
00:07:34,000 --> 00:07:43,000
In fact, when I was doing the when I gave this talk or gave this.

42
00:07:43,000 --> 00:07:52,000
Course a couple months ago, the creator of Polars actually reached out to me and I went through it with him.

43
00:07:52,000 --> 00:08:01,000
At that point in time and so he kind of gave it a thumbs up or a semi blessing and said, OK, yeah, we're good there.

44
00:08:01,000 --> 00:08:07,000
And it was kind of nice because he walked me through a lot of them and the mentality and the ideas of Polars.

45
00:08:07,000 --> 00:08:18,000
So if Richie's watching this, I'm sure he'll comment or correct things that I might say or misspeak at this point in time, because he seems to be pretty involved or at least.

46
00:08:18,000 --> 00:08:26,000
So so let's load some data here so you can load this data, which is.

47
00:08:26,000 --> 00:08:34,000
Actually, I'm going to let me let me reload this page here.

48
00:08:34,000 --> 00:08:38,000
Make sure that I have the latest here.

49
00:08:38,000 --> 00:08:45,000
OK, so so I'm you know, I'm not going to go into the details here, but I'm going to go into the details here.

50
00:08:46,000 --> 00:08:55,000
OK, so so you can load this data if you've checked it out, otherwise, otherwise, if you haven't checked it out, run this one down here.

51
00:08:55,000 --> 00:08:58,000
This will fetch it from the Internet.

52
00:08:58,000 --> 00:09:01,000
And so I'm just using in Polars.

53
00:09:01,000 --> 00:09:08,000
There's a read CSV function and that is making what we call a data frame here.

54
00:09:08,000 --> 00:09:12,000
And so this is the output of doing that.

55
00:09:12,000 --> 00:09:14,000
Anyone having issues doing that?

56
00:09:14,000 --> 00:09:20,000
So ideally, you're you're able to run what I'm doing and sort of follow along with me.

57
00:09:20,000 --> 00:09:23,000
So if you need help with that, let me know.

58
00:09:23,000 --> 00:09:25,000
This is the output. This is a data frame.

59
00:09:25,000 --> 00:09:30,000
So data frame for for those everyone in here probably knows.

60
00:09:30,000 --> 00:09:35,000
But for those who are watching, you might not know is basically a representation of tabular data.

61
00:09:35,000 --> 00:09:40,000
And generally, when you're doing analytics, you want the columns to be of the same type.

62
00:09:40,000 --> 00:09:43,000
So this is a Polars data frame at the top.

63
00:09:43,000 --> 00:09:44,000
You can see the shape of it.

64
00:09:44,000 --> 00:09:48,000
It's got fifty seven hundred rows and forty columns.

65
00:09:48,000 --> 00:09:52,000
This happens to be my tweet data for some portion of time.

66
00:09:52,000 --> 00:09:55,000
This is from Twitter. These are my tweets.

67
00:09:55,000 --> 00:09:59,000
And so this has analytic information about how I tweeted here.

68
00:09:59,000 --> 00:10:11,000
So what we're going to do is we're going to go through this and sort of assume that I was given this data set for maybe like a consulting gig or I'm like doing a data analysis on this.

69
00:10:11,000 --> 00:10:16,000
And this is kind of my process of going through this data set and exploring it and cleaning it up.

70
00:10:19,000 --> 00:10:26,000
I mean, this so so a couple things to note, if you are familiar with pandas here, I mean, this looks kind of like a panel's data frame.

71
00:10:26,000 --> 00:10:29,000
I mean, it says shape at the top. So the output is a little bit different.

72
00:10:29,000 --> 00:10:31,000
Panda says shape at the bottom.

73
00:10:31,000 --> 00:10:37,000
It's got it looks like column names, which should be familiar for people who are familiar with pandas.

74
00:10:37,000 --> 00:10:39,000
And then below that, it's got types here.

75
00:10:41,000 --> 00:10:45,000
Just I think everyone's here is probably aware about pan.

76
00:10:45,000 --> 00:10:46,000
Python is a slow language.

77
00:10:46,000 --> 00:10:48,000
Python is not a fast language.

78
00:10:48,000 --> 00:10:58,000
And one of the things that makes a data frame library fast is leveraging some lower level tooling to optimize memory.

79
00:10:58,000 --> 00:11:06,000
And so pandas in the case of pandas prior to two point oh leverage numpy under the covers as a pandas to you get pi under the covers.

80
00:11:07,000 --> 00:11:08,000
And arrow support.

81
00:11:09,000 --> 00:11:15,000
Polar is leveraging rust and a library called Arrow under the covers of rest implementation of this arrow.

82
00:11:16,000 --> 00:11:22,000
Arrow is an in memory representation to store.

83
00:11:24,000 --> 00:11:26,000
Analytic data or tabular data.

84
00:11:26,000 --> 00:11:37,000
And and the benefit you get there versus saying storing this in pure Python is that basically arrows going to say here is a column that represents the tweet ID.

85
00:11:37,000 --> 00:11:39,000
That's a 64 bit integer.

86
00:11:39,000 --> 00:11:41,000
So we've got 5700 of these.

87
00:11:41,000 --> 00:11:46,000
So it's going to make a buffer that's like 5700 times 64 64 bits.

88
00:11:46,000 --> 00:11:51,000
And all of that will be the tweet ID information in there.

89
00:11:52,000 --> 00:12:01,000
If you were doing that in Python, you would have a Python object for each of those ideas and there'd be a couple dozen bytes of overhead for each of those.

90
00:12:01,000 --> 00:12:11,000
So in addition to saving memory, pyro also supports low level operations that take advantage things like multi core or SIMD instructions.

91
00:12:11,000 --> 00:12:17,000
So when you do certain operations, they can be optimized and run very quickly.

92
00:12:17,000 --> 00:12:30,000
So a data frame library like this kind of gives you the best of both worlds and that you like you can write in a Python syntax but get C or rust speeds by doing that.

93
00:12:32,000 --> 00:12:33,000
And so.

94
00:12:33,000 --> 00:12:34,000
And so.

95
00:12:39,000 --> 00:12:49,000
We can we can scroll through this if we want to, but you can see that it's showing some amount of data here and some amount of columns.

96
00:12:49,000 --> 00:12:56,000
One thing to note here is if you're familiar with pandas, there is no index here.

97
00:12:56,000 --> 00:13:01,000
So pandas will stick in an index.

98
00:13:01,000 --> 00:13:09,000
You can think of an index as a row ID for some purposes, even though indexes don't have to be unique.

99
00:13:09,000 --> 00:13:13,000
And there is no index in in here.

100
00:13:13,000 --> 00:13:24,000
So that that causes, I guess, changes or effects that if you're coming from pandas, you just need to be aware that there isn't an index here.

101
00:13:24,000 --> 00:13:32,000
Also, pandas does have the ability to do hierarchical columns and hierarchical multi indexes.

102
00:13:32,000 --> 00:13:39,000
And pandas uses the same data structure to represent an index and to represent columns.

103
00:13:39,000 --> 00:13:44,000
And pollers does not have hierarchical columns either.

104
00:13:44,000 --> 00:13:48,000
So it's just all flat.

105
00:13:48,000 --> 00:13:56,000
So in these cells here, I've just listed out what you get just to sort of show you.

106
00:13:56,000 --> 00:14:04,000
So in in the pollers package itself, you have a good deal of functions of things that you can do.

107
00:14:04,000 --> 00:14:11,000
And then inside of the data frame, you've got a good deal of things that you can do.

108
00:14:11,000 --> 00:14:19,000
Now, if I did this with a pandas data frame, there's like 400 different things that you can do with a pandas data frame here.

109
00:14:19,000 --> 00:14:24,000
With pollers, you're looking at quite a few less than that.

110
00:14:24,000 --> 00:14:31,000
So 176, I think pandas is around like 430 or so attributes on a data frame.

111
00:14:31,000 --> 00:14:36,000
So a little bit less exposed there from that point of view.

112
00:14:36,000 --> 00:14:44,000
And then one thing to be aware of that there's also this notion of a column.

113
00:14:44,000 --> 00:14:49,000
And a column lets you do things that are called expressions.

114
00:14:49,000 --> 00:14:54,000
And so I'm just showing what is available on the expressions.

115
00:14:54,000 --> 00:14:59,000
And you can see that there there's a good deal of things that you can do to a column.

116
00:15:00,000 --> 00:15:09,000
So like taking the standard deviation, the square root, the sorting it, even things that you couldn't do, for example, like in pandas, pandas doesn't have like sine.

117
00:15:09,000 --> 00:15:16,000
It does have the ability to do some things that pandas doesn't do.

118
00:15:16,000 --> 00:15:21,000
OK, so let's let's explore our data frame.

119
00:15:21,000 --> 00:15:25,000
I mean, like if I just got this, this is kind of the process that I would do.

120
00:15:25,000 --> 00:15:28,000
And if I was in pandas, I'd maybe explore the index.

121
00:15:28,000 --> 00:15:31,000
And again, there is no index in pollers.

122
00:15:31,000 --> 00:15:34,000
So just be aware of that.

123
00:15:34,000 --> 00:15:36,000
Here are the columns.

124
00:15:36,000 --> 00:15:41,000
It's just going to give us a list of those columns in pandas.

125
00:15:41,000 --> 00:15:42,000
I'd probably do something like this.

126
00:15:42,000 --> 00:15:48,000
I'd probably like, say, trans take a sample of 20 rows and transpose that.

127
00:15:48,000 --> 00:15:50,000
And that kind of works, too.

128
00:15:50,000 --> 00:15:53,000
And so why do I like to take a sample?

129
00:15:53,000 --> 00:16:05,000
What I find is that often the first n rows of the data are kind of either not complete or might have test data in them or the schema hasn't been completely fleshed out.

130
00:16:05,000 --> 00:16:15,000
So by looking at the first amount of rows in the data, that's often not good enough to give me sufficient understanding of what's going on in the data.

131
00:16:15,000 --> 00:16:18,000
So just taking a sample of those rows is often better.

132
00:16:18,000 --> 00:16:37,000
And then transposing is just a little hack that I like to do because often transposing, if you're not aware, if it flips the rows and the columns, transposing lets me often see a little bit more data than I would if I don't transpose it.

133
00:16:37,000 --> 00:16:39,000
So I'll often do that just to see a little bit more data.

134
00:16:39,000 --> 00:16:55,000
Sometimes I do transposing for part of a math operation, but I think to be aware of is because we are column oriented here, when we transpose the data, you can see that all these columns basically went to strings because in order to represent all those strings was like the lowest common

135
00:16:55,000 --> 00:16:58,000
denominator that could represent everything.

136
00:16:58,000 --> 00:17:09,000
So you might lose some memory efficiency and probably change might to probably will if you transpose your data, if it has different types and columns.

137
00:17:09,000 --> 00:17:11,000
We can also inspect the shape here.

138
00:17:11,000 --> 00:17:17,000
So this is just a Python tuple saying the number of rows and the number of columns.

139
00:17:17,000 --> 00:17:22,000
Okay, so once I've got my data loaded up, I've kind of initially explored some of it.

140
00:17:22,000 --> 00:17:25,000
I might dive deep into a little bit more of it.

141
00:17:25,000 --> 00:17:28,000
And so one of the things I might want to look at is the types.

142
00:17:28,000 --> 00:17:35,000
And so there is D types, which again is similar to what you would see in Pandas.

143
00:17:35,000 --> 00:17:42,000
So we see like int64, UTF-8, float64, int64.

144
00:17:42,000 --> 00:17:45,000
So again, just to be pedantic here, these are not Python types.

145
00:17:45,000 --> 00:17:47,000
There is no UTF-8 type in Python.

146
00:17:47,000 --> 00:17:49,000
There's no int64 type in Python.

147
00:17:49,000 --> 00:17:51,000
There's no float64 type in Python.

148
00:17:51,000 --> 00:17:53,000
These are pi arrow types.

149
00:17:53,000 --> 00:17:55,000
And so this is for every column.

150
00:17:55,000 --> 00:18:02,000
This is the type that is represented that pi arrow is using to hold that data.

151
00:18:02,000 --> 00:18:08,000
And we can ask it to give us the estimated size and bytes of this data.

152
00:18:08,000 --> 00:18:13,000
So this is saying that this is like 2.9 megabytes of data.

153
00:18:13,000 --> 00:18:20,000
Okay, so just let me comment on data size for a little bit.

154
00:18:20,000 --> 00:18:25,000
So when I talk about Pandas, I like to say that Pandas is an in-memory tool

155
00:18:25,000 --> 00:18:30,000
or a small data tool in that the Pandas, the library, not Pandas, the API,

156
00:18:30,000 --> 00:18:35,000
but Pandas, the library requires that you have your data in memory.

157
00:18:35,000 --> 00:18:43,000
Polars is similar in that generally you will have your data in memory,

158
00:18:43,000 --> 00:18:45,000
but it can also do some streaming.

159
00:18:46,000 --> 00:18:51,000
So Polars has a mechanism to do lazy evaluation,

160
00:18:51,000 --> 00:18:55,000
and then you can batch chunks of data through it if you want to.

161
00:18:55,000 --> 00:18:59,000
And so if you have data that's larger than memory,

162
00:18:59,000 --> 00:19:04,000
it is possible to use Polars to do that.

163
00:19:04,000 --> 00:19:08,000
I said Pandas, the library, not Pandas, the API,

164
00:19:08,000 --> 00:19:14,000
because there are other libraries that aim to implement the Pandas API.

165
00:19:14,000 --> 00:19:17,000
So one example is Moden.

166
00:19:17,000 --> 00:19:22,000
Moden strives to implement the complete Pandas API

167
00:19:22,000 --> 00:19:25,000
as far as to implement their bugs,

168
00:19:25,000 --> 00:19:29,000
but allowing you to scale that out across multiple machines.

169
00:19:29,000 --> 00:19:31,000
Dask is somewhat similar.

170
00:19:31,000 --> 00:19:34,000
And then, like, I'm an advisor for a company called Ponder,

171
00:19:34,000 --> 00:19:40,000
which is actually a company that's founded by the creator of Moden.

172
00:19:40,000 --> 00:19:43,000
The idea there is that they're taking the Pandas API

173
00:19:43,000 --> 00:19:46,000
to things like BigQuery and Snowflake,

174
00:19:46,000 --> 00:19:48,000
so you can take your Pandas code

175
00:19:48,000 --> 00:19:53,000
and run your same Pandas code on large data there.

176
00:19:53,000 --> 00:20:00,000
So the Pandas API is going to give you sort of like the breadth

177
00:20:00,000 --> 00:20:02,000
of small data to big data.

178
00:20:02,000 --> 00:20:09,000
The Polars API is going to give you small data to sort of larger than memory,

179
00:20:09,000 --> 00:20:12,000
but not sort of scaling out to multiple machines,

180
00:20:12,000 --> 00:20:15,000
at least right now is the state of that.

181
00:20:15,000 --> 00:20:22,000
And so because it is, I would say, you know,

182
00:20:22,000 --> 00:20:24,000
Polars does have some support for streaming,

183
00:20:24,000 --> 00:20:32,000
but you're generally going to want to be holding it in memory

184
00:20:32,000 --> 00:20:36,000
if you want to, like, materialize that or visualize that

185
00:20:36,000 --> 00:20:38,000
or stick that into, like, machine learning.

186
00:20:38,000 --> 00:20:42,000
So you generally will want a little bit of extra space

187
00:20:42,000 --> 00:20:46,000
to be able to do further operations on your data.

188
00:20:49,000 --> 00:20:53,000
So this data set is so small that it's not really a big deal,

189
00:20:53,000 --> 00:20:56,000
but, you know, you might get other data sets that,

190
00:20:56,000 --> 00:21:00,000
where you're kind of edging up to your memory limits.

191
00:21:00,000 --> 00:21:03,000
And so I'm going to show some techniques

192
00:21:04,000 --> 00:21:08,000
for lowering the amount of memory requirements

193
00:21:08,000 --> 00:21:13,000
if memory is one of the main pushes for using something like Polars.

194
00:21:13,000 --> 00:21:17,000
I will note that a lot of it, like I said,

195
00:21:17,000 --> 00:21:19,000
a lot of this applies to Pandas as well.

196
00:21:19,000 --> 00:21:25,000
And so you'll see you can do similar things for Pandas.

197
00:21:25,000 --> 00:21:27,000
So here I'm doing the describe method.

198
00:21:27,000 --> 00:21:29,000
This is actually the same as in Pandas.

199
00:21:29,000 --> 00:21:34,000
And I like to do this when I get a new data set.

200
00:21:34,000 --> 00:21:36,000
So what does this give me?

201
00:21:36,000 --> 00:21:41,000
It gives me summary statistics for my data here.

202
00:21:41,000 --> 00:21:47,000
And this is pretty similar to what we see in Pandas.

203
00:21:47,000 --> 00:21:49,000
We have count.

204
00:21:49,000 --> 00:21:51,000
We have null count.

205
00:21:51,000 --> 00:21:55,000
So in Pandas count is the number of non-missing rows

206
00:21:55,000 --> 00:21:58,000
that has a different definition in Polars.

207
00:21:58,000 --> 00:22:02,000
We see the mean, the standard deviation, the minimum, the maximum,

208
00:22:02,000 --> 00:22:05,000
the median, which is the 50th quartile,

209
00:22:05,000 --> 00:22:08,000
and the 25th and 75th percentile.

210
00:22:08,000 --> 00:22:11,000
So this is kind of nice just to go through our data

211
00:22:11,000 --> 00:22:15,000
and kind of understand what's going on with it.

212
00:22:15,000 --> 00:22:18,000
Some of the things I like to do when I have something like this

213
00:22:18,000 --> 00:22:24,000
is to go through the maximum value and the minimum value

214
00:22:24,000 --> 00:22:27,000
and try and understand, you know,

215
00:22:27,000 --> 00:22:33,000
is the data type that it is using the appropriate one?

216
00:22:33,000 --> 00:22:38,000
And by default, Polars is going to use 64-bit floats and integers,

217
00:22:38,000 --> 00:22:41,000
which may or may not be appropriate.

218
00:22:41,000 --> 00:22:48,000
You can also ask for quantiles in there, like 25th percent quantile.

219
00:22:48,000 --> 00:22:50,000
I actually don't know if you can do this.

220
00:22:50,000 --> 00:22:51,000
Let's just try it.

221
00:22:51,000 --> 00:22:54,000
Let's say like 25th and 70th.

222
00:22:54,000 --> 00:22:59,000
Okay, so Pandas would support something like that, right,

223
00:22:59,000 --> 00:23:00,000
where you get both of those.

224
00:23:00,000 --> 00:23:03,000
Polars does not. It just supports the one.

225
00:23:03,000 --> 00:23:08,000
And so if you look at this, again, there's no index over here

226
00:23:08,000 --> 00:23:11,000
sort of saying like this is the description of this.

227
00:23:11,000 --> 00:23:16,000
This is just the result of taking the 25th percentile of all of those rows.

228
00:23:16,000 --> 00:23:21,000
Okay, so having said that, what I did up here is kind of a no-no.

229
00:23:21,000 --> 00:23:26,000
I said like on the data frame, calculate the quantile.

230
00:23:26,000 --> 00:23:30,000
Generally what we want to do is we want to do these things called expressions

231
00:23:30,000 --> 00:23:34,000
instead of working directly on the data.

232
00:23:34,000 --> 00:23:39,000
And what are these expressions?

233
00:23:39,000 --> 00:23:43,000
The basic idea here is that what we want to do is we want to do

234
00:23:43,000 --> 00:23:44,000
the expressions.

235
00:23:44,000 --> 00:23:48,000
The basic idea here is that you can think of Polars as a boundary.

236
00:23:48,000 --> 00:23:54,000
We've got our arrow data.

237
00:23:54,000 --> 00:23:56,000
We've got a rust layer on top of that.

238
00:23:56,000 --> 00:23:58,000
And then we've got a Python layer on top of that,

239
00:23:58,000 --> 00:24:00,000
which is the API that we're going through.

240
00:24:00,000 --> 00:24:05,000
And if we can keep things sort of in that rust layer and below,

241
00:24:05,000 --> 00:24:09,000
we have some speed and performance benefits that we get from that.

242
00:24:09,000 --> 00:24:13,000
If we are crossing that boundary up to the Python one,

243
00:24:13,000 --> 00:24:16,000
that can give us some flexibility, but it will also slow things down

244
00:24:16,000 --> 00:24:19,000
because then we have to go from rust to Python and back.

245
00:24:19,000 --> 00:24:25,000
So the expressions are a way, if we can express things informally

246
00:24:25,000 --> 00:24:31,000
using these expressions, it keeps us in that rust layer, so to speak.

247
00:24:31,000 --> 00:24:34,000
Okay, so it starts to look something like this,

248
00:24:34,000 --> 00:24:38,000
which is again completely different from what you would get in pandas.

249
00:24:38,000 --> 00:24:42,000
So we're going to say df select and then pl.all.

250
00:24:42,000 --> 00:24:48,000
So we're saying I want to run this select,

251
00:24:48,000 --> 00:24:50,000
which is going to select columns,

252
00:24:50,000 --> 00:24:54,000
and then this is an expression that represents all of the columns.

253
00:24:54,000 --> 00:24:59,000
So that all is from the Polars namespace there.

254
00:24:59,000 --> 00:25:03,000
You can also do things like saying pl.call and then give it a name,

255
00:25:03,000 --> 00:25:06,000
or you can say star to select all of the columns.

256
00:25:06,000 --> 00:25:11,000
But we'll do that saying all there.

257
00:25:11,000 --> 00:25:14,000
We can also do something like this where we say pl.call

258
00:25:14,000 --> 00:25:16,000
and then we can say pl.float64.

259
00:25:16,000 --> 00:25:21,000
So this would give us all of the columns that are float64 columns.

260
00:25:21,000 --> 00:25:28,000
So this is similar to like a select dtypes and pandas, this functionality here.

261
00:25:28,000 --> 00:25:33,000
You can also use exclude to exclude things,

262
00:25:33,000 --> 00:25:35,000
and this can also support regular expressions

263
00:25:35,000 --> 00:25:39,000
if you want to name things as regular expressions.

264
00:25:39,000 --> 00:25:42,000
So here I'm going to select the impressions column two times

265
00:25:42,000 --> 00:25:44,000
and give me a data frame with that.

266
00:25:44,000 --> 00:25:47,000
And when I do that, I get this error, duplicate error,

267
00:25:47,000 --> 00:25:56,000
and it says that the column with name impressions has more than one occurrence.

268
00:25:56,000 --> 00:25:59,000
So this is another thing that's a little bit different if you're used to pandas.

269
00:25:59,000 --> 00:26:01,000
Pandas would allow you to do this

270
00:26:01,000 --> 00:26:05,000
where you'd have the same column multiple times.

271
00:26:05,000 --> 00:26:10,000
Pollars is just saying you can't have columns that have the same name.

272
00:26:10,000 --> 00:26:12,000
So if we wanted to do something like that,

273
00:26:12,000 --> 00:26:22,000
we could do something like this where we say let's take out impressions,

274
00:26:22,000 --> 00:26:26,000
the impressions column, but let's also take out the impressions column again,

275
00:26:26,000 --> 00:26:30,000
but it has this expression suffix underscore two,

276
00:26:30,000 --> 00:26:33,000
so what that is going to do is it's going to change the name of the column

277
00:26:33,000 --> 00:26:35,000
and just stick the underscore two there,

278
00:26:35,000 --> 00:26:38,000
and pollars is fine if we want to do that.

279
00:26:41,000 --> 00:26:46,000
Okay, so what I'm going to do here is I'm going to say let's try and get,

280
00:26:46,000 --> 00:26:49,000
we saw that I couldn't do like the 25th percentile

281
00:26:49,000 --> 00:26:54,000
and the 75th percentile in one call to quantile.

282
00:26:54,000 --> 00:26:56,000
Let's see if we can do this another way.

283
00:26:56,000 --> 00:26:59,000
So I'm going to build up this little operation here.

284
00:26:59,000 --> 00:27:04,000
I'm going to take def.select and I'm going to say let's select all of these

285
00:27:04,000 --> 00:27:08,000
and do the 25th percentile, and then let's select all of these

286
00:27:08,000 --> 00:27:14,000
and do the 50th percentile and give that a suffix of Q5.

287
00:27:14,000 --> 00:27:19,000
And we can see that if we scroll over here,

288
00:27:19,000 --> 00:27:23,000
you can see that these have the suffix of Q5 over here.

289
00:27:23,000 --> 00:27:26,000
So this looks like that kind of worked.

290
00:27:26,000 --> 00:27:29,000
It gave us those.

291
00:27:29,000 --> 00:27:33,000
Now, if you were to do that in pandas,

292
00:27:33,000 --> 00:27:38,000
you'd probably want it vertically stacked instead of horizontally stacked like that.

293
00:27:38,000 --> 00:27:44,000
So I'll kind of show you how you might do something like that.

294
00:27:44,000 --> 00:27:51,000
So I'm going to say here let's take the 0th percentile here

295
00:27:51,000 --> 00:27:54,000
and then let's make a column.

296
00:27:54,000 --> 00:27:59,000
And so to make a column, you say this with columns.

297
00:27:59,000 --> 00:28:05,000
And here I'm going to say I want a lit, which is a literal column or a literal value.

298
00:28:05,000 --> 00:28:10,000
And the value that will be put in this is Q0.

299
00:28:10,000 --> 00:28:14,000
So I'm going to put the string Q0 in there and then I'm going to say alias.

300
00:28:14,000 --> 00:28:17,000
Alias is going to rename the column.

301
00:28:17,000 --> 00:28:24,000
So if we do this, we'll see that there should be a column called describe here at the end.

302
00:28:24,000 --> 00:28:27,000
And it has a value of Q0.

303
00:28:27,000 --> 00:28:32,000
So again, this syntax is completely different from pandas, right?

304
00:28:32,000 --> 00:28:37,000
If I were to do this, I would use like an assign to do this.

305
00:28:37,000 --> 00:28:39,000
There isn't an assign here.

306
00:28:39,000 --> 00:28:46,000
This is how you create columns using expressions in polars.

307
00:28:46,000 --> 00:28:56,000
Okay, so here what I'm going to do is I'm going to say let's do the literal here as describe

308
00:28:56,000 --> 00:29:00,000
and then we'll do the quantile there and we'll just stick that at the front.

309
00:29:00,000 --> 00:29:04,000
You can see that now is at the front.

310
00:29:04,000 --> 00:29:10,000
Okay, so this is kind of an attempt to emulate pandas describe functionality.

311
00:29:10,000 --> 00:29:23,000
So when I initially, so again, like I said, when I initially did this, I talked with the creator of polars to go through this.

312
00:29:23,000 --> 00:29:29,000
When I initially did this, polars had describe but it didn't have all of the features of pandas in describe.

313
00:29:29,000 --> 00:29:34,000
I think because of our discussions or maybe whatever, they since put those in.

314
00:29:34,000 --> 00:29:40,000
But when I initially did this, they didn't have like the 25th and 75th percentile there.

315
00:29:40,000 --> 00:29:45,000
So this is an attempt to stick those in there.

316
00:29:45,000 --> 00:29:48,000
So what are we doing here?

317
00:29:48,000 --> 00:29:50,000
This is a little bit more involved.

318
00:29:50,000 --> 00:29:53,000
I'm saying we're going to concatenate in verticals.

319
00:29:53,000 --> 00:29:55,000
We're going to stack these things on top of each other.

320
00:29:55,000 --> 00:29:57,000
And what am I going to concatenate?

321
00:29:57,000 --> 00:30:01,000
I'm going to concatenate everything from this list comprehension.

322
00:30:01,000 --> 00:30:03,000
So this is a list comprehension from there to there.

323
00:30:03,000 --> 00:30:07,000
I'm putting the star in front of it to unpack the values of that.

324
00:30:07,000 --> 00:30:10,000
So what are the values of that?

325
00:30:10,000 --> 00:30:23,000
That is saying we're going to take the DF with the quantile zero and we're going to say with these columns, which is each of these values here.

326
00:30:23,000 --> 00:30:25,000
So we're going to say Q0.

327
00:30:25,000 --> 00:30:29,000
And then we're going to select all of these columns and do the Q0.

328
00:30:29,000 --> 00:30:31,000
And that will be one entry in that.

329
00:30:31,000 --> 00:30:37,000
And then we'll do another one with Q0.25, another one with Q0.5, 0.75 and one.

330
00:30:37,000 --> 00:30:46,000
And so if we do that, we get something that looks like this.

331
00:30:46,000 --> 00:30:51,000
OK. And so we're taking these are all the ones that I created here.

332
00:30:51,000 --> 00:30:54,000
And then these are the other ones that came from there.

333
00:30:54,000 --> 00:30:58,000
And we're doing a vertical stacking of those.

334
00:30:58,000 --> 00:31:11,000
OK. So that's, you know, maybe a little bit more complicated and maybe more of a challenge.

335
00:31:11,000 --> 00:31:18,000
I mean, we are combining list comprehensions with this unpacking here to do that.

336
00:31:18,000 --> 00:31:31,000
But if you want to vertically stack things, it's a little bit harder to do.

337
00:31:31,000 --> 00:31:39,000
Because we have to do things like stick in a new literal as well so that we know what the rows represent.

338
00:31:39,000 --> 00:31:42,000
So I'd say that's sort of like a give and take.

339
00:31:42,000 --> 00:31:43,000
Like we don't have the index.

340
00:31:43,000 --> 00:31:46,000
The index and pandas would sort of represent that.

341
00:31:46,000 --> 00:31:58,000
And so we're having to kind of play games, making a literal to label what the rows are.

342
00:31:58,000 --> 00:31:59,000
OK. So what am I doing here?

343
00:31:59,000 --> 00:32:07,000
Here I'm saying let's select the time and the impressions and we'll do these quantiles, 0.25 and one for each of those.

344
00:32:07,000 --> 00:32:10,000
And then we'll transpose that.

345
00:32:10,000 --> 00:32:14,000
And so this is what that looks like.

346
00:32:14,000 --> 00:32:17,000
If I don't transpose that, I get something like this.

347
00:32:17,000 --> 00:32:20,000
So this is those different columns.

348
00:32:20,000 --> 00:32:25,000
And that looks OK, right?

349
00:32:25,000 --> 00:32:33,000
Transposing that, if I transpose that, because it doesn't have the index, I do have to say like include header is true.

350
00:32:33,000 --> 00:32:42,000
And then I have to say columns as well to get those column names in there.

351
00:32:42,000 --> 00:32:47,000
Again, if you want to, you can just explore like what happens if I transpose that without that.

352
00:32:47,000 --> 00:32:48,000
It looks like that.

353
00:32:48,000 --> 00:33:00,000
So those are some other, I guess, hoops that you might have to jump through if you're, you know, used to panda's index functionality here.

354
00:33:00,000 --> 00:33:09,000
OK. So this functionality that I put, I mean, up here it's kind of, I know that's not the world's greatest code.

355
00:33:09,000 --> 00:33:11,000
I mean, it's kind of unsatisfying.

356
00:33:11,000 --> 00:33:18,000
I'm going to make a function here, like put this into a function.

357
00:33:18,000 --> 00:33:20,000
And so let's try that with pipe.

358
00:33:20,000 --> 00:33:25,000
So I'm going to say let's pipe the PD describe function in here.

359
00:33:25,000 --> 00:33:35,000
And we can see that that does work as well.

360
00:33:35,000 --> 00:33:45,000
So this is a way where you can refactor your code.

361
00:33:45,000 --> 00:33:52,000
One of the things you want to be aware of is like pandas or pullers has the notion of being lazy here.

362
00:33:52,000 --> 00:34:01,000
And like in this case, like, can I make this lazy?

363
00:34:01,000 --> 00:34:04,000
Some things are, I guess, lazy.

364
00:34:04,000 --> 00:34:08,000
You're able to make lazy and some things you aren't able to make lazy.

365
00:34:08,000 --> 00:34:10,000
So what does laziness give you in pullers?

366
00:34:10,000 --> 00:34:18,000
Laziness gives you the ability to construct operations, a chain of operations.

367
00:34:18,000 --> 00:34:24,000
And then pullers can go through that and it can optimize, can do things like query optimization.

368
00:34:24,000 --> 00:34:29,000
So similar to like a SQL database where a SQL database would do query optimization.

369
00:34:29,000 --> 00:34:40,000
Lazy in pullers allows you to build up all the operations and then do them in an efficient manner.

370
00:34:40,000 --> 00:34:47,000
And so in this case, because we are using like describe, it's like, no, you can't really make describe lazy

371
00:34:47,000 --> 00:34:52,000
because when you're lazy, pullers wants to know all about the schema.

372
00:34:52,000 --> 00:34:58,000
And when you do something like a pivot, it makes it hard to know what the schema is.

373
00:34:58,000 --> 00:35:02,000
And so you can think of describe as a pivot.

374
00:35:02,000 --> 00:35:06,000
OK, let's look at the integer types here.

375
00:35:06,000 --> 00:35:08,000
So here's everything that's in integers.

376
00:35:08,000 --> 00:35:14,000
And I've got things like the number of apps that were open, the number of installs, number of follows.

377
00:35:14,000 --> 00:35:15,000
Again, this is Twitter data.

378
00:35:15,000 --> 00:35:21,000
And a lot of these look like small numbers.

379
00:35:21,000 --> 00:35:25,000
I mean, the tweet ID is a large number, but a lot of these look like small numbers.

380
00:35:25,000 --> 00:35:28,000
But I'm using a 64-bit integer to represent them.

381
00:35:28,000 --> 00:35:32,000
And so I might do something like this where I say, let's do this chain here.

382
00:35:32,000 --> 00:35:39,000
Let's select the integer types and then let's do a describe on those.

383
00:35:39,000 --> 00:35:46,000
OK, and if I look at these, so for example, like the app opens, that maxes out at zero.

384
00:35:46,000 --> 00:35:48,000
App installs is all zeros.

385
00:35:48,000 --> 00:35:52,000
Follows goes up to 191.

386
00:35:52,000 --> 00:35:55,000
Media views goes to like 16,000.

387
00:35:55,000 --> 00:35:59,000
And media engagements goes to 16,000 as well.

388
00:35:59,000 --> 00:36:07,000
So this lets me consider like, do I need to use 64-bit integers to represent this data?

389
00:36:07,000 --> 00:36:10,000
Probably not, right, for a zero.

390
00:36:10,000 --> 00:36:17,000
I mean, if everything's zero for the purposes of like machine learning, I could just drop that column because there's no information in it, right?

391
00:36:17,000 --> 00:36:24,000
A column that has no variance doesn't have any information.

392
00:36:24,000 --> 00:36:27,000
Something like follows, follows goes up to 191.

393
00:36:27,000 --> 00:36:37,000
So if I use like an unsigned 8-bit integer, I could probably represent that in follows, assuming that I never got more than like 255 follows.

394
00:36:37,000 --> 00:36:44,000
So let's look at like how we might cast these into a different type.

395
00:36:44,000 --> 00:36:47,000
So here's impressions, right?

396
00:36:47,000 --> 00:36:49,000
And it turns out that there is a cast on that.

397
00:36:49,000 --> 00:36:53,000
We can say, take impressions and cast it to an int 32.

398
00:36:53,000 --> 00:36:55,000
That looks like it works.

399
00:36:55,000 --> 00:37:02,000
However, if I'm doing this with square brackets, I'm accessing the column here with square brackets, I'm not using that expression syntax.

400
00:37:02,000 --> 00:37:09,000
And so I'm not being able to be lazy and Pullers isn't able to make optimizations with that.

401
00:37:09,000 --> 00:37:15,000
So I do want to use this expression style instead, which would I would do that like this.

402
00:37:15,000 --> 00:37:17,000
I'd say I want to select.

403
00:37:17,000 --> 00:37:20,000
So instead of square brackets, you're going to say select.

404
00:37:20,000 --> 00:37:27,000
And then I'm going to say, let's take the impressions column and I'm going to cast that to an int 32.

405
00:37:32,000 --> 00:37:36,000
If I want to stick that back into the data frame with an expression, I will do something like this.

406
00:37:36,000 --> 00:37:41,000
I'll say DF with columns and stick that back into the data frame.

407
00:37:41,000 --> 00:37:42,000
And here's impressions.

408
00:37:42,000 --> 00:37:46,000
You can see that now it says that it is a 32-bit integer.

409
00:37:46,000 --> 00:37:49,000
Let's try and see if we can get it down to an 8-bit integer.

410
00:37:49,000 --> 00:37:57,000
And unlike pandas, pandas, if you try and do this, pandas will happily proceed and give you overflow errors.

411
00:37:57,000 --> 00:38:01,000
Polar shares a little bit more smart about that.

412
00:38:01,000 --> 00:38:11,000
It says if you do that, you're basically going to be lossy because you've got values that are outside of the range of that.

413
00:38:11,000 --> 00:38:17,000
So the max value of impressions is this right here.

414
00:38:17,000 --> 00:38:21,000
So I'm going to cast it to an int 32 instead.

415
00:38:21,000 --> 00:38:25,000
And that looks like that works if I leave that as an int 32.

416
00:38:25,000 --> 00:38:30,000
So if you're curious about what the size range of these are, I wish I wish there was some way.

417
00:38:30,000 --> 00:38:36,000
I don't know that there's a way in Polar's to ask to get this in pandas.

418
00:38:36,000 --> 00:38:38,000
Generally, we go through NumPy.

419
00:38:38,000 --> 00:38:44,000
So NumPy has this thing called Iinfo to get integer info and Finfo to get floating point info.

420
00:38:44,000 --> 00:38:49,000
So you can pass in the NumPy types there, which should have the same range as the arrow types.

421
00:38:49,000 --> 00:38:56,000
But you can see that like an unsigned 8-bit integer is from 0 to 255.

422
00:38:56,000 --> 00:38:57,000
All right.

423
00:38:57,000 --> 00:39:02,000
So here's just some code that's trying to say, OK, let's take our data frame.

424
00:39:02,000 --> 00:39:06,000
Let's select the float columns and the integer columns.

425
00:39:06,000 --> 00:39:08,000
Let's do a describe on that.

426
00:39:08,000 --> 00:39:19,000
And then let's select the columns where if we go through this describe, I want the max value to be less than 255.

427
00:39:19,000 --> 00:39:22,000
OK. And if I do this, I get an error here.

428
00:39:22,000 --> 00:39:30,000
And it says that this less than is not supported for instances of string and integer.

429
00:39:30,000 --> 00:39:38,000
So what I need to do instead is a little bit more involved.

430
00:39:38,000 --> 00:39:42,000
I need to say like, let's pull off.

431
00:39:42,000 --> 00:39:44,000
So this is giving me this describe thing.

432
00:39:44,000 --> 00:39:46,000
And the second to last position is the max.

433
00:39:46,000 --> 00:39:49,000
And I'm checking whether that is less than some value here.

434
00:39:49,000 --> 00:40:01,000
So this is some code that I could do to say, what are the columns where the numeric values are less than 255?

435
00:40:01,000 --> 00:40:05,000
And these are those columns right here.

436
00:40:05,000 --> 00:40:14,000
And so then I could maybe take this and say, OK, well, let's look at like maybe an unsigned 16-bit integer.

437
00:40:14,000 --> 00:40:19,000
And that goes up to 65,000.

438
00:40:19,000 --> 00:40:26,000
So maybe let's see if there are columns that we can make unsigned 16-bit integers there.

439
00:40:26,000 --> 00:40:30,000
And so these are the ones that could be unsigned 16-bit.

440
00:40:30,000 --> 00:40:34,000
But if you look at these, these are the same as these ones up here that were less than 255.

441
00:40:34,000 --> 00:40:37,000
So you could do like some set operations there.

442
00:40:37,000 --> 00:40:45,000
But basically, most of those could be unsigned 8s.

443
00:40:45,000 --> 00:40:56,000
But maybe for growth purposes, if we're going to say like we might want greater than 255, we'll leave those as unsigned 16-bit.

444
00:40:56,000 --> 00:40:58,000
Again, unsigned can't go negative.

445
00:40:58,000 --> 00:41:06,000
So really, I don't think it makes sense to have negative retweets unless ELAN's changed that recently.

446
00:41:06,000 --> 00:41:10,000
So we should be OK with those unsigned values there.

447
00:41:10,000 --> 00:41:12,000
So what am I doing here?

448
00:41:12,000 --> 00:41:14,000
I'm saying with columns.

449
00:41:14,000 --> 00:41:25,000
And then I'm just using a list comprehension to say, take all these columns here and make a new column expression that casts those to 16-bit integers.

450
00:41:25,000 --> 00:41:28,000
Now, that's using a list comprehension.

451
00:41:28,000 --> 00:41:35,000
Generally, we want to stay with expressions rather than sticking in Python code in there.

452
00:41:35,000 --> 00:41:42,000
So again, that is one of those things you want to be aware of is when you're crossing sort of that Python boundary there.

453
00:41:42,000 --> 00:41:54,000
And if you've got like a list comprehension in the middle of your chain, that can make it so pullers has to cross the boundary between Rust and Python here.

454
00:41:54,000 --> 00:42:04,000
So instead of doing it that way, saying a list comprehension here, it turns out that we can just say with pl.call, we can just pass in the list of columns that we want.

455
00:42:06,000 --> 00:42:09,000
And that's the expression way to do that.

456
00:42:13,000 --> 00:42:17,000
OK, so what I've got here is just a benchmark here.

457
00:42:17,000 --> 00:42:19,000
I'm going to run this here.

458
00:42:19,000 --> 00:42:22,000
So what do I have in the first one?

459
00:42:22,000 --> 00:42:24,000
The first one I have a list comprehension.

460
00:42:24,000 --> 00:42:33,000
In the second one, I have the expression for the purposes of this, the micro benchmark that I'm running on my machine, the list comprehension is slightly slower.

461
00:42:33,000 --> 00:42:42,000
I will say like caveats with timing and benchmarking.

462
00:42:42,000 --> 00:42:52,000
My experience is if you are benchmarking, at least in Panda's world, you want to benchmark with the size of data that you're going to be using in production.

463
00:42:52,000 --> 00:43:00,000
Doesn't make it doesn't make much sense to benchmark something on 10 rows of data or 100 rows of data or a few thousand rows of data.

464
00:43:00,000 --> 00:43:08,000
If you've got millions of rows of data that you're going to be working with in production, because oftentimes there might be different paths that you're going through there.

465
00:43:08,000 --> 00:43:10,000
So just be aware of this.

466
00:43:10,000 --> 00:43:20,000
In this case, like both of these take like around 200 microseconds, which I can't really comprehend 200 microseconds.

467
00:43:20,000 --> 00:43:24,000
It's not like a huge deal for either of these.

468
00:43:24,000 --> 00:43:38,000
But if you had a few gigs of data, this might take a little bit longer, such that doing it the idiomatic Pollard's way with expressions,

469
00:43:38,000 --> 00:43:44,000
especially when you start chaining that with other operations is going to have an impact in what's going on there.

470
00:43:44,000 --> 00:43:47,000
OK, this one here, I'm being lazy.

471
00:43:47,000 --> 00:43:51,000
So again, Pollard's does have the ability to be lazy.

472
00:43:51,000 --> 00:44:01,000
And basically what laziness means is instead of being eager and materializing or evaluating all of these operations as we do them,

473
00:44:01,000 --> 00:44:09,000
what we do is we build up this chain and then Pollard's can analyze that and make certain optimizations here.

474
00:44:09,000 --> 00:44:14,000
In this case, because we're only doing one thing, we're just doing that with comments there.

475
00:44:14,000 --> 00:44:19,000
There's no benefit for laziness here.

476
00:44:19,000 --> 00:44:25,000
OK, so let's see, do we have any integers that need to be larger than those 16-bit integers?

477
00:44:25,000 --> 00:44:30,000
So here I'm just sticking in that magic number there.

478
00:44:30,000 --> 00:44:37,000
And it says the only column that I find here is the describe column, which isn't actually a numeric column.

479
00:44:37,000 --> 00:44:40,000
I said select the describe column and this column.

480
00:44:40,000 --> 00:44:51,000
So there's no columns in this data set that are larger than unsigned 16-bit integers here.

481
00:44:51,000 --> 00:44:58,000
I might have a chain that looks something like this, where I say with these columns here,

482
00:44:58,000 --> 00:45:07,000
we cast them as 8-bit unsigned integers and cast these ones as 16-bit and maybe impressions,

483
00:45:07,000 --> 00:45:13,000
we leave that as 32-bit and we get something that looks like this.

484
00:45:13,000 --> 00:45:21,000
OK, and it didn't complain here, so this conversion should be lossless

485
00:45:21,000 --> 00:45:28,000
in that we haven't lost any data or we haven't had any integer overflows by doing that.

486
00:45:28,000 --> 00:45:35,000
OK, so if we look through the data here, you see there's a bunch of these promoted values here

487
00:45:35,000 --> 00:45:38,000
where we have a bunch of nulls.

488
00:45:38,000 --> 00:45:44,000
It turns out that I haven't paid Twitter to promote anything, so those values are missing.

489
00:45:44,000 --> 00:45:49,000
So maybe I want to filter those out because if I'm doing analysis or machine learning,

490
00:45:49,000 --> 00:45:53,000
I'm not really getting any bang for my buck from those columns because they're not doing anything.

491
00:45:53,000 --> 00:46:01,000
So one of the things that we can do is do select and sadly on,

492
00:46:01,000 --> 00:46:04,000
it looks like a recent version of Jupiter broke the cell magic.

493
00:46:04,000 --> 00:46:07,000
I don't know if this works on yours or not.

494
00:46:07,000 --> 00:46:11,000
So I need to explore that.

495
00:46:11,000 --> 00:46:16,000
This seems to be a Jupiter issue.

496
00:46:16,000 --> 00:46:21,000
But if you look at the help for select, the documentation,

497
00:46:21,000 --> 00:46:24,000
I would say the documentation for pullers is pretty good.

498
00:46:24,000 --> 00:46:26,000
And so pull that documentation.

499
00:46:26,000 --> 00:46:34,000
My general recommendation for people who are working with libraries like this is to stay in Jupiter if you can

500
00:46:34,000 --> 00:46:37,000
or stay in your environment.

501
00:46:37,000 --> 00:46:41,000
And so if you can pull that documentation directly here, that might be useful.

502
00:46:41,000 --> 00:46:47,000
So you can see that it describes what's going on and there's examples of using that as well.

503
00:46:47,000 --> 00:46:50,000
So I think this is pretty good documentation.

504
00:46:50,000 --> 00:46:55,000
So what I'm going to try and do is I'm going to try, if you go up here, let's see select.

505
00:46:55,000 --> 00:47:01,000
It says these are the columns to select, accepts an expression input.

506
00:47:01,000 --> 00:47:03,000
So you can pass in an expression.

507
00:47:03,000 --> 00:47:06,000
Strings are passed as column names.

508
00:47:06,000 --> 00:47:09,000
Other non-expressions are passed as literals.

509
00:47:09,000 --> 00:47:16,000
So this should say that you can pass in a regular expression as well.

510
00:47:16,000 --> 00:47:24,000
And it doesn't say that, but you can.

511
00:47:24,000 --> 00:47:26,000
So here I'm going to pass in a regular expression here.

512
00:47:26,000 --> 00:47:30,000
I'm going to say, does this column start with promoted?

513
00:47:30,000 --> 00:47:34,000
OK, so here are all the columns that are promoted columns.

514
00:47:34,000 --> 00:47:38,000
So again, select is the puller's way of pulling out columns.

515
00:47:38,000 --> 00:47:40,000
And so here I'm going to say, let's do this.

516
00:47:40,000 --> 00:47:46,000
I'm going to say, select all of the columns, but then we're going to exclude the promoted columns.

517
00:47:46,000 --> 00:47:49,000
And that looks like that works.

518
00:47:49,000 --> 00:47:51,000
There aren't any promoted columns there.

519
00:47:51,000 --> 00:47:57,000
We see that there are only 22 columns now instead of all of our columns.

520
00:47:57,000 --> 00:48:00,000
So we might want to drop all those columns here.

521
00:48:00,000 --> 00:48:02,000
Again, you can pull up the documentation for a drop.

522
00:48:02,000 --> 00:48:04,000
Here I'm dropping off those promoted columns.

523
00:48:04,000 --> 00:48:08,000
That looks like that works as well.

524
00:48:08,000 --> 00:48:28,000
So another hint that I got from Richie is that your code's probably going to be better if you avoid using drop rather than using drop.

525
00:48:28,000 --> 00:48:35,000
So his point was when I was going over this with him, he's like, I don't care about the columns that I'm dropping.

526
00:48:35,000 --> 00:48:38,000
I care about the columns that I'm working with.

527
00:48:38,000 --> 00:48:44,000
So rather than saying drop these columns, you should be explicit and say, use these columns.

528
00:48:44,000 --> 00:48:50,000
And I don't know if that helps with their optimizations inside.

529
00:48:50,000 --> 00:48:54,000
There might be some optimizations where it helps because it should have like the scheme of what's going on here.

530
00:48:54,000 --> 00:48:56,000
But here I'm just saying, let's pull off these columns here.

531
00:48:56,000 --> 00:48:58,000
So now I know what the columns I have are.

532
00:48:58,000 --> 00:49:05,000
And then instead of doing the drop there, I'm going to come down here and say, let's select these columns.

533
00:49:05,000 --> 00:49:10,000
So being positive on what you're selecting rather than saying what you're refusing here.

534
00:49:10,000 --> 00:49:15,000
So now I've built up this chain here where I've said, OK, here's my raw data.

535
00:49:15,000 --> 00:49:20,000
And then I'm saying, I want to cast all of these columns to Uint8s.

536
00:49:20,000 --> 00:49:23,000
And then I want to cast all of these to Uint16s.

537
00:49:23,000 --> 00:49:27,000
And I cast all this one to a Uint32.

538
00:49:27,000 --> 00:49:30,000
And then I want to select these columns right here.

539
00:49:30,000 --> 00:49:32,000
Let's just run that and make sure that that works.

540
00:49:32,000 --> 00:49:33,000
It looks like it does work.

541
00:49:33,000 --> 00:49:34,000
It's not complaining.

542
00:49:34,000 --> 00:49:38,000
So we should be good there.

543
00:49:38,000 --> 00:49:43,000
Another thing I like to do is rename my columns.

544
00:49:43,000 --> 00:49:50,000
At least in pandas, that's super useful because I'm going to be like pulling off the columns directly by attribute access.

545
00:49:50,000 --> 00:49:55,000
That's how I prefer to do that because in Jupyter, I get better tab completion when I do that.

546
00:49:55,000 --> 00:50:03,000
However, it's not such a huge deal here in pollers because you don't really use attribute access to pull off columns in pollers.

547
00:50:03,000 --> 00:50:05,000
But let's say that I did want to.

548
00:50:05,000 --> 00:50:11,000
I could do something like this where I say, I'm going to rename these.

549
00:50:11,000 --> 00:50:17,000
And I want to take my current column names and rename those.

550
00:50:17,000 --> 00:50:19,000
In this case, I'm using a pipe here.

551
00:50:19,000 --> 00:50:21,000
Let's try and do that.

552
00:50:21,000 --> 00:50:23,000
And that looks like it works.

553
00:50:23,000 --> 00:50:26,000
So what's the benefit of doing a pipe here?

554
00:50:26,000 --> 00:50:30,000
Well, when you do a pipe, you get the current state of the data frame.

555
00:50:30,000 --> 00:50:32,000
And in this case, I have the current columns in there.

556
00:50:32,000 --> 00:50:37,000
So I can refer to the current state, not the original state that had all those extra columns.

557
00:50:37,000 --> 00:50:44,000
And what I'm doing here is I'm just saying, do a rename and I'm going to replace any spaces with underscores in there.

558
00:50:44,000 --> 00:50:47,000
You see that like tweet permalink now has an underscore in there.

559
00:50:47,000 --> 00:50:51,000
Again, not a huge thing for pullers per se and pandas.

560
00:50:51,000 --> 00:50:53,000
I do like that because it makes it easier to work with.

561
00:50:53,000 --> 00:51:00,000
However, I am using like a lambda here, which might slow down the laziness here.

562
00:51:00,000 --> 00:51:08,000
And this is using like a dictionary comprehension.

563
00:51:08,000 --> 00:51:16,000
So if you want to get rid of like the dictionary comprehension, we can do something like this where we say PL all.

564
00:51:16,000 --> 00:51:25,000
And because this is an expression, this all is going to refer to the current state of the data frame in our chain, not the original data frame.

565
00:51:25,000 --> 00:51:32,000
And then I'm going to say just map this alias here, which is a function to clean all those up.

566
00:51:32,000 --> 00:51:38,000
So this would be a more idiomatic pullers way of doing that.

567
00:51:38,000 --> 00:51:44,000
Now, I did stick a lazy in here that instead of doing anything actually just gives us the plan.

568
00:51:44,000 --> 00:51:50,000
So if I comment out the lazy, you can see that this gives us that result here.

569
00:51:50,000 --> 00:52:05,000
So this would be like an example of what I might do if I was given a data set, going through those columns, cleaning them up and finding which ones I might need or not need.

570
00:52:05,000 --> 00:52:08,000
OK, so we're at our first exercise here.

571
00:52:08,000 --> 00:52:09,000
This is for you.

572
00:52:09,000 --> 00:52:23,000
So your task is to select just the impressions and engagement columns and then select all the columns except impressions and engagements without using drop.

573
00:52:23,000 --> 00:52:30,000
And then use rename to rename the impression column to imp and engagement to E&G.

574
00:52:30,000 --> 00:52:35,000
So I'll just let you work on that for a little bit.

575
00:52:35,000 --> 00:52:38,000
If you have questions or need help, let me know.

576
00:52:38,000 --> 00:52:41,000
I'll give you a chance to practice your pullers.

577
00:52:41,000 --> 00:52:43,000
So there's a question.

578
00:52:43,000 --> 00:52:49,000
The question is like, what's the best way for like a comment and a question?

579
00:52:49,000 --> 00:52:58,000
One of the comments is like selecting being positive about the columns you want is kind of maybe more work or annoying.

580
00:52:58,000 --> 00:53:01,000
And so a couple of things on that.

581
00:53:01,000 --> 00:53:06,000
I want to focus on writing code that's easy to read, not necessarily easy to write.

582
00:53:06,000 --> 00:53:10,000
And so those might be odds with each other.

583
00:53:10,000 --> 00:53:19,000
But I think that being positive about what columns you have is slightly easier to read because you're making an emphasis on the columns you want.

584
00:53:19,000 --> 00:53:29,000
Now, there are cases where like in machine learning, if you're making a pipeline and you're dropping columns, if that pipeline changes such that you're adding new columns to it, you'll break your pipeline.

585
00:53:29,000 --> 00:53:33,000
But if you're being positive about that, you're going to prevent that.

586
00:53:33,000 --> 00:53:37,000
And then the other comment on that is what's the best way to do that?

587
00:53:37,000 --> 00:53:41,000
Well, I'm kind of showing you my process.

588
00:53:41,000 --> 00:53:50,000
So a lot of times, like when people will see a chain, like I write a lot of pandas code and I do chaining and pandas code and people are like, oh, that's a horrible chain or whatever.

589
00:53:50,000 --> 00:53:54,000
And what they don't see is that like I built that up.

590
00:53:54,000 --> 00:53:57,000
Right. I didn't start off with like this huge chain.

591
00:53:57,000 --> 00:54:00,000
I sort of built it up sort of like what I'm doing here in this class.

592
00:54:00,000 --> 00:54:03,000
And so will I use drop?

593
00:54:03,000 --> 00:54:14,000
I mean, yeah, it might make sense for me to use drop and then use find the columns and take the columns from that and then make a and then update my chain without the drop.

594
00:54:14,000 --> 00:54:16,000
So that's what I would do.

595
00:54:16,000 --> 00:54:25,000
So I'm not saying like don't ever use drop, but I would I would use that in cases where it makes sense or helps me to to write my code that way.

596
00:54:25,000 --> 00:54:31,000
OK, should we should we look at the solutions here or some solutions?

597
00:54:31,000 --> 00:54:38,000
OK, and I'm happy to if you have yours, if you want to, you know, ask about them, feel free to do that.

598
00:54:38,000 --> 00:54:44,000
OK, so we have our D.F. here.

599
00:54:44,000 --> 00:54:48,000
So the first one here select just the impressions columns.

600
00:54:48,000 --> 00:54:54,000
So I'm going to say D.F. and then I would say and I'm going to put it in parentheses here so I can just.

601
00:54:54,000 --> 00:54:59,000
So parentheses in case you don't know, do you have to put parentheses around your chains?

602
00:54:59,000 --> 00:55:05,000
No. The nice thing about that is when you're inside a parenthetical in Python, you kind of don't have to worry about white space rules.

603
00:55:05,000 --> 00:55:10,000
And I generally like to put each step on its own line so it reads like a recipe.

604
00:55:10,000 --> 00:55:12,000
So that's that's why I like to do that.

605
00:55:12,000 --> 00:55:19,000
So I'm going to say select P.L. and then I can say I should be able to say just impressions here as a list.

606
00:55:19,000 --> 00:55:25,000
And engagements if I spell it correctly with.

607
00:55:25,000 --> 00:55:28,000
So I'm going to spell it incorrectly just so.

608
00:55:28,000 --> 00:55:30,000
So I got a calm not found error.

609
00:55:30,000 --> 00:55:48,000
And then one thing is I think is a little bit more of a challenge with pullers and maybe pandas is like it does have like error originated just after this operation here.

610
00:55:48,000 --> 00:55:52,000
But the exception here is this column not found error.

611
00:55:52,000 --> 00:55:55,000
So you just have to make sure that you're reading this correctly.

612
00:55:55,000 --> 00:55:57,000
It's not like at the very end it's slightly up.

613
00:55:57,000 --> 00:55:59,000
But but I have a typo there.

614
00:55:59,000 --> 00:56:02,000
So let's see if I can fix that.

615
00:56:02,000 --> 00:56:06,000
And that looks like that now works.

616
00:56:06,000 --> 00:56:11,000
OK. Select all the columns except impressions and engagements without using drop.

617
00:56:11,000 --> 00:56:13,000
OK. So how would I do that?

618
00:56:13,000 --> 00:56:17,000
I should be able to do something like this where I say select.

619
00:56:17,000 --> 00:56:22,000
And then I should say like P.L. all.

620
00:56:22,000 --> 00:56:23,000
So that's all the columns.

621
00:56:23,000 --> 00:56:28,000
And then I should be able to say like exclude.

622
00:56:28,000 --> 00:56:32,000
And let's say exclude impressions.

623
00:56:32,000 --> 00:56:45,000
And exclude engagements.

624
00:56:45,000 --> 00:56:49,000
OK. And then rename.

625
00:56:49,000 --> 00:56:52,000
So I'm going to rename these.

626
00:56:52,000 --> 00:56:57,000
So I'm going to say select those and then let's do a rename.

627
00:56:57,000 --> 00:57:02,000
And we could pull up the documentation for rename but I can pass in a mapping there.

628
00:57:02,000 --> 00:57:05,000
So I should be able to say let's take impressions.

629
00:57:05,000 --> 00:57:08,000
And rename that to imp.

630
00:57:08,000 --> 00:57:18,000
And let's take engagements.

631
00:57:18,000 --> 00:57:35,000
And rename that to E&G.

632
00:57:35,000 --> 00:57:51,000
OK. What questions do you have?

633
00:57:51,000 --> 00:57:59,000
OK. So the question is can we do like a rename with like a lambda here?

634
00:57:59,000 --> 00:58:05,000
And we'll say call the upper.

635
00:58:05,000 --> 00:58:12,000
And so this function has no attribute keys.

636
00:58:12,000 --> 00:58:14,000
Yeah. So yeah.

637
00:58:14,000 --> 00:58:19,000
It looks like if we look at the implementation of this lazy frame, I mean, this, here's the rename.

638
00:58:19,000 --> 00:58:22,000
And it says it takes a mapping here.

639
00:58:22,000 --> 00:58:26,000
So it looks like that will not work.

640
00:58:26,000 --> 00:58:30,000
OK. Let's pull up the documentation here.

641
00:58:30,000 --> 00:58:41,000
So I'm going to have to say help df.rename because my Jupiter is broken.

642
00:58:41,000 --> 00:58:45,000
Yeah. And so this just wants a dictionary.

643
00:58:45,000 --> 00:58:50,000
It doesn't want, doesn't look like it works with a function here.

644
00:58:50,000 --> 00:58:55,000
OK. Other questions?

645
00:58:55,000 --> 00:59:07,000
Yeah.

646
00:59:07,000 --> 00:59:10,000
The question is what's the functional difference of adding select?

647
00:59:10,000 --> 00:59:15,000
So in this case, this select, well that one doesn't work, but this one right here,

648
00:59:15,000 --> 00:59:20,000
this is just saying pull off these two columns, right?

649
00:59:20,000 --> 00:59:33,000
But can you be like, I guess the question maybe goes to like can rename be lazy and be sort of taken into account in the query plan?

650
00:59:33,000 --> 00:59:35,000
So you can just stick in a lazy in there.

651
00:59:35,000 --> 00:59:42,000
And like if I tried to do like a pivot, it should complain about that.

652
00:59:42,000 --> 00:59:45,000
Like lazy has no pivot. So you can't pivot.

653
00:59:45,000 --> 00:59:49,000
Let's see if we can describe.

654
00:59:49,000 --> 00:59:56,000
You can't describe. So in the context of pan polars, you should be fine doing a rename there.

655
00:59:56,000 --> 01:00:01,000
The select here is just saying pull off these two columns first, right?

656
01:00:01,000 --> 01:00:11,000
So here's the result with the select. Here's the result without the select.

657
01:00:11,000 --> 01:00:18,000
Other questions?

658
01:00:18,000 --> 01:00:24,000
OK. Let's look at date columns here.

659
01:00:24,000 --> 01:00:27,000
So here's my chain that I built up.

660
01:00:27,000 --> 01:00:34,000
And so as again, I'm showing this process that I would do going through a new data set.

661
01:00:34,000 --> 01:00:38,000
And again, I wouldn't start off with this huge chain.

662
01:00:38,000 --> 01:00:43,000
I'm building this chain up. And so I'm showing you sort of my thought process when I'm going through the data building this up.

663
01:00:43,000 --> 01:00:49,000
So in this case, I do have a column in here called time and I want to convert it to a date time here.

664
01:00:49,000 --> 01:00:52,000
So I'm going to say cast PL dot date time.

665
01:00:52,000 --> 01:00:54,000
So let's do this.

666
01:00:54,000 --> 01:00:58,000
And I get a compute error.

667
01:00:58,000 --> 01:01:01,000
So let's just look at this compute error.

668
01:01:01,000 --> 01:01:06,000
It says trick conversion from string to date time failed for this value.

669
01:01:06,000 --> 01:01:09,000
And then it says consider using this stir p time.

670
01:01:09,000 --> 01:01:15,000
OK. So let.

671
01:01:15,000 --> 01:01:19,000
If you look at.

672
01:01:19,000 --> 01:01:26,000
Time time is a string. So similar to pandas, strings have a stir accessor on them.

673
01:01:26,000 --> 01:01:30,000
So this is kind of an idea taken from pandas.

674
01:01:30,000 --> 01:01:42,000
So in addition to the things that you can do directly, if things are strings, you can do these additional things which mostly mostly correlate with functionality that you'd find on like a Python string.

675
01:01:42,000 --> 01:01:49,000
Right. So so the issue here is I believe there's a plus here in our date string.

676
01:01:49,000 --> 01:01:52,000
And apparently it doesn't like that plus.

677
01:01:52,000 --> 01:01:55,000
When it's trying to convert that.

678
01:01:55,000 --> 01:01:58,000
So what I'm going to do is I'm going to try and tease this apart here.

679
01:01:58,000 --> 01:02:06,000
So I'm going to say like let's split on that plus and then like I'll do an apply and pull off the first value of that.

680
01:02:06,000 --> 01:02:09,000
And so if you split on that, we get something that looks like that.

681
01:02:09,000 --> 01:02:13,000
If I don't do the apply there, this is kind of interesting.

682
01:02:13,000 --> 01:02:20,000
You get if you look at this, you get a list with strings in it.

683
01:02:20,000 --> 01:02:28,000
Uh huh. Yeah. So so so this is I believe a a list or yes, I don't think it's a strict type.

684
01:02:28,000 --> 01:02:32,000
I think it's called a list type. And you can have like list types that are opt.

685
01:02:32,000 --> 01:02:36,000
This is not like a Python list. This is like a pie arrow list.

686
01:02:36,000 --> 01:02:43,000
So it's kind of smart about what's going on in there.

687
01:02:43,000 --> 01:02:48,000
Personally, I try and avoid like nested structures inside of tabular data if I can.

688
01:02:48,000 --> 01:02:52,000
But but you know, there are cases where you might have something that is nested.

689
01:02:52,000 --> 01:02:57,000
And so being able to like deal with that is kind of nice functionality to have in there.

690
01:02:57,000 --> 01:03:01,000
So so what I kind of want to do is just pull off that first part here.

691
01:03:01,000 --> 01:03:09,000
And so again, this is what that looks like.

692
01:03:09,000 --> 01:03:13,000
So let's look at what I can do with the split thing here.

693
01:03:13,000 --> 01:03:17,000
And again, you can pull up the the the documentation here.

694
01:03:17,000 --> 01:03:25,000
But one of the things that's in there is this are which is different from from pandas.

695
01:03:25,000 --> 01:03:34,000
So this because this is that list type, you have the ability to do a.

696
01:03:34,000 --> 01:03:37,000
Array operations here. So so maybe we come in here and say like,

697
01:03:37,000 --> 01:03:40,000
Let's inspect what you can do with an array here.

698
01:03:40,000 --> 01:03:45,000
So these are kind of like listy things that you would do with a list. Right.

699
01:03:45,000 --> 01:03:52,000
So that's kind of cool. So I'm going to say just split it and then get the thing that's in the zero index here.

700
01:03:52,000 --> 01:03:58,000
So that's instead of doing this with.

701
01:03:58,000 --> 01:04:02,000
This mechanism up here.

702
01:04:02,000 --> 01:04:07,000
Saying apply a lambda. This would be the pollers way of doing that.

703
01:04:07,000 --> 01:04:20,000
Saying array dot get zero.

704
01:04:20,000 --> 01:04:25,000
Yeah, you're crossing that that polar that rusts Python boundary. Right.

705
01:04:25,000 --> 01:04:32,000
And so you're kind of losing you're losing your speed benefit when you do that.

706
01:04:32,000 --> 01:04:38,000
OK, so so let's try this now. I'm going to say pull off that first part and then cast that to a date.

707
01:04:38,000 --> 01:04:41,000
And I still got an error.

708
01:04:41,000 --> 01:04:51,000
It says it failed and it says if you're using trying to cast a string UTF 8 to a temporal type, consider using stir P time.

709
01:04:51,000 --> 01:04:56,000
OK, so maybe I should take their hint that they're giving me and use this stir P time.

710
01:04:56,000 --> 01:04:59,000
So so now my chain looks like something like this.

711
01:04:59,000 --> 01:05:03,000
I'm going to take my time. I'm going to split on plus. I'm going to get the left hand side of that.

712
01:05:03,000 --> 01:05:05,000
And then with that, that should be a string.

713
01:05:05,000 --> 01:05:09,000
And then I'm going to say convert my string to a date.

714
01:05:09,000 --> 01:05:16,000
And this is the type. And then this is the format that is in there. Let's try that.

715
01:05:16,000 --> 01:05:18,000
And that looks like that works.

716
01:05:18,000 --> 01:05:28,000
So yeah, this is again, if you're coming from like pandas, this is maybe a little bit more involved than pandas parsing that.

717
01:05:28,000 --> 01:05:32,000
You could probably just stick that into a two date time in pandas.

718
01:05:32,000 --> 01:05:42,000
But this, you know, they want us to be explicit, which, you know, if you are explicit about like these date times here, these strings, it makes it easier to parse.

719
01:05:42,000 --> 01:05:51,000
And I get like if this is implemented in Rust, then yeah, forcing us to do things that make it easy from the Rust side kind of makes sense.

720
01:05:51,000 --> 01:06:03,000
So my chain would be updated to something like this, where I'd say, OK, now I want to have this time column, which is this chain of splitting that up.

721
01:06:03,000 --> 01:06:10,000
So let's just run that and make sure that that works. And it looks like time is now a time column there.

722
01:06:10,000 --> 01:06:15,000
OK, and I'm storing this in DF2. So I now I have this DF2 thing.

723
01:06:15,000 --> 01:06:21,000
Now, because I have converted that to a time, it also has a DT attribute here. Right.

724
01:06:21,000 --> 01:06:29,000
So we're seeing similar to pandas. Pandas has DT attributes, it has STR attributes. It doesn't have R, AR, R attributes.

725
01:06:29,000 --> 01:06:36,000
But there are various date manipulations that we can do on this now that it's converted to a date. Like we can say, what is the month?

726
01:06:36,000 --> 01:06:39,000
And this is kind of nice. We have this functionality.

727
01:06:39,000 --> 01:06:44,000
Now, could we pull this out of the string? Yeah, you could use a regular expression or whatever to pull this out of the string.

728
01:06:44,000 --> 01:06:53,000
But super nice here. Let's try and convert the time zone here. And here's the documentation for doing that.

729
01:06:53,000 --> 01:07:00,000
So I'm going to say replace time zone and we'll put it as UTC. And so now it is in UTC.

730
01:07:00,000 --> 01:07:18,000
And then if I want to convert that to America Denver, which is the time zone from where I tweeted, I might do something like this where I say convert it to UTC first of all and then convert that to America Denver.

731
01:07:18,000 --> 01:07:21,000
Okay, so yeah.

732
01:07:21,000 --> 01:07:47,000
So the comment is that this is microseconds instead of nanoseconds versus what it would be in pandas.

733
01:07:47,000 --> 01:08:00,000
I don't know off the top of my head. I mean, I guess that is one of the nice things about being precise about that stir P time is that you can tell it what it is.

734
01:08:00,000 --> 01:08:11,000
But yeah, I guess maybe the comment is more like if I've got like things happening at some very small granularity, maybe I'm doing particle physics or something.

735
01:08:11,000 --> 01:08:18,000
Maybe this is not the appropriate mechanism to keep track of the granularity of that time.

736
01:08:18,000 --> 01:08:35,000
Yeah, I don't I'll say I don't know. I haven't explored that or am super well versed in things that are very small points of time. But that might be something to consider.

737
01:08:35,000 --> 01:08:50,000
Okay, let's let's do another exercise here. So create a series with the months of the time column and then convert the time column to UTC and then convert the time column to America and New York.

738
01:08:50,000 --> 01:08:58,000
So again, just want to give you some practice in addition to just running those cells or writing out some code and doing that as well.

739
01:08:58,000 --> 01:09:03,000
So let you work on that for a little bit. And if you have questions or need help, let me know.

740
01:09:03,000 --> 01:09:12,000
And then I'm assuming we'll go straight into our break since it's it's like 1022. And I think our break is at 1030. So I think we'll jump.

741
01:09:12,000 --> 01:09:20,000
We'll work on this and then probably jump into our break. Okay. Hope everyone had a great break.

742
01:09:20,000 --> 01:09:28,000
Okay, let's look at the state exercise here says create a series with the months of the time columns.

743
01:09:28,000 --> 01:09:40,000
So if we so if we look at this DF to we do have that time column in there. So I'm going to say DF to.

744
01:09:40,000 --> 01:09:54,000
And we can say select to select columns. So I say select and then what column do I want? So I'm going to say PL call and then I'm going to say time column name.

745
01:09:54,000 --> 01:10:01,000
So this is the time column because that is a date column. It should have a DT accessor. Right.

746
01:10:01,000 --> 01:10:08,000
And I'm going to say select the month. And if I say select month like this, I think in pandas, this is a property.

747
01:10:08,000 --> 01:10:19,000
In this case, it says it did not accept expect a bound method. So what does that mean? Month in Python is an object. Right.

748
01:10:19,000 --> 01:10:29,000
It's a method. And apparently, pullers, you can't put a method inside of a data frame in pandas. You could actually.

749
01:10:29,000 --> 01:10:34,000
But you can't do that here. So we actually need to call the method.

750
01:10:34,000 --> 01:10:40,000
And that should be that. I think there's a month name as well. Let me just check here.

751
01:10:40,000 --> 01:10:54,000
Maybe it's month underscore name.

752
01:10:54,000 --> 01:11:04,000
OK. There is not a month name in there. Yeah. Question.

753
01:11:04,000 --> 01:11:12,000
Yeah. If we wanted to persist as a new column, then we would use the with column. Right. So let's do that.

754
01:11:12,000 --> 01:11:25,000
So if instead of doing the selector, I'd say with columns. And if I did this.

755
01:11:25,000 --> 01:11:31,000
Whoops.

756
01:11:31,000 --> 01:11:39,000
What this did was, yeah, this actually overrode our time column. So I might want to do something like this where I say alias.

757
01:11:39,000 --> 01:11:45,000
And say this is the month column. So that's that keeps the time column in there.

758
01:11:45,000 --> 01:11:57,000
But we should have a month column. If I can scroll over here right there.

759
01:11:57,000 --> 01:12:05,000
OK. The next one is convert time column to UTC. So since we're doing this in a data frame, I'll just do this in the same one here.

760
01:12:05,000 --> 01:12:19,000
I'll say, OK, let's make a time column that's UTC, which is this replace time zone UTC right here.

761
01:12:19,000 --> 01:12:34,000
So I'm going to say pl.call.time.dt replace time zone.

762
01:12:34,000 --> 01:12:43,000
And now it's in UTC right there. Again, if I if I want that to be new column, I can say alias.

763
01:12:43,000 --> 01:12:51,000
And then I can say this is the UTC time.

764
01:12:51,000 --> 01:12:54,000
There's UTC time.

765
01:12:54,000 --> 01:13:00,000
OK. And if I want to convert that to America time.

766
01:13:00,000 --> 01:13:08,000
So on Pandas you could do something like this where it's a pl.call and then I could say like let's take UTC time and let's do it on that.

767
01:13:08,000 --> 01:13:11,000
I want to do a dt.replace.

768
01:13:11,000 --> 01:13:14,000
Not replace.

769
01:13:14,000 --> 01:13:22,000
That is not replace. It is convert time zone. So we do a convert time zone.

770
01:13:22,000 --> 01:13:26,000
And we'll see America.

771
01:13:26,000 --> 01:13:30,000
New York.

772
01:13:30,000 --> 01:13:37,000
And I do that. I get an error here. Column not found. So let's look at that. It says there is not a column called UTC time.

773
01:13:37,000 --> 01:13:43,000
So what's going on here? It's saying you should have a column called UTC time, but I don't.

774
01:13:43,000 --> 01:13:49,000
That's because I just created that in this with columns call here.

775
01:13:49,000 --> 01:13:57,000
So in Pandas you can get around that by using a lambda. If you use a lambda with a sign, you can get around that.

776
01:13:57,000 --> 01:14:09,000
Let's see if we can get around it by just doing another call to with columns.

777
01:14:09,000 --> 01:14:16,000
And I'm going to alias this as NYC.

778
01:14:16,000 --> 01:14:26,000
So that looks like that worked. So if you want to refer to a column that you just made,

779
01:14:26,000 --> 01:14:30,000
you'll have to do a new with columns call.

780
01:14:30,000 --> 01:14:43,000
Does that make sense? This UTC column is the result of this previous with columns up here.

781
01:14:43,000 --> 01:14:51,000
Alternatively, if I wanted to, I could just take this whole thing and tack on this onto the end of it.

782
01:14:51,000 --> 01:14:58,000
And I wouldn't have to do a new call to with columns.

783
01:14:58,000 --> 01:15:02,000
Okay, questions before I keep going. Yeah.

784
01:15:02,000 --> 01:15:28,000
So the comment is you could do head to get the first number of rows,

785
01:15:28,000 --> 01:15:32,000
but what if you wanted to select different rows, not the 20th and 30th?

786
01:15:32,000 --> 01:15:39,000
So there is a filter. So select lets us select columns, filter lets us select rows.

787
01:15:39,000 --> 01:15:44,000
And so in that case, you would need to have some row that you could filter on.

788
01:15:44,000 --> 01:15:53,000
Right. So if you wanted to do it by like position, like Pandas has like ILOC, right, to get like the 20th row,

789
01:15:53,000 --> 01:16:01,000
then you'd have to make a column that has like the row count in it and then do a filter on that column.

790
01:16:01,000 --> 01:16:16,000
Yeah. So we don't have again, we don't have the index there. So we'd have to use something like that.

791
01:16:16,000 --> 01:16:26,000
Yeah, it has I've got an example here that will show you how to like emulate the index down below and insert that in there.

792
01:16:26,000 --> 01:16:32,000
OK, so after I've done this, I feel pretty good about what I have.

793
01:16:32,000 --> 01:16:36,000
I would go out and make a function that looks something like this. Right.

794
01:16:36,000 --> 01:16:43,000
And so this is my tweet Twitter function. This is taking the raw data and and cleaning it up.

795
01:16:43,000 --> 01:16:49,000
And again, I would make a chain like this. I would do this in Pandas as well.

796
01:16:49,000 --> 01:16:53,000
In Polar's, it's kind of encouraged to do a chain. In Pandas, you don't see that a lot.

797
01:16:53,000 --> 01:16:58,000
But again, Polar's because it can be lazy, can do some smart things about this.

798
01:16:58,000 --> 01:17:08,000
So if this, you know, if I was working for a client, what I would do is I would take this code here and I would put this right at the very top of my file right next to where I load the data.

799
01:17:08,000 --> 01:17:13,000
So what this does is it reads like a recipe. Whoops. I'm working with the raw data.

800
01:17:13,000 --> 01:17:24,000
And then I'm saying, OK, I'm going to convert these columns to integer eights, these two unsigned 16s, this one to an unsigned 32.

801
01:17:24,000 --> 01:17:27,000
This time I'm going to convert it to America time zone.

802
01:17:27,000 --> 01:17:31,000
And then I'm going to select these columns and then I'm going to rename the column names.

803
01:17:31,000 --> 01:17:34,000
All right. It looks like the series of steps that I'm going to do.

804
01:17:34,000 --> 01:17:40,000
Let's just run that and make sure that it works. And it looks like it does.

805
01:17:40,000 --> 01:17:46,000
OK, so this is also called chaining. In Pandas, I'm a big proponent of chaining.

806
01:17:46,000 --> 01:17:53,000
A lot of people say that it's ugly. Again, I think they don't see that like I build up the chains as I'm going.

807
01:17:53,000 --> 01:17:58,000
I don't just like write a big huge chain. I'm building it up as I'm testing it as I'm going.

808
01:17:58,000 --> 01:18:01,000
And I'm doing the same thing here in Polar's.

809
01:18:01,000 --> 01:18:07,000
Now, Polar's does have the added benefit that it does have this laziness, right, where we can say, let's be lazy here.

810
01:18:07,000 --> 01:18:13,000
In that case, it's not going to materialize these operations until I call collect at the end.

811
01:18:13,000 --> 01:18:20,000
And so if there are, it does have a query optimizer, so to speak, like a database.

812
01:18:20,000 --> 01:18:24,000
And so it can go in and be smart about how it's doing these things.

813
01:18:24,000 --> 01:18:31,000
But this should give me the same result as the one up above, but it should possibly be a little quicker.

814
01:18:31,000 --> 01:18:41,000
In fact, let's just do some timing here. And I'm going to time my lazy Twitter against my non-lazy one.

815
01:18:41,000 --> 01:18:47,000
And we'll see what happens here.

816
01:18:47,000 --> 01:18:55,000
OK, so my lazy one took 1.3 milliseconds on my machine. It's a MacBook Pro from last year or whatever.

817
01:18:55,000 --> 01:19:04,000
My non-lazy one took 1.47. Now, again, there aren't a lot of places for optimization here, so to speak.

818
01:19:04,000 --> 01:19:13,000
I mean, I'm just sort of pulling off columns and converting them where the query optimizer can't really do a lot of optimizations here.

819
01:19:13,000 --> 01:19:19,000
Now, what I'm going to do in this example is I'm going to actually add file reading into this.

820
01:19:19,000 --> 01:19:26,000
So here, instead of passing in my data frame, I'm going to pass in a file name, and I'm going to say scan CSV.

821
01:19:26,000 --> 01:19:33,000
Scan CSV is lazy, so it's not going to read the CSV. It's just going to say, I have this CSV file up here.

822
01:19:33,000 --> 01:19:40,000
I don't think I actually need this lazy down here because I think scan CSV is lazy.

823
01:19:40,000 --> 01:19:49,000
Oh, it says I don't have file. Let me get file here.

824
01:19:49,000 --> 01:19:58,000
So I have a local file right here. OK, so if you don't have that, I don't believe that works with URLs,

825
01:19:58,000 --> 01:20:03,000
so you do need to have that local if you want to try that out.

826
01:20:03,000 --> 01:20:10,000
So let me scroll back down here.

827
01:20:10,000 --> 01:20:16,000
OK, and that looks like it works. Again, if you leave off that collect there at the end,

828
01:20:16,000 --> 01:20:19,000
it's going to give you this query plan of what it's going to do.

829
01:20:19,000 --> 01:20:30,000
So again, there's not a lot of like fan out or group buys where it can do a lot more operations here, but let's just time this.

830
01:20:30,000 --> 01:20:40,000
So right now without collecting it, it takes 200 microseconds. If we do collect it, let's time it like this.

831
01:20:40,000 --> 01:20:47,000
And it takes 2.7 milliseconds to run. Now this is including reading the file, right?

832
01:20:47,000 --> 01:20:57,000
So this is not just doing the operations. This is reading the CSV file. So it takes 2.7 milliseconds to run.

833
01:20:57,000 --> 01:21:04,000
I'm just going to down here. I've got a pandas comparison.

834
01:21:04,000 --> 01:21:14,000
And I'm going to I'm going to read my pandas file and do this. So in pandas, this takes 38 milliseconds.

835
01:21:14,000 --> 01:21:26,000
In in pullers, this takes 2.7 milliseconds. So quite a bit faster here. Again, this is milliseconds, so it's not a huge deal like

836
01:21:26,000 --> 01:21:38,000
34 milliseconds versus 2 milliseconds is not a huge deal. But if your data gets bigger or you have more complicated operations, you can see where you might be getting some some benefits for this.

837
01:21:38,000 --> 01:21:45,000
OK, so this example is just showing the late the plan here. And

838
01:21:45,000 --> 01:21:51,000
this is what it can go through and optimize after this.

839
01:21:51,000 --> 01:22:01,000
OK, so one of the things that people complain about chains is they complain about that you can't debug them. So I think we'll do this exercise together.

840
01:22:01,000 --> 01:22:11,000
This exercise is use pipe to print the shape of the data frame after every step in the chain in the tweak Twitter function. So a lot of people when they see like a function like this, they're like,

841
01:22:11,000 --> 01:22:15,000
I can't understand what's going on here.

842
01:22:15,000 --> 01:22:18,000
Which whatever.

843
01:22:18,000 --> 01:22:27,000
I get that like it might be overwhelming, but once you like learn a little bit of pullers, it's it's not too bad. I sort of feel the same way about pandas.

844
01:22:27,000 --> 01:22:36,000
But but what I want to do is I want I want to use a pipe here and I want to debug this. So one of the things that people claim is that they want to look at the intermediate variables.

845
01:22:36,000 --> 01:22:43,000
And I don't personally want to do that. I want to have my input and my output and I don't care about what's made along the way.

846
01:22:43,000 --> 01:22:53,000
But let's assume that you did or maybe your data change and you need to look at that. And so one thing is that you could do is something like this where you can say like deaf and let's say debug.

847
01:22:53,000 --> 01:22:58,000
And we're going to take a data frame and maybe we'll take extra.

848
01:22:58,000 --> 01:23:01,000
And what I'll do is I'll just say print.

849
01:23:01,000 --> 01:23:05,000
I'll use an F string here and we'll say.

850
01:23:05,000 --> 01:23:08,000
Extra.

851
01:23:08,000 --> 01:23:12,000
And then we'll say D F dot shape.

852
01:23:15,000 --> 01:23:18,000
And then we'll return the data frame.

853
01:23:18,000 --> 01:23:24,000
OK, so what I'm going to do is I'm just going to put this right in here and I'm going to say dot pipe.

854
01:23:24,000 --> 01:23:32,000
And I'm going to say debug is the function I want to call. I'm going to say extra is equal to start.

855
01:23:32,000 --> 01:23:35,000
OK, let me just run this.

856
01:23:35,000 --> 01:23:41,000
And you can see that now it's putting out start there at the top.

857
01:23:41,000 --> 01:23:56,000
OK, so what I can do is I can just so this is kind of nice, especially if you're doing like joins or whatever. I can say like OK after I've updated my columns. Let's look at the size update columns.

858
01:23:56,000 --> 01:23:59,000
And then.

859
01:23:59,000 --> 01:24:03,000
Here I've selected my columns.

860
01:24:03,000 --> 01:24:10,000
And the last one is a rename. It's not going to do anything, but you could imagine if I had a filter or whatever it might do something.

861
01:24:10,000 --> 01:24:22,000
So there you can see that I'm starting off with five thousand seven hundred ninety one rows and forty columns after I update my columns. I still have forty columns. And then after I select my columns, I have twenty two columns.

862
01:24:22,000 --> 01:24:30,000
But something like this is especially nice. If you're doing a join, you can see if you do like a combinator explosion in there or something to catch that.

863
01:24:30,000 --> 01:24:39,000
Scroll back up there so you can see that code again. All I'm doing is making a function that takes the data frame as the first input and it's returning a data frame.

864
01:24:39,000 --> 01:24:48,000
That allows me to stick it inside of one of these chains here. And then I'm just having a side effect of printing that out on the site there.

865
01:24:48,000 --> 01:24:58,000
Any questions about that or comments?

866
01:24:58,000 --> 01:25:20,000
OK, I mean, if you wanted to go beyond this, you could say let's do the same thing. But instead of printing out to the screen, I'm going to make a global variable and update the global variable with the current state of the data frame. If you wanted that current state of the data frame to inspect it.

867
01:25:20,000 --> 01:25:27,000
The question is, could you do that inside of the width columns?

868
01:25:27,000 --> 01:25:38,000
Yeah, I don't know that that would work inside. I don't know that width columns will take a callable to do that. So that wouldn't really work.

869
01:25:38,000 --> 01:25:54,000
But yeah, I guess you just sort of have to treat width columns as sort of its own box of operations.

870
01:25:54,000 --> 01:26:07,000
OK, so we did this pandas comparison here. One thing to be aware of is like this would be like my pandas code to do what I'm doing.

871
01:26:07,000 --> 01:26:29,000
You know, which code is better? I'm just honestly more familiar with pandas. I've used it a lot more, right? So to me, this code is perfectly fine to a lot of people. Like it is doing some things like it's doing some dictionary comprehensions with unpacking in there, which a lot of people aren't familiar with.

872
01:26:29,000 --> 01:26:49,000
So I'm going to use this as a recipe. I'm going to rename my columns. I'm going to replace spaces with underscores. I'm going to drop these columns in here. I'm going to also drop these other columns.

873
01:26:49,000 --> 01:27:05,000
And these are the promoted columns. So this is just me not having to write out promoted in there. So I'm using a list comprehension to drop out those promoted columns. Again, if I wanted to go through that positive thing, I might do this and then select my columns and go in and say positively select these columns.

874
01:27:05,000 --> 01:27:25,000
And then I'm converting my impressions column. I can convert my engagements columns. All of these columns I'm converting to unsigned integers. All of these columns I'm converting to unsigned integers as well. This tweet permalink here, I'm converting that to a category, which I didn't do in pullers.

875
01:27:25,000 --> 01:27:48,000
And this one here is the date time conversion. So I think it's a little bit less code, or at least a little bit less of vertical lines. I'm putting in a bit faster in pullers.

876
01:27:48,000 --> 01:28:11,000
OK, so let's look at doing some manipulation of columns here. So here is my tweet DF here. Let's assume that I want like an engagement rate as a percentage. So what I'm going to say is I'm going to say take my engagement rate column and then let's apply this function right here, the percentage function.

877
01:28:11,000 --> 01:28:31,000
And we'll call that the engagement percent. And if we do that, we now have this engagement percent right here. So what's the engagement percentage of that? Let's just do a benchmark of that. And then I'm going to do a benchmark of this where I'm taking the expression and multiplying the expression by 100.

878
01:28:31,000 --> 01:28:52,000
OK, and you see that we've got a pretty big performance difference here. So top one here, 500 microseconds. This one here, 55 microseconds. Again, this is microseconds, but it is 10 times difference.

879
01:28:52,000 --> 01:29:17,000
Will that pan out to larger data sets? Maybe, maybe not. Again, general benchmark advice is to benchmark against the size of the data that you use. One of the things to be aware of is this apply. When you're doing apply, that's element-wise. So you're pulling off the individual values. You're crossing that Rust-Python boundary when you do that.

880
01:29:17,000 --> 01:29:44,000
So here I'm saying let's do apply with np.sign. Pollars does have a map, and so map is column-wise. And so you can see the difference there of np.sign versus map of sign, applying np.sign versus map of it. It's like 2 milliseconds versus 200 microseconds, about 10 times faster there for this data set.

881
01:29:44,000 --> 01:30:03,000
And again, unlike pandas, Pollars actually has sign in it itself, so you don't have to do any of this. You can just say .sign there to get the sign of that. Just for fun, I'll import the math library and look at calculating the Python math sign function here.

882
01:30:03,000 --> 01:30:23,000
You can see that Pollars is going to be faster than using NumPy and doing math.sign. In this case, math.sign is faster than using NumPy, which I'm not quite sure why that is.

883
01:30:23,000 --> 01:30:42,000
But yeah, probably because it's, well, yeah. I would think that converting to NumPy would be faster than converting to Python and applying math.sign to it, but apparently something's going on there.

884
01:30:42,000 --> 01:30:59,000
Okay, so another thing you might want to do is manipulate your string columns, right? And so here I'm saying I want to determine whether a string, a text, a tweet is a response. And so a response tweet to someone else would start with the at sign.

885
01:30:59,000 --> 01:31:21,000
And so in pandas you can do this, duradot starts with. In Pollars there is not a starts with. So like Python has starts with. Let's look at the attributes of str. And for whatever reason, they name this starts underscore with, which it is what it is.

886
01:31:21,000 --> 01:31:35,000
I mean, starts with without an underscore there, the world's best naming. Probably not, but I mean it's the convention in Python and pandas is sort of using it. So whatever, I mean it is what it is.

887
01:31:35,000 --> 01:31:52,000
So some of those things might not be exactly the same. So let's do starts with that and we'll just do a timing here of that. And we're going to compare that with an apply where we're pulling out the value and sticking it into a Python string and doing that starts with there.

888
01:31:52,000 --> 01:32:16,000
So generally in pandas my advice is avoid apply. However, if you're using strings in pandas and you're not using the pyro strings, you're going down a slow path anyway generally in pandas. And so apply might make sense for strings in pandas.

889
01:32:16,000 --> 01:32:31,000
So that is in Pollars here we can see that a lot of these things are optimized in Pollars. And so you get quite a big speed performance here by doing this with an expression if you can do it with an expression.

890
01:32:31,000 --> 01:32:48,000
Okay, so I might update my tweet Twitter to look something like this now. If I'm going to do like some machine learning or some further analysis on this, I might want to look at my tweets and pull off various features from that.

891
01:32:48,000 --> 01:33:04,000
So here I'm not going over all these, but I'm doing things like I'm going to make an is reply column. I'm going to make a length column that's just taking str.lengths s. I'm not sure why it's spelled lengths, but that's the naming there.

892
01:33:04,000 --> 01:33:22,000
And then I'm going to make an whether it's an add tweet, whether it has new lines, whether it has the number of lines in there. And so you can see I'm doing like str.contains str.contains str.countmatch str.contains.

893
01:33:22,000 --> 01:33:38,000
And then I'm also going to put some other columns here for time. I'm going to get like the hour when I tweeted, the day of the month, and the day of the week just to see if I'm doing some machine learning or further analysis, whether those have an impact on what's going on there.

894
01:33:38,000 --> 01:33:45,000
Okay, and you know, I can run this and inspect this and it looks like this is working.

895
01:33:45,000 --> 01:33:56,000
Okay, so I want to give you the chance to work with apply here. So we're going to do this. Calculate the engagement ratio by dividing engagements by impressions.

896
01:33:56,000 --> 01:34:08,000
And then calculate engagement ratio too by dividing the sum of replies, retweets, likes, user profile clicks, and detail expands by impressions.

897
01:34:08,000 --> 01:34:20,000
So I'll let you work on that for a moment and then we'll go over that. I'm going to give a hint for this first one that might just sort of help.

898
01:34:20,000 --> 01:34:36,000
I'm going to say, so I have like this twit df here, which has gone through my tweaked df. So I do have this twit df here and I want to make a new column.

899
01:34:36,000 --> 01:34:52,000
So I'm not going to make this ratio one, but to make a new column I say with columns. Okay, and then let's say instead of doing the ratio I'm just going to say I want to, maybe I had a good day so I'm going to have the double engagements column, right?

900
01:34:52,000 --> 01:35:07,000
So how would I do that? I would say call, I want to make an expression and so pl.call, right? And then we'll take engagements. And if I want to double it, I want to multiply it by two.

901
01:35:07,000 --> 01:35:19,000
And so these expressions respond to Python operators like this and it says incomplete input, it wants that parentheses there.

902
01:35:19,000 --> 01:35:34,000
So this is engagements multiplied by two and if I wanted to name this as like double engagements, I would do like an alias here. So alias, I'll just call it double eng.

903
01:35:34,000 --> 01:35:56,000
Now that doesn't work because it's like int doesn't have alias. That's because of operator precedence here so what I need to do is just wrap this inside of a parenthetical here, say do the multiplication, which should return another puller's expression and then once I've done that, alias that expression.

904
01:35:56,000 --> 01:36:02,000
So now you can see that there's the double eng right there.

905
01:36:02,000 --> 01:36:09,000
Does that kind of give you an idea or some boilerplate for what I might want to do for doing that?

906
01:36:09,000 --> 01:36:17,000
Oh, yeah.

907
01:36:17,000 --> 01:36:25,000
Well, mine have underscores in them because I'm using twit df.

908
01:36:25,000 --> 01:36:31,000
But yeah, yours might not. So yeah, you can do it either way.

909
01:36:31,000 --> 01:36:41,000
Okay, so hopefully now, again, I like to have the exercises here because it's one thing to watch me talk about it and it's like, oh, it goes in one ear and sort of makes sense.

910
01:36:41,000 --> 01:36:46,000
But then when you actually do it, that's like where it goes into muscle memory, right?

911
01:36:46,000 --> 01:36:54,000
And science tells us that if you actually do the exercise, you'll learn it a lot better because you're going to understand it better.

912
01:36:54,000 --> 01:36:59,000
Okay, so I'm just going to do this here. So I've got with columns here.

913
01:36:59,000 --> 01:37:05,000
So I think I can put in a bracket here and I'm just going to tack on these other ones in here.

914
01:37:05,000 --> 01:37:14,000
So I want to divide engagements by impressions. So I'm going to do that here.

915
01:37:14,000 --> 01:37:25,000
I'm going to just come in here and say take engagements and divide it by plcol.impressions.

916
01:37:25,000 --> 01:37:32,000
Okay, so that looks like that worked.

917
01:37:32,000 --> 01:37:41,000
It turns out that because I guess this is pullers just takes it looks like it's just taking the first column here and using that as the name.

918
01:37:41,000 --> 01:37:45,000
So it looks like it has updated the engagements with that. So I don't want it to update that.

919
01:37:45,000 --> 01:37:52,000
So again, same thing here. I'm going to come in here, wrap this with parentheses so that I do that whole expression.

920
01:37:52,000 --> 01:38:01,000
And then I want to alias the result of that. I guess alternatively, I could come in here and say like alias this as the ratio in here.

921
01:38:01,000 --> 01:38:06,000
But I kind of like to keep that math separate, like do the math and then do the alias.

922
01:38:06,000 --> 01:38:11,000
So we could call this like the ENG ratio if we wanted to.

923
01:38:11,000 --> 01:38:16,000
And now we should have that over here on the side.

924
01:38:16,000 --> 01:38:22,000
Okay, if we wanted that to be a percentage, we could we can multiply that by 100 to make it a percentage.

925
01:38:22,000 --> 01:38:28,000
The next one is calculated by summing up these columns and then dividing that by impressions.

926
01:38:28,000 --> 01:38:32,000
So this is just another example.

927
01:38:32,000 --> 01:38:39,000
You know, do I need to use a list comprehension? Maybe.

928
01:38:39,000 --> 01:38:47,000
There is a way to do a sum column wise.

929
01:38:47,000 --> 01:38:51,000
I don't I don't recall off the top of my head what that syntax is.

930
01:38:51,000 --> 01:38:57,000
So I'm just going to I'm just going to manually write this out.

931
01:38:57,000 --> 01:39:07,000
So we're going to take replies and we're going to add PL dot call dot retweets.

932
01:39:07,000 --> 01:39:14,000
And we're going to add PL dot call dot likes.

933
01:39:14,000 --> 01:39:20,000
And because I'm inside a parenthetical here, I'll just go to the next line. We'll add PL dot call.

934
01:39:20,000 --> 01:39:25,000
User profile clicks.

935
01:39:25,000 --> 01:39:29,000
And we will add PL dot call detail expands.

936
01:39:29,000 --> 01:39:36,000
So this should be according to Twitter's definition at some point in time, this should be the same thing as engagements.

937
01:39:36,000 --> 01:39:41,000
It's summing up these columns here.

938
01:39:41,000 --> 01:39:48,000
So I could come in here and say like alias this.

939
01:39:48,000 --> 01:39:55,000
This is an inch ratio to this should be the same as engagement.

940
01:39:55,000 --> 01:39:57,000
So engagement is like seven and three.

941
01:39:57,000 --> 01:39:59,000
So it should be like seven and three over here.

942
01:39:59,000 --> 01:40:01,000
So it looks like it looks like that is matching.

943
01:40:01,000 --> 01:40:05,000
So again, showing you how I like would build this up and like test it as I'm going.

944
01:40:05,000 --> 01:40:09,000
And then we want to divide this by the impressions here.

945
01:40:09,000 --> 01:40:12,000
So we've got that sum. We want to do the sum first.

946
01:40:12,000 --> 01:40:16,000
And then I'm not sure if there's a divide method. Let's just try it and see.

947
01:40:16,000 --> 01:40:19,000
There might there might be one there might not be.

948
01:40:19,000 --> 01:40:31,000
We'll divide it by PL dot call and impressions.

949
01:40:31,000 --> 01:40:34,000
OK, and it's claiming that the parentheses are off.

950
01:40:34,000 --> 01:40:37,000
So let's do that. It looks like that doesn't work.

951
01:40:37,000 --> 01:40:48,000
So I'm going to actually come in here and put that there.

952
01:40:48,000 --> 01:41:04,000
So I'm not seeing inch ratio to it's probably because my I need I think I need an extra parentheses around this right here.

953
01:41:04,000 --> 01:41:06,000
OK, there it goes. And it looks like those are the same.

954
01:41:06,000 --> 01:41:13,000
So so it looks like we're good there.

955
01:41:13,000 --> 01:41:24,000
So the point of this is that similar to Pandas, if you can, you want to do those math operations directly at like the Pandas level or the Polar's level.

956
01:41:24,000 --> 01:41:36,000
It keeps it fast rather than jumping to like an apply, which might be maybe I would say the naive way to do it, but it's going to be slow.

957
01:41:36,000 --> 01:41:49,000
OK, I think with our remaining time, we're going to look at aggregation and aggregation is I like to say aggregation is what your boss wants.

958
01:41:49,000 --> 01:41:59,000
Right. So, you know, it was spring break the other day and we went to a candy store in Jackson Hole.

959
01:41:59,000 --> 01:42:03,000
It was like a tourist trap, but it's like just a bunch of candy, right.

960
01:42:03,000 --> 01:42:05,000
And like weird candies and whatever.

961
01:42:05,000 --> 01:42:10,000
And I imagine that the boss of that candy store doesn't come in and say, tell me what happened today.

962
01:42:10,000 --> 01:42:17,000
And someone comes in and says, well, Suzy came in and bought one lollipop and two two sticks of gum and then a gummy bear.

963
01:42:17,000 --> 01:42:18,000
And then Billy came in.

964
01:42:18,000 --> 01:42:19,000
Your boss doesn't want that.

965
01:42:19,000 --> 01:42:21,000
Right. They want to say 58 people came in.

966
01:42:21,000 --> 01:42:24,000
They spent two thousand three hundred and sixty eight dollars.

967
01:42:24,000 --> 01:42:28,000
Right. So you're going to take all of that information and collapse it to a single value.

968
01:42:28,000 --> 01:42:32,000
Generally, when your boss wants something, they want some sort of aggregate.

969
01:42:32,000 --> 01:42:35,000
And so that's what I like to think when you're thinking reports.

970
01:42:35,000 --> 01:42:37,000
Generally, you're thinking some sort of aggregate.

971
01:42:37,000 --> 01:42:40,000
Right. I mean, some people say they want the details.

972
01:42:40,000 --> 01:42:42,000
They don't really want the details.

973
01:42:42,000 --> 01:42:48,000
Generally, they want the details to like make sure that you did your calculations correctly or something like that or dive into that.

974
01:42:48,000 --> 01:42:50,000
But really, they want these aggregations.

975
01:42:50,000 --> 01:42:56,000
So now let's look at some of these aggregations here and how we might do that.

976
01:42:56,000 --> 01:43:00,000
And generally, how are you going to do that is either with group by or with pivot.

977
01:43:00,000 --> 01:43:07,000
OK, so here I'm going to say, let's take my Twitter data and then I'm going to say I want to group by.

978
01:43:07,000 --> 01:43:10,000
And look at this. I'm putting an expression in here.

979
01:43:10,000 --> 01:43:15,000
So I'm taking the time column, pulling off the year and then aliasing that as year.

980
01:43:15,000 --> 01:43:20,000
And then my aggregation function here is going to be take the mean of that.

981
01:43:20,000 --> 01:43:24,000
OK, and this is the result here.

982
01:43:24,000 --> 01:43:28,000
Now, in pandas, you would see year in the index.

983
01:43:28,000 --> 01:43:32,000
Here we don't see that, but we are seeing that we have that alias year.

984
01:43:32,000 --> 01:43:35,000
Let me just for fun take off the alias.

985
01:43:35,000 --> 01:43:38,000
And we get an error here, duplicate error.

986
01:43:38,000 --> 01:43:41,000
Right. And so why is it complaining about duplicate error?

987
01:43:41,000 --> 01:43:49,000
Because we're using the time column and apparently it has more than one occurrence, which is a no, no in Polar's land.

988
01:43:49,000 --> 01:43:51,000
So we want to stick that alias in there.

989
01:43:51,000 --> 01:43:53,000
So we don't get that.

990
01:43:53,000 --> 01:43:57,000
So it's going to stick that instead of making an index, it's going to make a new column called year.

991
01:43:57,000 --> 01:44:04,000
And then we have for every year, we have the year, the year, the year, the year, the year, the year, the year.

992
01:44:04,000 --> 01:44:07,000
So we're getting entry there.

993
01:44:07,000 --> 01:44:10,000
In this case, it is including strings as well.

994
01:44:10,000 --> 01:44:13,000
It's giving us the mean value.

995
01:44:16,000 --> 01:44:27,000
And I should note as well, this is a place where you can get improvements kind of two places in Polar's.

996
01:44:27,000 --> 01:44:33,000
One is that Polar's, when you do grouping, Polar's can maximize those CPUs, right?

997
01:44:33,000 --> 01:44:40,000
So it can do parallel groups, it can do parallel operations on each of those groups because they're independent.

998
01:44:40,000 --> 01:44:50,000
And then if you're doing a lazy operation, it can further go through and figure out if there are other optimizations that it can make before it does the grouping if you're doing lazy.

999
01:44:50,000 --> 01:44:56,000
So this aggregation, this reporting is where you're going to see probably your most sort of bang for your buck.

1000
01:44:56,000 --> 01:45:04,000
So it's going to help you if using Polar's for performance is kind of what you're looking for or if that's your motivation for using Polar's.

1001
01:45:07,000 --> 01:45:13,000
Okay, so there's a question, how do I get the index in there or simulate the index?

1002
01:45:13,000 --> 01:45:16,000
And so there is this with row count.

1003
01:45:16,000 --> 01:45:18,000
So let me just show you that.

1004
01:45:18,000 --> 01:45:20,000
I'm just going to comment out these other things here.

1005
01:45:20,000 --> 01:45:24,000
You see that this sticks in this thing called row and R.

1006
01:45:24,000 --> 01:45:31,000
Okay, so you can think of that if you're used to Pandas as an index here.

1007
01:45:33,000 --> 01:45:39,000
So we're going to group by here, group by just like in Pandas is lazy, it doesn't do anything until we do an aggregation.

1008
01:45:39,000 --> 01:45:47,000
And here in this case, I'm going to call ag and in this case, I'm passing in a list of expressions, right?

1009
01:45:47,000 --> 01:45:52,000
And so this is another place where Polar's can be smart about this because we're passing in multiple things.

1010
01:45:52,000 --> 01:45:56,000
So it can say, oh, I'm going to do these at the same time if I have the CPUs to do them.

1011
01:45:56,000 --> 01:46:05,000
So in this case, it's saying with the impressions, take the mean value and with this number row, take the first entry for that.

1012
01:46:07,000 --> 01:46:15,000
Okay, so you see that there's number row, the first entry for the 2021 year was this value right there.

1013
01:46:16,000 --> 01:46:26,000
So the question going back to that question, like if I wanted to see row 20 or something like that, like 20 through 30, I might do something like this.

1014
01:46:30,000 --> 01:46:33,000
Or I might say, okay, let's do a filter here.

1015
01:46:33,000 --> 01:46:41,000
Filter lets us filter rows and then we can say PL.call and this should be row and R.

1016
01:46:42,000 --> 01:46:46,000
And I should be able to say greater than or equal to 20.

1017
01:46:48,000 --> 01:46:51,000
And I don't know if this works this way.

1018
01:46:57,000 --> 01:47:00,000
I don't know if it works like this. It might. Let's see if it does.

1019
01:47:05,000 --> 01:47:09,000
Okay, since the questions are related to truthiness of the expression of the ambiguous.

1020
01:47:12,000 --> 01:47:13,000
Okay.

1021
01:47:15,000 --> 01:47:21,000
So this is doing less than 30. So it wants us to combine it. It wants us to do something like this.

1022
01:47:23,000 --> 01:47:31,000
Less than 30 and PL.call row and R greater than or equal to 20.

1023
01:47:34,000 --> 01:47:37,000
Okay, so there's rows 20 through 30.

1024
01:47:38,000 --> 01:47:46,000
So a little bit more verbose than what you'd see using like a pandas or a pandas iLoc to get that.

1025
01:47:52,000 --> 01:48:00,000
Okay, so let's go to our next example here. We're grouping by year and we're getting the mean impressions and the mean replies by year.

1026
01:48:01,000 --> 01:48:04,000
So we want to say like how are we doing by each year?

1027
01:48:04,000 --> 01:48:13,000
And we see that, you know, for 2020, a thousand impressions on average and 2021 we're getting 3000.

1028
01:48:13,000 --> 01:48:19,000
And then we can also see that like our replies went up a bit per se as well.

1029
01:48:20,000 --> 01:48:34,000
Now, again, because we are doing these different aggregations, this is a place where pullers can be really quick, especially relative to just pandas, pandas the library.

1030
01:48:37,000 --> 01:48:45,000
Okay, so let's assume that we want to do instead of just by looking by year, we want to break this down to finer granularity. Let's do a month and year.

1031
01:48:45,000 --> 01:48:50,000
Okay, and so it turns out that you can pass in a list of expressions into the group by here.

1032
01:48:50,000 --> 01:48:54,000
So I want to say I want to group by year and then I also want to group by month in year.

1033
01:48:54,000 --> 01:48:57,000
And so let's run that and we get something that looks like this.

1034
01:48:57,000 --> 01:49:12,000
Now, if you look at this, the result here might be a little not satisfying here because it's going from like April of 2021 to August and then back to June of 2020.

1035
01:49:12,000 --> 01:49:18,000
So the results are not sorted and that's due to how pullers is doing this by default.

1036
01:49:18,000 --> 01:49:22,000
It's doing what's called a hash join. A hash is unordered, but it's fast.

1037
01:49:22,000 --> 01:49:26,000
And so it gets that done quickly, but it's not necessarily ordered here.

1038
01:49:26,000 --> 01:49:29,000
So we might need to do some further operations.

1039
01:49:29,000 --> 01:49:32,000
So we might say, like, let's sort this by year and month.

1040
01:49:32,000 --> 01:49:33,000
Again, we don't have an index.

1041
01:49:33,000 --> 01:49:36,000
We're just going to sort it by the columns and we get something that looks like that.

1042
01:49:36,000 --> 01:49:39,000
That looks like that's that's decent.

1043
01:49:39,000 --> 01:49:46,000
OK, another option here is to pass in this maintain order.

1044
01:49:46,000 --> 01:49:49,000
OK, and so we'll pass in maintain order here.

1045
01:49:49,000 --> 01:49:52,000
And that looks like that works as well.

1046
01:49:52,000 --> 01:49:57,000
As I was just telling the grouping, hey, I want you to maintain the order.

1047
01:49:57,000 --> 01:50:08,000
And again, according to Richie, the creator of pullers, this is preferable to calling sort by year.

1048
01:50:08,000 --> 01:50:09,000
Calling sort.

1049
01:50:09,000 --> 01:50:24,000
Apparently, there are some optimizations that can happen by knowing this at this point rather than just calling a generic sort after the fact.

1050
01:50:24,000 --> 01:50:27,000
Also, there is a fast path in pullers.

1051
01:50:27,000 --> 01:50:34,000
So if you call sort on a column, pullers will know that it's sorted and it can do further operations.

1052
01:50:34,000 --> 01:50:37,000
And you can if you think about that, I mean, if you've done a data structures class,

1053
01:50:37,000 --> 01:50:41,000
you have sorted data, it's easier to merge sorted data than data that's not sorted.

1054
01:50:41,000 --> 01:50:44,000
And so it can take advantage of that.

1055
01:50:44,000 --> 01:50:47,000
So here we're going to say I want to sort this.

1056
01:50:47,000 --> 01:50:50,000
But I'm also going to do this.

1057
01:50:50,000 --> 01:50:53,000
I'm going to say let's make a column called month.

1058
01:50:53,000 --> 01:50:57,000
But the column is going to be the year and the dash with the month in it.

1059
01:50:57,000 --> 01:51:05,000
So rather than having like this one up here had year and month as columns rather than having two columns, I just want a single column that represents that.

1060
01:51:05,000 --> 01:51:10,000
And so I'm going to do that and we'll get a result that looks like this.

1061
01:51:10,000 --> 01:51:12,000
And here month is this string right here.

1062
01:51:12,000 --> 01:51:14,000
So that's looking OK as well.

1063
01:51:14,000 --> 01:51:18,000
That might be preferable.

1064
01:51:18,000 --> 01:51:24,000
Now, if you've used pandas, you might be familiar with pandas has the notion of an offset alias.

1065
01:51:24,000 --> 01:51:26,000
Anyone familiar with that?

1066
01:51:26,000 --> 01:51:38,000
So an offset alias is where if you have a column that is a date, you can say I want to resample this at the quarter level and it will take everything from the same quarter.

1067
01:51:38,000 --> 01:51:45,000
And then you can do an aggregate on the same quarter or you can say I want to do everything from the same week and it will give you each week.

1068
01:51:45,000 --> 01:51:47,000
Polars has something similar.

1069
01:51:47,000 --> 01:51:49,000
It's called group by dynamic.

1070
01:51:49,000 --> 01:51:52,000
And so you can do something like this where it's kind of interesting.

1071
01:51:52,000 --> 01:51:56,000
You say index column, even though there isn't really an index in Polars, but you say index column.

1072
01:51:56,000 --> 01:52:01,000
So we use the time column and then I'm going to say every and this is two M.O.

1073
01:52:01,000 --> 01:52:03,000
This stands for two months.

1074
01:52:03,000 --> 01:52:04,000
So let's run this.

1075
01:52:04,000 --> 01:52:15,000
And if you look at this in this time column now, what you have is instead of having a string like we had in that different in that previous one, we now have a date time.

1076
01:52:15,000 --> 01:52:18,000
And if you look at the date times here, this is January 1st.

1077
01:52:18,000 --> 01:52:21,000
This is March 1st, May 1st, etc.

1078
01:52:21,000 --> 01:52:26,000
So this has done the aggregation using date manipulation.

1079
01:52:26,000 --> 01:52:33,000
If you think about this, this actually is just doing an aggregation based on the strings that I created.

1080
01:52:33,000 --> 01:52:37,000
But this is a little bit more intelligent down here.

1081
01:52:37,000 --> 01:52:40,000
OK, so you can do crazy things like this.

1082
01:52:40,000 --> 01:52:41,000
Look at this.

1083
01:52:41,000 --> 01:52:44,000
I'm going to do every and I'm going to do two W.

1084
01:52:44,000 --> 01:52:46,000
That stands for every two weeks.

1085
01:52:46,000 --> 01:52:50,000
And then I'm going to do every five, three days and every five hours.

1086
01:52:50,000 --> 01:52:52,000
Kind of silly.

1087
01:52:52,000 --> 01:52:56,000
But the point is, is that like this is kind of powerful, too, right?

1088
01:52:56,000 --> 01:53:02,000
If you wanted to do aggregations by different frequencies, you can do that.

1089
01:53:02,000 --> 01:53:05,000
And if you look at the date, the first one here is January 1st.

1090
01:53:05,000 --> 01:53:07,000
And it's like 2044.

1091
01:53:07,000 --> 01:53:12,000
And this next one should be like two weeks later and three days after that.

1092
01:53:12,000 --> 01:53:20,000
And it doesn't look like that's two weeks later.

1093
01:53:20,000 --> 01:53:22,000
So apparently there's a bug in here.

1094
01:53:22,000 --> 01:53:28,000
So maybe we found a bug.

1095
01:53:28,000 --> 01:53:29,000
OK, or maybe I have a bug.

1096
01:53:29,000 --> 01:53:32,000
Maybe my two weeks is not right.

1097
01:53:32,000 --> 01:53:33,000
This looks like three days.

1098
01:53:33,000 --> 01:53:36,000
So maybe my weeks is messed up.

1099
01:53:36,000 --> 01:53:39,000
Let me just for fun here.

1100
01:53:39,000 --> 01:53:50,000
Every and I'm going to say two W.

1101
01:53:50,000 --> 01:53:57,000
OK, so apparently it doesn't like combining dates and weeks.

1102
01:53:57,000 --> 01:54:07,000
And apparently the dates supersedes the week entry there.

1103
01:54:07,000 --> 01:54:13,000
OK, so I guess if we wanted every two weeks and three days, right, we could say two weeks is 14 days.

1104
01:54:13,000 --> 01:54:24,000
And we can make this 17, right, which should do what we wanted there.

1105
01:54:24,000 --> 01:54:30,000
OK, so oftentimes when I have this, right, I mean, I think this is a, you know, is this the best information ever?

1106
01:54:30,000 --> 01:54:33,000
I mean, it might be right, but I might want to plot this as well.

1107
01:54:33,000 --> 01:54:35,000
Right. And plotting is another way to visualize it.

1108
01:54:35,000 --> 01:54:37,000
I'm a huge fan of plotting.

1109
01:54:37,000 --> 01:54:43,000
Sadly, Pollers or maybe not sadly, depending on your point of view, Pollers doesn't have plotting in it.

1110
01:54:43,000 --> 01:54:49,000
So if you want to do plotting, you need to use some other library.

1111
01:54:49,000 --> 01:54:58,000
One of the reasons I really like Pandas is because I think I think like using Pandas lets me create plots and map plot level easier than map plot itself.

1112
01:54:58,000 --> 01:55:09,000
And actually think once you understand the interface of map plot of Pollers or sorry, once you understand the interface of how Pandas makes plots, it's really easy,

1113
01:55:09,000 --> 01:55:14,000
especially if you're using an aggregation to go from an aggregation to a plot.

1114
01:55:14,000 --> 01:55:18,000
And so I might want to look at like a time plot or a line plot of this data.

1115
01:55:18,000 --> 01:55:22,000
And what would I want? I would want this time to be in the index.

1116
01:55:22,000 --> 01:55:28,000
And then I want impressions and replies to be a little line for each of those columns here.

1117
01:55:28,000 --> 01:55:35,000
Really easy to do in Pandas. So let me just show you how we might do that.

1118
01:55:35,000 --> 01:55:39,000
We can actually say send this over to Pandas.

1119
01:55:39,000 --> 01:55:42,000
And so we say to Pandas. This is now not a Pollers data frame.

1120
01:55:42,000 --> 01:55:51,000
This is a Pandas data frame. And in Pandas, if you're doing a line plot, by default, it's going to plot the index in the x-axis.

1121
01:55:51,000 --> 01:55:56,000
So I'm going to say stick that time into the x-axis.

1122
01:55:56,000 --> 01:55:58,000
Now that is the index. You can see it's bold there.

1123
01:55:58,000 --> 01:56:07,000
And if I just call plot here, it's going to plot the index in the x-axis and each column will be its own line.

1124
01:56:07,000 --> 01:56:11,000
OK, so here is my plot from that.

1125
01:56:11,000 --> 01:56:20,000
Maybe I come in here and say fig size is equal to, and maybe just make this a little bit smaller, 8 by 3.

1126
01:56:20,000 --> 01:56:35,000
OK, so that's every two months here. But if I wanted to do like every three days, there's every three days, right, or every seven days.

1127
01:56:35,000 --> 01:56:50,000
So this makes it really easy to see that like in July of 2021, something happened. There were impressions like shot up, right?

1128
01:56:50,000 --> 01:56:58,000
OK, and so this is just showing what I just showed. I'm just changing the frequency there so you can change it to kind of play around with that.

1129
01:56:58,000 --> 01:57:09,000
But super powerful. I mean, once you sort of understand this, every parameter here, again, Pandas calls this an OPSA alias.

1130
01:57:09,000 --> 01:57:11,000
Actually, I'm going to pull up the documentation here.

1131
01:57:11,000 --> 01:57:20,000
One thing I don't like about Pandas is that the OPSA alias isn't documented in the in in documentation that you can access from Jupiter.

1132
01:57:20,000 --> 01:57:30,000
Let's see if that's the case here.

1133
01:57:30,000 --> 01:57:36,000
OK, yeah, so here's the opposite arguments right there.

1134
01:57:36,000 --> 01:57:48,000
Cool. So kudos to Polars for actually including that documentation in the library itself that you can access from the library.

1135
01:57:48,000 --> 01:57:56,000
OK, let's let's try and do a pivot. There's also a pivot in Pandas. Pivot is basically built on top of group by.

1136
01:57:56,000 --> 01:58:04,000
So here is there's a pivot and this looks pretty similar to Pandas. You say the values, you say what goes in the index.

1137
01:58:04,000 --> 01:58:09,000
Again, there's no index in Polars, but that's basically what you want to group by.

1138
01:58:09,000 --> 01:58:15,000
So if you're not familiar with Pandas, that might be a little bit weird, but basically the index is what you want to group by.

1139
01:58:15,000 --> 01:58:23,000
And then you can say what columns you want to be in the output and then you can specify an aggregation function.

1140
01:58:23,000 --> 01:58:28,000
OK, so.

1141
01:58:28,000 --> 01:58:36,000
One of the things that's sad about this is that or what I kind of miss is that we can't use a dynamic.

1142
01:58:36,000 --> 01:58:39,000
There's no dynamic pivot per se.

1143
01:58:39,000 --> 01:58:51,000
So if I'm making a pivot table and I wanted to do like dynamically by every three days and four hours, I can't do that with a pivot with the pivot function or method itself.

1144
01:58:51,000 --> 01:59:08,000
So I would need to come in here and do something like this where I use stir time and then with that new column till it to aggregate by that new column.

1145
01:59:08,000 --> 01:59:14,000
OK, so here what I'm doing is I'm just saying, OK, I'm going to make a new time column, which is year and month.

1146
01:59:14,000 --> 01:59:23,000
I'm going to pivot based on the month. I'm going to pull out the values for impressions and then I want columns to be whether it's a reply or not.

1147
01:59:23,000 --> 01:59:33,000
So this is what is this telling me? This is telling me, do I get more impressions if something is a reply or if it's not a reply?

1148
01:59:33,000 --> 01:59:40,000
And so we've got like a true faults column. True means that something is a reply. False means that it's not a reply.

1149
01:59:40,000 --> 01:59:45,000
And it looks like in general, if it's not a reply, I get more impressions, which makes sense.

1150
01:59:45,000 --> 01:59:53,000
Twitter doesn't really show your replies to everyone, only if someone is following both of those people.

1151
01:59:53,000 --> 02:00:02,000
So that would make sense.

1152
02:00:02,000 --> 02:00:10,000
OK, in this case, I'm saying let's look at so up here I just did. Let's look at the impressions here.

1153
02:00:10,000 --> 02:00:22,000
I'm saying let's look at the impressions and the replies and and then I'm going to aggregate or I'm going to in my columns as whether something is a reply or not.

1154
02:00:22,000 --> 02:00:29,000
So what do we have up here? We have here is something that is a reply. Faults for impressions.

1155
02:00:29,000 --> 02:00:37,000
Here's is a reply true for impressions. Here's replies that were replies and here's replies.

1156
02:00:37,000 --> 02:00:46,000
Sorry, here's replies that were faults and here's replies that is a reply was true.

1157
02:00:46,000 --> 02:00:58,000
So this is kind of need to think about this like what does if something is a reply, but the call we're aggregating the replies for something is OK.

1158
02:00:58,000 --> 02:01:06,000
Yeah, so the replies is the number of replies and this is an indicator whether something is reply.

1159
02:01:06,000 --> 02:01:14,000
So it's possible for replies, something that is replied to have a reply on it. Basically what that is saying.

1160
02:01:14,000 --> 02:01:23,000
So, yeah, the column headers here might be a little bit weird, right? I mean, it's it's building these up for us in pandas.

1161
02:01:23,000 --> 02:01:30,000
So we could have hierarchical columns here instead, which might make it a little bit easier to understand.

1162
02:01:30,000 --> 02:01:39,000
But again, pullers doesn't have hierarchical columns, so it's kind of just building those up based on the values that are in there.

1163
02:01:39,000 --> 02:01:52,000
Now you can do something like this. You can say like I want to separate specify the separator in there.

1164
02:01:52,000 --> 02:02:01,000
So instead of you can see how it's kind of building that up. This is the impressions column and then we're looking at whether is reply is faults.

1165
02:02:01,000 --> 02:02:08,000
This is the impression is columns, whether is reply is true. The replies column, whether is replies faults and the replies column is replies true.

1166
02:02:08,000 --> 02:02:15,000
This is just saying stick in a star as a separator. You know, if you want space in there, you can stick in a space.

1167
02:02:15,000 --> 02:02:25,000
That would work as well. And then here, let's just plot this here. So.

1168
02:02:25,000 --> 02:02:33,000
So we're going to set this. We're going to convert the pandas, stick that month in there and then we'll plot that.

1169
02:02:33,000 --> 02:02:40,000
We get something that looks like that.

1170
02:02:40,000 --> 02:02:46,000
OK, so we're going to do our aggregation exercise. Let you work on this for a little bit.

1171
02:02:46,000 --> 02:02:55,000
So let's just look at the exercise. The first one is what is what were the total impressions for each year?

1172
02:02:55,000 --> 02:03:03,000
So hint is when you hear something say for each year or by something, that's what you want to group by.

1173
02:03:03,000 --> 02:03:13,000
OK, so in this case, if we want the total impressions for each year, we need to group by the year and then we're going to pull off that impressions column and take the sum of it.

1174
02:03:13,000 --> 02:03:20,000
In this case, total impressions for each month, we need to get the month and then some of the impressions.

1175
02:03:20,000 --> 02:03:26,000
So once you have this first one, hopefully that that second one is not too hard.

1176
02:03:26,000 --> 02:03:33,000
This plot the previous, so just visualize those and then the next one here.

1177
02:03:33,000 --> 02:03:38,000
What is the total impressions for reply and non reply tweets for each month?

1178
02:03:38,000 --> 02:03:45,000
So that one's a little bit more complicated and then plotting that. So I'll let you work on those.

1179
02:03:45,000 --> 02:03:51,000
And again, just if you have questions, concerns, happy to go around and help.

1180
02:03:51,000 --> 02:03:55,000
But this is again your chance to get some of that muscle memory.

1181
02:03:55,000 --> 02:04:03,000
I found that grouping and aggregations, these pivoting, there's probably some of the most important things that people need to be able to do with their data.

1182
02:04:03,000 --> 02:04:10,000
And it tends to be one of those things that again, the more you practice it, the easier it's going to be.

1183
02:04:10,000 --> 02:04:18,000
Like this is one of those things that you need. You need to get the muscle memory to to really start understanding how it works.

1184
02:04:18,000 --> 02:04:22,000
So I'll let you work on that for a little bit.

1185
02:04:22,000 --> 02:04:28,000
Questions? Questions before you hop into this? What questions do you have?

1186
02:04:28,000 --> 02:04:33,000
OK. Just be quiet and let you work on it.

1187
02:04:33,000 --> 02:04:51,000
OK, I think we're going to run a look at the solutions here for this as we're getting to the end of our time here.

1188
02:04:51,000 --> 02:04:59,000
OK. So first of all, what were the impressions for each year?

1189
02:04:59,000 --> 02:05:07,000
And so we've got a couple options here. One is we can pull off that year column and that year value from the date and put that in there.

1190
02:05:07,000 --> 02:05:11,000
The other is we can do a group by dynamic.

1191
02:05:11,000 --> 02:05:23,000
So maybe I'll do. Let's do a dynamic one.

1192
02:05:23,000 --> 02:05:41,000
OK, so I want each year. So I should be able to say why here.

1193
02:05:41,000 --> 02:05:45,000
Yeah, so it says expected an integer in the duration string.

1194
02:05:45,000 --> 02:05:50,000
So. Wants us to specify how many years.

1195
02:05:50,000 --> 02:05:56,000
So if you look at this, this is the January 1st of 2020 and this is January 1st of 2021.

1196
02:05:56,000 --> 02:05:59,000
This is a little bit different than pandas and pandas with an offset a list.

1197
02:05:59,000 --> 02:06:05,000
You don't have to specify a number there, but it looks like pullers wants us to.

1198
02:06:05,000 --> 02:06:10,000
OK, so and we want the impressions for each year and we want the total.

1199
02:06:10,000 --> 02:06:16,000
So I've got replies in here. I don't want replies and then I want this to be the sum here.

1200
02:06:16,000 --> 02:06:22,000
OK, so there's the sum of the impressions for each year.

1201
02:06:22,000 --> 02:06:27,000
OK, if I wanted this for each month.

1202
02:06:27,000 --> 02:06:33,000
Once I've got it in this form, it's really easy. I just change this to one M.

1203
02:06:33,000 --> 02:06:37,000
And you can see this case that didn't work.

1204
02:06:37,000 --> 02:06:42,000
That looks like it's probably doing some sort of minute there.

1205
02:06:42,000 --> 02:06:47,000
So we'll try M.O. month. That looks like it did work.

1206
02:06:47,000 --> 02:06:52,000
OK, so now both of those work. Next one is to plot these.

1207
02:06:52,000 --> 02:06:57,000
So again, pullers doesn't have plotting, so convert that to pandas.

1208
02:06:57,000 --> 02:07:02,000
Stick that time into the index and if you just call plot there, there we go.

1209
02:07:03,000 --> 02:07:09,000
That's not particularly interesting. I would probably prefer to do a bar plot on this.

1210
02:07:09,000 --> 02:07:15,000
Slightly more interesting. I might even do a horizontal bar plot, change that to a bar H.

1211
02:07:15,000 --> 02:07:19,000
Something looks like that.

1212
02:07:19,000 --> 02:07:23,000
This one here probably would leave as a line plot.

1213
02:07:23,000 --> 02:07:29,000
Let me see something that looks like that.

1214
02:07:29,000 --> 02:07:39,000
OK, let's see. The next one here is what were the total impressions for reply and non-reply tweets for each month?

1215
02:07:39,000 --> 02:07:45,000
OK, so in this case, we're going to do a pivot on that.

1216
02:07:45,000 --> 02:07:49,000
So I'll take my pivot code and copy and paste it.

1217
02:07:49,000 --> 02:07:56,000
And I already have each, this is the month code here. And so I want to stick that into the index.

1218
02:07:56,000 --> 02:08:00,000
What do I want to look at? I want to look at the impressions.

1219
02:08:00,000 --> 02:08:04,000
And I want the columns to be whether it's a reply or not.

1220
02:08:04,000 --> 02:08:09,000
So that little tweak of that should be OK.

1221
02:08:09,000 --> 02:08:14,000
So I'm not afraid to take code and run with it.

1222
02:08:14,000 --> 02:08:21,000
Again, getting this code started off might be a little bit more involved.

1223
02:08:21,000 --> 02:08:31,000
So start with something simple, right? Start with something simple that you can check and make sure it's working and then build that up.

1224
02:08:31,000 --> 02:08:35,000
So let's plot this. So I'll convert it into pandas.

1225
02:08:35,000 --> 02:08:48,000
Let's set the index to the month and then we'll plot this.

1226
02:08:48,000 --> 02:08:53,000
Oh, thank you. Yeah, aggregate function to be the sum, total impressions.

1227
02:08:53,000 --> 02:08:59,000
Yeah, and there we go. We get something that looks like that.

1228
02:08:59,000 --> 02:09:10,000
OK, questions, concerns, thoughts?

1229
02:09:10,000 --> 02:09:15,000
OK, so we are at the end of our tutorial here.

1230
02:09:15,000 --> 02:09:26,000
Hopefully this gave you a chance to play around with pullers, see some of the places where it might be different or improve on other libraries that you're familiar with.

1231
02:09:26,000 --> 02:09:33,000
Again, sort of the elephant in the room is pandas.

1232
02:09:33,000 --> 02:09:38,000
My take on this, again, I'm pretty biased because I've used pandas for a long time.

1233
02:09:38,000 --> 02:09:43,000
My take on this is that I feel like I'm very comfortable with pandas.

1234
02:09:43,000 --> 02:09:49,000
The expressions for pullers take a little bit of getting used to.

1235
02:09:49,000 --> 02:09:53,000
It seems that pullers requires a little bit more typing than pandas does.

1236
02:09:53,000 --> 02:09:58,000
That might be a trade off that you're willing to make to get the speed performance.

1237
02:09:58,000 --> 02:10:07,000
Again, a lot of that speed performance is going to come when you start doing aggregations and groupings where it can be smart about that.

1238
02:10:07,000 --> 02:10:21,000
Pullers also, because of the streaming functionality that we didn't have time to go into, but it does have some streaming functionality, can do larger than memory data sets that pandas, the library can't do.

1239
02:10:21,000 --> 02:10:30,000
But again, pandas, the API can do with various other libraries that implement pandas that allow you to go out of core.

1240
02:10:30,000 --> 02:10:36,000
I do like to, so I showed my method for taking a data set and going through it and analyzing it.

1241
02:10:36,000 --> 02:10:39,000
I do like to do that.

1242
02:10:39,000 --> 02:10:42,000
Just go through the columns, check those types.

1243
02:10:42,000 --> 02:10:51,000
Again, when you're using libraries like pullers or pandas, going through those types can be often a significant memory savings by doing that.

1244
02:10:51,000 --> 02:10:56,000
And that can also speed up computation time as well.

1245
02:10:56,000 --> 02:11:01,000
Huge fan of chaining in pandas, but also chaining in pullers.

1246
02:11:01,000 --> 02:11:06,000
You don't have to chain, but you're kind of going to get dinged or penalized in pullers if you don't.

1247
02:11:06,000 --> 02:11:14,000
So you can get performance boost in pullers by doing chaining, especially if you're lazy because it does have query optimization.

1248
02:11:14,000 --> 02:11:17,000
So embrace chaining.

1249
02:11:17,000 --> 02:11:24,000
Again, when you go across that Rust Python threshold, things will slow you down there.

1250
02:11:24,000 --> 02:11:28,000
So find ways to do things in expressions if you can.

1251
02:11:28,000 --> 02:11:40,000
And then finally, these aggregations that we looked at are very powerful and let you answer questions very succinctly.

1252
02:11:40,000 --> 02:11:43,000
The syntax might be a little bit confusing or weird to you.

1253
02:11:43,000 --> 02:11:46,000
If that is the case, my advice is to practice them, right?

1254
02:11:46,000 --> 02:11:50,000
The more you practice these aggregations, the better they're going to make sense.

1255
02:11:50,000 --> 02:11:54,000
And you'll see sort of the pros and cons of them.

1256
02:11:54,000 --> 02:11:58,000
OK, any questions before we dismiss?

1257
02:12:10,000 --> 02:12:13,000
Yeah. Question.

1258
02:12:13,000 --> 02:12:18,000
If I'm happy with pandas generally, I'm not having performance issues.

1259
02:12:18,000 --> 02:12:23,000
I'm not like Rust isn't super interesting to me as a language.

1260
02:12:23,000 --> 02:12:28,000
Do I need to care about pullers?

1261
02:12:28,000 --> 02:12:37,000
I mean, and my sort of short answer would be if pandas works for you, you should be good.

1262
02:12:37,000 --> 02:12:41,000
Again, pandas, too, just came out a few weeks ago.

1263
02:12:41,000 --> 02:12:52,000
It does have the ability to use those pie arrow data types, which is going to get you both speed and memory improvements from that point of view.

1264
02:12:52,000 --> 02:13:04,000
You know, for doing exploratory data type stuff, pandas is going to have better integration with other libraries, right?

1265
02:13:04,000 --> 02:13:10,000
Because pandas has been around for a while. It's sort of the big player there.

1266
02:13:10,000 --> 02:13:18,000
So, yeah, if you don't have a compelling reason to use pullers,

1267
02:13:18,000 --> 02:13:24,000
I don't think there's a need to switch to it per se.

1268
02:13:24,000 --> 02:13:34,000
I guess my take on that would be like, I do think you would want to, at least if you're coming from pandas, to probably start embracing that chaining.

1269
02:13:34,000 --> 02:13:45,000
Because even though you don't have like query planning and optimization that you do in pandas, I do think it makes your code easier to read and debug and reuse.

1270
02:13:45,000 --> 02:13:50,000
So that would be my advice around that.

1271
02:13:50,000 --> 02:13:55,000
Other questions?

1272
02:13:55,000 --> 02:13:57,000
Okay, well, thank you for your time.

1273
02:13:57,000 --> 02:14:07,000
Hopefully you feel like you are better prepared to sort of figure out where pullers make sense, where it might not make sense.

1274
02:14:07,000 --> 02:14:13,000
If you want to connect with me on LinkedIn or Twitter, I'm there, happy to connect.

1275
02:14:13,000 --> 02:14:19,000
Yeah, and again, have a great conference.

1276
02:14:19,000 --> 02:14:27,000
Thanks for letting me present and good luck with your future tabular data endeavors.

