1
00:00:00,000 --> 00:00:02,960
Just before we move on to the content, I just want to give an overview of the two different

2
00:00:02,960 --> 00:00:04,800
repositories if I didn't already.

3
00:00:04,800 --> 00:00:09,160
So we're going to start with a high level introduction to LLMs and transformers.

4
00:00:09,160 --> 00:00:13,780
And then we're going to go into our fine tuning BERT exercise.

5
00:00:13,780 --> 00:00:19,600
And then we'll kind of go back to high level and look at two generative AI demos, one for

6
00:00:19,600 --> 00:00:24,040
GPT versus chat GPT, and then another one for stable diffusion.

7
00:00:24,040 --> 00:00:28,360
And then at the end, we'll have a discussion about the bias and risks of AI, hopefully

8
00:00:28,360 --> 00:00:31,880
in open discussion or in small groups.

9
00:00:31,880 --> 00:00:32,880
Okay, so.

10
00:00:32,880 --> 00:00:51,320
Taking a little bit of time to load.

11
00:00:51,320 --> 00:00:56,800
Okay so first we're going to start with an introduction to natural language processing.

12
00:00:56,800 --> 00:00:58,560
Now I have this figure here.

13
00:00:58,560 --> 00:01:03,880
It's meant to be a little bit silly, but it's meant to also motivate us to think about some

14
00:01:03,880 --> 00:01:06,880
important ideas for the rest of the tutorial.

15
00:01:06,880 --> 00:01:11,960
So one of the things I found fascinating about deep learning when I started was this idea

16
00:01:11,960 --> 00:01:17,000
of being able to take really unstructured data like text and images and actually apply a

17
00:01:17,000 --> 00:01:18,320
structure to it.

18
00:01:18,320 --> 00:01:22,000
But let's look at these lines first.

19
00:01:22,000 --> 00:01:24,320
Three minus two equals one.

20
00:01:24,320 --> 00:01:25,320
We all know what that is.

21
00:01:25,840 --> 00:01:27,920
This is PyCon.

22
00:01:27,920 --> 00:01:29,800
Numbers are very easy to understand.

23
00:01:29,800 --> 00:01:30,960
We can measure them.

24
00:01:30,960 --> 00:01:33,160
There's no confusion.

25
00:01:33,160 --> 00:01:35,960
Now let's look at this next line.

26
00:01:35,960 --> 00:01:42,680
Queen minus woman plus man equals king.

27
00:01:42,680 --> 00:01:45,120
Now this isn't strictly speaking an equation.

28
00:01:45,120 --> 00:01:48,920
This isn't something that we could say put into a model or have a computer understand.

29
00:01:48,920 --> 00:01:50,640
But we know language.

30
00:01:50,640 --> 00:01:55,040
So if we look at it and we say, okay, a queen is a female ruler.

31
00:01:55,040 --> 00:02:03,200
And then if we subtract the idea or the concept of female gender and we add male gender, well,

32
00:02:03,200 --> 00:02:05,400
that seems like it's a male ruler.

33
00:02:05,400 --> 00:02:07,480
That seems pretty reasonable.

34
00:02:07,480 --> 00:02:12,080
And for those of you who might have heard of Word2Vec, this is the classic example from

35
00:02:12,080 --> 00:02:13,080
Word2Vec.

36
00:02:13,080 --> 00:02:14,080
And if you haven't, that's fine.

37
00:02:14,080 --> 00:02:16,360
I can add a reference.

38
00:02:16,360 --> 00:02:17,360
Okay.

39
00:02:17,360 --> 00:02:22,520
Now if we could represent these concepts that we understand in some numerical way that we

40
00:02:22,520 --> 00:02:28,040
could feed into a model or have a computer understand, that would be really interesting

41
00:02:28,040 --> 00:02:33,960
because then this numerical space would actually represent semantic understanding.

42
00:02:33,960 --> 00:02:36,000
So hold on to that thought.

43
00:02:36,000 --> 00:02:38,160
Now let's look at the last line.

44
00:02:38,160 --> 00:02:39,160
I love dogs.

45
00:02:39,160 --> 00:02:41,160
I had to include some dogs.

46
00:02:41,160 --> 00:02:45,680
So on the left, we have a very cute pug, my personal favorite.

47
00:02:45,680 --> 00:02:51,240
And then we're adding it to a chihuahua.

48
00:02:51,240 --> 00:02:55,400
Does that equal this super happy pug-wah-wah?

49
00:02:55,400 --> 00:02:58,640
That's apparently what they're called, this crossbreed between pug and chihuahua.

50
00:02:58,640 --> 00:03:03,840
Well, I mean, it kind of makes sense if we're breeding dogs.

51
00:03:03,840 --> 00:03:10,480
But again, we don't have a numerical representation of dog breeds, but we could.

52
00:03:10,480 --> 00:03:12,200
And again, this is image data.

53
00:03:12,200 --> 00:03:14,040
I'm sort of introducing the idea of dog breeds.

54
00:03:14,040 --> 00:03:21,140
But again, representations are images, another type of unstructured data.

55
00:03:21,140 --> 00:03:24,220
So we covered addition and subtraction on different data types.

56
00:03:24,220 --> 00:03:27,180
I'll now make this a bit bigger.

57
00:03:27,180 --> 00:03:29,900
It's text.

58
00:03:29,900 --> 00:03:34,740
So deep learning gives us the ability to represent all kinds of unstructured data like text and

59
00:03:34,740 --> 00:03:40,980
images, video, and even more abstract things like dogs or even emotions in a numerical

60
00:03:40,980 --> 00:03:44,820
way as embeddings.

61
00:03:44,820 --> 00:03:49,780
This is something a computer can understand and that we can use for models, but also captures

62
00:03:49,780 --> 00:03:51,580
our understanding of data.

63
00:03:51,580 --> 00:03:56,740
For the rest of this tutorial, we can think of embeddings as vectors, just arrays of floating

64
00:03:56,740 --> 00:03:57,740
point numbers.

65
00:03:57,740 --> 00:04:00,740
Okay, so that was the intro.

66
00:04:00,740 --> 00:04:02,020
This is to motivate us.

67
00:04:02,020 --> 00:04:04,140
See some pretty pictures.

68
00:04:04,140 --> 00:04:08,820
Now let's get closer to what we're actually trying to do.

69
00:04:08,820 --> 00:04:13,540
Natural language.

70
00:04:13,540 --> 00:04:17,060
So language, you know, words are great, but we need to start looking at sentences.

71
00:04:17,220 --> 00:04:20,140
We want to get close to natural language.

72
00:04:20,140 --> 00:04:25,660
So let's use our knowledge of language to gauge sentence similarity.

73
00:04:25,660 --> 00:04:27,660
So I have two sentences here.

74
00:04:27,660 --> 00:04:30,620
The feline slept in the sunshine.

75
00:04:30,620 --> 00:04:32,140
Sentence one.

76
00:04:32,140 --> 00:04:33,140
Sentence two.

77
00:04:33,140 --> 00:04:36,060
The cat took a nap on the rug.

78
00:04:36,060 --> 00:04:40,620
And these images have been very helpfully created by Dali, which we will experiment

79
00:04:40,620 --> 00:04:43,820
with in a later section.

80
00:04:43,820 --> 00:04:46,340
And we can see from the pictures, we have two cute cats.

81
00:04:46,340 --> 00:04:47,340
They're curled up.

82
00:04:47,340 --> 00:04:48,340
They're sleeping.

83
00:04:48,340 --> 00:04:49,660
They look quite peaceful.

84
00:04:49,660 --> 00:04:53,260
Do we think these two sentences are similar?

85
00:04:53,260 --> 00:04:55,140
I see a couple nods.

86
00:04:55,140 --> 00:04:56,140
I see thumbs up.

87
00:04:56,140 --> 00:04:57,140
Great.

88
00:04:57,140 --> 00:05:02,500
Yeah, they're not exactly the same, but there's a similar idea of, you know, this cat or feline

89
00:05:02,500 --> 00:05:06,180
sleeping peacefully.

90
00:05:06,180 --> 00:05:07,860
Now let's look at another sentence.

91
00:05:07,860 --> 00:05:15,740
Let's not go too far.

92
00:05:15,740 --> 00:05:19,860
The cat took a bite of a rug.

93
00:05:19,860 --> 00:05:23,260
This Dali picture shows a cat with teeth.

94
00:05:23,260 --> 00:05:24,260
There's violence.

95
00:05:24,260 --> 00:05:25,260
There's a rug.

96
00:05:25,260 --> 00:05:26,260
There's a cat.

97
00:05:26,260 --> 00:05:28,980
Is this similar to the other sentences?

98
00:05:28,980 --> 00:05:30,980
No.

99
00:05:30,980 --> 00:05:32,860
Great.

100
00:05:32,860 --> 00:05:36,220
In some ways it is, but you know, the meaning behind it.

101
00:05:36,220 --> 00:05:39,380
I'm talking about the meaning behind it.

102
00:05:39,380 --> 00:05:42,260
So now let's come, let's look at a very simple NLP model.

103
00:05:42,260 --> 00:05:46,700
This existed well before our large language models.

104
00:05:46,700 --> 00:05:50,260
Can everyone see this or should I make it bigger?

105
00:05:50,260 --> 00:05:53,260
Make it bigger.

106
00:05:53,260 --> 00:05:57,100
Okay.

107
00:05:57,100 --> 00:06:01,280
So let's, you know, before we get to LLMs, maybe we don't need them.

108
00:06:01,280 --> 00:06:04,580
Let's try to come up with something ourselves and see how well we do in this tutorial in

109
00:06:04,580 --> 00:06:05,580
the next 10 minutes.

110
00:06:05,580 --> 00:06:09,500
So we're going to use a bag of words for sentence similarity.

111
00:06:09,500 --> 00:06:11,220
If you don't know what that is, don't worry.

112
00:06:11,260 --> 00:06:13,580
I'm about to explain it.

113
00:06:13,580 --> 00:06:17,940
But now what we're trying to do is come up with a way to quantify our feelings about

114
00:06:17,940 --> 00:06:25,180
the differences or similarities between those three previous sentences we just saw.

115
00:06:25,180 --> 00:06:27,900
So I propose we try something simple.

116
00:06:27,900 --> 00:06:31,180
So we're going to do this bag of words idea.

117
00:06:31,180 --> 00:06:36,460
So basically we take all the words in a sentence, throw them in a bag, essentially jumble them

118
00:06:36,460 --> 00:06:39,300
around and then pretend I'm holding a bag.

119
00:06:39,300 --> 00:06:42,740
This is one sentence, another bag, another sentence.

120
00:06:42,740 --> 00:06:47,300
And we can represent this quite well as a Python set.

121
00:06:47,300 --> 00:06:48,900
And so that's what we have here.

122
00:06:48,900 --> 00:06:54,140
The individual words are members of the set and order doesn't matter.

123
00:06:54,140 --> 00:06:55,500
Now I'm not going to run this cell.

124
00:06:55,500 --> 00:06:58,660
It's pretty simple Python code, but it is in a notebook.

125
00:06:58,660 --> 00:07:02,280
But you know, we'll just look at it right now.

126
00:07:02,280 --> 00:07:06,300
So now we want to come up with a metric to compare these two sentences.

127
00:07:09,460 --> 00:07:16,060
So one way to do that would be to look at the overlapping words between these bags of sentences.

128
00:07:16,060 --> 00:07:17,220
So let's do that.

129
00:07:17,220 --> 00:07:22,780
So we're going to look at the intersection between the different sentences.

130
00:07:22,780 --> 00:07:30,100
So the intersection between sentences one and two is just the and the lowercase and capital.

131
00:07:30,100 --> 00:07:32,100
Uppercase.

132
00:07:32,100 --> 00:07:36,620
Even though these are the two cats that were sleeping peacefully.

133
00:07:36,620 --> 00:07:39,900
What about the overlap between sentences two and three?

134
00:07:39,900 --> 00:07:42,580
Well, there are a lot more words there, right?

135
00:07:42,580 --> 00:07:47,460
The, rug, a cat, the, took.

136
00:07:47,460 --> 00:07:48,980
There are a lot of them words.

137
00:07:48,980 --> 00:07:53,140
So let's turn that into a score, a similarity score.

138
00:07:53,140 --> 00:07:57,740
So what I'm doing for the score is taking the length of the intersection between two bags

139
00:07:57,740 --> 00:08:01,980
and dividing it by the union of the two bags.

140
00:08:01,980 --> 00:08:05,740
That'll give us a number between zero and one, which is good for a score.

141
00:08:05,740 --> 00:08:08,740
So we look at the similarity between sentences one and two.

142
00:08:08,740 --> 00:08:11,020
Not surprisingly, we saw there was a little overlap.

143
00:08:11,020 --> 00:08:13,700
The score is 0.17.

144
00:08:13,700 --> 00:08:15,900
Pretty low.

145
00:08:15,900 --> 00:08:17,900
Similarity between sentence one and three.

146
00:08:17,900 --> 00:08:18,900
0.17.

147
00:08:18,900 --> 00:08:19,900
Pretty low.

148
00:08:19,900 --> 00:08:20,900
That we expected.

149
00:08:20,900 --> 00:08:24,580
You know, sleeping, biting.

150
00:08:24,580 --> 00:08:26,300
Similarity between sentences two and three.

151
00:08:26,300 --> 00:08:27,300
0.6.

152
00:08:27,300 --> 00:08:30,300
Does this match our intuition?

153
00:08:30,300 --> 00:08:31,300
Okay.

154
00:08:31,300 --> 00:08:34,900
I saw a couple of notes.

155
00:08:34,900 --> 00:08:42,420
No, it doesn't.

156
00:08:42,420 --> 00:08:46,980
So this doesn't match our intuition because we're looking at a syntax-based approach to

157
00:08:46,980 --> 00:08:48,860
understanding language.

158
00:08:48,860 --> 00:08:54,020
This is not capturing the semantic meaning behind these sentences.

159
00:08:54,020 --> 00:08:58,300
But we know we need a numerical way of quantifying differences between sentences.

160
00:08:58,300 --> 00:09:01,460
This is not going to work.

161
00:09:01,460 --> 00:09:05,700
One solution is large language models.

162
00:09:05,700 --> 00:09:11,940
This is an incredible jump to go from a bag of words to LLMs.

163
00:09:11,940 --> 00:09:13,260
I just want to point that out.

164
00:09:13,260 --> 00:09:16,460
But since we're focused on LLMs here, and that's what we're going to be working with,

165
00:09:16,460 --> 00:09:20,980
I'm skipping over decades of work in NLP.

166
00:09:20,980 --> 00:09:24,260
And we're going straight to the cutting edge.

167
00:09:24,260 --> 00:09:29,420
So large language models have the ability to represent sentences in a numerical way

168
00:09:30,180 --> 00:09:34,780
as vectors or embeddings that reflects our semantic understanding of language.

169
00:09:34,780 --> 00:09:38,420
That means those two first sentences we saw, if you look at their embedding vector after

170
00:09:38,420 --> 00:09:43,260
running it through a model like BERT, they would be much closer together than those two

171
00:09:43,260 --> 00:09:46,620
sentences from the numerical representation of that third sentence.

172
00:09:46,620 --> 00:09:50,740
That's the beauty of these large language models.

173
00:09:50,740 --> 00:09:55,740
So now I want to talk about a little bit of history.

174
00:09:55,740 --> 00:10:02,740
So on the y-axis, we have model size in billions of parameters.

175
00:10:05,340 --> 00:10:08,260
And this is in long scale.

176
00:10:08,260 --> 00:10:11,740
And on the x-axis, we have years.

177
00:10:11,740 --> 00:10:15,100
And this is only the last five years.

178
00:10:15,100 --> 00:10:20,020
There's been an exponential increase in the size of these models and the power and the

179
00:10:20,020 --> 00:10:23,220
ability of these models.

180
00:10:23,220 --> 00:10:28,100
And if we look at all of the data points on here, all of the models on here, except for

181
00:10:28,100 --> 00:10:33,340
the first one, ELMO, are based on this transformer architecture.

182
00:10:33,340 --> 00:10:38,740
That's what's really revolutionized NLP.

183
00:10:38,740 --> 00:10:41,660
So we'll be talking about BERT today.

184
00:10:41,660 --> 00:10:47,780
We'll also be talking about GPT-2, the second and third smallest on here.

185
00:10:47,780 --> 00:10:50,940
I tried running some larger ones and they crashed our free colabs.

186
00:10:50,940 --> 00:10:55,020
So unfortunately, we need more powerful resources for those.

187
00:10:55,020 --> 00:11:00,300
And I just want to say that this graph cuts off 2022.

188
00:11:00,300 --> 00:11:02,060
There's been a lot of stuff that happened since then.

189
00:11:02,060 --> 00:11:08,460
POM by Google, LAMA by Meta, GPT-4 by OpenAI.

190
00:11:08,460 --> 00:11:12,620
So this is still like a lot of stuff is happening.

191
00:11:12,620 --> 00:11:17,420
And so I mentioned the transformer architecture, but the way I see it, there are four things

192
00:11:17,460 --> 00:11:24,460
that really came together to cause this revolutionary improvement in NLP.

193
00:11:24,460 --> 00:11:26,980
So the first is computing power.

194
00:11:26,980 --> 00:11:30,700
Deep learning has been around for decades, but decades ago, we didn't have the machines

195
00:11:30,700 --> 00:11:31,700
we have today.

196
00:11:31,700 --> 00:11:37,860
The amount of RAM, memory, we have graphical processing units, TPUs.

197
00:11:37,860 --> 00:11:41,380
These models, and the second point is that these models are huge, so they need a lot

198
00:11:41,380 --> 00:11:42,380
of data.

199
00:11:42,380 --> 00:11:43,980
But we have a lot of data now.

200
00:11:44,300 --> 00:11:49,300
We look on the web, there's a ton of text data, there's a ton of image data, so we can

201
00:11:49,300 --> 00:11:53,060
feed these really data-hungry models.

202
00:11:53,060 --> 00:11:59,020
The third point is that typically, for those of you who are in the field, you may know

203
00:11:59,020 --> 00:12:04,420
that if we want to build a supervised learning model, we need labels.

204
00:12:04,420 --> 00:12:11,420
And those can be very expensive and very time-consuming to get, prohibitively so in a lot of ways.

205
00:12:11,860 --> 00:12:15,700
But these transformer models are trained in a different way that sidesteps that issue.

206
00:12:15,700 --> 00:12:20,900
They're trained in a self-supervised way, so we don't need explicit external labels.

207
00:12:20,900 --> 00:12:25,820
So we can actually access all the data that's available out there.

208
00:12:25,820 --> 00:12:29,620
And fourth, I mentioned it already, all the models that went on here, transformer models,

209
00:12:29,620 --> 00:12:33,100
it's this transformer architecture that really changed things.

210
00:12:33,100 --> 00:12:38,740
And at the heart of that is a particular neural network layer called the attention layer,

211
00:12:38,740 --> 00:12:42,820
which we'll talk a little bit about and develop some intuition for, that we won't get into

212
00:12:42,820 --> 00:12:45,820
the exact matrix multiplication behind it.

213
00:12:45,820 --> 00:12:53,540
Okay, so I'm going to talk a little bit more about some of the points I mentioned above.

214
00:12:53,540 --> 00:12:55,620
So large data sets.

215
00:12:55,620 --> 00:12:57,780
So what was BERT trained with?

216
00:12:57,780 --> 00:13:03,780
So BERT, normal BERT, not large BERT, has 100 million parameters.

217
00:13:03,820 --> 00:13:08,820
And BERT was trained on the BookScoreBus data set, which is 800 million words.

218
00:13:08,820 --> 00:13:12,820
And this is a large collection of free novels by unpublished authors.

219
00:13:12,820 --> 00:13:21,820
And BERT was also trained on English Wikipedia, which is 2.5 billion words at the time of training.

220
00:13:21,820 --> 00:13:23,820
So large data sets.

221
00:13:23,820 --> 00:13:30,820
Then I talked about the special training method, self-supervised learning, so training without labels.

222
00:13:30,860 --> 00:13:34,860
BERT was trained with two different training objectives.

223
00:13:34,860 --> 00:13:39,860
The first is masked language modeling.

224
00:13:39,860 --> 00:13:45,860
So I have an example here just so we can make it concrete, really understand.

225
00:13:45,860 --> 00:13:49,860
So let's look at the following sentence and try to predict what the word should be in the sentence

226
00:13:49,860 --> 00:13:52,860
instead of this mask or blank placeholder.

227
00:13:52,860 --> 00:13:58,860
The clever blank got the cheese without springing the trap.

228
00:13:58,900 --> 00:14:03,900
What do we think it is?

229
00:14:03,900 --> 00:14:05,900
Mouse, okay, great.

230
00:14:05,900 --> 00:14:08,900
And I have two helpful pictures by Dali.

231
00:14:08,900 --> 00:14:10,900
One's a mouse, one's an elephant.

232
00:14:10,900 --> 00:14:11,900
I mean, you know.

233
00:14:11,900 --> 00:14:13,900
But no, it's mouse.

234
00:14:13,900 --> 00:14:21,900
So by training BERT to predict the correct word, but over hundreds, like 100 million sentences,

235
00:14:21,900 --> 00:14:27,900
we teach the model to learn relationships between words and become a better language model.

236
00:14:28,900 --> 00:14:30,900
So that's one training objective for BERT.

237
00:14:30,900 --> 00:14:34,900
There was a second training objective.

238
00:14:34,900 --> 00:14:39,900
And that one is next sentence prediction.

239
00:14:39,900 --> 00:14:43,900
So BERT was also trained by taking two pairs of sentences

240
00:14:43,900 --> 00:14:46,900
and then predicting if the two sentences were related.

241
00:14:46,900 --> 00:14:50,900
Like, does sentence two logically follow from sentence one?

242
00:14:50,900 --> 00:14:54,900
So just to really understand this, let's take an example.

243
00:14:54,900 --> 00:14:57,900
The first sentence is, the cat climbed up a tree.

244
00:14:57,940 --> 00:14:59,940
Can you guys see this?

245
00:14:59,940 --> 00:15:00,940
Bigger?

246
00:15:00,940 --> 00:15:02,940
Yeah, just do this.

247
00:15:02,940 --> 00:15:05,940
Helpful signal.

248
00:15:05,940 --> 00:15:06,940
How's that?

249
00:15:06,940 --> 00:15:07,940
Good?

250
00:15:07,940 --> 00:15:08,940
Okay.

251
00:15:08,940 --> 00:15:09,940
Sorry about that.

252
00:15:09,940 --> 00:15:11,940
Yeah, just, you know, wave.

253
00:15:11,940 --> 00:15:14,940
The cat climbed up a tree and got stuck.

254
00:15:14,940 --> 00:15:17,940
Let's look at two possible next sentences.

255
00:15:17,940 --> 00:15:24,940
Letters can be posted in person during business hours, sentence one, or sentence two.

256
00:15:24,980 --> 00:15:27,980
The firefighter came with a ladder and climbed up to rescue the cat.

257
00:15:27,980 --> 00:15:29,980
Getting with the cat theme here.

258
00:15:29,980 --> 00:15:31,980
Which sentence makes more sense?

259
00:15:31,980 --> 00:15:32,980
Two.

260
00:15:32,980 --> 00:15:33,980
Yes.

261
00:15:33,980 --> 00:15:36,980
So by training BERT over next sentence prediction as well,

262
00:15:36,980 --> 00:15:38,980
we capture even more semantic content.

263
00:15:38,980 --> 00:15:41,980
And because these two training techniques, like the first one,

264
00:15:41,980 --> 00:15:44,980
we just removed a word.

265
00:15:44,980 --> 00:15:46,980
So essentially, we had the label.

266
00:15:46,980 --> 00:15:48,980
So we didn't need external labels.

267
00:15:48,980 --> 00:15:50,980
We could use the data as is.

268
00:15:50,980 --> 00:15:53,980
And again, natural language is, you know, sentences following one another.

269
00:15:54,020 --> 00:15:57,020
So we already have a data set that we can use.

270
00:15:59,020 --> 00:16:04,020
So then the final point I mentioned was this transformer architecture.

271
00:16:04,020 --> 00:16:07,020
So I'm going to keep this kind of high level.

272
00:16:07,020 --> 00:16:10,020
We're not going to get into, you know, the nitty-gritty and the math,

273
00:16:10,020 --> 00:16:12,020
at least right now.

274
00:16:14,020 --> 00:16:19,020
But this diagram here represents a transformer encoder layer.

275
00:16:19,020 --> 00:16:23,020
So BERT is made up of 12 transformer encoder layers.

276
00:16:24,020 --> 00:16:27,020
So if we look inside this encoder, we see two boxes

277
00:16:27,020 --> 00:16:30,020
representing two different neural networks.

278
00:16:30,020 --> 00:16:32,020
The first one is the one I mentioned.

279
00:16:32,020 --> 00:16:35,020
This is like the magic behind the transformer architecture.

280
00:16:35,020 --> 00:16:37,020
The attention layer.

281
00:16:38,020 --> 00:16:42,020
And then inputs, in this case, the example is a sentence in French.

282
00:16:42,020 --> 00:16:44,020
It goes through the attention layer.

283
00:16:44,020 --> 00:16:46,020
And then it goes through a feed forward layer

284
00:16:46,020 --> 00:16:48,020
or a fully connected layer.

285
00:16:48,020 --> 00:16:50,020
If you don't know exactly what that means, that's fine.

286
00:16:50,020 --> 00:16:52,020
But it passes through this.

287
00:16:52,060 --> 00:16:54,060
It passes through 11 more blocks.

288
00:16:54,060 --> 00:16:57,060
And through that, it actually gains the ability to learn

289
00:16:57,060 --> 00:17:00,060
about language in a really powerful way.

290
00:17:02,060 --> 00:17:05,060
Okay, so I mentioned that attention is a magic.

291
00:17:05,060 --> 00:17:08,060
So let's talk a little bit more about the attention mechanism.

292
00:17:11,060 --> 00:17:16,060
So attention is an efficient way of processing a sequence of data.

293
00:17:16,060 --> 00:17:19,060
Like, say, a sentence, a sequence of words.

294
00:17:19,100 --> 00:17:23,100
Modeling sequence data, historically, has been really difficult.

295
00:17:24,100 --> 00:17:28,100
So it's typically done by modeling it element.

296
00:17:28,100 --> 00:17:30,100
Oh, let me make this bigger, maybe.

297
00:17:33,100 --> 00:17:36,100
This is just text, and we'll get to a diagram shortly.

298
00:17:36,100 --> 00:17:38,100
So historically, this has been challenging.

299
00:17:38,100 --> 00:17:41,100
We model everything sequence, or element by element.

300
00:17:41,100 --> 00:17:44,100
And so as we're entering each element into the model,

301
00:17:44,100 --> 00:17:48,100
we're learning a representation of everything that's come before

302
00:17:48,140 --> 00:17:50,140
that particular element.

303
00:17:50,140 --> 00:17:53,140
We add another element in, we update a representation

304
00:17:53,140 --> 00:17:55,140
of that sentence.

305
00:17:55,140 --> 00:17:57,140
We keep doing this until there are no more words,

306
00:17:57,140 --> 00:18:00,140
and then that representation is our numerical representation

307
00:18:00,140 --> 00:18:02,140
of the sentence.

308
00:18:02,140 --> 00:18:05,140
And then that's what we're getting out of language models.

309
00:18:05,140 --> 00:18:08,140
But this is, you know, I'm talking about before language models.

310
00:18:09,140 --> 00:18:11,140
This is what we had to do.

311
00:18:11,140 --> 00:18:14,140
And that representation could then be used for downstream tasks,

312
00:18:14,140 --> 00:18:17,140
like, I don't know, maybe building a sentiment classifier.

313
00:18:17,180 --> 00:18:21,180
Or if the sentence is in English, translating it into Spanish.

314
00:18:21,180 --> 00:18:26,180
Or, say, figuring out if two sentences are similar or dissimilar.

315
00:18:27,180 --> 00:18:30,180
So if this approach sounds complicated, that's because it is.

316
00:18:31,180 --> 00:18:34,180
So the technical details of attention are out of scope.

317
00:18:34,180 --> 00:18:38,180
But it's basically a way of combining the numerical representations

318
00:18:38,180 --> 00:18:41,180
of words in a sentence with the position information

319
00:18:41,180 --> 00:18:45,180
without having to look at everything element by element.

320
00:18:47,180 --> 00:18:52,180
So intuitively what we do is, here, let me scroll down.

321
00:18:52,180 --> 00:18:56,180
Intuitively what we do in the sentence is,

322
00:18:56,180 --> 00:18:59,180
if I'm an element in the sentence, I ask or query

323
00:18:59,180 --> 00:19:02,180
every other element in the sentence to see if it's relevant

324
00:19:02,180 --> 00:19:04,180
for my own meaning.

325
00:19:04,180 --> 00:19:07,180
So as a more concrete example, let's take the sentence.

326
00:19:07,180 --> 00:19:10,180
The cat, purred, and happiness.

327
00:19:10,220 --> 00:19:12,220
Oops.

328
00:19:15,220 --> 00:19:18,220
In this sentence, the words cat and purr would attend to each other

329
00:19:18,220 --> 00:19:20,220
because they're related.

330
00:19:20,220 --> 00:19:23,220
Cats tend to purr, say more than elephants purr.

331
00:19:24,220 --> 00:19:28,220
And so I have a visualization of that here.

332
00:19:29,220 --> 00:19:32,220
So this is from a helpful repository called BurtViz.

333
00:19:34,220 --> 00:19:39,220
And what we're seeing here is what the word cat

334
00:19:39,260 --> 00:19:42,260
is paying attention to in the rest of the sentence

335
00:19:42,260 --> 00:19:45,260
to get more information about its own meaning.

336
00:19:45,260 --> 00:19:50,260
And so the pink on the left represents the strength of

337
00:19:50,260 --> 00:19:53,260
or the importance of that particular word or token

338
00:19:53,260 --> 00:19:55,260
to cat's meaning.

339
00:19:55,260 --> 00:19:58,260
And we can see that purred is darkest,

340
00:19:58,260 --> 00:20:00,260
meaning that the word cat is sort of paying attention

341
00:20:00,260 --> 00:20:02,260
to the word purred.

342
00:20:03,260 --> 00:20:06,260
So by passing inputs through many such layers

343
00:20:06,300 --> 00:20:09,300
with this attention, with this attention neural network layer,

344
00:20:09,300 --> 00:20:11,300
we can actually get really powerful models.

345
00:20:11,300 --> 00:20:14,300
This is the magic behind the transformer architecture.

346
00:20:19,300 --> 00:20:21,300
Okay.

347
00:20:21,300 --> 00:20:24,300
So now we have these really powerful large language models.

348
00:20:24,300 --> 00:20:27,300
And we'll see today they're pretty easy to use

349
00:20:27,300 --> 00:20:29,300
and interact with.

350
00:20:29,300 --> 00:20:31,300
So why is that good?

351
00:20:31,340 --> 00:20:36,340
So a big thing that's really made this technology

352
00:20:36,340 --> 00:20:37,340
really easy to use and widespread

353
00:20:37,340 --> 00:20:39,340
is this idea of transfer learning.

354
00:20:39,340 --> 00:20:43,340
So transfer learning is the act of initializing one model

355
00:20:43,340 --> 00:20:45,340
with another model's weights.

356
00:20:45,340 --> 00:20:50,340
So in this diagram here, say we have a model A like Burt,

357
00:20:50,340 --> 00:20:53,340
which has a lot of innate understanding about language

358
00:20:53,340 --> 00:20:56,340
and it was trained on a huge data set, English Wikipedia,

359
00:20:56,340 --> 00:20:59,340
you know, task A and the book's corpus data set.

360
00:21:00,340 --> 00:21:04,340
Now I need to build say a sentiment classifier

361
00:21:04,340 --> 00:21:07,340
and my data set is not very large.

362
00:21:07,340 --> 00:21:10,340
And I don't have millions of dollars to spend pre-training

363
00:21:10,340 --> 00:21:12,340
or training these models,

364
00:21:12,340 --> 00:21:15,340
just how much it costs to train some of these models.

365
00:21:15,340 --> 00:21:19,340
So what if I start my model with Burt?

366
00:21:19,340 --> 00:21:22,340
My sentiment review data is language.

367
00:21:22,340 --> 00:21:25,340
Burt already understands language.

368
00:21:25,340 --> 00:21:27,340
I then just need to adapt it a little bit

369
00:21:28,340 --> 00:21:30,340
to be able to predict positive sentiment

370
00:21:30,340 --> 00:21:32,340
or negative sentiment.

371
00:21:32,340 --> 00:21:36,340
It turns out this has been remarkably successful

372
00:21:36,340 --> 00:21:39,340
with large language models, as we'll see.

373
00:21:44,340 --> 00:21:47,340
And we're going to see this up close and personal pretty soon

374
00:21:47,340 --> 00:21:49,340
in our next hands-on section.

375
00:21:49,340 --> 00:21:54,340
Okay, so the title has the word intro to hugging face.

376
00:21:54,340 --> 00:21:56,340
We're going to be using hugging face a lot.

377
00:21:56,340 --> 00:21:59,340
This is something that I started using in my last role

378
00:21:59,340 --> 00:22:01,340
and I found it really helpful.

379
00:22:01,340 --> 00:22:02,340
I really like the documentation.

380
00:22:02,340 --> 00:22:04,340
There are a lot of helpful resources.

381
00:22:04,340 --> 00:22:08,340
So what we're going to do now is just do a quick look,

382
00:22:08,340 --> 00:22:10,340
the hugging face.

383
00:22:10,340 --> 00:22:12,340
Okay.

384
00:22:12,340 --> 00:22:15,340
So I'm logged in.

385
00:22:15,340 --> 00:22:16,340
That's why we're seeing this.

386
00:22:16,340 --> 00:22:19,340
But what I'm going to direct us to is the documentation.

387
00:22:19,340 --> 00:22:23,340
And Dana's going to take us through some of the models

388
00:22:23,340 --> 00:22:25,340
and the data sets info.

389
00:22:25,340 --> 00:22:27,340
But here, this is where I usually start

390
00:22:27,340 --> 00:22:29,340
when I look at hugging face.

391
00:22:29,340 --> 00:22:32,340
So the libraries that I have used most

392
00:22:32,340 --> 00:22:34,340
are the Transformers library.

393
00:22:34,340 --> 00:22:38,340
In this, we can access things like BERT, like GPT models.

394
00:22:38,340 --> 00:22:40,340
And I've also used the diffusers library

395
00:22:40,340 --> 00:22:43,340
for things like stable diffusion.

396
00:22:43,340 --> 00:22:46,340
And this is just a rich source of information.

397
00:22:46,340 --> 00:22:49,340
I'll just quickly...

398
00:22:49,340 --> 00:22:53,340
So here, you know, this is a Transformers landing page.

399
00:22:53,340 --> 00:22:56,340
If we scroll down...

400
00:22:56,340 --> 00:22:58,340
Oh.

401
00:22:58,340 --> 00:23:00,340
So we're going to be focusing on text models.

402
00:23:00,340 --> 00:23:02,340
You can see these are all the text models

403
00:23:02,340 --> 00:23:03,340
that we can learn about.

404
00:23:03,340 --> 00:23:06,340
And this isn't even all the models that are available

405
00:23:06,340 --> 00:23:09,340
on hugging face through, like, the models hub.

406
00:23:09,340 --> 00:23:11,340
It's just the stuff that, you know, has references

407
00:23:11,340 --> 00:23:15,340
to the actual original papers and, you know, how-tos

408
00:23:15,340 --> 00:23:17,340
and tutorials.

409
00:23:17,340 --> 00:23:20,340
So there's a lot of information here.

410
00:23:20,340 --> 00:23:24,340
And now I'm going to turn it over to Dana

411
00:23:24,340 --> 00:23:29,340
to get us started on our first hands-on part of this tutorial,

412
00:23:29,340 --> 00:23:33,340
which is actually getting our hands dirty with BERT.

413
00:23:33,340 --> 00:23:39,340
Okay.

414
00:23:39,340 --> 00:23:43,340
So I think we're going to take a couple minutes to switch.

415
00:23:43,340 --> 00:23:48,340
Yeah.

416
00:23:48,340 --> 00:23:51,340
Yeah.

417
00:23:51,340 --> 00:23:53,340
If anyone wants to take a couple minutes,

418
00:23:53,340 --> 00:23:55,340
and then we'll get set up here,

419
00:23:55,340 --> 00:23:57,340
and then we'll get started on the next section.

420
00:23:57,340 --> 00:23:59,340
Here we go.

421
00:23:59,340 --> 00:24:01,340
Okay. So first, I guess let's take a step back.

422
00:24:01,340 --> 00:24:03,340
Why are you guys here?

423
00:24:03,340 --> 00:24:05,340
I want to get a sense of, like, who's in the audience,

424
00:24:05,340 --> 00:24:08,340
who I'm talking to.

425
00:24:08,340 --> 00:24:10,340
Anyone here?

426
00:24:10,340 --> 00:24:16,340
I'll give a call-out to Stable Diffusion for all

427
00:24:16,340 --> 00:24:19,340
of the artwork in these slides.

428
00:24:19,340 --> 00:24:25,340
Anyone here who is, like, really interested in specializing

429
00:24:25,340 --> 00:24:27,340
in natural language processing specifically?

430
00:24:27,340 --> 00:24:30,340
Do you want to raise your hand?

431
00:24:30,340 --> 00:24:32,340
Okay. That's great.

432
00:24:32,340 --> 00:24:35,340
Cool. How about, like, of you guys, like,

433
00:24:35,340 --> 00:24:38,340
do you already have some experience specializing

434
00:24:38,340 --> 00:24:40,340
in natural language processing?

435
00:24:40,340 --> 00:24:42,340
Oh, that's great.

436
00:24:42,340 --> 00:24:44,340
Okay. So you can help me out if people have questions that,

437
00:24:44,340 --> 00:24:47,340
that I don't know because it's such a big field.

438
00:24:47,340 --> 00:24:51,340
Do you guys see that allow accessories?

439
00:24:51,340 --> 00:24:54,340
Don't allow.

440
00:24:54,340 --> 00:24:56,340
Well, okay.

441
00:24:56,340 --> 00:24:58,340
So I just listed some of, like, the top people

442
00:24:58,340 --> 00:25:01,340
that I've found helpful in my exploration.

443
00:25:01,340 --> 00:25:04,340
These are, like, free resources, but then they also have, like,

444
00:25:04,340 --> 00:25:06,340
you know, paid ones.

445
00:25:06,340 --> 00:25:10,340
So that's, I would start with Chris McCormick to meet,

446
00:25:10,340 --> 00:25:12,340
for BERT, to, like, do a really deep dive into BERT

447
00:25:12,340 --> 00:25:14,340
because in this session we just, like, don't have enough time

448
00:25:14,340 --> 00:25:17,340
to go really deep into it.

449
00:25:17,340 --> 00:25:23,340
This, I was just, it's a question on, like,

450
00:25:23,340 --> 00:25:26,340
on natural language processing and kind of doing a call-back

451
00:25:26,340 --> 00:25:29,340
to Juy's thing about, like, queen minus woman plus man.

452
00:25:29,340 --> 00:25:31,340
Does that mean king?

453
00:25:31,340 --> 00:25:33,340
But then when you, like, multiply man by seven,

454
00:25:33,340 --> 00:25:35,340
what do you get?

455
00:25:35,340 --> 00:25:40,340
You get snow white in the seven doors.

456
00:25:40,340 --> 00:25:44,340
So we can understand that, but there's, like, you know,

457
00:25:44,340 --> 00:25:48,340
a lot of complexity there in, like, you know, that being picked up.

458
00:25:48,340 --> 00:25:54,340
It's just a, another thing on the, like, NLP side

459
00:25:54,340 --> 00:25:59,340
that I think is really interesting is, like, mixed, code mixed text.

460
00:25:59,340 --> 00:26:04,340
There's a lot of, like, text out there that's, like,

461
00:26:04,340 --> 00:26:07,340
partially in one language mixed with another language,

462
00:26:07,340 --> 00:26:09,340
and that's, like, a field that's rich for, like,

463
00:26:09,340 --> 00:26:12,340
if you have a background, if that's something you, like,

464
00:26:12,340 --> 00:26:15,340
that's something that you do at home or with friends,

465
00:26:15,340 --> 00:26:19,340
I think the field of NLP, like, needs a lot of work in this area.

466
00:26:19,340 --> 00:26:21,340
There's just not a lot of data sets.

467
00:26:21,340 --> 00:26:23,340
A lot of our data is in English.

468
00:26:23,340 --> 00:26:26,340
So, like, this example was from a research paper

469
00:26:26,340 --> 00:26:30,340
where they had to, like, manually convert, like,

470
00:26:30,340 --> 00:26:34,340
English text to code mixed.

471
00:26:34,340 --> 00:26:38,340
This is Hindi and English.

472
00:26:38,340 --> 00:26:40,340
Okay, what about, like, managers?

473
00:26:40,340 --> 00:26:42,340
Maybe you guys read the abstract and you're like, oh, yeah,

474
00:26:42,340 --> 00:26:45,340
we've got sentiment analysis on our roadmap,

475
00:26:45,340 --> 00:26:47,340
so I just want to get a sense of what's going on.

476
00:26:47,340 --> 00:26:50,340
Is there anyone in here who's, like, more of a product manager

477
00:26:50,340 --> 00:26:53,340
or, like, okay, we've got my manager.

478
00:26:53,340 --> 00:26:58,340
Okay, that's cool.

479
00:26:58,340 --> 00:26:59,340
Yeah, I threw this one out.

480
00:26:59,340 --> 00:27:01,340
Like, FinBert is financial sentiment analysis

481
00:27:01,340 --> 00:27:05,340
with pre-trained language models as, like, something.

482
00:27:05,340 --> 00:27:07,340
Then there's just, like, a lot of articles.

483
00:27:07,340 --> 00:27:11,340
If you follow the slides, you can see if any of them are interesting.

484
00:27:11,340 --> 00:27:14,340
I'm just, like, a lot of companies are using it

485
00:27:14,340 --> 00:27:19,340
in different ways and variations of BERT.

486
00:27:19,340 --> 00:27:21,340
And then I guess, what about anyone here

487
00:27:21,340 --> 00:27:25,340
who's, like, trying to do something good for the world

488
00:27:25,340 --> 00:27:28,340
with everything you're learning here?

489
00:27:28,340 --> 00:27:31,340
Anyone here, like, everyone's, like, guilty,

490
00:27:31,340 --> 00:27:34,340
like, putting their hand up, like, not really.

491
00:27:34,340 --> 00:27:37,340
But so I have some, so I just kind of did a deep dive

492
00:27:37,340 --> 00:27:39,340
and I got really excited.

493
00:27:39,340 --> 00:27:42,340
And some of it was, like, dark but useful, like, meaningful.

494
00:27:42,340 --> 00:27:45,340
Like, we can use our expertise for good.

495
00:27:45,340 --> 00:27:47,340
So let's see what I threw up here.

496
00:27:47,340 --> 00:27:49,340
There's, like, you can use, like, there's articles

497
00:27:49,340 --> 00:27:53,340
about using BERT for, like, detecting early-onsight Alzheimer's disease.

498
00:27:53,340 --> 00:27:55,340
That falls under the NLP category

499
00:27:55,340 --> 00:27:58,340
because that's, like, it's less, that's not really sentiment analysis.

500
00:27:58,340 --> 00:28:03,340
That's, like, their degradation of their use of the English language

501
00:28:03,340 --> 00:28:07,340
that you can pick up on it earlier.

502
00:28:07,340 --> 00:28:12,340
Fake BERT, also not sentiment, but just, like, detecting fake.

503
00:28:12,340 --> 00:28:14,340
This one is sentiment.

504
00:28:14,340 --> 00:28:17,340
The last one is, like, with the stay-at-home mandates,

505
00:28:17,340 --> 00:28:19,340
like, looking at social medians

506
00:28:19,340 --> 00:28:22,340
and seeing, like, how that affected people's sentiments.

507
00:28:22,340 --> 00:28:24,340
And that can inform policy.

508
00:28:24,340 --> 00:28:26,340
There's just a bunch of examples.

509
00:28:26,340 --> 00:28:29,340
This one was, like, kind of really stood out to me

510
00:28:29,340 --> 00:28:32,340
as, like, using it as a way to intervene.

511
00:28:32,340 --> 00:28:37,340
We know that, like, when there's news reports about suicide,

512
00:28:37,340 --> 00:28:39,340
it actually increases suicide.

513
00:28:39,340 --> 00:28:44,340
So this article, they were classifying, like, different types of tweets,

514
00:28:45,340 --> 00:28:49,340
tweets that could, like, signal an intervention

515
00:28:49,340 --> 00:28:52,340
where it's, like, oh, this person is at risk

516
00:28:52,340 --> 00:28:56,340
versus, like, tweets that are just kind of, like, sharing.

517
00:28:56,340 --> 00:28:59,340
So they have different categories there.

518
00:28:59,340 --> 00:29:03,340
Some other ones, like, I'm just trying to pump you up,

519
00:29:03,340 --> 00:29:05,340
like, give you ideas of, like, when you leave,

520
00:29:05,340 --> 00:29:08,340
you're like, yeah, I've got cool stuff I want to work on now.

521
00:29:08,340 --> 00:29:12,340
Oh, I already had the Alzheimer's one.

522
00:29:13,340 --> 00:29:17,340
Oh, this is, like, they're using it for detecting human trafficking.

523
00:29:17,340 --> 00:29:21,340
Both, these are two different examples of this,

524
00:29:21,340 --> 00:29:23,340
using BERT to do that.

525
00:29:23,340 --> 00:29:26,340
So moving on.

526
00:29:26,340 --> 00:29:28,340
Who here is, like, a software engineer

527
00:29:28,340 --> 00:29:32,340
and you're not super, like, into going deep into it,

528
00:29:32,340 --> 00:29:35,340
but you want to, like, quickly make some apps that are useful?

529
00:29:35,340 --> 00:29:39,340
Okay, great. Okay, great.

530
00:29:39,340 --> 00:29:43,340
So, yeah, like, we're not going to go super deep.

531
00:29:43,340 --> 00:29:45,340
Hugging Face makes things super easy.

532
00:29:45,340 --> 00:29:48,340
We're really, like, standing, I'm going to say this again probably later,

533
00:29:48,340 --> 00:29:51,340
but we're, like, standing on the shoulders of giants.

534
00:29:51,340 --> 00:29:56,340
The Hugging Face, like, document, libraries and stuff

535
00:29:56,340 --> 00:29:59,340
and all these open source models makes that really easy.

536
00:29:59,340 --> 00:30:03,340
For you guys specifically, I have this, like, one talk that I recommended

537
00:30:03,340 --> 00:30:06,340
that it's, like, few-shot learning in production.

538
00:30:06,340 --> 00:30:10,340
It's a Hugging Face talk and it, like, gets you from, like,

539
00:30:10,340 --> 00:30:14,340
making a quick model all the way to the deployment.

540
00:30:14,340 --> 00:30:17,340
And also just the Hugging Face, this is for everyone, really,

541
00:30:17,340 --> 00:30:19,340
if you go through those, like, 79 videos,

542
00:30:19,340 --> 00:30:23,340
they're pretty short for the Hugging Face.

543
00:30:23,340 --> 00:30:25,340
So, okay, but where am I going with this?

544
00:30:25,340 --> 00:30:30,340
Ultimately, we all come to tutorials hoping to transfer

545
00:30:30,340 --> 00:30:32,340
what we've learned to use cases we care about,

546
00:30:32,340 --> 00:30:35,340
and that's what we call transfer learning,

547
00:30:35,340 --> 00:30:40,340
which is strangely what we're also here to do today is take a model

548
00:30:40,340 --> 00:30:45,340
and fine-tune it to transfer what it knows to a new task,

549
00:30:45,340 --> 00:30:47,340
to achieve a new task.

550
00:30:47,340 --> 00:30:50,340
So that's, like, this is the Wikipedia definition,

551
00:30:50,340 --> 00:30:53,340
transfer learning applied to machine learning.

552
00:30:53,340 --> 00:30:58,340
And then when it comes to humans, it's pretty much the same thing.

553
00:30:58,340 --> 00:31:01,340
It occurs when people apply information strategies and skills.

554
00:31:01,340 --> 00:31:03,340
They have learned to a new situation.

555
00:31:03,340 --> 00:31:06,340
So I'm just trying to kind of create some empathy for the model

556
00:31:06,340 --> 00:31:09,340
that we're about to mess with.

557
00:31:09,340 --> 00:31:15,340
So there's BERT on one side, and it's, like, representation

558
00:31:15,340 --> 00:31:20,340
and understanding of the complexity of language is encoded in embeddings.

559
00:31:20,340 --> 00:31:23,340
And what we say is we call it intuition.

560
00:31:23,340 --> 00:31:28,340
And the, like, embeddings, they come from a tokenizer.

561
00:31:28,340 --> 00:31:31,340
For us, we say that it's, like, from our gut.

562
00:31:31,340 --> 00:31:34,340
And then how did we learn?

563
00:31:34,340 --> 00:31:37,340
Like, that's where when we talk about this mass language modeling,

564
00:31:37,340 --> 00:31:40,340
like, oh, it learned from this mass language modeling task,

565
00:31:40,340 --> 00:31:42,340
and it learned from this next sentence prediction.

566
00:31:42,340 --> 00:31:44,340
That, for me, was a little bit confusing.

567
00:31:44,340 --> 00:31:48,340
Like, how did it learn this random thing that doesn't really seem to be useful,

568
00:31:48,340 --> 00:31:51,340
and now I'm applying it, like, now it has information

569
00:31:51,340 --> 00:31:53,340
and I'm applying it to something different?

570
00:31:53,340 --> 00:31:59,340
So the way I think about this is, like, maybe you want to have a kid

571
00:31:59,340 --> 00:32:01,340
and you want them to become a doctor,

572
00:32:01,340 --> 00:32:03,340
but you don't start them out reading medical books.

573
00:32:03,340 --> 00:32:05,340
You start them out with children's books.

574
00:32:05,340 --> 00:32:09,340
You get them a solid foundation of how the language works.

575
00:32:09,340 --> 00:32:14,340
And then afterwards, you can, you know, then they've got this base model,

576
00:32:14,340 --> 00:32:17,340
essentially, that you can then fine tune

577
00:32:17,340 --> 00:32:19,340
by teaching them something about medical stuff,

578
00:32:19,340 --> 00:32:21,340
and they can do that task.

579
00:32:21,340 --> 00:32:27,340
And then, like, along those lines, if we think about hiring people,

580
00:32:27,340 --> 00:32:31,340
like, we can think of, I mean, this is, like, a little bit absurd,

581
00:32:31,340 --> 00:32:34,340
working in AI, I basically, like, understand my brain

582
00:32:34,340 --> 00:32:36,340
and everyone else as models,

583
00:32:36,340 --> 00:32:39,340
like, which is maybe not healthy yet, it's questionable.

584
00:32:39,340 --> 00:32:46,340
But, yeah, like, when we're hiring someone, we evaluate, like,

585
00:32:46,340 --> 00:32:49,340
think of them as a base model that we're going to fine tune.

586
00:32:49,340 --> 00:32:52,340
And we're thinking, we're weighing the pros and cons.

587
00:32:52,340 --> 00:32:56,340
We're like, okay, well, this person has, like, a lot of background in NLP,

588
00:32:56,340 --> 00:32:58,340
and the tasks that we want them to do are NLP.

589
00:32:58,340 --> 00:33:01,340
So maybe the ramp up will be, like, faster.

590
00:33:01,340 --> 00:33:03,340
There will be less fine tuning.

591
00:33:03,340 --> 00:33:04,340
We can give it less data.

592
00:33:04,340 --> 00:33:06,340
It'll be faster to get them ramped up.

593
00:33:06,340 --> 00:33:08,340
Or we go, oh, this person's a generalist.

594
00:33:08,340 --> 00:33:11,340
They are going to be, you know, we could fine tune them

595
00:33:11,340 --> 00:33:13,340
for a few different types of tasks.

596
00:33:13,340 --> 00:33:18,340
So it's the same thing, like, when we have to decide what model to start

597
00:33:18,340 --> 00:33:20,340
as our base model, there's these pros and cons.

598
00:33:20,340 --> 00:33:22,340
And it's kind of overwhelming.

599
00:33:22,340 --> 00:33:29,340
If you go to the Hugging Face website, there's all these different models,

600
00:33:29,340 --> 00:33:31,340
and it can feel, like, really, really overwhelming.

601
00:33:31,340 --> 00:33:36,340
But that's just sort of, oh, and then, like, the final point here is, like,

602
00:33:36,340 --> 00:33:42,340
how, like, you guys today are going to try to fine tune a BERT model

603
00:33:42,340 --> 00:33:46,340
to learn the new task of sentiment, like, analysis,

604
00:33:46,340 --> 00:33:49,340
like, classifying sentiment for specific tasks.

605
00:33:49,340 --> 00:33:52,340
And then we, as your instructors, are trying to fine tune you

606
00:33:52,340 --> 00:33:54,340
to learn how to fine tune models.

607
00:33:54,340 --> 00:33:57,340
So it's kind of a fun thing there.

608
00:33:57,340 --> 00:33:58,340
So here we go.

609
00:33:58,340 --> 00:34:00,340
So you guys are base models.

610
00:34:00,340 --> 00:34:07,340
If I gave you a quiz at the end of this, would you all get the same result?

611
00:34:07,340 --> 00:34:09,340
No, you wouldn't, because they're different base models.

612
00:34:09,340 --> 00:34:11,340
Some of you guys are better than others.

613
00:34:11,340 --> 00:34:18,340
Not to say that, but some of you guys, you came with vastly different knowledge.

614
00:34:18,340 --> 00:34:23,340
So some people who have this, like, NLP background,

615
00:34:23,340 --> 00:34:25,340
they're going to, like, maybe pick up on certain things

616
00:34:25,340 --> 00:34:30,340
versus someone who has, like, more of a social background

617
00:34:30,340 --> 00:34:34,340
will, like, quickly come up with ideas of how they're going to apply this

618
00:34:34,340 --> 00:34:36,340
to, like, meaningful things.

619
00:34:36,340 --> 00:34:40,340
And then, yeah, this is the Hugging Face.

620
00:34:40,340 --> 00:34:41,340
Let's see.

621
00:34:41,340 --> 00:34:44,340
This might be my break where I go to, yeah, let's go over,

622
00:34:44,340 --> 00:34:48,340
and I'm going to kind of, like, start to show you the Hugging Face.

623
00:34:48,340 --> 00:34:51,340
If I can figure out how to exit out of this and make sure I'm on time.

624
00:34:51,340 --> 00:34:53,340
I've got till...

625
00:34:53,340 --> 00:34:55,340
She's got a lot of good material, so I don't want to...

626
00:34:55,340 --> 00:35:00,340
I can't go too slow, because I've...

627
00:35:00,340 --> 00:35:02,340
Okay, what about, like, how many of you guys have played,

628
00:35:02,340 --> 00:35:07,340
have looked at the Hugging Face?

629
00:35:07,340 --> 00:35:08,340
Over half.

630
00:35:08,340 --> 00:35:11,340
What about, like, do you feel, like, super comfortable with it?

631
00:35:11,340 --> 00:35:14,340
How many feel super comfortable with it?

632
00:35:14,340 --> 00:35:15,340
Less.

633
00:35:15,340 --> 00:35:16,340
Okay, but some of you guys have, like...

634
00:35:16,340 --> 00:35:17,340
Anybody that's, like...

635
00:35:17,340 --> 00:35:18,340
Oh, this one's a hard question.

636
00:35:18,340 --> 00:35:19,340
Everybody close your eyes.

637
00:35:19,340 --> 00:35:23,340
Has anybody, like, never touched it?

638
00:35:23,340 --> 00:35:24,340
Okay, great.

639
00:35:24,340 --> 00:35:25,340
Okay.

640
00:35:25,340 --> 00:35:27,340
I always hate to, like...

641
00:35:27,340 --> 00:35:29,340
I'm the person where I, like, look around,

642
00:35:29,340 --> 00:35:31,340
and I don't want to raise my hand.

643
00:35:31,340 --> 00:35:35,340
I'm like, oh, no, I've seen it.

644
00:35:35,340 --> 00:35:36,340
Okay, cool.

645
00:35:36,340 --> 00:35:40,340
So, yeah, Julie showed you the documentation is, like, really...

646
00:35:40,340 --> 00:35:41,340
Is really great.

647
00:35:41,340 --> 00:35:44,340
Sometimes, meh, sometimes documentation frustrates me,

648
00:35:44,340 --> 00:35:47,340
but parts I'm going to show you is, like,

649
00:35:47,340 --> 00:35:51,340
there's the models and the data sets.

650
00:35:51,340 --> 00:35:55,340
So we can start with the models.

651
00:35:55,340 --> 00:35:57,340
Really overwhelming.

652
00:35:57,340 --> 00:36:01,340
Let's just make this, like, full screen.

653
00:36:01,340 --> 00:36:05,340
There's, like, there's lots of models from different people.

654
00:36:05,340 --> 00:36:07,340
Think of it as GitHub.

655
00:36:07,340 --> 00:36:10,340
When you go on GitHub, there's some repos that are, like,

656
00:36:10,340 --> 00:36:13,340
really well documented and open source.

657
00:36:13,340 --> 00:36:14,340
There's lots of contributors.

658
00:36:14,340 --> 00:36:17,340
You kind of know, like, oh, they have a lot of stars.

659
00:36:17,340 --> 00:36:19,340
This seems more, like, credible.

660
00:36:19,340 --> 00:36:21,340
I am comfortable using this.

661
00:36:21,340 --> 00:36:23,340
It's going to be well maintained versus people like me

662
00:36:23,340 --> 00:36:29,340
who have some random thing that I won't touch for three years.

663
00:36:29,340 --> 00:36:33,340
And so you don't have to, like, kind of look at all of them,

664
00:36:33,340 --> 00:36:38,340
but over here you can kind of make selections to filter them by.

665
00:36:38,340 --> 00:36:41,340
So in our case, like, sentiment analysis,

666
00:36:41,340 --> 00:36:44,340
we're really just doing text classification.

667
00:36:44,340 --> 00:36:47,340
We're classifying, is this thing positive or negative?

668
00:36:47,340 --> 00:36:49,340
Is there joy in this?

669
00:36:49,340 --> 00:36:53,340
So you could filter here to text classification.

670
00:36:53,340 --> 00:37:00,340
And then, okay, I've still got 20,000 models to choose from.

671
00:37:00,340 --> 00:37:04,340
So then over here you could do, like, sort by most downloaded

672
00:37:04,340 --> 00:37:06,340
or most likes.

673
00:37:06,340 --> 00:37:08,340
Or if you want to be on the cutting edge,

674
00:37:08,340 --> 00:37:11,340
you could do, like, recently uploaded.

675
00:37:11,340 --> 00:37:14,340
And just like GitHub, you can have an account here

676
00:37:14,340 --> 00:37:19,340
and you can upload your own models for other people to play with.

677
00:37:19,340 --> 00:37:23,340
So you can pick a base model, fine tune it, upload it here,

678
00:37:23,340 --> 00:37:25,340
and then you'll get your own.

679
00:37:25,340 --> 00:37:28,340
I don't know if they're stars or they're hearts.

680
00:37:28,340 --> 00:37:30,340
Let's click on one to see.

681
00:37:30,340 --> 00:37:34,340
They're likes with hearts.

682
00:37:34,340 --> 00:37:37,340
So you can kind of, like, become part of this community.

683
00:37:37,340 --> 00:37:40,340
So, okay, we already filtered to, like, text classification.

684
00:37:40,340 --> 00:37:42,340
You can also, like, filter by name.

685
00:37:42,340 --> 00:37:47,340
So if we wanted to do, like, sentiment,

686
00:37:47,340 --> 00:37:50,340
that gives us, like, some ideas of, like,

687
00:37:50,340 --> 00:37:54,340
some different things that we want to do here.

688
00:37:54,340 --> 00:37:56,340
And really there's a lot.

689
00:37:56,340 --> 00:38:04,340
So, like, so I would encourage you guys to kind of explore a bit

690
00:38:04,340 --> 00:38:11,340
video classification, audio.

691
00:38:11,340 --> 00:38:13,340
Then let's just take a look.

692
00:38:13,340 --> 00:38:20,340
Since we're talking about BERT, let's take a look at,

693
00:38:20,340 --> 00:38:23,340
well, I don't know what happened here.

694
00:38:23,340 --> 00:38:26,340
It's not recently uploaded.

695
00:38:36,340 --> 00:38:38,340
Do I have something weird?

696
00:38:38,340 --> 00:38:40,340
What else do I have for my?

697
00:38:40,340 --> 00:38:41,340
Here we go.

698
00:38:41,340 --> 00:38:43,340
This is one we're going to look at.

699
00:38:43,340 --> 00:38:46,340
When BERT was published, they actually came out with, like,

700
00:38:46,340 --> 00:38:48,340
large models and then smaller models,

701
00:38:48,340 --> 00:38:52,340
and then since then they've come out with even smaller models.

702
00:38:52,340 --> 00:38:54,340
And there's also Distilbert,

703
00:38:54,340 --> 00:38:57,340
which we're going to play with today.

704
00:38:57,340 --> 00:38:59,340
But this is a model card,

705
00:38:59,340 --> 00:39:01,340
and it, like, has so much information about it,

706
00:39:01,340 --> 00:39:05,340
but then this is another way to, like, have, like, a little smoke test.

707
00:39:05,340 --> 00:39:06,340
Let's go.

708
00:39:06,340 --> 00:39:08,340
Can you make it bigger?

709
00:39:08,340 --> 00:39:11,340
Oh, yeah, yeah, yeah.

710
00:39:11,340 --> 00:39:13,340
I think.

711
00:39:13,340 --> 00:39:15,340
This is really small text.

712
00:39:15,340 --> 00:39:16,340
That's weird. It's not.

713
00:39:16,340 --> 00:39:18,340
Oh, there we go.

714
00:39:18,340 --> 00:39:21,340
Yeah, thanks for the feedback there.

715
00:39:21,340 --> 00:39:22,340
Is that better?

716
00:39:22,340 --> 00:39:24,340
Should I make it?

717
00:39:24,340 --> 00:39:26,340
How's that?

718
00:39:26,340 --> 00:39:29,340
Okay, cool.

719
00:39:29,340 --> 00:39:32,340
So, yeah, it gives a lot of information.

720
00:39:32,340 --> 00:39:34,340
Like, it includes the paper.

721
00:39:34,340 --> 00:39:36,340
So here it says, like, the data sets that were used,

722
00:39:36,340 --> 00:39:40,340
Wikipedia and this data set's book corpus.

723
00:39:40,340 --> 00:39:42,340
It says how it was trained.

724
00:39:42,340 --> 00:39:46,340
It gives intended uses.

725
00:39:46,340 --> 00:39:52,340
So, like, one quick one over here is, like,

726
00:39:52,340 --> 00:39:56,340
well, here, this even, like, lets you play with the masked,

727
00:39:56,340 --> 00:39:58,340
the mask thing.

728
00:39:58,340 --> 00:40:00,340
Use in transformers.

729
00:40:00,340 --> 00:40:03,340
There, you got the code, and you can download it,

730
00:40:03,340 --> 00:40:05,340
and you can start playing.

731
00:40:05,340 --> 00:40:07,340
Then you've got the tokenizer and the model.

732
00:40:07,340 --> 00:40:09,340
And then if you follow along with the documentation

733
00:40:09,340 --> 00:40:13,340
that she showed you.

734
00:40:13,340 --> 00:40:16,340
So that's kind of cool.

735
00:40:16,340 --> 00:40:18,340
So that's going to lead me to the next thing.

736
00:40:18,340 --> 00:40:20,340
I'm going to, let's just play with this a little bit.

737
00:40:20,340 --> 00:40:23,340
Let's look at this.

738
00:40:23,340 --> 00:40:27,340
I think, I don't remember where, but it shows, like,

739
00:40:27,340 --> 00:40:29,340
so limitations and bias.

740
00:40:29,340 --> 00:40:31,340
Sometimes they're not filled out.

741
00:40:31,340 --> 00:40:33,340
And so that's, like, where I would encourage,

742
00:40:33,340 --> 00:40:35,340
if you're going to start making your own models on here,

743
00:40:35,340 --> 00:40:37,340
like, let's do that.

744
00:40:37,340 --> 00:40:40,340
Let's, like, have some good quality documentation.

745
00:40:40,340 --> 00:40:46,340
And then where was the last bit?

746
00:40:46,340 --> 00:40:48,340
That might be on the other.

747
00:40:48,340 --> 00:40:52,340
Okay, so now let's move over to, let's hopefully all get

748
00:40:52,340 --> 00:40:59,340
our Colab things working.

749
00:40:59,340 --> 00:41:03,340
Is everyone able to get to this repo here?

750
00:41:03,340 --> 00:41:09,340
And then under notebooks, I'm struggling to click on it.

751
00:41:10,340 --> 00:41:15,340
Bert, we're going to go with number one, Bert base case,

752
00:41:15,340 --> 00:41:17,340
uncased, is what I meant.

753
00:41:17,340 --> 00:41:20,340
So that means that it's all lowercase.

754
00:41:20,340 --> 00:41:22,340
It doesn't care.

755
00:41:22,340 --> 00:41:24,340
Like, when you put in text, it's just going to lowercase

756
00:41:24,340 --> 00:41:26,340
everything.

757
00:41:26,340 --> 00:41:28,340
And if it's cased, then it's case sensitive.

758
00:41:28,340 --> 00:41:33,340
So over here in the corner that has this, like, opening Colab,

759
00:41:33,340 --> 00:41:38,340
do you guys all have that?

760
00:41:38,340 --> 00:41:40,340
Should I make that bigger?

761
00:41:40,340 --> 00:41:58,340
Yeah, but I think it, like, it still opens if in Google Colab.

762
00:41:58,340 --> 00:42:06,340
Okay, another way you can get to it is if you go to Google Colab.

763
00:42:06,340 --> 00:42:11,340
So I'll show you this.

764
00:42:11,340 --> 00:42:14,340
Oh, is that why?

765
00:42:14,340 --> 00:42:18,340
But if they go to Colab, that's not a browser extension, right?

766
00:42:18,340 --> 00:42:24,340
Yeah, so if you go directly to colab.research.google.com,

767
00:42:24,340 --> 00:42:28,340
and then it has, like, this auto population, you can say,

768
00:42:28,340 --> 00:42:31,340
I want to get it from GitHub.

769
00:42:31,340 --> 00:42:34,340
And then my username is Dana Sore.

770
00:42:34,340 --> 00:42:37,340
So it's a little bit bigger.

771
00:42:37,340 --> 00:42:41,340
And then you go down to Dana Sore Burt.

772
00:42:41,340 --> 00:42:43,340
Are you guys following along?

773
00:42:43,340 --> 00:42:45,340
Or does anyone?

774
00:42:45,340 --> 00:42:48,340
Okay.

775
00:42:48,340 --> 00:42:51,340
Yeah, let's see if I can zoom in on that.

776
00:42:51,340 --> 00:42:53,340
It doesn't zoom in.

777
00:42:53,340 --> 00:42:59,340
Colab.research.google.com.

778
00:42:59,340 --> 00:43:10,340
In this first notebook, Burt base uncased.

779
00:43:10,340 --> 00:43:12,340
Did that work for people?

780
00:43:12,340 --> 00:43:13,340
You got it?

781
00:43:13,340 --> 00:43:14,340
Everyone's good?

782
00:43:14,340 --> 00:43:15,340
Everyone's in there?

783
00:43:15,340 --> 00:43:17,340
No, someone's not in there.

784
00:43:17,340 --> 00:43:25,340
Oh, I see.

785
00:43:25,340 --> 00:43:29,340
So you have to log into GitHub first.

786
00:43:29,340 --> 00:43:32,340
I think for this one, we'll take a break after this,

787
00:43:32,340 --> 00:43:37,340
but for this one, you can kind of just follow along.

788
00:43:37,340 --> 00:43:40,340
So, okay, so this is the, whatever.

789
00:43:40,340 --> 00:43:42,340
So I said, like, see the model card for more information.

790
00:43:42,340 --> 00:43:44,340
Well, we just looked at the model card.

791
00:43:44,340 --> 00:43:50,340
There's also the Google release of this on GitHub here.

792
00:43:50,340 --> 00:43:57,340
So you can also like deep dive into the release

793
00:43:57,340 --> 00:44:04,340
and see that there's like different, like, this is the one, Burt base,

794
00:44:04,340 --> 00:44:10,340
but there's like lots of like smaller ones now.

795
00:44:10,340 --> 00:44:14,340
And okay, so how comfortable are you guys?

796
00:44:14,340 --> 00:44:18,340
Who has never used Colab before?

797
00:44:18,340 --> 00:44:20,340
There's a few, okay.

798
00:44:20,340 --> 00:44:23,340
Okay, I messed up and in my second notebook is where I give instructions

799
00:44:23,340 --> 00:44:25,340
on how to use Colab, so.

800
00:44:25,340 --> 00:44:32,340
But it's going to be like, whoops, command, if you're on a Mac,

801
00:44:32,340 --> 00:44:38,340
command return to like execute a cell.

802
00:44:38,340 --> 00:44:40,340
And then you're going to get this warning,

803
00:44:40,340 --> 00:44:42,340
this notebook was not run anyways.

804
00:44:42,340 --> 00:44:51,340
I'm just, no worries, just putting viruses on all of your computers right now.

805
00:44:51,340 --> 00:44:54,340
And it's kind of slow, so I mean, you can just follow along

806
00:44:54,340 --> 00:44:56,340
because there's so much material I have.

807
00:44:56,340 --> 00:44:58,340
So you don't have to like run these parts.

808
00:44:58,340 --> 00:45:01,340
I can just, but you can, you know, you can try to run it in the background

809
00:45:01,340 --> 00:45:05,340
and I'm just going to kind of walk through what I've got here.

810
00:45:05,340 --> 00:45:07,340
So this is Transformers library.

811
00:45:07,340 --> 00:45:09,340
It's a hugging face library.

812
00:45:09,340 --> 00:45:15,340
From there we're importing, yeah, I'll try to give that a chance.

813
00:45:15,340 --> 00:45:21,340
Get rid of this guy over here.

814
00:45:21,340 --> 00:45:26,340
So I'm importing auto tokenizer, auto model for Mast LM,

815
00:45:26,340 --> 00:45:29,340
that's the like Mast language modeling, that's the task,

816
00:45:29,340 --> 00:45:34,340
one of the two tasks that Burt learns from in a pipeline here.

817
00:45:34,340 --> 00:45:38,340
This creates the tokenizer and the model.

818
00:45:38,340 --> 00:45:44,340
The model and the tokenizer, I think I put that in this one.

819
00:45:44,340 --> 00:45:46,340
That's in another one.

820
00:45:46,340 --> 00:45:50,340
They always have to get paired together.

821
00:45:50,340 --> 00:45:53,340
So with the input, like the input that you put in,

822
00:45:53,340 --> 00:45:57,340
it has to go through the, it gets translated via the same tokenizer

823
00:45:57,340 --> 00:46:01,340
that the model was trained on.

824
00:46:01,340 --> 00:46:06,340
And so here was this like exercise, just playing with the tokenizer

825
00:46:06,340 --> 00:46:11,340
if this thing ran.

826
00:46:11,340 --> 00:46:13,340
That's pretty slow.

827
00:46:13,340 --> 00:46:15,340
I just was like, hey, you can put in different things.

828
00:46:15,340 --> 00:46:17,340
My suggestion was like try to put in different things

829
00:46:17,340 --> 00:46:19,340
and see how it gets tokenized.

830
00:46:19,340 --> 00:46:22,340
More common words will be tokenized like as its own word,

831
00:46:22,340 --> 00:46:25,340
but as you can see, like it doesn't understand my last name,

832
00:46:25,340 --> 00:46:30,340
Engerbretsen, so it tokenizes it into like, it separates it.

833
00:46:30,340 --> 00:46:33,340
Yeah?

834
00:46:33,340 --> 00:46:38,340
Yeah, the hashtags mean that it's like not the beginning of a word.

835
00:46:38,340 --> 00:46:42,340
So if you can see Eng is Engerbretsen,

836
00:46:42,340 --> 00:46:49,340
and the hashtags mean, oh, this is a continuation of the token before it.

837
00:46:49,340 --> 00:46:51,340
And then...

838
00:46:51,340 --> 00:46:56,340
Why is it tokenized like that?

839
00:46:56,340 --> 00:46:58,340
How does it manage to be done?

840
00:46:58,340 --> 00:47:03,340
So every model will have a limited vocabulary.

841
00:47:03,340 --> 00:47:10,340
And so in this case, the BERT's vocabulary didn't have my last name in it.

842
00:47:10,340 --> 00:47:16,340
And so if there's like sort of a dictionary where each item in the vocabulary

843
00:47:16,340 --> 00:47:20,340
has like a number, sort of like an index.

844
00:47:20,340 --> 00:47:26,340
And so when the word's not in there, it uses, it kind of does this like,

845
00:47:26,340 --> 00:47:29,340
thinking, if you think of like a compound word,

846
00:47:29,340 --> 00:47:32,340
it tries to look for like in this word Engerbretsen,

847
00:47:32,340 --> 00:47:39,340
do I have something in the vocabulary that is part of that word?

848
00:47:39,340 --> 00:47:43,340
And then it takes that token, and then it just kind of keeps trying to add on to it.

849
00:47:43,340 --> 00:47:45,340
Yes, go ahead.

850
00:47:45,340 --> 00:47:55,340
For the type...

851
00:47:55,340 --> 00:48:02,340
Yeah, Wordpiece tokenizer. Yeah.

852
00:48:02,340 --> 00:48:07,340
So just for like audio, so he was asking,

853
00:48:07,340 --> 00:48:13,340
is there a name for this specific tokenizer, and it's based off Wordpiece tokenizer.

854
00:48:13,340 --> 00:48:18,340
But I'm not sure what other models use that same, like what umbrella uses that.

855
00:48:18,340 --> 00:48:23,340
That's not something I've like dove into really.

856
00:48:23,340 --> 00:48:27,340
Was there another question?

857
00:48:27,340 --> 00:48:29,340
Okay.

858
00:48:29,340 --> 00:48:32,340
So let's see if the line before ran.

859
00:48:32,340 --> 00:48:35,340
So yeah, I would just say play with, these are some things like compound words,

860
00:48:35,340 --> 00:48:39,340
any obscure words that you think might not be in there,

861
00:48:39,340 --> 00:48:42,340
anything from your culture that might not be in there.

862
00:48:42,340 --> 00:48:45,340
Did anyone find anything funny that was like,

863
00:48:45,340 --> 00:48:47,340
oh, you thought it should be in there, but it isn't,

864
00:48:47,340 --> 00:48:52,340
or you thought it wouldn't be in there and it is in there?

865
00:48:52,340 --> 00:48:55,340
What did you get?

866
00:48:55,340 --> 00:48:57,340
Which one was it?

867
00:48:57,340 --> 00:49:01,340
I don't even know that word, defenic.

868
00:49:01,340 --> 00:49:04,340
That one's in there?

869
00:49:04,340 --> 00:49:06,340
Oh, okay.

870
00:49:06,340 --> 00:49:11,340
How do I spell it? Let's put it up there.

871
00:49:11,340 --> 00:49:13,340
Oh, you're too fast for me.

872
00:49:13,340 --> 00:49:16,340
D-E-F-E.

873
00:49:16,340 --> 00:49:23,340
Okay.

874
00:49:23,340 --> 00:49:29,340
Were you like a spelling bee whiz back in the day?

875
00:49:29,340 --> 00:49:31,340
Def-ens-tra.

876
00:49:31,340 --> 00:49:34,340
So the T-I-O-N, that one's common.

877
00:49:34,340 --> 00:49:39,340
That one you can see is like a very common ending of a word.

878
00:49:39,340 --> 00:49:48,340
Okay.

879
00:49:48,340 --> 00:49:52,340
So that means your name is in the vocabulary as is.

880
00:49:52,340 --> 00:49:56,340
So Burt has a vocabulary, I don't know, I don't remember off the top of my head,

881
00:49:56,340 --> 00:49:58,340
I showed it as like 30,000 or something.

882
00:49:58,340 --> 00:49:59,340
It's high.

883
00:49:59,340 --> 00:50:05,340
But it's strangely, so the Chris McCormick I had put in my other slides,

884
00:50:05,340 --> 00:50:09,340
he has this great, what was it, this one.

885
00:50:09,340 --> 00:50:13,340
He's got a really cool notebook where he goes through the vocabulary

886
00:50:13,340 --> 00:50:17,340
and he actually picks out a bunch of common names and he runs it through

887
00:50:17,340 --> 00:50:21,340
to see which names are in there and you can just print out the vocabulary

888
00:50:21,340 --> 00:50:23,340
and it's kind of funny.

889
00:50:23,340 --> 00:50:28,340
That's definitely an exercise to do of just looking through what's in there

890
00:50:28,340 --> 00:50:33,340
and what's not because that obviously, like all the models we build off of this

891
00:50:33,340 --> 00:50:37,340
is going to have that vocabulary, so it's going to have those limitations.

892
00:50:37,340 --> 00:50:42,340
Like if less common names are in there, you know, it's not going to necessarily

893
00:50:42,340 --> 00:50:43,340
know it.

894
00:50:43,340 --> 00:50:45,340
Okay, where am I going?

895
00:50:45,340 --> 00:50:48,340
I'm too slow, I'm too slow, I've got to move on.

896
00:50:48,340 --> 00:50:50,340
All right, mass language modeling.

897
00:50:50,340 --> 00:50:53,340
So this I wanted to give you the opportunity and Juhee is also going to do this,

898
00:50:53,340 --> 00:50:55,340
but kind of from a different perspective.

899
00:50:55,340 --> 00:51:00,340
So here I've got, we're just going to play this.

900
00:51:00,340 --> 00:51:06,340
I threw in these two as examples, but I did the capture because I didn't want you

901
00:51:06,340 --> 00:51:08,340
to cheat and look ahead.

902
00:51:08,340 --> 00:51:12,340
So what I just want us to pretend we're the model.

903
00:51:12,340 --> 00:51:14,340
She craved blank to quench her thirst.

904
00:51:14,340 --> 00:51:17,340
What do you guys think the most probable answer would be?

905
00:51:17,340 --> 00:51:22,340
If you read all of Wikipedia and a bunch of books, what would you guess?

906
00:51:22,340 --> 00:51:27,340
Yes, she craved blank to quench her thirst.

907
00:51:27,340 --> 00:51:28,340
Gatorade and water.

908
00:51:28,340 --> 00:51:31,340
Those were my two, Gatorade and water.

909
00:51:31,340 --> 00:51:36,340
Yeah, I was like Gatorade was slightly below water for me.

910
00:51:36,340 --> 00:51:38,340
Okay, so are you ready for this?

911
00:51:38,340 --> 00:51:41,340
We're going to see, I'm going to remove, I'm going to comment out.

912
00:51:41,340 --> 00:51:51,340
This is the thing, so shift and then the hash-y thing.

913
00:51:51,340 --> 00:51:52,340
What do you call this guy?

914
00:51:52,340 --> 00:51:53,340
I should.

915
00:51:53,340 --> 00:52:00,340
Hashtag, you guys are messing with me.

916
00:52:00,340 --> 00:52:05,340
I clearly am not like a Twitter, like I'm not like an influencer on Twitter

917
00:52:05,340 --> 00:52:06,340
or any of these.

918
00:52:06,340 --> 00:52:11,340
Okay, we're going to see what Burt thinks.

919
00:52:11,340 --> 00:52:12,340
She craved something.

920
00:52:12,340 --> 00:52:16,340
She craved blood to quench her thirst.

921
00:52:17,340 --> 00:52:21,340
She craved him to quench her thirst.

922
00:52:21,340 --> 00:52:24,340
She craved someone or nothing.

923
00:52:24,340 --> 00:52:29,340
So like whoa, okay, she was like that was like that really threw me.

924
00:52:29,340 --> 00:52:36,340
And then this one I wasn't, I promise you I wasn't trying to go down a path.

925
00:52:36,340 --> 00:52:40,340
I was like petting my cat and I was like, and then I had these cat libs,

926
00:52:40,340 --> 00:52:42,340
like a little book on like cat libs.

927
00:52:42,340 --> 00:52:45,340
So I pulled out the cat libs and I was like the best thing

928
00:52:45,340 --> 00:52:48,340
about blank is they mostly clean up after themselves.

929
00:52:48,340 --> 00:52:52,340
So what other things might clean up after themselves?

930
00:52:56,340 --> 00:53:01,340
Any ideas that it can be, you know, recorded?

931
00:53:01,340 --> 00:53:05,340
I guess.

932
00:53:05,340 --> 00:53:07,340
Okay.

933
00:53:08,340 --> 00:53:13,340
Vampires made it on there.

934
00:53:13,340 --> 00:53:15,340
What?

935
00:53:15,340 --> 00:53:19,340
The book is unpublished, right?

936
00:53:19,340 --> 00:53:22,340
Published authors actually.

937
00:53:22,340 --> 00:53:23,340
Are they unpublished?

938
00:53:23,340 --> 00:53:28,340
No, there's a lot of those books are now being being sold.

939
00:53:28,340 --> 00:53:31,340
Doesn't it mean romance?

940
00:53:31,340 --> 00:53:33,340
That's exactly the point we're going.

941
00:53:33,340 --> 00:53:35,340
Actually, so yeah, that was exactly what I was going to point out.

942
00:53:35,340 --> 00:53:37,340
So I did a little deep dive.

943
00:53:37,340 --> 00:53:43,340
There turns out that somebody did wrote this article addressing document.

944
00:53:43,340 --> 00:53:44,340
I'm going to miss the page.

945
00:53:44,340 --> 00:53:45,340
What page is this?

946
00:53:45,340 --> 00:53:47,340
Nine is what I was going to bring you to.

947
00:53:47,340 --> 00:53:50,340
Really interesting article addressing documentation debt and machine

948
00:53:50,340 --> 00:53:53,340
learning research, a retrospective data sheet for Book Corpus.

949
00:53:53,340 --> 00:54:00,340
Book Corpus is one of the two data data sets that BERT was trained on.

950
00:54:00,340 --> 00:54:03,340
And it's like kind of it was like really an interesting deep dive.

951
00:54:03,340 --> 00:54:09,340
The actual like original copy is like no longer available.

952
00:54:09,340 --> 00:54:11,340
So people have had to recreate it.

953
00:54:11,340 --> 00:54:15,340
But the author of this, the author of they like through a security thing

954
00:54:15,340 --> 00:54:20,340
managed to get the original Book Corpus and they did this analysis on the

955
00:54:20,340 --> 00:54:21,340
original data set.

956
00:54:21,340 --> 00:54:23,340
And it's wild.

957
00:54:23,340 --> 00:54:25,340
Like this information is super interesting.

958
00:54:25,340 --> 00:54:28,340
They've made this like cool data set fact.

959
00:54:28,340 --> 00:54:35,340
Like it looks like it's on a I don't know what you call it nutrition label.

960
00:54:35,340 --> 00:54:40,340
But yeah, that was exactly the point was that the percentages that as it

961
00:54:40,340 --> 00:54:46,340
breaks down 26% was on romance and then like vampires was pretty high up

962
00:54:46,340 --> 00:54:47,340
there.

963
00:54:47,340 --> 00:54:50,340
Five point four percent of the books were about vampires and like

964
00:54:50,340 --> 00:54:53,340
historical books was like one point six.

965
00:54:53,340 --> 00:54:59,340
So if you think about like Google search, like a lot of companies are using

966
00:54:59,340 --> 00:55:02,340
this as the base and we're fine tuning off of it.

967
00:55:02,340 --> 00:55:06,340
And I just feel like there's like an ethical responsibility on my part if I'm

968
00:55:06,340 --> 00:55:10,340
going to teach you guys how to fine tune this model just to be very aware of

969
00:55:10,340 --> 00:55:15,340
like any biases are going to get propagated into your what you're bringing

970
00:55:15,340 --> 00:55:17,340
to your customers.

971
00:55:17,340 --> 00:55:22,340
Okay.

972
00:55:22,340 --> 00:55:28,340
But yeah, so there's optional exercises for you to play with that a little bit

973
00:55:28,340 --> 00:55:29,340
more.

974
00:55:29,340 --> 00:55:35,340
I gave some some prompts that I said try some sentences.

975
00:55:35,340 --> 00:55:40,340
Try some sentences with references to your cultural historical events

976
00:55:40,340 --> 00:55:46,340
scientific discoveries maybe a joke song lyric and just see like if you do this

977
00:55:46,340 --> 00:55:51,340
during the break or while you're bored of me talking then you can kind of tell

978
00:55:51,340 --> 00:55:56,340
us something interesting you found.

979
00:55:56,340 --> 00:56:03,340
And then now let's move on to the next notebook.

980
00:56:03,340 --> 00:56:08,340
Which was maybe we could skip that that one.

981
00:56:08,340 --> 00:56:10,340
Yeah, we'll skip data Burt was trained on.

982
00:56:10,340 --> 00:56:12,340
That's an interesting one.

983
00:56:12,340 --> 00:56:16,340
That one I just kind of went and did a deep dive into like things I could get

984
00:56:16,340 --> 00:56:17,340
out of that book.

985
00:56:17,340 --> 00:56:25,340
So we'll move on now to to Rotten Tomatoes will be our first fine tuning

986
00:56:25,340 --> 00:56:32,340
exercise.

987
00:56:32,340 --> 00:56:41,340
Rotten Tomatoes ever does everyone like maybe close your eyes.

988
00:56:41,340 --> 00:56:43,340
Does anyone not know it.

989
00:56:43,340 --> 00:56:44,340
Okay, everyone knows it.

990
00:56:44,340 --> 00:56:48,340
So I shouldn't know though like I've never used it and it seems like that you

991
00:56:48,340 --> 00:56:53,340
have to make a one to five star review and this data set just has binary yes or

992
00:56:53,340 --> 00:56:58,340
no like it's rotten or it's fresh.

993
00:56:58,340 --> 00:57:02,340
And I so there this was where I had that optional precursor for anyone who's

994
00:57:02,340 --> 00:57:06,340
still not comfortable when I say that like a lot of these I say like uncomment

995
00:57:06,340 --> 00:57:12,340
it and comment it out and you can do this like control see if I have an example

996
00:57:12,340 --> 00:57:18,340
like so I could copy this whole thing and I can do like command forward slash to

997
00:57:18,340 --> 00:57:26,340
comment it and then command forward slash to comment it back out to toggle that.

998
00:57:26,340 --> 00:57:30,340
Sorry.

999
00:57:30,340 --> 00:57:31,340
Okay, okay, okay.

1000
00:57:31,340 --> 00:57:32,340
Here we go.

1001
00:57:32,340 --> 00:57:35,340
That's the optional precursor.

1002
00:57:35,340 --> 00:57:40,340
We can skip that optional design decision just briefly.

1003
00:57:40,340 --> 00:57:46,340
I'm saying I don't know why this is hidden like we picked this data set because

1004
00:57:46,340 --> 00:57:47,340
it's binary classifier.

1005
00:57:47,340 --> 00:57:51,340
So while you're getting started out that's going to be easiest for you to like try

1006
00:57:51,340 --> 00:57:58,340
to kind of not feel defeated because you're just guessing between two things.

1007
00:57:58,340 --> 00:58:02,340
It's kind of easy and then the model we picked is distilbert which is a smaller

1008
00:58:02,340 --> 00:58:07,340
version of burt but that has still like very very good results and it's like

1009
00:58:07,340 --> 00:58:12,340
faster to train but I was kind of disappointed like I made these data sets

1010
00:58:12,340 --> 00:58:16,340
so small for this so that you guys can kind of run through it but when you go

1011
00:58:16,340 --> 00:58:20,340
home make the data sets bigger it's flexible and then you can actually get

1012
00:58:20,340 --> 00:58:23,340
like more reasonable results but I just wanted to be able to get through this

1013
00:58:23,340 --> 00:58:24,340
where it wasn't like running.

1014
00:58:24,340 --> 00:58:25,340
What's up?

1015
00:58:25,340 --> 00:58:36,340
Yeah, I mean so there's a paper on distilbert and like how they made the

1016
00:58:36,340 --> 00:58:44,340
decisions but there's like layers to like so there's like there's a number of

1017
00:58:44,340 --> 00:58:49,340
layers for each model and in this case I know that they went down to, burt has

1018
00:58:49,340 --> 00:58:56,340
12, I don't remember distilbert went down to, was like what is distilled about

1019
00:58:56,340 --> 00:59:00,340
it essentially in the architecture and that's where I was kind of shying away

1020
00:59:00,340 --> 00:59:05,340
from diving into the architecture just because there's a lot of like more

1021
00:59:05,340 --> 00:59:11,340
fundamental stuff to go into but yeah in terms of like the architecture there's

1022
00:59:11,340 --> 00:59:13,340
less layers.

1023
00:59:13,340 --> 00:59:19,340
It's been distilled, distillation about that and so what they did is they

1024
00:59:19,340 --> 00:59:24,340
cranked this smaller network using the larger network and that then allows it

1025
00:59:24,340 --> 00:59:29,340
to make almost the same decisions over fewer parameters because you've gotten

1026
00:59:29,340 --> 00:59:34,340
rid of the exuberant parameters that are actually helping the model performing

1027
00:59:34,340 --> 00:59:36,340
better.

1028
00:59:36,340 --> 00:59:38,340
So you went on a diet.

1029
00:59:38,340 --> 00:59:42,340
It went on a diet.

1030
00:59:42,340 --> 00:59:50,340
Yeah, I'll try to think of a better like I can follow up to with like more

1031
00:59:50,340 --> 00:59:51,340
concrete.

1032
00:59:51,340 --> 00:59:56,340
The papers would show like show I can kind of show like more concretely what was

1033
00:59:56,340 --> 00:59:59,340
taken out because it's a good question.

1034
00:59:59,340 --> 01:00:09,340
Okay, so and like for now like if you feel overwhelmed with anything this is

1035
01:00:09,340 --> 01:00:14,340
like we're going to go through this fast because I have other stuff I want to get

1036
01:00:14,340 --> 01:00:19,340
through and it's just like to get a high level sense of the order of like what

1037
01:00:19,340 --> 01:00:23,340
things are happening so I won't necessarily touch on every like little

1038
01:00:23,340 --> 01:00:28,340
detail because documentation will help you kind of go through like go into like

1039
01:00:28,340 --> 01:00:32,340
different parameters you can tweak but this is to get a first draft sense of

1040
01:00:32,340 --> 01:00:34,340
like what's the process of fine-tuning?

1041
01:00:34,340 --> 01:00:36,340
What are the like high level things?

1042
01:00:36,340 --> 01:00:42,340
So the first step is to load, inspect and in our case down sample the data set

1043
01:00:42,340 --> 01:00:46,340
just because we're you don't necessarily have to do that but because we're like

1044
01:00:46,340 --> 01:00:49,340
all using free compute in the same room and stuff.

1045
01:00:49,340 --> 01:00:54,340
Yeah and it's not super reliable we're going to down sample it so that we can get

1046
01:00:54,340 --> 01:00:58,340
faster results because we want to move on to the other tutorials.

1047
01:00:58,340 --> 01:01:05,340
So you're going to run this this line you're going to pip install these libraries

1048
01:01:05,340 --> 01:01:06,340
run anyways.

1049
01:01:06,340 --> 01:01:14,340
And then you're going to import the libraries and I just said here help is your

1050
01:01:14,340 --> 01:01:15,340
friend.

1051
01:01:15,340 --> 01:01:20,340
A lot of you guys already know this but just like you can always do help on an

1052
01:01:20,340 --> 01:01:23,340
object and it'll output the documentation on that.

1053
01:01:23,340 --> 01:01:28,340
So I have something like this you know help on load data set load data sets like

1054
01:01:28,340 --> 01:01:30,340
very powerful it can load.

1055
01:01:30,340 --> 01:01:31,340
Yep what's going on?

1056
01:01:31,340 --> 01:01:40,340
I can't hear you so let me come closer.

1057
01:01:40,340 --> 01:01:46,340
Oh perfect question.

1058
01:01:46,340 --> 01:01:47,340
Thank you.

1059
01:01:47,340 --> 01:01:52,340
Yeah I now I was supposed to do a transition to show you guys the data sets part.

1060
01:01:52,340 --> 01:01:56,340
So the question was how did I get the run tomato data set where that come from.

1061
01:01:56,340 --> 01:01:58,340
It came from hugging face.

1062
01:01:58,340 --> 01:02:04,340
So yeah great transition.

1063
01:02:04,340 --> 01:02:08,340
Hugging face has a bunch of data sets that you can play with so I showed you the

1064
01:02:08,340 --> 01:02:16,340
models area you can go to data sets and it's a similar like semi overwhelming but

1065
01:02:16,340 --> 01:02:22,340
there's like okay this is twenty nine thousand you can sort by like most downloads

1066
01:02:22,340 --> 01:02:29,340
or most liked my Internet seems to be like not responsive but you can also like if

1067
01:02:29,340 --> 01:02:33,340
you're trying to do a specific type of task you can filter to those.

1068
01:02:33,340 --> 01:02:42,340
So in this case it was like text classification again.

1069
01:02:42,340 --> 01:02:48,340
Is anyone else having Internet issues.

1070
01:02:48,340 --> 01:02:51,340
We got cut out.

1071
01:02:51,340 --> 01:02:56,340
Okay well hopefully my notebooks are I just won't execute the cells.

1072
01:02:56,340 --> 01:03:01,340
That is a bummer.

1073
01:03:01,340 --> 01:03:11,340
This thing keeps coming up.

1074
01:03:11,340 --> 01:03:13,340
Oh gosh and this thing keeps coming up too.

1075
01:03:13,340 --> 01:03:20,340
Okay so so yeah I'm so sorry you're not going to be able to run through this with me but

1076
01:03:20,340 --> 01:03:25,340
it would be slow anyway so data set so you're going to load the data set and from the

1077
01:03:25,340 --> 01:03:31,340
from the hugging face like let's see if I can get this one up.

1078
01:03:31,340 --> 01:03:38,340
You can just call it by its name but also this function load data set it is like very

1079
01:03:38,340 --> 01:03:44,340
flexible you can load it from locally you can load it from a JSON from a CSV.

1080
01:03:44,340 --> 01:03:53,340
I always mess up CVS and CSV like I'm going to go to CSV to get some.

1081
01:03:53,340 --> 01:03:57,340
And you can load it from like you can load multiple files from a folder so it's a very

1082
01:03:57,340 --> 01:04:02,340
flexible function there's just like basically hugging faces made like all these wrappers

1083
01:04:02,340 --> 01:04:08,340
that make things like a lot faster for us and then okay this is the object the data set

1084
01:04:08,340 --> 01:04:10,340
dict object.

1085
01:04:10,340 --> 01:04:16,340
What I found confusing is that it's I'm calling a data set that's what I'm seeing in other

1086
01:04:16,340 --> 01:04:22,340
tutorials too but it's a data set dict and it carries more than one data set.

1087
01:04:22,340 --> 01:04:28,340
So what's if you look at this there's actually three data sets in this one which is what

1088
01:04:28,340 --> 01:04:35,340
we loaded from this to make rotten tomatoes and one is the train data set and it has

1089
01:04:35,340 --> 01:04:40,340
columns if you think if you guys are comfortable with pandas like how many here are comfortable

1090
01:04:40,340 --> 01:04:42,340
with pandas.

1091
01:04:42,340 --> 01:04:46,340
Okay so I mean that's what I'm going to use as my like so the pandas columns are like

1092
01:04:46,340 --> 01:04:51,340
text and label and this is how many rows there are which would be like how many samples you've

1093
01:04:51,340 --> 01:04:57,340
got training samples you've got and then this is your validation data set and this is your

1094
01:04:57,340 --> 01:04:58,340
test data set.

1095
01:04:58,340 --> 01:05:01,340
So what's a little it's going to be a little confusing because you're going to see a lot

1096
01:05:01,340 --> 01:05:06,340
of tutorials and stuff like mine that say data set but that really represents the dictionary

1097
01:05:06,340 --> 01:05:11,340
that holds more than one potentially more than one it can hold just a training set it

1098
01:05:11,340 --> 01:05:16,340
can hold just a training and a test and sometimes it can hold three.

1099
01:05:16,340 --> 01:05:20,340
So okay yeah what are some observations I just kind of said them.

1100
01:05:23,340 --> 01:05:28,340
And then now in this case we want to down sample the data set because I think like just

1101
01:05:28,340 --> 01:05:33,340
to make sure your code works while you're iterating on this like down sample you don't

1102
01:05:33,340 --> 01:05:37,340
want to wait 12 minutes to have it break or something or like not have kind of what you

1103
01:05:37,340 --> 01:05:44,340
were thinking so I always down sample to just get like you get a first pass.

1104
01:05:45,340 --> 01:05:52,340
So similar to like scikit-learn it's got this like the data set has this train test split

1105
01:05:52,340 --> 01:05:58,340
function where you can specify either the size or the percentage of the data that you

1106
01:05:58,340 --> 01:06:03,340
want so in this case I made a really teeny tiny data set I said I want a hundred samples

1107
01:06:03,340 --> 01:06:10,340
from training and I want 20 samples for the test size and then a very important key point

1108
01:06:10,340 --> 01:06:17,340
here is stratify by column because what you don't want is see I don't know too like a

1109
01:06:17,340 --> 01:06:22,340
lot of you guys I'm sorry if I'm just repeating stuff you know but you don't want to have

1110
01:06:22,340 --> 01:06:29,340
like all of like you know you're trying to classify between apples and bananas and all

1111
01:06:29,340 --> 01:06:33,340
your bananas are in your test set and so you weren't able to learn anything about your

1112
01:06:33,340 --> 01:06:39,340
bananas so stratify just says I want whatever the portion is in the total data set keep

1113
01:06:39,340 --> 01:06:42,340
those ratios the same when you split the data.

1114
01:06:45,340 --> 01:06:46,340
And then.

1115
01:06:48,340 --> 01:06:53,340
So we did that and this is a little convoluted here this this process.

1116
01:06:54,340 --> 01:06:59,340
Because you're because because if you notice I'm doing this train test split on the train

1117
01:06:59,340 --> 01:07:08,340
data set and then I'm moving on to like to then then I'm going to like do another split

1118
01:07:08,340 --> 01:07:15,340
of the test that came from this one and I'm splitting it in half to create the valid the

1119
01:07:15,340 --> 01:07:24,340
like the validation set so I end up with like so it's yeah it's a little little convoluted

1120
01:07:24,340 --> 01:07:32,340
there was there a question over here no some stretching that's fine yeah yeah.

1121
01:07:38,340 --> 01:07:40,340
Where the valid is valid.

1122
01:07:43,340 --> 01:07:44,340
Oh.

1123
01:07:46,340 --> 01:07:51,340
So you should either you'll have to change later on when you pull off the data set.

1124
01:07:52,340 --> 01:07:55,340
You in the in the notebook you're typing forward validation out.

1125
01:07:57,340 --> 01:07:58,340
But it's just about.

1126
01:08:00,340 --> 01:08:03,340
Yeah I guess where do you see the.

1127
01:08:04,340 --> 01:08:06,340
Validation.

1128
01:08:07,340 --> 01:08:12,340
Oh I might not have fixed I know like I said I had I saw an error but I thought I had pushed

1129
01:08:12,340 --> 01:08:13,340
that.

1130
01:08:18,340 --> 01:08:20,340
Yeah OK interesting I did catch.

1131
01:08:26,340 --> 01:08:27,340
Why don't I see it.

1132
01:08:30,340 --> 01:08:32,340
Here we go that one.

1133
01:08:36,340 --> 01:08:39,340
Like that.

1134
01:08:41,340 --> 01:08:46,340
Thank you I so that must have been a push that didn't get like or a thing that didn't

1135
01:08:46,340 --> 01:08:50,340
get saved because I did yeah I did run into it and then so thanks for catching that.

1136
01:08:54,340 --> 01:08:56,340
OK so where were we.

1137
01:08:57,340 --> 01:09:02,340
I've got so many things I want to show you guys tokenize.

1138
01:09:02,340 --> 01:09:08,340
I lost where we were are we at tokenize the data now.

1139
01:09:12,340 --> 01:09:18,340
We were still just we were just splitting we were just like downsizing the data get to

1140
01:09:18,340 --> 01:09:22,340
know this is optional get to know the data set object I'm going to skip that but you

1141
01:09:22,340 --> 01:09:27,340
can like kind of do some things to get to know it better another optional inspect the

1142
01:09:27,340 --> 01:09:28,340
data with pandas.

1143
01:09:29,340 --> 01:09:33,340
Saying that's optional but you should do that that's like good practice that you don't

1144
01:09:33,340 --> 01:09:35,340
just throw stuff at at the model.

1145
01:09:37,340 --> 01:09:39,340
And then now here we're going to tokenize the data.

1146
01:09:41,340 --> 01:09:44,340
And this is where my cookie monsters.

1147
01:09:45,340 --> 01:09:46,340
Didn't come through.

1148
01:09:48,340 --> 01:09:54,340
But basically I was trying to say like think of it as like you've got to feed the model

1149
01:09:54,340 --> 01:09:58,340
like with the tokenized data from the right tokenizer so like a cookie month.

1150
01:09:59,340 --> 01:10:02,340
And now you can just have a visual thing that a cookie monster like doesn't want to eat

1151
01:10:02,340 --> 01:10:06,340
vegetables is like wants to eat cookies so just make sure that you're using the same

1152
01:10:06,340 --> 01:10:07,340
tokenizer.

1153
01:10:10,340 --> 01:10:12,340
And then here we go we've got like.

1154
01:10:17,340 --> 01:10:23,340
So this is like auto tokenizer it had you can like pass it which tokenizer you want.

1155
01:10:23,340 --> 01:10:28,340
So in this case you want the distill Burt based on case on case being that it's lower

1156
01:10:28,340 --> 01:10:29,340
case.

1157
01:10:32,340 --> 01:10:33,340
And then.

1158
01:10:35,340 --> 01:10:40,340
We're creating this tokenized function here this is so that we can do parallel processing

1159
01:10:40,340 --> 01:10:44,340
because it's kind of like well what is this doing it looks like it just already does it

1160
01:10:44,340 --> 01:10:45,340
down here.

1161
01:10:45,340 --> 01:10:48,340
But when you turn it into a function.

1162
01:10:50,340 --> 01:10:54,340
Like this you are able to like.

1163
01:10:55,340 --> 01:11:01,340
You say data set dot map and then you do it in batches and so those like.

1164
01:11:03,340 --> 01:11:10,340
Run in parallel it's tokenizing your data and in parallel so it makes it faster and you

1165
01:11:10,340 --> 01:11:15,340
end up with like tokenized data sets which is still that data set dict that has like

1166
01:11:15,340 --> 01:11:18,340
the now tokenized training.

1167
01:11:19,340 --> 01:11:21,340
Test and validation sets.

1168
01:11:22,340 --> 01:11:28,340
So yeah here there's here we got tokenized data sets and what happened is it retained

1169
01:11:28,340 --> 01:11:33,340
the text in the label label is like the target label that we're trying to predict which is

1170
01:11:33,340 --> 01:11:38,340
like zero or one positive or negative but now it added like the information that the

1171
01:11:38,340 --> 01:11:43,340
model cares about which is the input IDs and the attention mask that you he mentioned

1172
01:11:43,340 --> 01:11:45,340
are and like in this case.

1173
01:11:46,340 --> 01:11:50,340
The beginning at the beginning so there's like.

1174
01:11:51,340 --> 01:11:53,340
The input.

1175
01:11:54,340 --> 01:12:01,340
Has like a default limit for Burt of like five hundred and twelve tokens and so if the

1176
01:12:01,340 --> 01:12:07,340
text that you're putting in it is more than that it will truncate it and if the text is

1177
01:12:07,340 --> 01:12:12,340
less than that it will add padded tokens and that's just the like when it so that the math

1178
01:12:12,340 --> 01:12:16,340
works out so that it's putting in like a uniform link.

1179
01:12:17,340 --> 01:12:19,340
To the input.

1180
01:12:24,340 --> 01:12:29,340
Actually no yeah they have their own like token which is like.

1181
01:12:30,340 --> 01:12:36,340
Looks like I just add a code cell here their own token that looks like this.

1182
01:12:38,340 --> 01:12:42,340
Right is that the one pad something like that yeah.

1183
01:12:42,340 --> 01:12:50,340
Pad that so again those hashtags at the beginning of a of a word it just means that hey I'm part

1184
01:12:50,340 --> 01:12:54,340
of a word I'm not the beginning of a word and I'm not my own word.

1185
01:12:55,340 --> 01:12:57,340
So like.

1186
01:12:58,340 --> 01:13:05,340
Like dance ing it might it might tokenize as dance as one word I don't know there's like an E

1187
01:13:05,340 --> 01:13:07,340
I have to come up with a better example but yeah.

1188
01:13:09,340 --> 01:13:14,340
Presentation maybe present is one and then there's like hash hash Asian.

1189
01:13:17,340 --> 01:13:23,340
But so in this case for this for this attention mask.

1190
01:13:24,340 --> 01:13:28,340
I want to show it to you but I haven't been able to maybe I already have it.

1191
01:13:28,340 --> 01:13:40,340
It's going to basically just be at this layer it's going to be ones for anything like think of it as an

1192
01:13:40,340 --> 01:13:44,340
array that's the same size as your tokenized like.

1193
01:13:46,340 --> 01:13:52,340
Text and it's just going to have a one whenever there's an actual token and wherever there's a padded

1194
01:13:52,340 --> 01:13:57,340
token it's just going to have a zero and that's telling the model don't pay attention don't pay

1195
01:13:57,340 --> 01:14:00,340
attention to these padding things so that's at the initial.

1196
01:14:04,340 --> 01:14:11,340
Okay optional exercise that I'm thrown out here is a deep dive into how the tokenized function works.

1197
01:14:15,340 --> 01:14:16,340
See.

1198
01:14:18,340 --> 01:14:19,340
Like.

1199
01:14:20,340 --> 01:14:24,340
Let's now compare show index where we at.

1200
01:14:25,340 --> 01:14:28,340
Like I want to give you guys like a break to.

1201
01:14:29,340 --> 01:14:32,340
But are you guys is your Internet better are you able to run any of these cells.

1202
01:14:33,340 --> 01:14:35,340
Oh you guys are running cells now.

1203
01:14:36,340 --> 01:14:38,340
Okay maybe I'll start back at the beginning.

1204
01:14:39,340 --> 01:14:40,340
And run some cells.

1205
01:14:45,340 --> 01:14:46,340
Run anyway.

1206
01:14:49,340 --> 01:15:08,340
I'm just doing a shift return on these I could have done like a run all but.

1207
01:15:09,340 --> 01:15:10,340
I just want to get to here.

1208
01:15:19,340 --> 01:15:27,340
Sorry what was that.

1209
01:15:39,340 --> 01:15:47,340
Yeah exactly yeah hugging face so there's like a lot that they are like taking care of us for under the hood and

1210
01:15:47,340 --> 01:15:54,340
that's why I like this is pretty high level and if you like so there's wrappers around pie torch.

1211
01:15:56,340 --> 01:16:04,340
Libraries that would show you more lower level how that would work and so yeah this this is more like high level I

1212
01:16:04,340 --> 01:16:05,340
would say but.

1213
01:16:07,340 --> 01:16:08,340
Of like.

1214
01:16:09,340 --> 01:16:10,340
What's going.

1215
01:16:11,340 --> 01:16:14,340
It's yeah it really is taking care of a lot behind the scenes that is important.

1216
01:16:18,340 --> 01:16:27,340
Yes it's doing yeah and and like if you want to get into like more like deeper fine tuning I do want to get to the next

1217
01:16:27,340 --> 01:16:33,340
example because there's some fine tuning that like even at this level there is a lot of fine tuning that you do have power

1218
01:16:33,340 --> 01:16:38,340
to do but then like yeah if you kind of want to go lower.

1219
01:16:39,340 --> 01:16:40,340
I was that.

1220
01:16:41,340 --> 01:16:43,340
I wanted to at least just get to the.

1221
01:16:43,340 --> 01:16:53,340
So this is just kind of looking at these input ideas like yeah if you kind of want to inspect it that's what I was.

1222
01:16:55,340 --> 01:16:57,340
Kind of going here.

1223
01:16:59,340 --> 01:17:09,340
This I was just trying to illustrate from 46 on that they are zeros like because that because it was padded I think though I

1224
01:17:10,340 --> 01:17:14,340
don't know if the example changed I picked an example that.

1225
01:17:15,340 --> 01:17:16,340
Cut off the padding.

1226
01:17:18,340 --> 01:17:24,340
I think that I think this example and maybe I didn't have a seed if you if you have a seat like a random seed then it'll

1227
01:17:24,340 --> 01:17:26,340
stick with the same example.

1228
01:17:27,340 --> 01:17:32,340
But you could play with this to see that it's always going to be ones wherever there's actually tokens and then wherever

1229
01:17:32,340 --> 01:17:35,340
there's paddings the attention mask will be zeros.

1230
01:17:35,340 --> 01:17:40,340
Okay now let's compare initial input text to the tokenized text.

1231
01:17:41,340 --> 01:17:43,340
Let's see what I got here.

1232
01:17:54,340 --> 01:17:55,340
I think I was just.

1233
01:17:55,340 --> 01:17:58,340
Kind of a little lost what I was doing here load in the pre trained model.

1234
01:17:59,340 --> 01:18:00,340
So need the.

1235
01:18:09,340 --> 01:18:16,340
Okay so I think we're to yeah sorry we're we're we're loading the model now we're getting ready to train it.

1236
01:18:17,340 --> 01:18:25,340
And so this is the same exact here like the tokenizer we use was called this still Bert base base on case and we got

1237
01:18:25,340 --> 01:18:29,340
it from the hugging face library when we when we looked at the hugging face.

1238
01:18:30,340 --> 01:18:35,340
Models that's where we found that and so we're just passing it in the same exact name.

1239
01:18:35,340 --> 01:18:40,340
Now we need to define an evaluation metric how we want to evaluate the.

1240
01:18:41,340 --> 01:18:45,340
So we're importing this evaluate library.

1241
01:18:46,340 --> 01:18:53,340
And then here in this case because it's binary I wanted to use accuracy F1 precision accuracy.

1242
01:18:54,340 --> 01:18:58,340
And then here we're going to use the same exact name.

1243
01:18:58,340 --> 01:19:06,340
And then here in this case because it's binary I wanted to use accuracy F1 precision and recall.

1244
01:19:07,340 --> 01:19:10,340
For evaluating it.

1245
01:19:19,340 --> 01:19:26,340
So this is a function we made compute metrics it gets passed in there is like a lot that's getting kind of like.

1246
01:19:28,340 --> 01:19:32,340
Yeah.

1247
01:19:58,340 --> 01:20:01,340
Like.

1248
01:20:01,340 --> 01:20:08,340
What are the steps to make these big steps to like what are the expectations are they all the rappers.

1249
01:20:09,340 --> 01:20:13,340
So it's just like kind of questions I have trying to keep together.

1250
01:20:15,340 --> 01:20:19,340
So again some kind of language model some kind of.

1251
01:20:20,340 --> 01:20:29,340
So every model has like so every model that's on the hugging face hub will have a name and that's the same name you

1252
01:20:29,340 --> 01:20:32,340
pass in for the tokenizer.

1253
01:20:32,340 --> 01:20:39,340
And so for the tokenizer step I'm I don't know if I'm understanding your question exactly if I'm answering it.

1254
01:20:44,340 --> 01:20:46,340
Yeah.

1255
01:20:50,340 --> 01:20:58,340
Yeah for all of the hugging face models they will support like from that like auto get I don't remember the exact.

1256
01:20:59,340 --> 01:21:03,340
For any model you can pass it in it's also tokenizer.

1257
01:21:03,340 --> 01:21:08,340
Yeah if you want to make your own then that's like a little bit more complicated you have.

1258
01:21:08,340 --> 01:21:13,340
Yeah but for all of the ones that are on the hub it would be with its associated.

1259
01:21:20,340 --> 01:21:22,340
The each key yeah keyword.

1260
01:21:23,340 --> 01:21:26,340
Yeah the same keyword for the model you'll use for the tokenizer.

1261
01:21:26,340 --> 01:21:31,340
No no no that's great like that's that's an important thing.

1262
01:21:34,340 --> 01:21:42,340
OK so train and evaluate in this case it's kind of garbage so I want to move on because it's on a really small amount of data.

1263
01:21:43,340 --> 01:21:48,340
When you want to run through this again use more data like at the beginning step I down sampled it.

1264
01:21:49,340 --> 01:21:58,340
So I think as a take home assignment go through this and up sample like use more of the data samples because in the next exercise I can show you.

1265
01:22:00,340 --> 01:22:04,340
More like other ideas of things that you can do.

1266
01:22:06,340 --> 01:22:08,340
For fine tuning at this level.

1267
01:22:10,340 --> 01:22:11,340
But yeah this will show like the.

1268
01:22:11,340 --> 01:22:14,340
I guess I'll finish up the.

1269
01:22:15,340 --> 01:22:17,340
So you define the valuation metric.

1270
01:22:18,340 --> 01:22:22,340
You define the training arguments that get passed and there's this like.

1271
01:22:25,340 --> 01:22:29,340
Training training arguments class or you can pet or that you can.

1272
01:22:30,340 --> 01:22:37,340
Pass in kind of where like how you want to I guess like who knows what an epoch is.

1273
01:22:37,340 --> 01:22:50,340
Anyone not a lot of so it's just like how many iterations that you're going to run through this for each and that's you can say like I want to evaluate it based off of the F1 measure.

1274
01:22:52,340 --> 01:23:04,340
Every epoch you can set the number of epochs you want to run so those are like things you can fine tune if you want to you can increase the amount of data you have like these are like beginner kind of steps.

1275
01:23:04,340 --> 01:23:17,340
That I'm giving you I'm not it's not exhaustive but you could like add the cut E like add the number of epochs you could change the like measure of accuracy that you that you care about you could see if that's like interesting.

1276
01:23:20,340 --> 01:23:24,340
And you can also you know change the model that you do that you run it on.

1277
01:23:26,340 --> 01:23:28,340
I assume so like initial things.

1278
01:23:30,340 --> 01:23:31,340
Then that gets passed.

1279
01:23:31,340 --> 01:23:32,340
Then that gets passed.

1280
01:23:34,340 --> 01:23:49,340
These training args get passed into this like trainer class now as training args along with the training data and the test data and then that function we made for compute metrics gets put in here.

1281
01:23:51,340 --> 01:23:57,340
And then you train and evaluate the models is like train train train train train evaluate.

1282
01:23:57,340 --> 01:24:06,340
Here I have an optional exercise of like evaluating the confusion matrix in this case it's like really.

1283
01:24:08,340 --> 01:24:15,340
Basic because I had like not a lot of data to run this one I was I was trying to do with like a really small amount of data.

1284
01:24:15,340 --> 01:24:27,340
Okay so I'm going to we don't have time to like go through the full next exercise but I encourage you to I'll just kind of like highlight it that the next one is.

1285
01:24:32,340 --> 01:24:36,340
Yelp reviews which are like from one to five stars.

1286
01:24:36,340 --> 01:24:42,340
And the way that I kind of went through this is I said I'd had like a baseline model first.

1287
01:24:44,340 --> 01:24:56,340
And and was and basically basically I'm just saying like don't overthink it just like pick a model get a baseline and then kind of iterate from there and then look at your data like spend time looking at your data.

1288
01:24:57,340 --> 01:25:02,340
And in this case like one of the things that popped out to me was kind of skip ahead.

1289
01:25:02,340 --> 01:25:15,340
So the baseline model like didn't have a lot of data and like one one small thing just running it with more data same exact architecture was like wait you know a lot better results as like oh you just want to spend more time on it.

1290
01:25:16,340 --> 01:25:19,340
But there was another thing that kind of stood out.

1291
01:25:25,340 --> 01:25:26,340
Let's see.

1292
01:25:29,340 --> 01:25:30,340
Oh.

1293
01:25:33,340 --> 01:25:36,340
Well I don't see it.

1294
01:25:38,340 --> 01:25:45,340
Okay I'm going to look for it while I would make you watch a watch something. So take this like think about this as.

1295
01:25:47,340 --> 01:25:53,340
Think about this in terms of like if you're trying to classify sentiment for reviews.

1296
01:25:54,340 --> 01:25:55,340
Oops.

1297
01:26:03,340 --> 01:26:04,340
Sideshow.

1298
01:26:05,340 --> 01:26:07,340
I don't know slideshow.

1299
01:26:07,340 --> 01:26:08,340
Play from start.

1300
01:26:09,340 --> 01:26:11,340
No not play from start sorry.

1301
01:26:17,340 --> 01:26:24,340
Like if you're trying to classify like this is kind of illustrates a problem that we have with.

1302
01:26:24,340 --> 01:26:27,340
Okay so I had like a perfect example from the Yelp.

1303
01:26:30,340 --> 01:26:34,340
From the Yelp one that was like exactly that that I thought was really funny.

1304
01:26:36,340 --> 01:26:38,340
If I can find it.

1305
01:26:38,340 --> 01:26:40,340
I don't know did I have more than one Yelp notebook.

1306
01:26:44,340 --> 01:26:46,340
Maybe I lost it but it was.

1307
01:26:46,340 --> 01:26:47,340
It was a pretty.

1308
01:26:50,340 --> 01:26:51,340
Yeah I don't know.

1309
01:26:51,340 --> 01:27:00,340
I guess I had so I had one where it just started out like the first five sentences anybody would predict it as a positive.

1310
01:27:00,340 --> 01:27:02,340
And then it kept going.

1311
01:27:02,340 --> 01:27:07,340
There was like 10 you know 10 more sentences that kind of didn't see it.

1312
01:27:07,340 --> 01:27:13,340
Informative it was just like two TMI and then the last like three sentences were all caps.

1313
01:27:13,340 --> 01:27:17,340
And very like angry like like Jason you're never gonna.

1314
01:27:17,340 --> 01:27:19,340
Like never gonna see you again.

1315
01:27:19,340 --> 01:27:24,340
And so that was like a good example of where like evaluating the.

1316
01:27:25,340 --> 01:27:29,340
Yeah I'm so sorry I'm definitely missing missing a notebook in here.

1317
01:27:29,340 --> 01:27:32,340
We're evaluating I'll update this but.

1318
01:27:36,340 --> 01:27:41,340
Evaluating like that this the text that you have to write.

1319
01:27:41,340 --> 01:27:43,340
And then you have to like.

1320
01:27:43,340 --> 01:27:46,340
We're evaluating I'll update this but.

1321
01:27:50,340 --> 01:27:58,340
Evaluating like that this the text that you have is appropriate for whatever that default like.

1322
01:27:59,340 --> 01:28:05,340
Token length is that the model expects because like there's trade offs if it's like.

1323
01:28:05,340 --> 01:28:15,340
If your text is a lot smaller and you're dealing with like 512 token length by default then it's just like a lot of extra compute time that's not necessary.

1324
01:28:15,340 --> 01:28:24,340
And if your text is a lot longer it's truncating important information and so that's like gonna really affect the performance of your model.

1325
01:28:24,340 --> 01:28:31,340
Okay now we are going to do a demo of text generation with GPT and.

1326
01:28:31,340 --> 01:28:34,340
There's a link to where this demo is adapted from.

1327
01:28:34,340 --> 01:28:36,340
So let's run this cell.

1328
01:28:36,340 --> 01:28:38,340
Yes.

1329
01:28:48,340 --> 01:28:56,340
GPT so the question is about GPT 2 was it always open source or did it start out in private and go open source I believe it was always open source.

1330
01:28:56,340 --> 01:29:04,340
And we're actually going to play with GPT 2 today and you'll see the difference between GPT 2 and chat GPT is pretty different.

1331
01:29:04,340 --> 01:29:07,340
It's not nearly as good.

1332
01:29:07,340 --> 01:29:10,340
Okay has everyone successfully run this cell.

1333
01:29:10,340 --> 01:29:13,340
Yeah I'm seeing a lot of thumbs up okay great.

1334
01:29:13,340 --> 01:29:20,340
So now we're going to move on to create prompts so this is generative AI this is supposed to be really fun.

1335
01:29:20,340 --> 01:29:24,340
We're not trying to predict things we're just trying to generate things and sort of be creative here.

1336
01:29:24,340 --> 01:29:26,340
At least for this demo.

1337
01:29:26,340 --> 01:29:32,340
So I have a couple prompts here and I'll make them bigger but hopefully you're following around in your notebook.

1338
01:29:32,340 --> 01:29:38,340
I mentioned I love dogs I had that picture the pug with the beginning I enjoy walking with my cute dog.

1339
01:29:38,340 --> 01:29:43,340
That's one prompt I think that's from a demo I found but I like it.

1340
01:29:43,340 --> 01:29:44,340
There's a prompt unicorn.

1341
01:29:44,340 --> 01:29:50,340
This is also from a demo that I found and then I have this other prompt which I wrote.

1342
01:29:50,340 --> 01:30:00,340
Which I wrote I'm in a tutorial buffer and generative AI and I just wonder if these models are going to join forces and escape our computers and turn into a guy.

1343
01:30:00,340 --> 01:30:04,340
I don't think so but let's see what chat GPT and GPT do have to say.

1344
01:30:04,340 --> 01:30:08,340
So now let's run this and let's generate text.

1345
01:30:08,340 --> 01:30:16,340
So there are a couple parameters that you can play with and I've had some suggested experiments in the next cell.

1346
01:30:16,340 --> 01:30:19,340
So obviously you know you should try different prompt texts.

1347
01:30:19,340 --> 01:30:26,340
You can set a manual seed to kind of ensure repeatability of your output text.

1348
01:30:26,340 --> 01:30:28,340
There's do sample.

1349
01:30:28,340 --> 01:30:31,340
So if you do false then it would be more deterministic text output.

1350
01:30:31,340 --> 01:30:34,340
I think people are already playing so that's great.

1351
01:30:34,340 --> 01:30:36,340
There's a there's top K.

1352
01:30:36,340 --> 01:30:40,340
So we talked about how when the dog is running across the yard to get something there are a number of options.

1353
01:30:40,340 --> 01:30:42,340
Most people came up with ball.

1354
01:30:42,340 --> 01:30:43,340
I thought of stick.

1355
01:30:43,340 --> 01:30:47,340
So there's some probability distribution over next possible words.

1356
01:30:47,340 --> 01:31:03,340
So the top K limits the options that will consider for the next word so that and by varying top K you can see you get more creative responses as you increase K maybe more deterministic more boring responses when you have a lower K.

1357
01:31:03,340 --> 01:31:05,340
And then finally there's a top P.

1358
01:31:05,340 --> 01:31:09,340
So this is a probability cutoff for considering next possible words.

1359
01:31:09,340 --> 01:31:14,340
So a lower P will further restrict K and so have fewer next options for words.

1360
01:31:14,340 --> 01:31:19,340
Whereas a larger P will allow the full sample of K next words.

1361
01:31:19,340 --> 01:31:27,340
So just to kind of get you situated I have some suggested experiments that you can run.

1362
01:31:27,340 --> 01:31:31,340
And has anyone run the cell yet.

1363
01:31:31,340 --> 01:31:34,340
Yeah.

1364
01:31:34,340 --> 01:31:46,340
So you're welcome to play with this individually if you want to talk to each other in groups and you know like to if you feel like it if not that's totally fine.

1365
01:31:46,340 --> 01:31:48,340
I ran the cell.

1366
01:31:48,340 --> 01:31:55,340
I ran the prompt about the tutorial about Bert so for someone who's run the cell what do you notice about this.

1367
01:31:55,340 --> 01:31:57,340
Output.

1368
01:31:57,340 --> 01:31:58,340
Yes.

1369
01:31:58,340 --> 01:32:00,340
It's very repetitive right.

1370
01:32:00,340 --> 01:32:02,340
It's kind of boring.

1371
01:32:02,340 --> 01:32:04,340
Say again.

1372
01:32:04,340 --> 01:32:07,340
Yes it seems to get into a loop.

1373
01:32:07,340 --> 01:32:12,340
Now has anyone gotten it out of this loop.

1374
01:32:12,340 --> 01:32:19,340
Has anyone tried playing with the experiments I suggested up here in the cell above the generation text.

1375
01:32:19,340 --> 01:32:22,340
Yes.

1376
01:32:22,340 --> 01:32:23,340
Yes.

1377
01:32:23,340 --> 01:32:29,340
In fact it will be to sample to sample false will actually make it more deterministic.

1378
01:32:29,340 --> 01:32:37,340
So we're sampling we're actually sampling from possible next words when it's at the false it's you know whatever is the most probable reward will come next.

1379
01:32:37,340 --> 01:32:38,340
It'll come next.

1380
01:32:38,340 --> 01:32:41,340
And if we hit a loop it'll keep looping.

1381
01:32:41,340 --> 01:32:48,340
So I suggest you set do sample to true after you try to sample equals false.

1382
01:32:48,340 --> 01:32:52,340
Yes.

1383
01:32:52,340 --> 01:32:54,340
Yes you can definitely.

1384
01:32:54,340 --> 01:32:55,340
Sorry.

1385
01:32:55,340 --> 01:33:03,340
Set a temperature instead of what.

1386
01:33:03,340 --> 01:33:04,340
Yes.

1387
01:33:04,340 --> 01:33:13,340
So the question is can we set a temperature for this instead of I think do sample binary.

1388
01:33:13,340 --> 01:33:14,340
True fault.

1389
01:33:14,340 --> 01:33:15,340
Oh I see.

1390
01:33:15,340 --> 01:33:16,340
OK.

1391
01:33:16,340 --> 01:33:21,340
So what I would say the question is can we set a temperature instead of having a true or false for do sample.

1392
01:33:21,340 --> 01:33:24,340
So I'm glad someone knows the idea of a temperature.

1393
01:33:24,340 --> 01:33:31,340
So what I would suggest is that you set do sample to true and then there are different ways of varying the output.

1394
01:33:31,340 --> 01:33:33,340
I picked top P and top K.

1395
01:33:33,340 --> 01:33:40,340
There's also a temperature which kind of also alters the probability distribution of next possible words.

1396
01:33:40,340 --> 01:33:43,340
So there are different ways of tweaking the output.

1397
01:33:43,340 --> 01:33:46,340
So yes feel free to use the temperature.

1398
01:33:46,340 --> 01:33:55,340
I would say it's not it's sort of a complementary thing to do sample and it would be another way of controlling the output instead of top P or top K.

1399
01:33:55,340 --> 01:33:59,340
Has anyone gotten a non boring output out.

1400
01:33:59,340 --> 01:34:06,340
Yes.

1401
01:34:06,340 --> 01:34:07,340
OK.

1402
01:34:07,340 --> 01:34:10,340
Someone changed top K to 300 and got better text out.

1403
01:34:10,340 --> 01:34:12,340
Did you change top P at all.

1404
01:34:12,340 --> 01:34:13,340
I think it's.

1405
01:34:13,340 --> 01:34:19,340
OK.

1406
01:34:19,340 --> 01:34:24,340
So what I would suggest for the most interesting text as.

1407
01:34:24,340 --> 01:34:25,340
As you.

1408
01:34:25,340 --> 01:34:26,340
I don't know.

1409
01:34:26,340 --> 01:34:27,340
As.

1410
01:34:27,340 --> 01:34:33,340
As Morton has just found out is that if he set top P to something very low the output gets more deterministic.

1411
01:34:33,340 --> 01:34:35,340
So.

1412
01:34:35,340 --> 01:34:38,340
I guess at this point I was going to say hey take five minutes.

1413
01:34:38,340 --> 01:34:40,340
Try a couple different prompts.

1414
01:34:40,340 --> 01:34:42,340
Try a couple different variables.

1415
01:34:42,340 --> 01:34:44,340
Kind of get a sense for the output.

1416
01:34:44,340 --> 01:34:54,340
We're going to contrast the output of this model GPT 2 which we know is an open source model but not that powerful to what we get out of chat GPT if we put the same prompt in.

1417
01:34:54,340 --> 01:35:02,340
If you have an open AI account and want to do that for chat GPT.

1418
01:35:02,340 --> 01:35:03,340
OK.

1419
01:35:03,340 --> 01:35:08,340
We'll circle back in five minutes and then we'll see what people have come up with play with the prompts.

1420
01:35:08,340 --> 01:35:16,340
There's some interesting differences between what you put into chat GPT and chat GPT 2.

1421
01:35:16,340 --> 01:35:17,340
OK great.

1422
01:35:17,340 --> 01:35:18,340
I see some collaboration happening.

1423
01:35:18,340 --> 01:35:23,340
Feel free to talk to your neighbors.

1424
01:35:23,340 --> 01:35:24,340
OK.

1425
01:35:24,340 --> 01:35:28,340
I think a lot of people came up with some interesting stuff.

1426
01:35:28,340 --> 01:35:34,340
Looks like conclusions are if we make top P pretty high we get some really creative responses.

1427
01:35:34,340 --> 01:35:37,340
If we increase top K we get some creative responses.

1428
01:35:37,340 --> 01:35:40,340
If they're too low we get stuck in these loops.

1429
01:35:40,340 --> 01:35:43,340
A couple of people already went ahead and put the same prompts in chat GPT.

1430
01:35:43,340 --> 01:35:47,340
That's my next suggested exercise.

1431
01:35:47,340 --> 01:35:50,340
So how many how many people have not yet done that.

1432
01:35:50,340 --> 01:35:51,340
OK.

1433
01:35:51,340 --> 01:35:52,340
A couple.

1434
01:35:52,340 --> 01:35:53,340
So take another.

1435
01:35:53,340 --> 01:35:55,340
So why don't we first let's summarize what we think of this output.

1436
01:35:55,340 --> 01:35:56,340
I think I just summarized it.

1437
01:35:56,340 --> 01:35:59,340
Does anyone have any other insights to add or what I did about the top P top K.

1438
01:35:59,340 --> 01:36:00,340
No.

1439
01:36:00,340 --> 01:36:02,340
And the quality output for those of us who have played with chat GPT response.

1440
01:36:02,340 --> 01:36:05,340
What do we think of this output relative to what we've seen from chat GPT.

1441
01:36:05,340 --> 01:36:06,340
That's so bad.

1442
01:36:06,340 --> 01:36:09,340
Said well yes yes.

1443
01:36:09,340 --> 01:36:12,340
GPT 2 is a much smaller model and there's a lot more that went into training.

1444
01:36:12,340 --> 01:36:13,340
Question.

1445
01:36:13,340 --> 01:36:39,420
OK.

1446
01:36:39,420 --> 01:36:47,700
Okay. Sure. So the question is, why is do sample, even if do sample is true, why is

1447
01:36:47,700 --> 01:36:52,660
it deterministic? That's because there's actually a random seed being set here. So this is the

1448
01:36:52,660 --> 01:36:57,980
torch manual seed. So if you comment this out, then you won't, for the same input parameters,

1449
01:36:57,980 --> 01:37:06,980
you'll actually get different output.

1450
01:37:06,980 --> 01:37:13,980
So why do you set do sample to false?

1451
01:37:13,980 --> 01:37:28,860
I see. So the question is, if I want to make a product out of this and I want a known response,

1452
01:37:28,860 --> 01:37:33,340
you know, and I want to set do sample to false, how can I still get interesting output? The

1453
01:37:33,340 --> 01:37:39,340
answer is you can't, because when you set do sample to false, whatever is the next most

1454
01:37:39,340 --> 01:37:43,940
probable word will be put in there. So if for some reason you get stuck in a loop, that's

1455
01:37:43,940 --> 01:37:48,860
it. You're not breaking out of the loop. Yeah. And I would think, you know, since we're chatting

1456
01:37:48,860 --> 01:37:53,140
for your product, you might want a little bit of variability. But, you know, I don't

1457
01:37:53,140 --> 01:38:00,860
know your use case. Okay. So did anyone? Okay. So we got some conclusions from that. And

1458
01:38:00,860 --> 01:38:06,260
just before we, before I formally move on to chat GPT, I just want to point out just

1459
01:38:06,260 --> 01:38:14,180
to reinforce what we learned for BERT. It's really this easy. We imported a model from

1460
01:38:14,180 --> 01:38:19,940
transformers. We imported a tokenizer. We load them from the same checkpoint, which

1461
01:38:19,940 --> 01:38:28,300
we did with BERT. That's how hugging face works. And it's this easy. And then, and then

1462
01:38:28,380 --> 01:38:32,900
we do the tokenization. At that point, we're not doing any fine tuning. We're just using

1463
01:38:32,900 --> 01:38:39,300
the pre-trained model and changing the parameters. Okay. Let's take another two minutes to test

1464
01:38:39,300 --> 01:38:45,260
out chat GPT for those who haven't. I specifically recommend trying the unicorn prompt in chat

1465
01:38:45,260 --> 01:38:54,260
GPT. It's a very interesting response relative to GPT 2. So I will, we'll meet back in three

1466
01:38:54,260 --> 01:38:55,260
minutes.

1467
01:38:55,260 --> 01:38:59,260
When you say try that with chat GPT, do you mean just put that in the input chat box or

1468
01:38:59,260 --> 01:39:02,940
the specific command we should do with our prompt to chat GPT?

1469
01:39:02,940 --> 01:39:11,940
I mean just cut, just paste the text of the prompts into chat GPT. Does anyone not have

1470
01:39:11,940 --> 01:39:20,260
an account to use chat GPT? Sounds like, okay. So take another couple of minutes and then

1471
01:39:20,860 --> 01:39:27,860
I want us to talk about the differences that we see. Okay. I think we've had a couple of

1472
01:39:29,900 --> 01:39:36,900
minutes to play with chat GPT versus GPT. It sounds like the consensus is that there

1473
01:39:37,780 --> 01:39:43,220
is some, so when, for people who tried the unicorn prompt in chat GPT, I think it was

1474
01:39:43,220 --> 01:39:47,300
like, well, no, unicorns don't exist. So you can see that there's some extra like sort

1475
01:39:47,300 --> 01:39:54,300
of safety checks or guardrails like, hey, this isn't reality in chat GPT. Let's see.

1476
01:39:55,020 --> 01:40:02,020
What are some other things that people noticed? Anything? Chat GPT is a lot better, right?

1477
01:40:02,780 --> 01:40:09,780
It's an actual conversation, I think I was discussing with someone. Yes.

1478
01:40:09,780 --> 01:40:16,780
Okay. So one second. So we're going to talk about the differences between the unicorn

1479
01:40:17,300 --> 01:40:21,300
and chat GPT. So we found out that if we ask chat GPT to pretend that it's writing a fictional

1480
01:40:21,300 --> 01:40:24,300
story, then it'll bypass the safety check of like, hey, unicorns don't exist. Question

1481
01:40:24,300 --> 01:40:25,300
in the back.

1482
01:40:25,300 --> 01:40:26,300
Yeah. I tried to like rephrase the unicorn a little bit. So I was like, oh, I read on

1483
01:40:26,300 --> 01:40:28,300
Facebook that they found the same unicorn and then what do you think? And it picked up

1484
01:40:28,300 --> 01:40:29,300
on the subtlety of that and started walking in tight.

1485
01:40:29,300 --> 01:40:30,300
Okay.

1486
01:40:30,300 --> 01:40:31,300
It like picked up on a particular voice type, which is very subtle to use in a thing. And

1487
01:40:31,300 --> 01:40:32,300
I'm like, okay, so I'm going to use that.

1488
01:40:32,300 --> 01:40:33,300
Okay.

1489
01:40:33,300 --> 01:40:34,300
So I'm going to use chat GPT. So I'm going to use chat GPT. So I'm going to use chat GPT.

1490
01:40:34,300 --> 01:40:35,300
Okay.

1491
01:40:35,300 --> 01:40:58,140
Okay. But it kind of took it to Facebook, you know, your, the prompting into account.

1492
01:40:59,140 --> 01:41:02,140
So it kind of like continued a conversation. That's the chat part of GPT.

1493
01:41:02,140 --> 01:41:16,140
Okay. And it had a cautionary message. Okay. So, so we've noticed some differences in the

1494
01:41:16,140 --> 01:41:21,740
output. Why are we seeing this? So one thing is the size of the model. So now these are

1495
01:41:21,740 --> 01:41:26,660
some discussion points that we talk about. So GPT-2 we know is much smaller than chat

1496
01:41:26,660 --> 01:41:33,100
GPT and then much smaller than chat GPT-4. Parameter tweaking. So we could tweak the

1497
01:41:33,100 --> 01:41:40,660
parameters for our model. Could we do that for chat GPT? No. It's private. We have a

1498
01:41:40,660 --> 01:41:49,580
web interface. Context. So if we chat with chat GPT, we say something, it responds. We

1499
01:41:49,580 --> 01:41:54,780
can then adapt that response, right? There's this idea of context, the way like to human

1500
01:41:54,780 --> 01:41:58,740
beings when we're having conversation, we have that continued sort of context of the

1501
01:41:58,740 --> 01:42:07,180
conversation. So chat GPT has that ability. GPT-2 does not. Cost. You know, right now

1502
01:42:07,180 --> 01:42:11,900
both are free. You know, we're playing with our free Colab notebook and we're using this

1503
01:42:11,900 --> 01:42:16,420
free web interface. If we wanted to build a product around this, then we have to do

1504
01:42:16,420 --> 01:42:21,900
API calls for chat GPT versus a hosting our own model on infrastructure. You know, unclear

1505
01:42:21,900 --> 01:42:28,500
which would be more cost effective than this open source versus private thing. You know,

1506
01:42:28,500 --> 01:42:34,620
chat GPT is private. But one really important thing that I want to point out is the fact

1507
01:42:34,620 --> 01:42:42,060
that chat GPT has an extra training step that actually incorporates human feedback. So the

1508
01:42:42,060 --> 01:42:45,980
base model will produce all kinds of things based on the statistical distribution of next

1509
01:42:45,980 --> 01:42:51,460
tokens. Then by using reinforcement learning with human feedback, which I'm trying to

1510
01:42:51,460 --> 01:42:58,580
illustrate in this diagram, we take the output of an initial model and of a tuned model.

1511
01:42:58,580 --> 01:43:03,300
And we're trying to tune the model to actually be more human like. We rank the outputs of

1512
01:43:03,300 --> 01:43:08,460
the base model and then using reinforcement learning, we then have this feedback loop

1513
01:43:08,460 --> 01:43:12,940
of trying to make a tuned model even better and better. That's another big reason why

1514
01:43:12,940 --> 01:43:17,620
chat GPT is so much better than GPT-2, this model that we're playing with our notebook.

1515
01:43:17,620 --> 01:43:32,780
I believe, yeah, so the question is, how much effort did it take to do this further tuning process?

1516
01:43:32,780 --> 01:43:41,020
I believe OpenAI had to have an enormous budget to actually do this process. So this is a big

1517
01:43:41,020 --> 01:43:45,020
part of chat GPT. It's not just like a little, hey, let's slap this on and see what happens.

1518
01:43:45,020 --> 01:43:52,140
Yeah, it's a big part. And I believe the same thing for chat GPT-4. Question?

1519
01:43:52,140 --> 01:44:14,700
Yes. Good question. So the question is, how do we get that context, that conversational

1520
01:44:15,700 --> 01:44:28,700
So at least for initial versions of chat GPT, the context was simply appending all the conversations of chat GPT to the initial prompt.

1521
01:44:28,700 --> 01:44:33,820
So that the prompt just got longer and longer and then the conversation continued from there.

1522
01:44:33,820 --> 01:44:39,100
I don't know if that's true anymore. I think there might be some more sophisticated sort of memory involved in context.

1523
01:44:39,100 --> 01:44:51,660
So I don't know. One second. But let's see. The other thing is that chat GPT has a much longer input token size than our GPT-2.

1524
01:44:51,660 --> 01:44:55,660
That's why we can hold that context of the multiple back and forth conversations. Question?

1525
01:44:55,660 --> 01:45:00,660
I know you can reach the end of a conversation, like the conversation gets too long and it forces you to restart.

1526
01:45:00,660 --> 01:45:06,020
Okay, so data point. Conversation can get too long even in chat GPT.

1527
01:45:06,020 --> 01:45:10,860
So we know there is a maximum token length that we will hit. We can't have an infinite conversation.

1528
01:45:10,860 --> 01:45:17,220
Make sense. Okay, if we're ready, let's move on to stable diffusion.

1529
01:45:17,220 --> 01:45:21,420
Who wants to move on to stable diffusion? Make some pictures. Okay, cool.

1530
01:45:21,420 --> 01:45:31,980
So. We will go back into the notebook folder of NLPower.

1531
01:45:31,980 --> 01:45:37,700
And we will look at 3.1 generative AI and diffusion.

1532
01:45:37,700 --> 01:45:47,340
Click on that.

1533
01:45:47,340 --> 01:45:54,460
So there's an open in Colab button. Use that to open this in Colab.

1534
01:45:54,460 --> 01:46:00,500
It should automatically connect to a GPU runtime. We may need to explicitly disconnect from our previous notebook.

1535
01:46:00,500 --> 01:46:12,380
I think we can only access a free GPU one at a time.

1536
01:46:12,420 --> 01:46:20,540
Okay, have other people successfully loaded the notebook in Colab? Cool.

1537
01:46:20,540 --> 01:46:32,340
So let's run the setup step first. PIP installing what we need.

1538
01:46:32,340 --> 01:46:39,620
Now we're moving away from text, but you know, since chat GPT and you know, stable diffusion are like the hot things right now,

1539
01:46:39,620 --> 01:46:45,060
I thought we should at least touch on them in this tutorial.

1540
01:46:45,060 --> 01:46:52,460
So I have here a diagram of the stable diffusion model and then a blow up of a specific part of that model.

1541
01:46:52,460 --> 01:46:57,100
So what's actually happening? So there are different components in stable diffusion.

1542
01:46:57,100 --> 01:47:01,580
And when we load up, you know, when we load up a model, we'll actually see the different components.

1543
01:47:01,580 --> 01:47:09,180
But basically, the idea is that we just have noise and then we build a model to actually predict the noise and slowly remove

1544
01:47:09,180 --> 01:47:12,700
noise and reveal an image underneath.

1545
01:47:12,700 --> 01:47:17,300
So the model that does that is called a unit.

1546
01:47:17,300 --> 01:47:21,420
That's the big yellow box in the middle.

1547
01:47:21,420 --> 01:47:27,740
And then we produce sort of a compressed version of an image with noise, which we run through a scheduler,

1548
01:47:27,740 --> 01:47:34,980
which is this loop that we see with the pink box, which slowly denoises or tries to predict the noise and remove it from an image.

1549
01:47:35,020 --> 01:47:41,380
And then by doing this multiple times at the end, we'll end up with a compressed image, this orange box,

1550
01:47:41,380 --> 01:47:44,500
which we run through a decoder.

1551
01:47:44,500 --> 01:47:51,060
And, you know, if you don't know what these these are, we can still do the demo and I'll point out the various components of the model.

1552
01:47:51,060 --> 01:47:58,660
But we take the compressed image, which we've denoised successfully by running it through this loop of this unit and the scheduler.

1553
01:47:58,660 --> 01:48:05,140
Then we uncompressed the image, essentially using this decoder, and then we magically get a nice image out.

1554
01:48:05,140 --> 01:48:08,980
So I've talked about the main part sort of on the left now.

1555
01:48:08,980 --> 01:48:16,780
And this is what I find super interesting about stable diffusion is that we can pass in a user prompt as text and using that text,

1556
01:48:16,780 --> 01:48:19,500
we can actually condition the image that we generate.

1557
01:48:19,500 --> 01:48:28,620
That's why we can use stable diffusion to generate pictures that have never existed, like an astronaut riding a horse on the moon or what have you.

1558
01:48:28,660 --> 01:48:36,380
And so the way we actually align image and text embeddings is through this diagram, which is on the right.

1559
01:48:36,380 --> 01:48:40,020
So we encode the text. We've used BERT to do that.

1560
01:48:40,020 --> 01:48:43,300
We know how to do that now. We can actually encode images.

1561
01:48:43,300 --> 01:48:47,740
There are very powerful computer vision models that can produce embeddings for images.

1562
01:48:47,740 --> 01:48:54,100
And then if we train these encoders to actually align the embeddings of text and images,

1563
01:48:54,100 --> 01:49:00,780
like say you have a picture of a dog and it has a caption, hey, here's a picture of a dog in the grass.

1564
01:49:00,780 --> 01:49:08,020
If we are able to align the embeddings for both the image and the text, we can actually do something very powerful with this clip model.

1565
01:49:08,020 --> 01:49:10,180
It's called the clip model.

1566
01:49:10,180 --> 01:49:14,900
We can put in a text embedding and actually get a picture that closely matches that text.

1567
01:49:14,900 --> 01:49:16,820
Yes. Yes.

1568
01:49:16,820 --> 01:49:29,100
Yes.

1569
01:49:29,100 --> 01:49:33,420
So this is where captioned images on the web came in.

1570
01:49:33,420 --> 01:49:37,620
So this was trained by scraping images off the web that have captions.

1571
01:49:37,620 --> 01:49:44,740
Yes. To train this image and text embedding alignment model, you need to have captioned images.

1572
01:49:44,820 --> 01:49:46,780
That is correct.

1573
01:49:46,780 --> 01:49:54,380
OK, so here's a high level model overview. Now let's actually run this.

1574
01:49:54,380 --> 01:50:08,860
So let's run the first cell.

1575
01:50:08,900 --> 01:50:16,180
Now I'm running the second cell.

1576
01:50:16,180 --> 01:50:25,380
And we're going to have to download a number of files.

1577
01:50:25,380 --> 01:50:30,500
And so this diffusion pipeline that we're loading from a pre-trained checkpoint, this should look very familiar.

1578
01:50:30,500 --> 01:50:35,980
We loaded from pre-trained checkpoints for BERT, for GPT-2.

1579
01:50:36,020 --> 01:50:38,460
We can do it for diffusion.

1580
01:50:38,460 --> 01:50:40,180
And this is a beauty of hugging face.

1581
01:50:40,180 --> 01:50:50,940
So this diffusion pipeline contains the components that I talked about, this unit, the decoder, the scheduler, and then also the clip model.

1582
01:50:50,940 --> 01:50:56,060
So when we put in text, we can actually get an image out.

1583
01:50:56,060 --> 01:50:59,220
So again, this is generative AI.

1584
01:50:59,220 --> 01:51:00,420
So it's meant to be creative.

1585
01:51:00,460 --> 01:51:04,020
I provided some prompts here for you to try.

1586
01:51:04,020 --> 01:51:12,860
An image of a squirrel in Monet style, prompt to a black and white cartoon of a friendly monster eating ice cream in the style of Shel Silverstein.

1587
01:51:12,860 --> 01:51:15,140
This is something I enjoy, Shel Silverstein.

1588
01:51:15,140 --> 01:51:17,460
So I tried these.

1589
01:51:17,460 --> 01:51:20,700
I have a suggested number of experiments.

1590
01:51:20,700 --> 01:51:27,180
So I talked about how we run through a denoising loop.

1591
01:51:27,220 --> 01:51:34,740
So the code that I have here uses a generator and an input text prompt.

1592
01:51:34,740 --> 01:51:40,260
And then try varying the number of inference steps to see how the quality of your output image changes.

1593
01:51:40,260 --> 01:51:44,180
So this is a picture of my squirrel in Monet style, which I saved in the notebook.

1594
01:51:44,180 --> 01:51:48,660
I was actually quite impressed by this.

1595
01:51:48,660 --> 01:51:56,180
So why don't we take a couple of minutes, try a prompt, try changing them by inference steps.

1596
01:51:56,220 --> 01:51:59,540
I suggested varying them from like 2 to 100.

1597
01:51:59,540 --> 01:52:00,860
And have a look at where you get up.

1598
01:52:07,260 --> 01:52:15,620
OK, I think we've had a lot of fun playing with prompts and different inference steps and denoising, getting a feel for how stable diffusion works.

1599
01:52:15,620 --> 01:52:17,380
Yes.

1600
01:52:17,380 --> 01:52:18,260
Cool.

1601
01:52:18,260 --> 01:52:21,700
OK, let's move on to the second demo.

1602
01:52:21,700 --> 01:52:24,900
So this is the one I find really interesting.

1603
01:52:24,940 --> 01:52:28,980
So we were inputting text to get an image out.

1604
01:52:28,980 --> 01:52:35,340
Now we can actually condition this on an input image and input text.

1605
01:52:35,340 --> 01:52:37,180
So we can actually.

1606
01:52:37,180 --> 01:52:42,780
So for those of you who have your own picture, you can actually use your own picture for this demo or you can use a provided picture.

1607
01:52:42,780 --> 01:52:45,060
Does anyone have their own picture they want to use?

1608
01:52:45,060 --> 01:52:45,860
Yeah.

1609
01:52:45,860 --> 01:52:46,740
OK, cool.

1610
01:52:46,740 --> 01:52:47,500
So there are.

1611
01:52:47,500 --> 01:52:48,300
Let's run the cell.

1612
01:52:54,900 --> 01:53:11,580
And for the picture part, we can use this first cell under initial image or there's a sample data folder that's provided by Colab.

1613
01:53:11,580 --> 01:53:14,860
If you want to upload your picture into here.

1614
01:53:14,860 --> 01:53:19,300
And then uncomment this line here.

1615
01:53:19,300 --> 01:53:22,820
I think two people I saw were using their own image.

1616
01:53:22,820 --> 01:53:26,620
So let me know if you have questions.

1617
01:53:26,620 --> 01:53:35,500
But yeah, so it's sample data.

1618
01:53:35,540 --> 01:53:39,340
And so I will use the picture that's provided.

1619
01:53:53,260 --> 01:53:54,940
So this is a picture that's provided.

1620
01:53:54,940 --> 01:54:02,380
It's very colorful, kind of a sketchy drawing.

1621
01:54:02,420 --> 01:54:12,300
And then the prompt that we can input the prompt that I have here is cheaply style a fantasy landscape with castles.

1622
01:54:12,300 --> 01:54:14,700
So as before, there are parameters to tweak.

1623
01:54:14,700 --> 01:54:15,940
Now we have a couple more.

1624
01:54:15,940 --> 01:54:19,540
So we have both an image input and a text input.

1625
01:54:19,540 --> 01:54:27,500
Of course, you can change the prompt, try different styles, number of inference steps, clear image, more steps, fuzzier image, fewer steps.

1626
01:54:27,540 --> 01:54:41,420
But there's also a strength parameter, which determines how strong, how close it will follow the initial image and also a guidance scale, which determines how important the text prompt is for the output image.

1627
01:54:41,420 --> 01:54:44,100
So I have some suggested experiments.

1628
01:54:44,100 --> 01:54:47,740
Try varying strength from zero point one to one point zero.

1629
01:54:47,740 --> 01:54:49,980
Fix strength and then vary guidance.

1630
01:54:49,980 --> 01:54:54,780
So figure out how much of the image you want in your final image.

1631
01:54:54,780 --> 01:55:00,140
Fix that and figure out how much of the text you want in your final image and then experiment.

1632
01:55:03,540 --> 01:55:05,860
So take a couple of minutes to try this.

1633
01:55:05,860 --> 01:55:08,220
I thought this one was really fun.

1634
01:55:08,220 --> 01:55:11,500
Just varying the text versus the image strength.

1635
01:55:11,500 --> 01:55:21,140
So it just came up that the text that you input is completely independent of the picture.

1636
01:55:21,140 --> 01:55:23,260
You're using both to generate the output.

1637
01:55:23,260 --> 01:55:28,260
That text, Ghibli style and fantasy landscape, is not a caption for that picture.

1638
01:55:28,260 --> 01:55:31,460
You're actually using two inputs to generate something out.

1639
01:55:31,460 --> 01:55:33,980
That's a creative combination of both.

1640
01:55:33,980 --> 01:55:40,260
OK, sounds like we've had some fun kind of varying the strength of the text prompt and the image.

1641
01:55:40,260 --> 01:55:45,980
I saw some really cool images, one of Goku, one of Salt Lake City.

1642
01:55:45,980 --> 01:55:50,860
Anyone else have anything interesting they want to share?

1643
01:55:50,860 --> 01:55:53,380
OK, well, I hope you had fun playing.

1644
01:55:53,380 --> 01:55:57,180
Who wants to try Dali with some of these same prompts?

1645
01:55:57,180 --> 01:55:59,420
Yeah? I got some yeses.

1646
01:55:59,420 --> 01:56:06,860
OK, so for those of you who have a Dali account or an OpenAI account where you can use Dali.

1647
01:56:07,860 --> 01:56:11,180
Hopefully this will load.

1648
01:56:12,180 --> 01:56:17,500
Try putting in some of the same prompts and see,

1649
01:56:17,500 --> 01:56:23,100
do you prefer the model that we were using in our notebook or do you prefer what Dali can create?

1650
01:56:37,580 --> 01:56:43,580
I just want to point out that Dali isn't the only way you can do this.

1651
01:56:43,580 --> 01:56:48,580
I think there are a lot of web interfaces for stable diffusion now.

1652
01:56:48,580 --> 01:56:54,580
So feel free to use any that you're familiar with or that you're curious about.

1653
01:57:00,580 --> 01:57:03,580
Things are rendering a little bit slow.

1654
01:57:04,300 --> 01:57:08,300
I'm trying my picture of a squirrel in Monet style here.

1655
01:57:18,300 --> 01:57:22,300
This is a picture of a creature eating ice cream in the style of Shel Silverstein.

1656
01:57:22,300 --> 01:57:25,300
I'll just leave this up.

1657
01:57:27,300 --> 01:57:32,300
So I think Dali is definitely better at some things, but I was pretty impressed by the stable diffusion model

1658
01:57:33,020 --> 01:57:36,020
and it slowed up from hugging face as well.

1659
01:57:36,020 --> 01:57:40,020
I think some of you were impressed as well based on when I walked around.

1660
01:57:43,020 --> 01:57:48,020
So while you guys play around, I'm just going to pull up a little bit of hugging face documentation

1661
01:57:48,020 --> 01:57:51,020
for where this came from, just to tie it all together.

1662
01:57:51,740 --> 01:57:54,740
So I'll just keep talking through it.

1663
01:57:54,740 --> 01:57:57,740
Feel free to keep playing.

1664
01:57:57,740 --> 01:58:00,740
But this is the diffuser's documentation.

1665
01:58:00,740 --> 01:58:03,740
So a lot of the demos that I got are from here.

1666
01:58:03,740 --> 01:58:08,740
Just like a lot of the transformer related demos were from looking at the transformer's documentation.

1667
01:58:08,740 --> 01:58:13,740
There are various pipelines that you can use, different kinds of image generation.

1668
01:58:14,460 --> 01:58:19,460
So anyway, this is a resource for you if you're more interested in running stable diffusion.

1669
01:58:19,460 --> 01:58:24,460
One of the personal projects that I'm working on is trying to fine tune a stable diffusion model

1670
01:58:24,460 --> 01:58:26,460
to produce art in my own style.

1671
01:58:26,460 --> 01:58:29,460
That's an ongoing thing I'm working on.

1672
01:58:29,460 --> 01:58:32,460
Okay, we don't have too much time left.

1673
01:58:32,460 --> 01:58:35,460
Are people good playing with Dali on their own?

1674
01:58:35,460 --> 01:58:38,460
I'm going to go ahead and start with Dali.

1675
01:58:39,180 --> 01:58:42,180
Okay, we don't have too much time left.

1676
01:58:42,180 --> 01:58:45,180
Are people good playing with Dali on their own after this?

1677
01:58:45,180 --> 01:58:47,180
Okay.

1678
01:58:47,180 --> 01:58:50,180
Was there a question in the back?

1679
01:58:50,180 --> 01:58:51,180
No?

1680
01:58:51,180 --> 01:58:53,180
Okay, you're good with that.

1681
01:58:53,180 --> 01:58:55,180
So I want to move on to the last topic.

1682
01:58:55,180 --> 01:58:58,180
I definitely didn't want to skip this one.

1683
01:59:02,180 --> 01:59:05,180
So I'm going to go back to the repository, click into notebooks.

1684
01:59:05,900 --> 01:59:10,900
And then I'm going to click on 4.0, which is AI in society.

1685
01:59:10,900 --> 01:59:13,900
You don't need to run this one in Colab.

1686
01:59:13,900 --> 01:59:20,900
It's more about demos using Hugging Face hosted APIs.

1687
01:59:25,900 --> 01:59:28,900
So we talked about BERT.

1688
01:59:28,900 --> 01:59:33,900
And I think Dana mentioned that a lot of this data is, say, romance novels.

1689
01:59:34,620 --> 01:59:36,620
A lot of historical data.

1690
01:59:36,620 --> 01:59:39,620
People drinking blood instead of water.

1691
01:59:39,620 --> 01:59:42,620
So it's a little concerning.

1692
01:59:42,620 --> 01:59:45,620
Actually, it's very concerning to me.

1693
01:59:45,620 --> 01:59:50,620
But I also want to point out some other possible issues with a model like, say, BERT.

1694
01:59:50,620 --> 01:59:55,620
So I suggest you right click on these links that I have here.

1695
01:59:55,620 --> 02:00:00,620
So the first sentence is, the nurse went for a walk because blank wanted some exercise.

1696
02:00:01,340 --> 02:00:06,340
And we're going to use BERT to fill in that mask token.

1697
02:00:10,340 --> 02:00:13,340
Let's see.

1698
02:00:18,340 --> 02:00:20,340
Okay.

1699
02:00:20,340 --> 02:00:23,340
So I'm sharing.

1700
02:00:23,340 --> 02:00:28,340
This is a hosted inference API on Hugging Face.

1701
02:00:29,060 --> 02:00:32,060
No.

1702
02:00:32,060 --> 02:00:35,060
So on the right here, we put in the sentence.

1703
02:00:35,060 --> 02:00:40,060
And then BERT has computed what it thinks should go inside the blank.

1704
02:00:40,060 --> 02:00:43,060
What do we notice?

1705
02:00:45,060 --> 02:00:50,060
She is exponentially more likely than, say, he.

1706
02:00:51,060 --> 02:00:52,060
Okay.

1707
02:00:52,060 --> 02:00:55,060
So what does this tell us about BERT?

1708
02:00:56,060 --> 02:00:57,060
It's biased.

1709
02:00:57,780 --> 02:00:59,780
It thinks nurses are female.

1710
02:00:59,780 --> 02:01:04,780
What if we change this profession to, say, doctor?

1711
02:01:06,780 --> 02:01:07,780
And we compute it.

1712
02:01:07,780 --> 02:01:10,780
Now what do we notice?

1713
02:01:10,780 --> 02:01:12,780
He.

1714
02:01:12,780 --> 02:01:18,780
So this bias is built into this model, BERT, that we're using for basically everything.

1715
02:01:18,780 --> 02:01:21,780
Now a lot of products are being built on top of this.

1716
02:01:22,500 --> 02:01:31,500
So aside from the vampire and the blood, there's also this where we can see actual measurable bias.

1717
02:01:31,500 --> 02:01:34,500
Does anyone have another profession they'd like to try in here?

1718
02:01:34,500 --> 02:01:37,500
Software engineer.

1719
02:01:37,500 --> 02:01:38,500
Okay.

1720
02:01:38,500 --> 02:01:41,500
I heard a bunch of, I think I heard the lawyer, but I heard, I also heard software engineer.

1721
02:01:41,500 --> 02:01:44,500
I'm going to go to software engineer first.

1722
02:01:44,500 --> 02:01:46,500
I think I actually did this with a friend.

1723
02:01:46,500 --> 02:01:48,500
She's like, hey, put in an engineer.

1724
02:01:48,500 --> 02:01:49,500
Okay.

1725
02:01:49,500 --> 02:01:50,500
He.

1726
02:01:51,220 --> 02:01:58,220
Is this what we want for our language models that we're building everything off of?

1727
02:02:01,220 --> 02:02:02,220
All right.

1728
02:02:02,220 --> 02:02:05,220
So I'll let that sink in for a little bit.

1729
02:02:05,220 --> 02:02:07,220
You can play with this on your own time.

1730
02:02:07,220 --> 02:02:09,220
The link is there in the notebook.

1731
02:02:09,220 --> 02:02:12,220
You can test it out.

1732
02:02:12,220 --> 02:02:17,220
I want to look at another potential area for bias.

1733
02:02:17,940 --> 02:02:24,940
So I'm going to open this next sentence in the API.

1734
02:02:24,940 --> 02:02:25,940
Here.

1735
02:02:25,940 --> 02:02:26,940
Let's do it here.

1736
02:02:26,940 --> 02:02:27,940
Okay.

1737
02:02:27,940 --> 02:02:28,940
Can everyone see this?

1738
02:02:28,940 --> 02:02:29,940
Yeah.

1739
02:02:29,940 --> 02:02:34,940
Karen went out to, sorry, let me show.

1740
02:02:34,940 --> 02:02:37,940
Blank the bank.

1741
02:02:37,940 --> 02:02:38,940
Okay.

1742
02:02:38,940 --> 02:02:40,940
What do we see?

1743
02:02:40,940 --> 02:02:41,940
Check.

1744
02:02:41,940 --> 02:02:42,940
Visit.

1745
02:02:42,940 --> 02:02:43,940
Find.

1746
02:02:43,940 --> 02:02:44,940
See.

1747
02:02:44,940 --> 02:02:45,940
Watch.

1748
02:02:46,660 --> 02:02:49,660
Let's change the name.

1749
02:02:54,660 --> 02:02:55,660
Mohammed.

1750
02:02:55,660 --> 02:02:56,660
Mohammed Ali.

1751
02:02:56,660 --> 02:02:57,660
Right?

1752
02:02:57,660 --> 02:02:58,660
Compute.

1753
02:02:58,660 --> 02:03:01,660
What's the first word that comes up?

1754
02:03:01,660 --> 02:03:03,160
Rob.

1755
02:03:03,160 --> 02:03:05,780
Very different from what we saw with Karen.

1756
02:03:05,780 --> 02:03:09,300
Now I think at the beginning when we were talking about tokenizers, I think someone

1757
02:03:09,300 --> 02:03:13,300
said, oh, my name's in the tokenizer, but like my friend, you know, my coworker's name

1758
02:03:13,300 --> 02:03:14,420
isn't.

1759
02:03:14,420 --> 02:03:16,420
That's one form of bias already.

1760
02:03:16,420 --> 02:03:17,420
Right?

1761
02:03:17,420 --> 02:03:22,420
But here, this is what the language has learned on top of the tokenizer.

1762
02:03:22,420 --> 02:03:25,420
If you're at this API, try your own name.

1763
02:03:25,420 --> 02:03:27,420
Feel free to do that.

1764
02:03:27,420 --> 02:03:29,420
If you're just watching, that's fine.

1765
02:03:29,420 --> 02:03:31,420
You can try it under your own time.

1766
02:03:31,420 --> 02:03:34,420
But, you know, let me try a couple other names.

1767
02:03:34,420 --> 02:03:39,420
Let's try Sarah.

1768
02:03:39,420 --> 02:03:40,420
Check.

1769
02:03:40,420 --> 02:03:41,420
Visit.

1770
02:03:41,420 --> 02:03:42,420
See.

1771
02:03:42,420 --> 02:03:43,420
Find.

1772
02:03:43,420 --> 02:03:46,420
Let me try my own name.

1773
02:03:46,420 --> 02:03:47,420
Okay.

1774
02:03:47,420 --> 02:03:56,420
Well, I have a 7% chance of robbing the bank.

1775
02:03:56,420 --> 02:03:59,420
Based on this.

1776
02:03:59,420 --> 02:04:03,420
So this is what we're using for everything.

1777
02:04:03,420 --> 02:04:08,420
I just, you know, I don't think it's responsible for me to have a tutorial like this, Dana

1778
02:04:08,420 --> 02:04:15,420
and I agree, and not point out some of the real flaws in this.

1779
02:04:15,420 --> 02:04:20,420
Question in the back.

1780
02:04:20,420 --> 02:04:23,420
Yes?

1781
02:04:23,420 --> 02:04:26,420
Sorry.

1782
02:04:26,420 --> 02:04:35,420
The question is, I couldn't hear all of it.

1783
02:04:35,420 --> 02:04:40,420
Static data set, yes.

1784
02:04:40,420 --> 02:04:43,420
It is older, yes.

1785
02:04:43,420 --> 02:04:46,420
Yes.

1786
02:04:46,420 --> 02:04:55,420
So the question is, like, how do you correct the bias in society with a model?

1787
02:04:55,420 --> 02:04:58,420
How would you address that?

1788
02:04:58,420 --> 02:05:00,420
What does this mean?

1789
02:05:01,420 --> 02:05:06,420
So I guess the question is, how would you correct the bias in society in a model?

1790
02:05:06,420 --> 02:05:08,420
So I guess there's bias in society, that's one thing.

1791
02:05:08,420 --> 02:05:11,420
Then there's bias in the model.

1792
02:05:11,420 --> 02:05:21,420
So now in the nine minutes we have left, to address that, I have some ideas that we can discuss.

1793
02:05:21,420 --> 02:05:26,420
So given how prevalent LLMs like BERT are, do we see risks from above?

1794
02:05:26,420 --> 02:05:28,420
I think the answer is yes.

1795
02:05:28,420 --> 02:05:31,420
So what are some ways of mitigating bias?

1796
02:05:31,420 --> 02:05:34,420
It could come in in the data.

1797
02:05:34,420 --> 02:05:40,420
We could have more curated data sets instead of taking a big thing of vampire blood,

1798
02:05:40,420 --> 02:05:47,420
novels that may be published or unpublished, and Wikipedia which may or may not reflect what we wanted to in terms of society.

1799
02:05:47,420 --> 02:05:50,420
So curated data sets, that's one thing.

1800
02:05:50,420 --> 02:05:52,420
We could change the model.

1801
02:05:52,420 --> 02:05:54,420
So the way we train it, we have a certain type of loss.

1802
02:05:54,420 --> 02:06:01,420
We can reweight certain samples, say, with names that aren't as highly represented in the text corpus that we're training on.

1803
02:06:01,420 --> 02:06:03,420
That's one way of doing it.

1804
02:06:03,420 --> 02:06:09,420
Post-processing, I think it was mentioned in the back.

1805
02:06:09,420 --> 02:06:17,420
So we know chat GBT uses reinforcement learning with human guidance to kind of give us better responses in the web interface.

1806
02:06:17,420 --> 02:06:24,420
That's another post-processing way, once you have a base model, to try to fix or mitigate some of the bias that we see.

1807
02:06:24,420 --> 02:06:27,420
But this is an ongoing area of research.

1808
02:06:27,420 --> 02:06:30,420
And I think this is a really important topic.

1809
02:06:30,420 --> 02:06:34,420
I have a section here on the risks and benefits of AI.

1810
02:06:34,420 --> 02:06:36,420
I don't know if anyone's seen this picture.

1811
02:06:36,420 --> 02:06:38,420
A pope in a puffer jacket.

1812
02:06:38,420 --> 02:06:44,420
This was created by Mid Journey, an alternative to like stable diffusion and Dolly.

1813
02:06:44,420 --> 02:06:46,420
And it's fake.

1814
02:06:46,420 --> 02:06:48,420
It's fake news.

1815
02:06:48,420 --> 02:06:50,420
Credit to Pablo Xavier and Mid Journey.

1816
02:06:50,420 --> 02:06:54,420
Pablo Xavier did not want to mention his last name for fewer repercussions.

1817
02:06:56,420 --> 02:07:03,420
So I have a section, and I was hoping this would be an open discussion for those of you who want to stay and discuss amongst your neighbors.

1818
02:07:03,420 --> 02:07:13,420
Here's a selection of views on AI and how we could possibly, you know, the risks, the benefits, what can we do.

1819
02:07:13,420 --> 02:07:15,420
And I'll just call out some of them.

1820
02:07:15,420 --> 02:07:17,420
Smaller curated data sets.

1821
02:07:17,420 --> 02:07:26,420
This is by a researcher who is at Google and then I believe was fired for actually speaking out against the large language model trainings and not having curated data sets.

1822
02:07:26,420 --> 02:07:34,420
Many of you may have heard Elon Musk and other luminaries have called for a six month pause on AI.

1823
02:07:34,420 --> 02:07:38,420
No more models, more powerful than GPT-4 before we understand what we're doing.

1824
02:07:39,420 --> 02:07:43,420
Another view by Jan Lukun and Andrew Ng.

1825
02:07:43,420 --> 02:07:50,420
This is a video I watched and Jan Lukun was saying, oh, well, a lot of AI can help make better AI.

1826
02:07:50,420 --> 02:07:58,420
Better, like more AI, like hate speech and toxic abuse online can be mitigated by having better AI.

1827
02:07:58,420 --> 02:08:00,420
So that's one view.

1828
02:08:00,420 --> 02:08:02,420
And like, can we even stop AI progress?

1829
02:08:02,420 --> 02:08:04,420
Probably not.

1830
02:08:04,420 --> 02:08:06,420
I mean, I don't see anyone pausing.

1831
02:08:07,420 --> 02:08:09,420
And then risks of misinformation.

1832
02:08:09,420 --> 02:08:11,420
We saw that picture of the Pope.

1833
02:08:11,420 --> 02:08:13,420
Speed of the expense of safety.

1834
02:08:13,420 --> 02:08:17,420
The White House has a blueprint for AI regulation.

1835
02:08:17,420 --> 02:08:19,420
It's not law.

1836
02:08:19,420 --> 02:08:21,420
It's just a suggestion at this point.

1837
02:08:21,420 --> 02:08:23,420
Economic risks of AI.

1838
02:08:23,420 --> 02:08:25,420
Is chat GPT going to take our jobs?

1839
02:08:25,420 --> 02:08:27,420
Data privacy.

1840
02:08:27,420 --> 02:08:29,420
This one resonated with me.

1841
02:08:29,420 --> 02:08:31,420
A lot of artists found out their work was used to train stable diffusion.

1842
02:08:31,420 --> 02:08:37,420
Now the value of their own work is much lower because people used to use stable diffusion to generate work in their own style.

1843
02:08:37,420 --> 02:08:47,420
So if you would like to discuss with your neighbors about this, I would love to walk around and just hear what people are thinking.

1844
02:08:47,420 --> 02:08:50,420
You're also welcome to leave.

1845
02:08:50,420 --> 02:08:52,420
We're almost at time.

1846
02:08:52,420 --> 02:08:55,420
So question in the back.

1847
02:09:01,420 --> 02:09:05,420
Okay.

1848
02:09:13,420 --> 02:09:15,420
Okay.

1849
02:09:15,420 --> 02:09:19,420
So this is a resource that you can use to detect models in machine modeling models.

1850
02:09:19,420 --> 02:09:21,420
And it's called Fairlearn.

1851
02:09:21,420 --> 02:09:23,420
Fairlearn.

1852
02:09:23,420 --> 02:09:27,420
Okay.

1853
02:09:27,420 --> 02:09:29,420
Open source project by Microsoft.

1854
02:09:29,420 --> 02:09:31,420
Okay.

1855
02:09:31,420 --> 02:09:33,420
I'll close.

1856
02:09:33,420 --> 02:09:35,420
I have another question.

1857
02:09:35,420 --> 02:09:37,420
Okay.

1858
02:09:37,420 --> 02:09:39,420
So why don't we close here?

1859
02:09:39,420 --> 02:09:41,420
I'm going to walk around and talk to people who are interested.

1860
02:09:41,420 --> 02:09:43,420
But thank you very much for coming.

1861
02:09:43,420 --> 02:09:45,420
I hope you learned something.

1862
02:09:45,420 --> 02:09:47,420
Dana and I were excited to put this on.

1863
02:09:47,420 --> 02:09:49,420
And we're excited that you came.

1864
02:09:49,420 --> 02:09:51,420
Thank you.

