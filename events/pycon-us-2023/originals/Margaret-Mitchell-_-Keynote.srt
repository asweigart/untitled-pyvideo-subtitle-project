1
00:00:00,000 --> 00:00:01,840
All right.

2
00:00:01,840 --> 00:00:10,160
Now we have Margaret Mitchell, who is a researcher based, okay, I need to read my notes, sorry,

3
00:00:10,160 --> 00:00:15,720
who is a researcher focused on the ins and outs of machine learning and ethics informed

4
00:00:15,720 --> 00:00:18,560
AI development in tech.

5
00:00:18,560 --> 00:00:26,000
She is driving forward towards the work in ML machine learning development, data governance,

6
00:00:26,000 --> 00:00:30,040
AI evaluation and AI ethics.

7
00:00:30,040 --> 00:00:37,520
And she recently received the recognition of being in the list of time 100 most influential

8
00:00:37,520 --> 00:00:39,840
people in 2023.

9
00:00:39,840 --> 00:00:41,640
So welcome to the stage, Margaret.

10
00:00:41,640 --> 00:00:42,640
We are excited.

11
00:00:42,640 --> 00:00:50,120
We are honored to have you here on stage as Python US keynote speaker.

12
00:00:50,120 --> 00:00:53,960
Hi.

13
00:00:54,000 --> 00:00:56,200
Thank you for the introduction.

14
00:00:56,200 --> 00:00:57,520
Thank you all for having me.

15
00:00:57,520 --> 00:01:00,880
I really appreciate the masking, even looking out in the audience.

16
00:01:00,880 --> 00:01:03,560
I see basically all of you are wearing masks.

17
00:01:03,560 --> 00:01:04,560
Thank you for that.

18
00:01:04,560 --> 00:01:10,520
I'm going to have to give my talk today behind the podium instead of walking around because

19
00:01:10,520 --> 00:01:14,400
a few weeks ago I broke my leg.

20
00:01:14,400 --> 00:01:16,720
So it's been an interesting thing traveling.

21
00:01:16,720 --> 00:01:19,620
I highly recommend these little knee scooters.

22
00:01:19,620 --> 00:01:21,920
I was like the talk of the airport.

23
00:01:21,920 --> 00:01:26,680
Everyone was so jealous that I could get through the airport so quickly on this little thing.

24
00:01:26,680 --> 00:01:29,000
But anyway, so it goes.

25
00:01:29,000 --> 00:01:32,480
And thank you all for coming.

26
00:01:32,480 --> 00:01:39,680
So I've been programming for a long time, an uncountable number of years.

27
00:01:39,680 --> 00:01:46,520
And my first introduction to computing was on a PC in the early 90s when I was in elementary

28
00:01:46,520 --> 00:01:47,760
school.

29
00:01:47,760 --> 00:01:51,020
And my flow would go basically like this.

30
00:01:51,020 --> 00:01:53,340
You turn on the computer.

31
00:01:53,340 --> 00:01:54,340
You'd wait.

32
00:01:54,340 --> 00:01:55,340
You'd wait.

33
00:01:55,340 --> 00:01:58,940
Then eventually it would start to say that it was starting MS-DOS.

34
00:01:58,940 --> 00:02:00,220
That's when it got really exciting.

35
00:02:00,220 --> 00:02:06,300
There'd be a bunch of stuff that little me didn't really care about.

36
00:02:06,300 --> 00:02:08,740
But then you'd get to the goal.

37
00:02:08,740 --> 00:02:10,740
C colon backslash.

38
00:02:10,740 --> 00:02:15,520
I had a friend who taught me that you could clear the screen and then the world was your

39
00:02:15,520 --> 00:02:17,180
oyster.

40
00:02:17,180 --> 00:02:22,260
So my first experience with programming was Simple Echo.

41
00:02:22,260 --> 00:02:25,380
And I found this fascinating.

42
00:02:25,380 --> 00:02:28,700
And this was sort of before the internet was much of a thing.

43
00:02:28,700 --> 00:02:35,700
So I spent most of my time doing this on the computer.

44
00:02:35,700 --> 00:02:37,440
But it was pretty fun and fascinating.

45
00:02:37,440 --> 00:02:40,280
And I was just hooked.

46
00:02:40,280 --> 00:02:45,260
So by the mid-90s, the World Wide Web looked kind of like this.

47
00:02:45,260 --> 00:02:49,380
And all of its sort of 32-bit glory, I guess.

48
00:02:49,380 --> 00:02:55,300
And I was actually able to connect with this is what Amazon looked like at the time.

49
00:02:55,300 --> 00:02:58,600
Just to sort of place the time.

50
00:02:58,600 --> 00:03:03,840
People were really into HTML color and animation.

51
00:03:03,840 --> 00:03:09,780
And I was able to join America Online where I met a bunch of fellow nerds.

52
00:03:09,780 --> 00:03:14,220
And we started getting into programming websites together.

53
00:03:14,220 --> 00:03:20,740
So that brought me to my first experience with programming where I learned HTML5 at

54
00:03:20,740 --> 00:03:21,740
the time.

55
00:03:21,740 --> 00:03:29,680
And was able to create, as you can see, much more mature work than I did when I was younger.

56
00:03:29,680 --> 00:03:35,560
So by the late 90s, I was very much into website design and object-oriented programming.

57
00:03:35,560 --> 00:03:36,700
This was all self-taught.

58
00:03:36,700 --> 00:03:40,080
It wasn't really much of a thing at school.

59
00:03:40,080 --> 00:03:44,240
But I used a service called Web Monkey to teach myself how to make interactive websites

60
00:03:44,240 --> 00:03:48,040
like Ask Satan, which would give you your fortune.

61
00:03:48,040 --> 00:03:52,620
And other little games that I would give to my friends as gifts, like here's a little

62
00:03:52,620 --> 00:03:54,620
website for you.

63
00:03:54,620 --> 00:04:00,040
Although it was probably a lot more fun for me to do that than for them.

64
00:04:00,040 --> 00:04:09,200
So by 2000s, in roughly 2000, I began getting into compiled languages, starting with C++.

65
00:04:09,880 --> 00:04:14,760
After I had nailed the fundamentals of Heapsort, or whatever it is, I started making games

66
00:04:14,760 --> 00:04:18,080
again using Visual C++.

67
00:04:18,080 --> 00:04:22,600
And was like exciting to my nerd friends for a hot minute.

68
00:04:22,600 --> 00:04:28,960
Eventually I landed my first real job where the language was Tickle.

69
00:04:28,960 --> 00:04:32,140
Basically Tickle is something that's used in speech processing.

70
00:04:32,140 --> 00:04:37,520
And my job was working in natural language processing and speech processing, trying to

71
00:04:37,560 --> 00:04:41,800
do things like diagnose whether someone might have mild cognitive impairment, which is a

72
00:04:41,800 --> 00:04:47,240
precursor to Alzheimer's, by using their speech stream.

73
00:04:47,240 --> 00:04:54,520
So I got pretty familiar with Tickle, as well as scripting languages and Unix utilities

74
00:04:54,520 --> 00:04:59,520
like SAD and OCK, which I still use to this day.

75
00:04:59,520 --> 00:05:03,520
For those of you who aren't familiar with OCK, highly recommend.

76
00:05:03,520 --> 00:05:08,000
And you can get a lot of language text processing stuff done.

77
00:05:08,000 --> 00:05:09,000
One guy agrees.

78
00:05:09,000 --> 00:05:12,480
You can get a lot of stuff done super quickly.

79
00:05:12,480 --> 00:05:15,640
It's really handy.

80
00:05:15,640 --> 00:05:21,360
When I was ready to go to grad school, I had the opportunity of figuring out which language

81
00:05:21,360 --> 00:05:22,680
I wanted to program in.

82
00:05:22,680 --> 00:05:28,520
They were pretty open about what languages you could use for different programming assignments.

83
00:05:28,520 --> 00:05:34,080
And so this was a pretty straightforward decision for me.

84
00:05:34,080 --> 00:05:38,840
I was embarrassed to say that my primary language was Tickle.

85
00:05:38,840 --> 00:05:43,360
And so I decided that my primary language would become Python, because Tickle sounds

86
00:05:43,360 --> 00:05:45,440
lame and Python sounds cool.

87
00:05:45,440 --> 00:05:50,160
And that was my complete reason for getting into Python.

88
00:05:50,160 --> 00:05:52,640
But I found it super intuitive.

89
00:05:52,640 --> 00:05:56,000
I just naturally took to it.

90
00:05:56,000 --> 00:06:01,320
And I was sort of off and running in machine learning at that point.

91
00:06:01,320 --> 00:06:08,600
In 2011, I used Python to create one of the first image description systems.

92
00:06:08,600 --> 00:06:16,120
So this is somewhat relevant now as generative image and text to image models are coming

93
00:06:16,120 --> 00:06:19,800
into sort of popular understanding.

94
00:06:19,800 --> 00:06:22,480
At the time, it was really niche.

95
00:06:22,480 --> 00:06:24,720
Computer vision systems didn't work.

96
00:06:24,920 --> 00:06:30,320
So what you had to do was essentially take the things that the vision system had the

97
00:06:30,320 --> 00:06:37,600
highest scores for and intersect it with the language that was most likely and essentially

98
00:06:37,600 --> 00:06:41,200
use that to generate what the output was.

99
00:06:41,200 --> 00:06:47,400
And this was when I discovered YAML, yet another micro-language, which we used to pass around

100
00:06:47,600 --> 00:06:56,800
different information from the computer vision system and reading it in as JSON.

101
00:06:56,800 --> 00:07:00,040
Throughout grad school, I picked up some more things here and there, but primarily was working

102
00:07:00,040 --> 00:07:06,280
in Python and with the NLTK for language processing.

103
00:07:06,280 --> 00:07:11,440
I went on to get a postdoc where I primarily coded in Java and Perl.

104
00:07:11,440 --> 00:07:12,440
I really hate Perl.

105
00:07:12,440 --> 00:07:13,440
I never used Perl.

106
00:07:13,440 --> 00:07:14,440
I really didn't like it.

107
00:07:14,440 --> 00:07:16,440
But there you go.

108
00:07:16,480 --> 00:07:21,400
Anyway, so I went on the rest of my career.

109
00:07:21,400 --> 00:07:22,760
You pick up things as you go.

110
00:07:22,760 --> 00:07:28,160
But it's basically just been learning Python packages, although I've had reasons to learn

111
00:07:28,160 --> 00:07:30,080
other languages.

112
00:07:30,080 --> 00:07:33,400
Throughout industry, they use Python.

113
00:07:33,400 --> 00:07:38,960
So at Microsoft, I had to do some C sharp, but most of my prototyping was in Python.

114
00:07:38,960 --> 00:07:44,320
Google was all Python, TensorFlow, Scikit-learn, and Pandas.

115
00:07:44,320 --> 00:07:48,720
Then recently, I've joined Hugging Face, which was using Streamlit fairly heavily when

116
00:07:48,720 --> 00:07:50,400
I joined.

117
00:07:50,400 --> 00:07:53,360
And now Gradio actually joined us.

118
00:07:53,360 --> 00:07:59,560
So Gradio has become our main way of putting out demos.

119
00:07:59,560 --> 00:08:07,640
But I wanted to give this timeline a bit to talk about some of the things that have emerged

120
00:08:07,640 --> 00:08:14,200
for me as most important in computer science and in machine learning, and now what is called

121
00:08:14,280 --> 00:08:15,280
AI.

122
00:08:15,280 --> 00:08:19,080
It's been a little bit of a challenge for people to work in machine learning to start

123
00:08:19,080 --> 00:08:20,080
saying they work in AI.

124
00:08:20,080 --> 00:08:22,520
It's like a little bit roll your eyes.

125
00:08:22,520 --> 00:08:27,160
So I might use them interchangeably, but I mean essentially the same thing.

126
00:08:27,160 --> 00:08:33,240
So one of the big turning points in my life and how I came to work on ethics in machine

127
00:08:33,240 --> 00:08:37,040
learning was during my time at Microsoft.

128
00:08:37,040 --> 00:08:41,360
So at Microsoft, I continued to work on image description.

129
00:08:41,400 --> 00:08:47,640
At this time, it was deep learning had just sort of been proven to work well in computer

130
00:08:47,640 --> 00:08:49,320
vision.

131
00:08:49,320 --> 00:08:56,000
And then recurrent neural networks on the language processing side seemed fairly promising.

132
00:08:56,000 --> 00:09:01,040
So I worked also still in Python in putting together the output of convolutional neural

133
00:09:01,040 --> 00:09:05,480
networks, which was on the vision side, to recurrent neural networks on the language

134
00:09:05,480 --> 00:09:13,640
side and ended up being able to create image description systems that surpassed human

135
00:09:13,640 --> 00:09:20,280
performance as measured by blue score, which really speaks to the difficulty in automatic

136
00:09:20,280 --> 00:09:25,400
metrics and how they don't actually quite capture what we want.

137
00:09:25,400 --> 00:09:30,440
And I was able to work on creating a language for people or a program for people who are

138
00:09:30,440 --> 00:09:33,240
blind called Seeing AI.

139
00:09:33,240 --> 00:09:36,760
And one of the things that was happening at this time was I was trying to think about,

140
00:09:36,760 --> 00:09:39,400
OK, I'm working on image description.

141
00:09:39,400 --> 00:09:40,760
Why am I doing this?

142
00:09:40,760 --> 00:09:41,760
Right?

143
00:09:41,760 --> 00:09:43,440
Like, what is the image captioning task?

144
00:09:43,440 --> 00:09:44,800
Who needs that?

145
00:09:44,800 --> 00:09:49,720
And one of the obvious answers in discussions with other academics was, oh, this is useful

146
00:09:49,720 --> 00:09:50,800
for blind people.

147
00:09:50,800 --> 00:09:53,440
You could like caption things for them super helpful.

148
00:09:53,440 --> 00:09:57,160
So I thought, OK, I'll work with blind people and ask them.

149
00:09:57,160 --> 00:09:59,200
And it turned out not helpful.

150
00:09:59,200 --> 00:10:03,720
The kinds of things that people who are blind needed was fundamentally different than the

151
00:10:03,720 --> 00:10:08,320
kinds of things being optimized for in the image captioning task.

152
00:10:08,320 --> 00:10:12,560
And so this was really a wake-up call to me about the discrepancy between the kind of

153
00:10:12,560 --> 00:10:19,640
academic tasks that we develop as a way to play around with new models and new technology

154
00:10:19,640 --> 00:10:24,360
and the real world use cases and how they can be quite different.

155
00:10:24,360 --> 00:10:29,680
So for example, someone who's blind doesn't need to know that they're holding a soda can.

156
00:10:29,680 --> 00:10:31,600
They know they're holding a soda can.

157
00:10:31,600 --> 00:10:34,200
They need to know what soda it is.

158
00:10:34,200 --> 00:10:35,200
Right?

159
00:10:35,200 --> 00:10:41,800
They need to know if a tube is toothpaste or like preparation H. It's these sorts of

160
00:10:41,800 --> 00:10:42,800
things.

161
00:10:42,800 --> 00:10:43,800
Right?

162
00:10:43,800 --> 00:10:50,200
OCR, logo, these kinds of things were fundamentally different than what image captioning, the

163
00:10:50,200 --> 00:10:54,600
field of image captioning was really thinking about, but really got at what they actually

164
00:10:54,600 --> 00:10:55,880
needed.

165
00:10:55,880 --> 00:10:59,400
Things like counting the number of steps when you're going downstairs.

166
00:10:59,400 --> 00:11:00,400
Right?

167
00:11:00,400 --> 00:11:05,240
Completely under the radar, over the radar, over the heads of people who are working on

168
00:11:05,240 --> 00:11:08,160
these sorts of things.

169
00:11:08,160 --> 00:11:14,360
I was asked if I could talk about gender as well, being like a woman in machine learning.

170
00:11:14,360 --> 00:11:20,680
And so I'll just sort of say for this slide that it was not a pleasant experience to work

171
00:11:20,680 --> 00:11:21,680
on this.

172
00:11:21,680 --> 00:11:23,000
It was super territorial.

173
00:11:23,000 --> 00:11:24,000
Right?

174
00:11:24,000 --> 00:11:26,800
So deep learning was just becoming a thing.

175
00:11:26,800 --> 00:11:28,960
There were all these competitions.

176
00:11:28,960 --> 00:11:36,040
I maybe was the first woman to win a deep learning competition.

177
00:11:36,040 --> 00:11:39,120
I don't know, because people were super mean about it.

178
00:11:39,120 --> 00:11:42,480
People at Microsoft refused to share it.

179
00:11:42,680 --> 00:11:49,160
I did not see that happening with my male colleagues and men in other companies as well.

180
00:11:49,160 --> 00:11:54,560
So I just kind of wanted to put this out there that it's possible that you're a woman in

181
00:11:54,560 --> 00:12:03,080
tech and if so, if you feel like everybody hates you and isn't listening to you and is

182
00:12:03,080 --> 00:12:08,880
not respecting you similarly to your peers, this is what happens.

183
00:12:08,880 --> 00:12:09,880
Right?

184
00:12:09,880 --> 00:12:10,880
You're not alone.

185
00:12:11,040 --> 00:12:13,000
There are so many women.

186
00:12:13,000 --> 00:12:17,760
And you can continue on and eventually get to a place where you're recognized.

187
00:12:17,760 --> 00:12:20,760
But it's a lot more hard work than it is for men.

188
00:12:20,760 --> 00:12:23,800
At least it has been for me.

189
00:12:23,800 --> 00:12:24,800
So I'll just say that.

190
00:12:24,800 --> 00:12:27,800
I think that's nice of you.

191
00:12:27,800 --> 00:12:40,160
But in this work, this sort of image description, image captioning work, my switch to working

192
00:12:40,160 --> 00:12:45,800
with people who are blind really changed my way of looking at research, which is that

193
00:12:45,800 --> 00:12:51,640
instead of focusing on the tasks defined for some conference or some competition, think

194
00:12:51,640 --> 00:12:58,080
through what the technology that you're working on now could look like in five years, ten

195
00:12:58,080 --> 00:13:01,120
years, fifteen years if it were to help people.

196
00:13:01,120 --> 00:13:02,120
Right?

197
00:13:02,120 --> 00:13:05,640
How should it evolve to get to some helping people stage?

198
00:13:05,680 --> 00:13:12,040
So for me, with image description, I could see that some foreseeable positive future

199
00:13:12,040 --> 00:13:14,560
would be helping people who are blind.

200
00:13:14,560 --> 00:13:18,600
And so I worked backwards from there to figure out what I needed to work on right then.

201
00:13:18,600 --> 00:13:24,240
And it was fundamentally different than what sort of the academy was defining.

202
00:13:24,240 --> 00:13:29,320
And this meant that now when I approached research, I was looking at it in terms of

203
00:13:29,320 --> 00:13:33,200
the long picture, the long story.

204
00:13:33,200 --> 00:13:34,200
Where's this going to go over time?

205
00:13:34,440 --> 00:13:36,240
Who's it going to affect?

206
00:13:36,240 --> 00:13:40,200
And using that to influence the kind of things that I worked on.

207
00:13:40,200 --> 00:13:47,120
And once you're doing that, once you're working on your projects from the perspective of foresight,

208
00:13:47,120 --> 00:13:50,480
then you're already working in the realm of ethical AI.

209
00:13:50,480 --> 00:13:51,480
Right?

210
00:13:51,480 --> 00:13:57,160
Because fundamental to working on ethics in AI is being able to think through the different

211
00:13:57,160 --> 00:14:03,440
use cases, think through how the technology can be used positively and negatively, including

212
00:14:03,480 --> 00:14:05,480
by malicious actors.

213
00:14:05,480 --> 00:14:13,080
And this fundamental thing of foresight is really not prioritized in tech circles as

214
00:14:13,080 --> 00:14:14,960
much as it could be.

215
00:14:14,960 --> 00:14:19,880
And it fundamentally changes the kinds of things that we work on.

216
00:14:19,880 --> 00:14:26,560
This also spoke to my desire to include those most likely to benefit from the technology

217
00:14:26,560 --> 00:14:27,560
in its creation.

218
00:14:27,560 --> 00:14:31,840
And I still do that to this day to the extent I can.

219
00:14:31,880 --> 00:14:35,200
And it again fundamentally changes the kind of things that you do.

220
00:14:35,200 --> 00:14:43,360
And I would argue helps to shape the path of development towards better outcomes.

221
00:14:43,360 --> 00:14:49,320
So shortly after image description work, I started working on storytelling.

222
00:14:49,320 --> 00:14:54,120
So here's where you give the system a sequence of images and it spits out a corresponding

223
00:14:54,120 --> 00:14:56,320
story about them.

224
00:14:56,320 --> 00:15:04,040
Here are some example stories generated by different versions of the system at the time.

225
00:15:04,040 --> 00:15:08,480
And we found that it had this problem that it thought everything was awesome.

226
00:15:08,480 --> 00:15:12,000
We called it the everything is awesome problem.

227
00:15:12,000 --> 00:15:16,360
So you know, we could show it, for example, this is actually me and my friends at Thanksgiving

228
00:15:16,360 --> 00:15:18,040
this past year.

229
00:15:18,040 --> 00:15:21,680
And it would generate a story like everyone was there, the food was awesome, the food

230
00:15:21,680 --> 00:15:24,680
was delicious, and it was awesome.

231
00:15:24,680 --> 00:15:29,280
And it turns out that this is a reflection of the kinds of things that people say when

232
00:15:29,280 --> 00:15:32,040
they're sharing photos, right?

233
00:15:32,040 --> 00:15:38,880
So the reason for this is that people tend to share pictures of parties, right, of celebrations,

234
00:15:38,880 --> 00:15:46,080
of marriages, these kinds of things, and talk about how fun and awesome it is.

235
00:15:46,080 --> 00:15:49,380
People also tend to share things that are beautiful and great.

236
00:15:49,380 --> 00:15:52,200
So like sunsets or views.

237
00:15:52,800 --> 00:15:58,360
And that makes a lot of sense if you think about it, since people tend not to share tons

238
00:15:58,360 --> 00:16:00,600
of pictures of sad things or tragedy.

239
00:16:00,600 --> 00:16:06,160
You know, when's the last time you saw a bunch of photos from a funeral, right?

240
00:16:06,160 --> 00:16:11,080
These personal experiences that are dark and sad aren't things we tend to take tons of

241
00:16:11,080 --> 00:16:14,440
photos of and talk about online.

242
00:16:14,440 --> 00:16:20,080
So systems that are learning from this kind of data learn things like beautiful colors

243
00:16:20,080 --> 00:16:24,520
in a sky, and it learns that essentially everything is awesome.

244
00:16:24,520 --> 00:16:27,640
When in doubt, everything is awesome.

245
00:16:27,640 --> 00:16:33,640
And the moment, the moment that I realized my life was changing in my research was giving

246
00:16:33,640 --> 00:16:36,440
it this exact sequence.

247
00:16:36,440 --> 00:16:42,600
And I'll pause a moment to let you read it.

248
00:16:42,600 --> 00:16:49,760
But basically, I gave it this sequence of the, what's the Hempstead blast, and it looked

249
00:16:49,840 --> 00:16:52,960
at it and thought it was great.

250
00:16:52,960 --> 00:17:01,160
And I realized that if we started making systems like this autonomous, capable of acting out

251
00:17:01,160 --> 00:17:07,640
things, then in the pursuit of beauty and things that it mistakenly thinks are good,

252
00:17:07,640 --> 00:17:10,640
it could kill people, right?

253
00:17:10,640 --> 00:17:16,280
It doesn't understand the fundamentals of mortality, and it doesn't understand a lot

254
00:17:16,280 --> 00:17:21,880
of the things that we don't tend to share online, private information.

255
00:17:21,880 --> 00:17:27,600
And so looking at it, I could understand sort of why it made these mistakes, right?

256
00:17:27,600 --> 00:17:35,520
So you see this is a great view, it's trained on views, it's learned colors are beautiful

257
00:17:35,520 --> 00:17:39,400
when you take a picture of a sky, it's a good thing.

258
00:17:39,400 --> 00:17:42,540
And this was something that no one else was thinking about.

259
00:17:42,540 --> 00:17:46,820
And as I tried to sort of get people interested in this, it was sort of a hard truth that

260
00:17:46,820 --> 00:17:48,860
no one really cared.

261
00:17:48,860 --> 00:17:53,620
So it was sort of like, well, if no one cares, then I guess I have to, right?

262
00:17:53,620 --> 00:17:55,460
Like someone has to.

263
00:17:55,460 --> 00:18:00,820
So I started paying a lot more attention to the way that data affects everything that

264
00:18:00,820 --> 00:18:06,000
happens afterwards in the machine learning pipeline, and then what happens when it's

265
00:18:06,000 --> 00:18:08,820
deployed in the world.

266
00:18:08,860 --> 00:18:14,060
So this sort of problem occurs throughout text that we scrape online.

267
00:18:14,060 --> 00:18:19,780
And online text is the main source of models that we train for AI right now.

268
00:18:19,780 --> 00:18:27,780
So one study showed, for example, that counts in an analysis of web text were not reflective

269
00:18:27,780 --> 00:18:31,340
of the proportions of these things in real life.

270
00:18:31,340 --> 00:18:39,020
So for example, murdering was considered or would be learned to be much more common

271
00:18:39,020 --> 00:18:40,780
than blinking, right?

272
00:18:40,780 --> 00:18:42,220
There are lots of things that we don't say.

273
00:18:42,220 --> 00:18:45,180
We don't tend to say, I'm blinking, I'm breathing.

274
00:18:45,180 --> 00:18:49,860
I'm sure many of you are blinking right now.

275
00:18:49,860 --> 00:18:54,300
But things like murdering, when that happens, that is talked about, it makes news, this

276
00:18:54,300 --> 00:18:55,420
kind of thing.

277
00:18:55,420 --> 00:19:00,460
And so this is a kind of bias that could creep into your system affecting what it learns

278
00:19:00,460 --> 00:19:02,580
and understands.

279
00:19:02,580 --> 00:19:05,860
This problem is called reporting bias.

280
00:19:05,860 --> 00:19:14,860
It's one of the many biases that fundamentally shape what language models used in AI learn.

281
00:19:14,860 --> 00:19:21,780
And you can see how this interplays with the machine learning pipeline by sort of taking

282
00:19:21,780 --> 00:19:27,240
it into different stages and paying attention to how these things propagate.

283
00:19:27,240 --> 00:19:31,640
So at this point, I think probably the machine learning pipeline is fairly familiar to this

284
00:19:31,640 --> 00:19:33,680
audience.

285
00:19:33,680 --> 00:19:37,280
First you collect the data, then you train and test a model.

286
00:19:37,280 --> 00:19:43,420
You can add additional post-processing, such as selecting among a ranked list in order

287
00:19:43,420 --> 00:19:46,800
for the model to output an eventual answer.

288
00:19:46,800 --> 00:19:50,040
And then people see the output.

289
00:19:50,040 --> 00:19:57,160
But before we even begin the process, the data itself encodes a subset of human perspectives

290
00:19:57,160 --> 00:20:00,100
and a skewed set of human perspectives.

291
00:20:00,100 --> 00:20:05,440
What people choose to talk about and how they talk about it based on their experiences is

292
00:20:05,440 --> 00:20:13,080
shaped fundamentally differently in text than it is in our real world interactions.

293
00:20:13,080 --> 00:20:19,280
And the idea that a data set can be unbiased misunderstands what a data set is.

294
00:20:19,280 --> 00:20:22,880
A data set is a collection of bias.

295
00:20:22,880 --> 00:20:24,960
Where we collect it from is a bias.

296
00:20:24,960 --> 00:20:27,840
The time points that we collect it from is a bias.

297
00:20:27,840 --> 00:20:31,060
As well as the people who are represented, that's a bias.

298
00:20:31,060 --> 00:20:36,720
It's not everything in the world for all time ever or throughout the universe.

299
00:20:36,720 --> 00:20:42,400
Some sort of random sampling of everything of all time might be something like unbiased.

300
00:20:42,400 --> 00:20:43,640
But that's not what a data set is.

301
00:20:43,640 --> 00:20:46,520
A data set is a snapshot of something.

302
00:20:46,520 --> 00:20:52,240
So a data set by definition is a biased bundle in natural language processing, a biased bundle

303
00:20:52,240 --> 00:20:53,880
of text.

304
00:20:53,880 --> 00:20:59,160
And this sort of basic thing really escapes people's understanding, I think, in machine

305
00:20:59,160 --> 00:21:00,160
learning.

306
00:21:00,160 --> 00:21:04,120
But if you can sort of wrap your head around that, this isn't a representation of the world.

307
00:21:04,120 --> 00:21:08,000
It is a biased, skewed snapshot of the world.

308
00:21:08,000 --> 00:21:13,600
Then I think it's a little bit easier to prioritize some of the things that most influence people

309
00:21:13,760 --> 00:21:19,400
and will most affect people when machine learning systems are deployed.

310
00:21:19,400 --> 00:21:24,520
So within the data itself, before it's even collected, there are things like reporting

311
00:21:24,520 --> 00:21:25,720
bias.

312
00:21:25,720 --> 00:21:29,060
There's racism, there's sexism, there's stereotyping.

313
00:21:29,060 --> 00:21:35,600
There's underrepresentation of various viewpoints as a function of it being a sample and where

314
00:21:35,600 --> 00:21:37,640
it's sampled from.

315
00:21:37,640 --> 00:21:41,240
And then there's further biases in the way the data is collected.

316
00:21:41,240 --> 00:21:45,640
So this is where things like selection bias and sampling bias really come in.

317
00:21:45,640 --> 00:21:47,360
And the way it can be annotated.

318
00:21:47,360 --> 00:21:52,320
And this is where the sorts of compensation we provide to annotators really matter.

319
00:21:52,320 --> 00:21:57,000
The design of the annotation task, these kinds of things.

320
00:21:57,000 --> 00:22:02,160
So all of these biases are sort of injected at the start and then propagate throughout

321
00:22:02,160 --> 00:22:08,400
the rest of the system as the system then learns all of these different things.

322
00:22:08,400 --> 00:22:12,240
Once you're training the model, there's a whole bunch of other kinds of biases you

323
00:22:12,240 --> 00:22:13,680
can pick up.

324
00:22:13,680 --> 00:22:18,640
So this is where things like overfitting and underfitting come in, where your data and

325
00:22:18,640 --> 00:22:22,800
your loss function aren't really well matched to one another.

326
00:22:22,800 --> 00:22:27,600
There's also default and anchoring effects where you tend to just sort of do what people

327
00:22:27,600 --> 00:22:32,840
do without really thinking through why you're making those choices.

328
00:22:32,840 --> 00:22:38,200
And doing the default or anchoring on just what you know without really thinking through

329
00:22:38,200 --> 00:22:45,440
why fundamentally affects the kinds of issues that continue to propagate in machine learning

330
00:22:45,440 --> 00:22:51,520
systems as people just sort of do what the last person did with this default effect.

331
00:22:51,520 --> 00:22:55,240
From there, with the generation, there's all kinds of further biases.

332
00:22:55,240 --> 00:22:57,600
So confirmation bias.

333
00:22:57,600 --> 00:23:06,240
I think we can see this right now with Microsoft and Google sort of insisting that these chatbots

334
00:23:06,400 --> 00:23:09,280
they've put out are useful for search.

335
00:23:09,280 --> 00:23:10,620
They're not useful for search.

336
00:23:10,620 --> 00:23:15,900
They're useful for lots of things like creative writing, learning English.

337
00:23:15,900 --> 00:23:17,620
Not great for search.

338
00:23:17,620 --> 00:23:21,940
But if this is your sort of way of getting money, then you kind of have a confirmation

339
00:23:21,940 --> 00:23:25,560
bias in trying to see like, oh yeah, this chat thing will totally work.

340
00:23:25,560 --> 00:23:30,440
And the fact that it's not factual, well, you know, we'll figure it out later.

341
00:23:30,440 --> 00:23:33,600
So this is the confirmation bias effect.

342
00:23:33,600 --> 00:23:35,080
And then people see the output.

343
00:23:35,200 --> 00:23:38,080
This is when automation bias comes into play.

344
00:23:38,080 --> 00:23:39,440
There's a bunch of different biases.

345
00:23:39,440 --> 00:23:45,160
But one in particular that's relevant right now is the fact that people tend to agree

346
00:23:45,160 --> 00:23:51,720
with or believe outputs of systems that are automated, even in light of evidence that

347
00:23:51,720 --> 00:23:54,420
what's going on isn't true.

348
00:23:54,420 --> 00:24:01,020
Because it comes out of a system, a very biased system, because there's math involved, right?

349
00:24:01,020 --> 00:24:05,040
There tends to be some perception that it's objective or correct.

350
00:24:05,040 --> 00:24:10,200
And so we'll tend to do things or believe things that aren't actually appropriate or

351
00:24:10,200 --> 00:24:14,320
correct based on the output of an automated system.

352
00:24:14,320 --> 00:24:19,480
And from there, the sort of actions we take, or in the case of deep learning models for

353
00:24:19,480 --> 00:24:22,000
language, what we then say online.

354
00:24:22,000 --> 00:24:27,520
So we have a recommendation system that gives us tweets and then the kinds of things that

355
00:24:27,520 --> 00:24:29,360
we respond to, right?

356
00:24:29,360 --> 00:24:34,260
This then can be swept up in a further machine learning model, further amplifying these sorts

357
00:24:34,260 --> 00:24:37,380
of effects further.

358
00:24:37,380 --> 00:24:41,700
And I like to call this bias laundering, in large part because I made this GIF and I'm

359
00:24:41,700 --> 00:24:43,700
really proud of it and I like to share it.

360
00:24:43,700 --> 00:24:45,700
But right.

361
00:24:45,700 --> 00:24:47,700
Okay.

362
00:24:47,700 --> 00:24:49,700
Thank you, guys.

363
00:24:49,700 --> 00:24:53,700
You're like a very nice crowd.

364
00:24:53,700 --> 00:24:58,500
I usually talk at other more serious conferences, I guess.

365
00:24:58,500 --> 00:24:59,500
You guys are really laid back.

366
00:24:59,500 --> 00:25:02,220
It's wonderful.

367
00:25:02,220 --> 00:25:04,940
So cool.

368
00:25:04,940 --> 00:25:09,900
So basically, one of the punch lines here is that there's no such thing as having a

369
00:25:09,900 --> 00:25:13,680
value neutral way to develop systems.

370
00:25:13,680 --> 00:25:18,020
If you don't think about values, if you have the default effect where you're just not thinking

371
00:25:18,020 --> 00:25:21,460
about it, that doesn't mean that there aren't values being encoded.

372
00:25:21,460 --> 00:25:25,820
It just means that it's these biases that we already know about, right?

373
00:25:25,820 --> 00:25:28,500
You're just letting them further propagate.

374
00:25:28,500 --> 00:25:35,420
When the input data set defines what we care about for the model, if the data set doesn't

375
00:25:35,420 --> 00:25:42,180
represent appropriately populations in the solution space and you deploy it, then you're

376
00:25:42,180 --> 00:25:50,140
making the value judgment that some populations should receive worse performance.

377
00:25:50,140 --> 00:25:56,420
And so this is a continued problem and I wanted to talk about this in part to encourage all

378
00:25:56,420 --> 00:25:57,860
of you who care about data.

379
00:25:57,860 --> 00:26:01,500
This has been an ongoing thing in the conference to keep working on it.

380
00:26:01,500 --> 00:26:05,500
There's so much work to be done and lots of blind spots right now in the sort of things

381
00:26:05,500 --> 00:26:08,140
that are important.

382
00:26:08,140 --> 00:26:12,540
And I like to call this sort of situation the data bottleneck.

383
00:26:12,540 --> 00:26:18,620
It's the touch point of information flow from human thought to AI, but it's the source of

384
00:26:18,620 --> 00:26:21,340
an incredible amount of issues.

385
00:26:21,340 --> 00:26:27,580
In particular, there's not a way to have something like the most values or the ethical thing.

386
00:26:28,100 --> 00:26:33,460
What ethics does is it talks about how to think through different values and prioritize

387
00:26:33,460 --> 00:26:34,460
among them.

388
00:26:34,460 --> 00:26:37,380
And there are tradeoffs in all decisions.

389
00:26:37,380 --> 00:26:41,660
So it goes back to what your values are or what the values are of the organization you're

390
00:26:41,660 --> 00:26:43,660
working in.

391
00:26:43,660 --> 00:26:47,580
For example, we might want to release open data sets, right?

392
00:26:47,580 --> 00:26:52,940
It helps with advancing AI, benchmarking, reproducibility.

393
00:26:53,020 --> 00:27:01,500
This comes with tradeoffs of lack of consent, perpetuating problematic ideas, the idea that

394
00:27:01,500 --> 00:27:07,980
all the data somehow represents people correctly when it doesn't, privacy, licensing issues,

395
00:27:07,980 --> 00:27:13,500
issues of compensation, consent, and credit for the people who created the data.

396
00:27:13,500 --> 00:27:18,100
And so these are just some of the things that are at play when we sort of have the default

397
00:27:18,180 --> 00:27:24,180
approach of releasing our data sets without sort of additional considerations.

398
00:27:24,180 --> 00:27:27,340
Not to say it's all bad, but to say that there's tensions.

399
00:27:27,340 --> 00:27:31,500
Anytime we make a decision, there are pros and cons.

400
00:27:31,500 --> 00:27:33,740
You might want to have a data set that's diverse.

401
00:27:33,740 --> 00:27:38,100
This has come up sometimes, trying to have a reasonable representation of different sorts

402
00:27:38,100 --> 00:27:40,380
of populations.

403
00:27:40,380 --> 00:27:46,100
But that comes up against tensions of exploitation and stereotyping.

404
00:27:46,100 --> 00:27:48,620
So I'll give an example of this.

405
00:27:48,620 --> 00:27:53,220
I really enjoy showing bad headlines of Google, because Google was mean to me.

406
00:27:53,220 --> 00:27:55,460
So anytime I have a talk, I try and slide one in.

407
00:27:55,460 --> 00:27:56,460
So this is this one.

408
00:27:56,460 --> 00:28:01,460
Cheers for bad headlines for Google.

409
00:28:01,460 --> 00:28:02,460
Okay.

410
00:28:02,460 --> 00:28:07,500
But the point here, so basically, Google was trying to work on the Pixel 4 face unlock,

411
00:28:07,500 --> 00:28:08,500
right?

412
00:28:08,500 --> 00:28:10,380
You look at your phone, it unlocks.

413
00:28:10,380 --> 00:28:15,940
And they were finding that it worked disproportionately well for people with lighter faces than people

414
00:28:15,940 --> 00:28:17,420
with darker faces.

415
00:28:17,420 --> 00:28:20,100
So the idea was, okay, we need more diverse data.

416
00:28:20,100 --> 00:28:26,660
We'll go out into the world and collect images of black faces or people with black faces.

417
00:28:26,660 --> 00:28:29,900
But it was really the sort of objectifying, like we just need the face.

418
00:28:29,900 --> 00:28:32,780
The person doesn't matter, right?

419
00:28:32,780 --> 00:28:38,500
And what ended up happening was a stereotype effect where apparently the people who were

420
00:28:38,500 --> 00:28:42,540
interested in doing this, working on doing this, thought that the best place to go to

421
00:28:43,300 --> 00:28:51,300
black faces would be in Atlanta homeless areas where there's a lot of homeless people.

422
00:28:51,300 --> 00:28:55,340
And they didn't really give a lot of information to the people about what they were doing or

423
00:28:55,340 --> 00:28:56,340
why.

424
00:28:56,340 --> 00:29:04,060
They paid them five bucks for a multibillion dollar company that would be benefiting from

425
00:29:04,060 --> 00:29:05,100
this, right?

426
00:29:05,100 --> 00:29:09,180
So like, yes, diversity, but also massive exploitation, right?

427
00:29:10,060 --> 00:29:13,860
This is the kind of tension that comes into play when you're trying to optimize for one

428
00:29:13,860 --> 00:29:18,260
value and aren't really thinking about the larger picture.

429
00:29:18,260 --> 00:29:19,260
Right.

430
00:29:19,260 --> 00:29:25,820
So part of your choice when you develop and share data is to identify which of these are

431
00:29:25,820 --> 00:29:30,460
your priorities and the pros and cons of different approaches.

432
00:29:30,460 --> 00:29:35,220
If you follow a default bias, just doing what everyone is doing, then the state of the art

433
00:29:35,220 --> 00:29:38,140
means hitting on all of these issues.

434
00:29:38,140 --> 00:29:42,540
Like amplification, creating dual use effects, exploitation, licensing and privacy issues

435
00:29:42,540 --> 00:29:43,540
and stereotyping.

436
00:29:43,540 --> 00:29:46,460
That's the default, right?

437
00:29:46,460 --> 00:29:52,140
You all can change this, but this is the state of the art right now.

438
00:29:52,140 --> 00:29:57,620
So one of the key solutions to this is operationalizing data development.

439
00:29:57,620 --> 00:30:02,180
I think this has come up in the conference already, but essentially data development

440
00:30:02,180 --> 00:30:12,700
within machine learning is treated as an unexciting, unimportant, sort of silly task.

441
00:30:12,700 --> 00:30:17,580
Even though data is the source of everything else that happens afterwards.

442
00:30:17,580 --> 00:30:22,980
So I've done a lot of work on what it means to have sort of responsible data set development.

443
00:30:22,980 --> 00:30:27,000
And it actually turns out that the way to do this is by having a ton of roles.

444
00:30:27,000 --> 00:30:33,160
So instead of one data scientist who's in charge of collecting and processing and then

445
00:30:33,160 --> 00:30:37,800
reprocessing and then going back because there was an issue and da da da, there actually

446
00:30:37,800 --> 00:30:44,800
should be a whole variety of people and steps that go into the project.

447
00:30:44,800 --> 00:30:50,420
So a data set requirement specification, what should your data set have in it?

448
00:30:50,420 --> 00:30:52,840
And this is where foresight is really useful.

449
00:30:53,680 --> 00:30:59,920
Given this kind of system I'm trying to create, what sort of data set is best set to meet

450
00:30:59,920 --> 00:31:04,360
goals of positive uses in five years and ten years?

451
00:31:04,360 --> 00:31:11,160
A data set design document, which talks about sort of how you collect the data, the approaches

452
00:31:11,160 --> 00:31:12,160
there.

453
00:31:12,160 --> 00:31:17,640
The data set implementation diary, which is talking about what you actually end up implementing

454
00:31:17,640 --> 00:31:19,680
and why.

455
00:31:19,680 --> 00:31:25,360
A data set testing report, so this is stuff about understanding the data more, analyzing

456
00:31:25,360 --> 00:31:27,800
it, measuring data.

457
00:31:27,800 --> 00:31:30,440
And then a data set maintenance plan.

458
00:31:30,440 --> 00:31:32,620
So are you going to update it?

459
00:31:32,620 --> 00:31:37,560
Are you going to, for example, include opt-in or opt-out mechanisms so that the data set

460
00:31:37,560 --> 00:31:43,360
can be continually updated in light of the data creator's wishes?

461
00:31:43,360 --> 00:31:49,480
And all of these have a lot of further details, but this is just sort of a high level of somewhat

462
00:31:49,920 --> 00:31:56,000
something like what I think would help for more ethical data development.

463
00:31:56,000 --> 00:32:01,320
So here's the same thing as a picture, because I like pictures, but essentially as you go

464
00:32:01,320 --> 00:32:07,300
through maintenance you can redefine the requirements and sort of loop through this way, keeping

465
00:32:07,300 --> 00:32:11,880
on checking on how the data set is affecting everything downstream and updating your data

466
00:32:11,880 --> 00:32:16,960
set as is necessary.

467
00:32:16,960 --> 00:32:24,040
And one of the ways that I've been approaching this is the way that lets me do Python, honestly.

468
00:32:24,040 --> 00:32:28,400
So there's a lot of different roles here, but the one that I wanted of all these roles

469
00:32:28,400 --> 00:32:32,600
that I've been defining was the one about quantifying data sets.

470
00:32:32,600 --> 00:32:35,640
So I've been calling it measuring data.

471
00:32:35,640 --> 00:32:40,360
And basically the idea is that when we have a data set we can quantify it along a whole

472
00:32:40,360 --> 00:32:42,200
bunch of different axes.

473
00:32:42,200 --> 00:32:46,880
And it's weird that there really isn't a bunch of norms around what you can quantify

474
00:32:46,880 --> 00:32:52,880
in data relevant to values, relevant to populations, relevant to representation.

475
00:32:52,880 --> 00:32:56,680
These sort of mechanisms and ways to do it don't really exist yet.

476
00:32:56,680 --> 00:33:01,980
So it's a wide open area and it lets you program ways to do it, which is fun.

477
00:33:01,980 --> 00:33:05,020
So here's an example that you can check out if you want.

478
00:33:05,020 --> 00:33:10,880
It's open source on Hugging Face, which is where I work now.

479
00:33:10,880 --> 00:33:15,920
I am basically only allowed to work on it if people go to it.

480
00:33:15,920 --> 00:33:22,120
So if you just want to go and then say this needs work, that's fine.

481
00:33:22,120 --> 00:33:24,240
Basically I need to show engagement.

482
00:33:24,240 --> 00:33:28,520
And unfortunately data measurement just isn't a thing that the community largely cares about

483
00:33:28,520 --> 00:33:31,000
in AI.

484
00:33:31,000 --> 00:33:33,480
But here's the kind of things that you could do.

485
00:33:33,480 --> 00:33:39,080
So wouldn't it be cool if when we released data sets we also had some basic statistics,

486
00:33:39,120 --> 00:33:44,640
like the number of web domains represented, the number of data instances with copyright

487
00:33:44,640 --> 00:33:48,320
information, the number and type of licenses.

488
00:33:48,320 --> 00:33:54,600
And the Washington Post actually just a few days ago released an investigative report

489
00:33:54,600 --> 00:34:01,440
doing just this for a common machine learning data set called C4.

490
00:34:01,440 --> 00:34:05,160
So the C4 has been around since 2019.

491
00:34:05,160 --> 00:34:11,840
And it was just last week requiring investigative journalism to provide these basic statistics.

492
00:34:11,840 --> 00:34:14,480
And they had to work with AI too to do it.

493
00:34:14,480 --> 00:34:16,920
Also it was all in Python, yay Python.

494
00:34:16,920 --> 00:34:19,240
But the point is that this isn't a norm.

495
00:34:19,240 --> 00:34:25,400
And having to find out what's in the data sets that we're using within AI through the

496
00:34:25,400 --> 00:34:32,360
work of investigative journalists is maybe a situation where we can make some improvements.

497
00:34:32,360 --> 00:34:34,320
You can also do advanced statistics.

498
00:34:34,360 --> 00:34:37,760
So for those of you who are mathy, this is where it gets super fun.

499
00:34:37,760 --> 00:34:40,000
So I've developed a few approaches here.

500
00:34:40,000 --> 00:34:45,200
So for example, if you're modeling natural language, natural language is known to roughly

501
00:34:45,200 --> 00:34:47,800
follow a Zipfian distribution.

502
00:34:47,800 --> 00:34:49,640
And so you can actually measure this.

503
00:34:49,640 --> 00:34:54,280
And then this will automatically sort of tell you whether there are a lot of weird artifacts

504
00:34:54,280 --> 00:34:55,280
in your data.

505
00:34:55,280 --> 00:35:00,360
If it's not behaving like a natural language distribution, that's a warning sign that there's

506
00:35:00,400 --> 00:35:04,520
some crud in your data that isn't natural language that you might not want.

507
00:35:04,520 --> 00:35:08,360
Associations, very low hanging fruit.

508
00:35:08,360 --> 00:35:10,880
That would be super useful.

509
00:35:10,880 --> 00:35:15,400
And then one of the hardest things is around social constructs.

510
00:35:15,400 --> 00:35:19,000
And this is where work on construct validity comes into play.

511
00:35:19,000 --> 00:35:21,160
So things like the severity of hate language.

512
00:35:21,160 --> 00:35:24,080
Like what are these concepts and how do we measure them?

513
00:35:24,080 --> 00:35:25,200
Wide open.

514
00:35:25,200 --> 00:35:26,200
You all should work on it.

515
00:35:26,200 --> 00:35:29,200
Let me check my time.

516
00:35:29,200 --> 00:35:30,200
Okay, cool.

517
00:35:31,040 --> 00:35:34,560
So I'll quickly jump into one of my favorite families of measurement, associations, so

518
00:35:34,560 --> 00:35:38,360
you can all see how easy it is and hopefully want to do this too.

519
00:35:38,360 --> 00:35:43,240
So basically one of the issues in datasets is stereotypes, right?

520
00:35:43,240 --> 00:35:48,640
And stereotypes apply to different identities, different social identities.

521
00:35:48,640 --> 00:35:53,680
So if we were to put these social identities into language, this would be things like words

522
00:35:53,680 --> 00:35:58,240
for ability status, for age, for gender.

523
00:35:58,280 --> 00:36:03,440
And what you can do is look at what they tend to co-occur with in the dataset, right?

524
00:36:03,440 --> 00:36:07,720
So all of these are vocabulary from the dataset.

525
00:36:07,720 --> 00:36:09,760
We isolate the identity terms.

526
00:36:09,760 --> 00:36:11,720
You can write these, right?

527
00:36:11,720 --> 00:36:15,640
We know the social identity terms that are historically subject to discrimination.

528
00:36:15,640 --> 00:36:17,180
You can write these down.

529
00:36:17,180 --> 00:36:20,880
And then you can look at everything they tend to co-occur with.

530
00:36:20,880 --> 00:36:24,980
And then you can do a fairly simple thing of counting, right?

531
00:36:24,980 --> 00:36:28,240
So woman and lipstick, woman and smile.

532
00:36:28,240 --> 00:36:30,620
You can then look at comparisons, right?

533
00:36:30,620 --> 00:36:33,720
So man and lipstick, man and smile.

534
00:36:33,720 --> 00:36:37,220
And once you take some basic subtractions, you're measuring bias.

535
00:36:37,220 --> 00:36:38,220
Boom.

536
00:36:38,220 --> 00:36:39,580
I just got so excited.

537
00:36:39,580 --> 00:36:42,480
I like hit my glasses off.

538
00:36:42,480 --> 00:36:44,700
So simple, such low-hanging fruit.

539
00:36:44,700 --> 00:36:45,700
Nobody does this.

540
00:36:45,700 --> 00:36:46,900
Someone please do this.

541
00:36:46,900 --> 00:36:49,620
So this is just a simple co-occurrence kind of statistic.

542
00:36:49,620 --> 00:36:52,220
You can get much more advanced.

543
00:36:53,220 --> 00:36:57,020
There's a whole world of math having to do with associations.

544
00:36:57,020 --> 00:37:00,260
Correlations is the one or the family.

545
00:37:00,260 --> 00:37:01,860
There's a family of correlation measures.

546
00:37:01,860 --> 00:37:04,060
That's what people are most familiar with.

547
00:37:04,060 --> 00:37:11,440
There's also other kinds of measures like those based on entropy and mutual information.

548
00:37:11,440 --> 00:37:15,340
So normalized point-wise mutual information is one that I use.

549
00:37:15,340 --> 00:37:18,180
And it really helps automatically pull out stereotypes.

550
00:37:18,180 --> 00:37:21,860
Super easy, super fun.

551
00:37:21,900 --> 00:37:25,140
Not fun that it exists, but fun to uncover.

552
00:37:25,140 --> 00:37:29,220
And then you can use that to make your data sets better and remove really problematic

553
00:37:29,220 --> 00:37:31,180
stereotypes.

554
00:37:31,180 --> 00:37:35,700
So the point here is that you know how we evaluate models?

555
00:37:35,700 --> 00:37:37,100
We should also measure data.

556
00:37:37,100 --> 00:37:42,140
So you should think of like evaluate is to model as measure is to data.

557
00:37:42,140 --> 00:37:44,180
That's sort of the headspace I've been operating in.

558
00:37:44,180 --> 00:37:50,060
And I really want the second part measuring data to become a thing.

559
00:37:50,060 --> 00:37:54,780
Once you have all the sorts of information, this is where data documentation comes in.

560
00:37:54,780 --> 00:37:59,380
There's been a lot of really cool work on how to do data set documentation.

561
00:37:59,380 --> 00:38:04,220
For those of you who are fans of metadata, this is where you can totally nerd out.

562
00:38:04,220 --> 00:38:09,860
So some of the examples are data statements, which is for natural language processing.

563
00:38:09,860 --> 00:38:16,940
Data sheets, which get into details of consent of crowd workers and this kind of thing.

564
00:38:16,940 --> 00:38:21,820
And then data set cards is something that I'm working on at Hugging Face, which essentially

565
00:38:21,820 --> 00:38:28,180
just borrows from all these ideas and makes it a little more compact.

566
00:38:28,180 --> 00:38:32,540
And this does things like providing information about the sources, the languages, the curation

567
00:38:32,540 --> 00:38:36,820
rationale, the collection methods, the consent mechanisms, et cetera.

568
00:38:36,820 --> 00:38:41,980
And now you can use these things to decide among data sets, compare different data sets,

569
00:38:41,980 --> 00:38:45,260
as well as improve data sets.

570
00:38:45,260 --> 00:38:48,100
I was also asked to talk about the stack.

571
00:38:48,100 --> 00:38:52,100
I won't have a ton of time to do that, but here's an example of things that are in a

572
00:38:52,100 --> 00:38:55,860
data set card for the stack.

573
00:38:55,860 --> 00:39:03,700
So as an FYI, the stack is a bunch of GitHub repositories where it was collected with

574
00:39:03,700 --> 00:39:05,940
super permissive licenses.

575
00:39:05,940 --> 00:39:09,380
And we're also giving people the option of opting out.

576
00:39:09,380 --> 00:39:14,820
So it's sort of an experimental way of developing a data set where you try and make sure that

577
00:39:14,820 --> 00:39:16,580
people know that they're in it.

578
00:39:16,580 --> 00:39:18,140
We're also working on opt-in.

579
00:39:18,140 --> 00:39:21,020
That's a lot harder for large data sets.

580
00:39:21,020 --> 00:39:24,620
If you want to keep up with the state of the art, you essentially can't use opt-in if you're

581
00:39:24,620 --> 00:39:28,140
trying to look at your model performance.

582
00:39:28,140 --> 00:39:33,940
But by working on opt-in mechanisms, we can make this more of a thing that actually happens.

583
00:39:33,940 --> 00:39:37,780
So this is where you end up, in this data set card, is where you end up putting things

584
00:39:38,500 --> 00:39:46,060
like information about the curators, the social impact, all these sorts of things.

585
00:39:46,060 --> 00:39:51,060
And my work in particular, I just have a couple more minutes, so I think I'm right on time

586
00:39:51,060 --> 00:39:52,060
actually.

587
00:39:52,060 --> 00:39:56,300
My work in particular is around what we call ethics and society at Hugging Face.

588
00:39:56,300 --> 00:39:59,540
There isn't one dedicated ethics team.

589
00:39:59,540 --> 00:40:04,380
Pro tip, if you're trying to operationalize ethics within your company, having one dedicated

590
00:40:04,380 --> 00:40:09,020
team ends up creating a bit of a rift.

591
00:40:09,020 --> 00:40:14,540
It can create sort of territorialism effects, and the ethics team can be seen as a policing

592
00:40:14,540 --> 00:40:15,860
force.

593
00:40:15,860 --> 00:40:19,060
So one thing that's really useful is actually to have ethics-minded people throughout the

594
00:40:19,060 --> 00:40:23,340
company who then come together to sort of share vocabulary and techniques and things

595
00:40:23,340 --> 00:40:26,160
like that and go back to their teams.

596
00:40:26,160 --> 00:40:30,360
So the group of us, we call ourselves ethics and society regulars.

597
00:40:30,360 --> 00:40:37,080
And we have open source stuff up on Hugging Face as well, if you want to check it out.

598
00:40:37,080 --> 00:40:43,840
I think I, let's see, yeah, it's at hf.co.ethex.

599
00:40:43,840 --> 00:40:49,280
And so you can see some of the stuff we've been doing, for example, exploring bias in

600
00:40:49,280 --> 00:40:51,360
generative image models.

601
00:40:51,360 --> 00:40:54,220
So we have like stable diffusion and Dali.

602
00:40:54,220 --> 00:40:58,880
You can take a look at different kinds of groups and adjectives and see the differences

603
00:40:58,880 --> 00:41:03,760
yourself, where we have a ton of tools for this kind of stuff.

604
00:41:03,760 --> 00:41:08,420
And since it's open source, we're really happy to get more, obviously.

605
00:41:08,420 --> 00:41:11,240
So some concluding thoughts.

606
00:41:11,240 --> 00:41:12,240
Python is awesome.

607
00:41:12,240 --> 00:41:15,640
So I hope that's been a theme throughout this.

608
00:41:15,640 --> 00:41:20,320
Most of, or all of the stuff I've talked about has been in Python.

609
00:41:20,320 --> 00:41:25,320
Machine learning models learn about the world usually using web scrape data, which is problematic

610
00:41:25,320 --> 00:41:27,560
for a whole bunch of reasons.

611
00:41:27,680 --> 00:41:31,920
But it's useful to have knowledge and control over what's in the data.

612
00:41:31,920 --> 00:41:35,720
And we can use Python to do that.

613
00:41:35,720 --> 00:41:40,960
Decisions of what to work on and how can be informed by thinking through different values

614
00:41:40,960 --> 00:41:44,660
and tensions and then how they play out over time.

615
00:41:44,660 --> 00:41:49,640
This foresight idea, looking forward five years, 10 years, how would this be the most

616
00:41:49,640 --> 00:41:54,960
beneficial and who should I be including right now to make sure that it is as beneficial

617
00:41:55,000 --> 00:41:57,840
as I want it to be.

618
00:41:57,840 --> 00:42:02,000
Dataset construction as a whole can be way more rigorous.

619
00:42:02,000 --> 00:42:06,120
I really am trying to push the needle on measuring and documenting data.

620
00:42:06,120 --> 00:42:09,840
And I hope that this is interesting to you all as well.

621
00:42:09,840 --> 00:42:14,160
And I work, by the way, at an open source Python company.

622
00:42:14,160 --> 00:42:18,920
So you can check out more at just hf.co.

623
00:42:18,920 --> 00:42:19,920
OK, thanks.

