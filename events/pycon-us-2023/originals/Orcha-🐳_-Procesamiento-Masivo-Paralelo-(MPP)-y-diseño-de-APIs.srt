1
00:00:00,000 --> 00:00:05,000
Hola, hola. Y seguimos con nuestra segunda charla de las Python charlas.

2
00:00:05,000 --> 00:00:08,600
Ahora venimos con una super charla con dos ponentes.

3
00:00:08,600 --> 00:00:12,000
Uno de ellos es Miguel Robledo. Miguel es profesional.

4
00:00:12,000 --> 00:00:17,000
Lo voy a decir, pero aquí en su biografía dice profesional, idiot.

5
00:00:17,000 --> 00:00:23,000
Tiene 24 años de experiencia, evangelist y es Python conestador.

6
00:00:23,000 --> 00:00:28,000
También tenemos nuestro segundo speaker de esta charla.

7
00:00:30,000 --> 00:00:34,000
Es Javier. Javier en tu biografía no quiere aparecer, pero bueno, aquí llegó.

8
00:00:34,000 --> 00:00:39,000
Javier es un ingeniero de computadoras al que le encanta seguir adquiriendo conocimiento

9
00:00:39,000 --> 00:00:41,000
y nunca deja de aprender.

10
00:00:41,000 --> 00:00:49,000
Crea las herramientas de hardware, de open source para sus propias necesidades.

11
00:00:49,000 --> 00:00:55,000
¿Qué más hace? Por lo general, él suele trabajar en proyectos propios,

12
00:00:55,000 --> 00:00:57,000
pero también desarrolla soluciones para empresas.

13
00:00:57,000 --> 00:01:01,000
Vamos a darle una super bienvenida a estos chicos en su charla.

14
00:01:01,000 --> 00:01:09,000
Muchas gracias, Deni.

15
00:01:09,000 --> 00:01:12,000
La charla de la que vamos a hablar hoy es Orca.

16
00:01:12,000 --> 00:01:15,000
Es un framework de procesamiento masivo paralelo.

17
00:01:15,000 --> 00:01:20,000
Yo voy a hablar un poquito más de esta parte y luego de cómo diseñar una buena API,

18
00:01:20,000 --> 00:01:23,000
que va a ser lo que se va a encargar principalmente Miguel en este caso.

19
00:01:23,000 --> 00:01:25,000
En este caso, ¿quiénes somos?

20
00:01:25,000 --> 00:01:27,000
Después de la superpresentación de Deni, poco más hay que decir.

21
00:01:27,000 --> 00:01:29,000
Yo soy Javi, ingeniero de computadores.

22
00:01:29,000 --> 00:01:33,000
Me encanta cacharrear el mundo hardware.

23
00:01:33,000 --> 00:01:37,000
Estos son algunos de los lenguajes de programación que hablo y luego Linux para siempre.

24
00:01:37,000 --> 00:01:39,000
Aunque esa historia nunca fue del todo así.

25
00:01:39,000 --> 00:01:41,000
Y luego Miguel.

26
00:01:41,000 --> 00:01:45,000
Yo soy autodidacta. Llevo programando unos diez años así.

27
00:01:45,000 --> 00:01:48,000
Hay algunos de los lenguajes que sé usar.

28
00:01:48,000 --> 00:01:52,000
También me gusta mucho cacharrear y trastear con distintos lenguajes y librerías.

29
00:01:52,000 --> 00:01:56,000
Saber que ideas se puede sacar de cada uno.

30
00:01:56,000 --> 00:01:58,000
Perfecto.

31
00:01:58,000 --> 00:02:03,000
Para ponernos un poco de un texto, ahora mismo tanto Miguel como yo estamos trabajando en Teldat.

32
00:02:03,000 --> 00:02:07,000
Teldat es una compañía que entre otras cosas se dedica a hacer routers de núcleo.

33
00:02:07,000 --> 00:02:12,000
Routers como este bicho de aquí, de 20 kilos, para que luego digan que los sysadmin no hacen deporte.

34
00:02:12,000 --> 00:02:15,000
Que hay racks de servidores enormes.

35
00:02:15,000 --> 00:02:19,000
Entre todas las cosas que hace Teldat, también tenemos nuestro sistema operativo propio.

36
00:02:19,000 --> 00:02:21,000
Que en este caso se llama OSDX.

37
00:02:21,000 --> 00:02:23,000
Hay distintos tipos de servicios que ejecutamos.

38
00:02:23,000 --> 00:02:25,000
Y aquí he puesto algún ejemplo de unos cuantos.

39
00:02:25,000 --> 00:02:27,000
Como SSH, DNS, etc.

40
00:02:27,000 --> 00:02:33,000
Pero podéis imaginaros que estos routers de núcleo, como son por lo general los que van en los data centers y en grandes oficinas.

41
00:02:33,000 --> 00:02:37,000
Este conjunto de aplicaciones es una muestra muy chiquitita de lo que hay.

42
00:02:37,000 --> 00:02:43,000
Para este tipo de cosas, de servicios, hay un montón de tests que llamamos test de integración.

43
00:02:43,000 --> 00:02:47,000
Primero es que probamos que el propio equipo por sí solo sea capaz de ejecutarlos correctamente.

44
00:02:47,000 --> 00:02:52,000
Pero luego también que varios equipos ejecutando OSDX puedan utilizar estos servicios entre sí.

45
00:02:52,000 --> 00:02:55,000
Es decir, que un SSH funcione entre dos equipos, etc.

46
00:02:55,000 --> 00:03:00,000
Estos tests de integración, como ya podéis haceros a la idea, son un poco complejos.

47
00:03:00,000 --> 00:03:02,000
Porque al final son probar un sistema operativo.

48
00:03:02,000 --> 00:03:04,000
Entonces, ¿cuáles son los tests de un sistema operativo?

49
00:03:04,000 --> 00:03:06,000
Pues eso es un poco lo que hacemos.

50
00:03:06,000 --> 00:03:09,000
Pero esto era la parte fácil, por decirlo así.

51
00:03:09,000 --> 00:03:13,000
Porque en tel data hacemos también escenarios SD-1, que significa Software Defined One.

52
00:03:13,000 --> 00:03:19,000
En este caso, este es un ejemplo de uno de los escenarios que probamos activamente en nuestros equipos.

53
00:03:19,000 --> 00:03:25,000
¿Qué sucede? Que cada uno de estos bichos es uno de los routers de núcleo que he comentado antes.

54
00:03:25,000 --> 00:03:29,000
En este caso estamos hablando de 12 routers que hay que probar todos simultáneamente.

55
00:03:29,000 --> 00:03:36,000
Una de las características de SD-1 es que como se definen por software, en este caso, si alguno de los links o conexiones se cae,

56
00:03:36,000 --> 00:03:40,000
automáticamente entre todos los equipos se recupera la conexión.

57
00:03:40,000 --> 00:03:43,000
Y la idea es que siempre, siempre, siempre haya conectividad de red.

58
00:03:43,000 --> 00:03:45,000
O idealmente es lo que se busca.

59
00:03:45,000 --> 00:03:53,000
Entonces, este tipo de escenarios son especialmente complejos de simular, especialmente difíciles de ejecutar y de trabajar con ellos.

60
00:03:53,000 --> 00:03:55,000
Entonces, ¿qué sucede?

61
00:03:55,000 --> 00:04:00,000
La situación de la que venimos es que tenemos tres smoke que tienen que ser rápidos y tardan dos horas.

62
00:04:00,000 --> 00:04:02,000
Así que de rápidos tienen poco.

63
00:04:02,000 --> 00:04:07,000
Y a los desarrolladores les da pereza ejecutar los smoke porque tardan mucho más en hacer las integraciones.

64
00:04:07,000 --> 00:04:09,000
Los nightly también tienen que ejecutarse una noche.

65
00:04:09,000 --> 00:04:12,000
¿Qué sucede? Que cada nightly dura ocho horas por sí solo.

66
00:04:12,000 --> 00:04:19,000
Y si alguno de los tres se bloquea, bloquea todo el pipeline y pasaba muy a menudo que se iban al día siguiente a las ejecuciones.

67
00:04:19,000 --> 00:04:20,000
Así que, terrible.

68
00:04:20,000 --> 00:04:27,000
Y luego los weekly, que tenemos weekly, hay que decir, tardan 15 horas en ejecutarse y sucede exactamente lo mismo.

69
00:04:27,000 --> 00:04:31,780
Y luego el

70
00:04:31,780 --> 00:04:35,780
de los weekly se bloquea por un casual, todo el pipeline está bloqueado hasta el lunes siguiente.

71
00:04:35,780 --> 00:04:37,780
Así que no sirven nada estos test.

72
00:04:37,780 --> 00:04:39,780
¿Por qué tanto tiempo?

73
00:04:39,780 --> 00:04:41,780
Porque no se paralizan.

74
00:04:41,780 --> 00:04:48,780
Así que va mi jefe y me dice, tenemos que buscar alguna herramienta que nos permita paralizar un poco las cosas que ya existan.

75
00:04:48,780 --> 00:04:51,780
A lo cual yo digo, vale, perfecto, creo la mía.

76
00:04:51,780 --> 00:04:53,780
Bueno, esto no es del todo cierto.

77
00:04:54,560 --> 00:04:58,560
Encontramos estas dos que seguro que al menos la primera os es conocida a todos.

78
00:04:58,560 --> 00:05:01,560
¿Qué sucede? Que fueron descartadas casi desde el principio.

79
00:05:01,560 --> 00:05:04,560
Porque estas dos herramientas están orientadas a microservicios,

80
00:05:04,560 --> 00:05:07,560
lo cual directamente ya no encaja con nuestros escenarios.

81
00:05:07,560 --> 00:05:10,560
Y luego además no lidian bien con topologías de red complejas.

82
00:05:10,560 --> 00:05:17,560
Es decir, viendo la imagen que os he enseñado antes, cada una de las flechas, por decirlo así, era una conexión de red.

83
00:05:17,560 --> 00:05:23,560
Así que en este caso no eran herramientas útiles para lo que queríamos hacer.

84
00:05:23,560 --> 00:05:26,560
Así que decidimos hacer nuestra propia herramienta.

85
00:05:26,560 --> 00:05:32,560
En este caso una herramienta que fuese controlable, es decir, saber cuándo empiezan, acaban las ejecuciones, etc.

86
00:05:32,560 --> 00:05:40,560
Que sea resiliente, es decir, si alguno de los test por un casual se queda bloqueado, saberlo y poder tener margen de actuación con respecto a ello.

87
00:05:40,560 --> 00:05:47,560
Y por último, pero no menos importante, que sea flexible, que pueda trabajar con cualquier tipo de requisito o de recurso que se le plantee.

88
00:05:47,560 --> 00:05:52,560
En el caso del que venimos, el principal recurso con el que teníamos que trabajar fue la memoria RAM.

89
00:05:52,560 --> 00:06:01,560
Pero a medida que los escenarios evolucionen y son más complejos, se incluyó en estos recursos la CPU, la conectividad de red, los discos duros, etc.

90
00:06:01,560 --> 00:06:06,560
Es un gran conjunto de cosas que daban muchísimos problemas a la hora de ejecutar test.

91
00:06:06,560 --> 00:06:09,560
Así que vamos a ver un poco cómo es Orca en sí.

92
00:06:09,560 --> 00:06:14,560
Orca se compone de estas herramientas.

93
00:06:14,560 --> 00:06:17,560
En este caso, el procesor es el corazón.

94
00:06:17,560 --> 00:06:28,560
Esto es lo que internamente va a encargarse de recibir todas las peticiones, todos los escenarios, todo lo que se quiera ejecutar, de ordenarlo según prioridad y de hacer la propia ejecución en sí.

95
00:06:28,560 --> 00:06:33,560
Orca, por otro lado, digamos que sería el cuerpo de este framework.

96
00:06:33,560 --> 00:06:38,560
Define todas las interacciones con el procesor, como se puede ver, manejando la lógica interna que pueda tener.

97
00:06:38,560 --> 00:06:41,560
Pero luego también hace las veces de API con el manager.

98
00:06:41,560 --> 00:06:46,560
Y el manager, en este caso, para introducirlo, es, por decirlo así, lo que un desarrollador va a implementar.

99
00:06:46,560 --> 00:06:54,560
Importante destacar que, aunque la casuística de la que partimos es para realizar la ejecución de test, Orca no está restringida a eso.

100
00:06:54,560 --> 00:06:58,560
Se puede ejecutar cualquier cosa, aunque explicaré un poquito más de eso adelante.

101
00:06:58,560 --> 00:07:03,560
Vale, el manager define, por decirlo así, eso, lo que tiene que ejecutar o lo que tiene que implementar cada desarrollador.

102
00:07:03,560 --> 00:07:06,560
Y luego tenemos los mensajes y las peticiones.

103
00:07:06,560 --> 00:07:09,560
Un mensaje es la unidad mínima de comunicación.

104
00:07:09,560 --> 00:07:12,560
Un mensaje define qué es lo que tiene que ejecutar.

105
00:07:12,560 --> 00:07:17,560
Y luego la petición es el mensaje con esteroides, como me gusta llamarlo allí, a mí.

106
00:07:17,560 --> 00:07:25,560
Tiene un montón de información, tiene todo aquello necesario para que las peticiones funcionen, para que el orquestador sepa qué hacer con ellas.

107
00:07:25,560 --> 00:07:28,560
El proceso lo entiende de peticiones, no sabe nada de mensajes.

108
00:07:28,560 --> 00:07:34,560
Y las peticiones tienen todo el conocimiento de dominio suficiente como para saber hacer la ejecución que se le pide que haga.

109
00:07:34,560 --> 00:07:37,560
Y por último, pero no menos importante, tenemos los plugins.

110
00:07:37,560 --> 00:07:41,560
Los plugins definen el punto de entrada del framework en este caso.

111
00:07:41,560 --> 00:07:44,560
Porque como es un framework, por sí solo no hace nada.

112
00:07:44,560 --> 00:07:54,560
Entonces, los plugins definen cuáles son las opciones de línea de comandos en caso de que las haya, los recursos y requisitos que se van a utilizar, etcétera, etcétera.

113
00:07:54,560 --> 00:08:00,560
Todo este sistema lo que permite, entre otras cosas, es tener un componente completamente desacoplado y escalable.

114
00:08:00,560 --> 00:08:05,560
Porque las propias clases están definidas para añadir todo aquello que sea necesario para una ejecución.

115
00:08:06,560 --> 00:08:13,560
En particular, como cosas especialmente interesantes de Orca, que queremos destacar, es que es cero dependencias.

116
00:08:13,560 --> 00:08:17,560
Orca se escribe única y exclusivamente utilizando el módulo de multiprocesing de Python.

117
00:08:17,560 --> 00:08:21,560
Así que con tener Python 3 se puede ejecutar Orca.

118
00:08:21,560 --> 00:08:27,560
Sigue una arquitectura cliente-servidor, en el que hay un único servicio de Orca y NClient que envía peticiones.

119
00:08:27,560 --> 00:08:32,560
Y luego tiene una arquitectura de plugins que, como he dicho antes, al ser un framework, todo se acopla al propio orquestador.

120
00:08:32,560 --> 00:08:36,560
Así que vamos a ver un poquito cómo es algo más en detalle por dentro.

121
00:08:36,560 --> 00:08:39,560
Por una parte, esto es una visión general de lo que sería el manager.

122
00:08:39,560 --> 00:08:47,560
El manager tiene aquí un método setup que sirve a predecir todas las configuraciones y aquellas cosas que se pueden necesitar para una ejecución en cuestión.

123
00:08:47,560 --> 00:08:49,560
Y luego tiene estos métodos de aquí.

124
00:08:49,560 --> 00:08:55,560
Siguiendo un poco la lógica de cómo sería la recepción de mensajes, vamos a ir viendo cuál es cada uno de ellos.

125
00:08:55,560 --> 00:09:01,560
Por una parte, el procesor recibirá un mensaje y dirá, manager, conviértemelo en una petición.

126
00:09:01,560 --> 00:09:10,560
Y el manager hará toda su lógica interna para convertir el mensaje, que repito, es la unidad mínima de información, en una petición, que es lo que el procesor entiende y sabe qué hacer con ello.

127
00:09:10,560 --> 00:09:17,560
Cuando la petición ya se ha evaluado correctamente y ha satisfecho su condición, pasaría a ejecutarse.

128
00:09:17,560 --> 00:09:19,560
En este caso es el método en start.

129
00:09:19,560 --> 00:09:25,560
En el método en start, por ejemplo, se pueden reservar recursos, se pueden crear las topologías de red para el escenario que estaba explicando antes.

130
00:09:25,560 --> 00:09:29,560
Se hacen todas aquellas gestiones como sería de inicialización.

131
00:09:29,560 --> 00:09:34,560
La acción en sí está en un rompito en este caso porque es algo propio de la petición.

132
00:09:34,560 --> 00:09:39,560
Una de las cosas que le da mucha flexibilidad a Orca es que no está restringido al manager en sí.

133
00:09:39,560 --> 00:09:49,560
Es decir, cada petición puede tener acciones distintas, de manera que un único manager puede ejecutar distintos tipos de peticiones según las necesidades que se tengan.

134
00:09:49,560 --> 00:09:53,560
Y por último, pero no menos importante, está el onfinish, que se encargaría de liberar recursos.

135
00:09:53,560 --> 00:09:56,560
Este método de aquí, lo que vemos un poquito más adelante, es sumidero.

136
00:09:56,560 --> 00:10:05,560
Es decir, cuando sucede cualquier tipo de evento descontrolado, por decirlo así, siempre se acaba en esa función para poder hacer siempre la liberación de recursos.

137
00:10:05,560 --> 00:10:12,560
Por todas las peticiones, que es de las partes más importantes del orquestrador, tienen esta apariencia.

138
00:10:12,560 --> 00:10:14,560
Este es el ciclo debido de una petición.

139
00:10:14,560 --> 00:10:24,560
Viendo un poco cómo estaba definido antes, cuando una petición se acaba de crear, es decir, se ha llamado el convert to petition, está en este estado created.

140
00:10:24,560 --> 00:10:30,560
En este estado de la petición todavía no hace nada, no está encolada, no se han evaluado sus requisitos, no se han evaluado sus recursos.

141
00:10:30,560 --> 00:10:33,560
Está preparada para entrar ya en la pila de ejecución.

142
00:10:33,560 --> 00:10:39,560
Entonces, cuando se ha creado, en cualquier momento se puede encancelar, pero lo habitual es que pase a estar encolada.

143
00:10:39,560 --> 00:10:46,560
La cola que utiliza Orca internamente es una cola de prioridades, que se ordena siempre por la prioridad de la petición en cuestión.

144
00:10:46,560 --> 00:10:53,560
Como son basadas en números de coma flotante, la prioridad más baja es menos infinito y la más alta es más infinito.

145
00:10:53,560 --> 00:11:03,560
Lo interesante de la cola es que cada vez que se encola una petición automáticamente se reordena de forma que siempre la petición más prioritaria es la primera que se ejecuta.

146
00:11:03,560 --> 00:11:09,560
Cuando la petición está en la cola se evalúa cada cierto tiempo la condición.

147
00:11:09,560 --> 00:11:17,560
Una condición puede ser, tengo recursos suficientes, tengo topología de red desplegada, tengo conectividad a la internet, puede ser cualquier cosa.

148
00:11:17,560 --> 00:11:20,560
Lo único importante es que la condición devuelva un valor booleano.

149
00:11:20,560 --> 00:11:27,560
Cuando la condición se evalúa verdadero, pasa a ejecutarse la petición. Si se evalúa falso, sigue encolada.

150
00:11:27,560 --> 00:11:35,560
Y cuando la petición se está ejecutando, pasaría por todo el pipeline que he explicado anteriormente, por el método onStart, por el action y por el onFinish.

151
00:11:35,560 --> 00:11:44,560
Y finalmente la petición se finaliza. En este estado, en el estado onFinish, perdón, en la diapositiva, la petición ya no puede volver atrás.

152
00:11:44,560 --> 00:11:47,560
Es un estado en el que la petición ya se descarta completamente.

153
00:11:48,560 --> 00:11:56,560
Como cosa interesante, está en el estado broken, que se llega siempre que haya una excepción no controlada en cualquier etapa del pipeline.

154
00:11:56,560 --> 00:12:03,560
Y luego, por último, el cancel, que como he dicho antes, y se puede ver, son en esas tres etapas cuando se cancela.

155
00:12:03,560 --> 00:12:11,560
Lo interesante es que luego ya lo pones todo junto y este es el esquema final de cómo funciona Orca internamente a grosso modo.

156
00:12:11,560 --> 00:12:24,560
Llega un mensaje, el mensaje se convierte en petición, la petición va a estar creada, se encola, cuando satisface su condición va a estar ejecutándose, onStart, action, onFinish, y la petición se finaliza.

157
00:12:24,560 --> 00:12:27,560
Y eso es un poquito el interior del orquestrador.

158
00:12:28,560 --> 00:12:34,560
De todo esto nace Orca Vagrant, que es, por decirlo así, el primer plugin oficial del propio orquestrador.

159
00:12:34,560 --> 00:12:40,560
Vagrant, para que no lo sepáis, es una tecnología de virtualización basada en máquinas virtuales.

160
00:12:40,560 --> 00:12:44,560
A mí me gusta decir que es el Docker de máquinas virtuales, permite un control muy parecido.

161
00:12:44,560 --> 00:12:48,560
Y una de las cosas interesantes es que es controlada desde la consola de comandos.

162
00:12:48,560 --> 00:12:58,560
Así que, este primer plugin de Orca es un plugin que sencillamente se encarga de enviar comandos, capturar salidas de comandos, y enviar señales cuando es necesario.

163
00:12:58,560 --> 00:13:03,560
Así que es un plugin muy sencillo de implementar y el orquestrador lo puso bastante fácil para hacerlo.

164
00:13:03,560 --> 00:13:11,560
De todo esto, haciendo un poco de retrospectiva de dónde venimos, tenemos que los tests tienen que ser rápidos,

165
00:13:11,560 --> 00:13:21,560
así que pasamos de 2 horas de ejecución a solo 20 minutos con el orquestrador, desaparecen los tests weekly, que antes eran 15 horas de ejecución,

166
00:13:21,560 --> 00:13:24,560
y ahora son test nightly, los test nightly son los antiguos weekly.

167
00:13:24,560 --> 00:13:28,560
Y tenemos que los nightly se ejecutan en una noche en 8 horas.

168
00:13:28,560 --> 00:13:32,560
Es decir, una mejora en tiempo muy considerable.

169
00:13:32,560 --> 00:13:36,560
En este caso, gracias a que Orca paraleliza los tests.

170
00:13:36,560 --> 00:13:40,560
Así que, ya vemos que el orquestrador tiene como mucha potencia y mucho alcance.

171
00:13:40,560 --> 00:13:47,560
Sin embargo, ¿qué sucede cuando alguien externo se une al juego y dice, voy a hacer mi propio plugin?

172
00:13:47,560 --> 00:13:49,560
Y aquí entra Miguel.

173
00:13:50,560 --> 00:13:52,560
Gracias Javi.

174
00:13:52,560 --> 00:13:57,560
Pues como mencionaba Javi, tenemos test de integración para el sistema operativo en sí.

175
00:13:57,560 --> 00:13:59,560
Para eso usamos un framework de testeo que se llama Robot Framework.

176
00:13:59,560 --> 00:14:04,560
No voy a meter mucho en ello, pero la idea es que testea como si fuese un humano, solo que muy rápido.

177
00:14:04,560 --> 00:14:09,560
Entonces, testemos los mismos routers de núcleo que con Mininet, como máquinas virtuales y tal,

178
00:14:09,560 --> 00:14:13,560
pero también testemos dispositivos físicos, hardware de verdad.

179
00:14:13,560 --> 00:14:19,560
Y en general las nightly suelen durar unas 2 horas así y no se paralelizan 3 dispositivos unas 6 horas.

180
00:14:19,560 --> 00:14:21,560
Da tiempo en una noche.

181
00:14:21,560 --> 00:14:26,560
El problema viene cuando tienes 4 versiones, ahí ya no da tiempo en una sola noche,

182
00:14:26,560 --> 00:14:28,560
porque se testarán en cada dispositivo.

183
00:14:28,560 --> 00:14:34,560
Entonces de ahí salió la idea de hacer un plugin de Orca para poder paralizar estos test.

184
00:14:34,560 --> 00:14:42,560
Y está compuesto de Orca para hacer el manejo de recursos y de dispositivos, cuáles están en uso, cuáles no.

185
00:14:42,560 --> 00:14:45,560
Docker para poder aislar el entorno de test.

186
00:14:45,560 --> 00:14:51,560
Y un poco de magia negra para poder conectar los dispositivos físicos a máquinas virtuales dentro del contenedor,

187
00:14:51,560 --> 00:14:53,560
porque Docker no está pensado para eso.

188
00:14:53,560 --> 00:14:58,560
Entonces, esta sería la estructura de un plugin más sencillito.

189
00:14:58,560 --> 00:15:08,560
Simplemente ejecuta comandos dentro de un contenedor de Docker, pero las piezas son las mismas en todos los plugins.

190
00:15:08,560 --> 00:15:12,560
Tienes una petición, la petición representa la tarea en sí.

191
00:15:12,560 --> 00:15:19,560
El manager que es el que maneja las peticiones, convierte mensajes en peticiones, reserva y libera recursos,

192
00:15:19,560 --> 00:15:22,560
y también determina cuando una petición se puede ejecutar o no.

193
00:15:22,560 --> 00:15:26,560
Luego el plugin que es simplemente burocracia para decirle a Orca cómo usar nuestro plugin.

194
00:15:26,560 --> 00:15:30,560
Entonces, la estructura de una petición es eso, es una estructura de datos.

195
00:15:30,560 --> 00:15:37,560
Lo único importante es que tiene que tener el campo priority, que es lo que es la prioridad de la petición,

196
00:15:37,560 --> 00:15:41,560
y el método action, que es lo que se va a ejecutar en sí.

197
00:15:41,560 --> 00:15:45,560
El resto es simplemente cosas para poder ejecutar la petición.

198
00:15:45,560 --> 00:15:51,560
El manager lo que hace es convertir un mensaje en una petición, ese es el convert to petition,

199
00:15:51,560 --> 00:15:56,560
y luego el predicate determina si se puede ejecutar o no una petición.

200
00:15:56,560 --> 00:16:01,560
Además está el método onStart y onFinish, que sirve para reservar y liberar recursos.

201
00:16:01,560 --> 00:16:07,560
En este caso no hay uno onStart porque solo queremos destruir el contenedor cuando acaba la ejecución.

202
00:16:07,560 --> 00:16:11,560
Ese método es el que se llama cuando se cancelan las peticiones, cuando acaban y todo eso.

203
00:16:11,560 --> 00:16:13,560
Siempre se llama al final de una petición.

204
00:16:13,560 --> 00:16:19,560
El plugin, esto lo estamos cambiando, bueno, lo hemos estado cambiando en el vuelo aquí,

205
00:16:19,560 --> 00:16:25,560
esto es simplemente boilerplate y va a cambiar un poco, así que no voy a hacer mucho hincapié en esto.

206
00:16:25,560 --> 00:16:33,560
Una cosa que me di cuenta al implementar el plugin este es que Orca Vagrant y Orca Robot son bastante distintos

207
00:16:33,560 --> 00:16:40,560
y había que hacer algunos cambios dentro de Orca porque Orca asumía ciertas cosas de cómo debería funcionar un plugin.

208
00:16:40,560 --> 00:16:47,560
Por ejemplo, en Orca Vagrant convertir un mensaje en una petición es trivial, es simplemente pasar datos de un lado a otro.

209
00:16:47,560 --> 00:16:53,560
Mientras que Orca Robot tiene que saber qué recursos se van a usar en un test y eso no es un cálculo trivial.

210
00:16:53,560 --> 00:16:59,560
Tienes que saber qué test se van a ejecutar, si hay algunos que necesitan un cliente de wifi y tal.

211
00:16:59,560 --> 00:17:04,560
Y ese cálculo tarda un par de segundos, no es muy lento pero son un par de segundos.

212
00:17:04,560 --> 00:17:07,560
En ese par de segundos pueden ocurrir bastantes cosas.

213
00:17:07,560 --> 00:17:10,560
Entre ellas puedes cancelar una petición antes de que exista mientras se está convirtiendo.

214
00:17:10,560 --> 00:17:16,560
Y a Orca no le gustaba mucho porque te decía que esa petición no existe y luego si la intentabas cancelar otra vez

215
00:17:16,560 --> 00:17:19,560
decías que ya la has cancelado y se seguía ejecutando siempre.

216
00:17:19,560 --> 00:17:25,560
Entonces requirió añadir un mecanismo para poder cancelar mensajes y cuando una petición se crea

217
00:17:25,560 --> 00:17:27,560
a partir de un mensaje cancelado simplemente se ignora.

218
00:17:27,560 --> 00:17:36,560
Y luego también Vagrant cuando quieres acabar con el proceso simplemente matas el proceso y libera todos sus recursos.

219
00:17:36,560 --> 00:17:41,560
Docker no siempre es así, tienes que desinicializar el contenedor y hacer una secuencia.

220
00:17:41,560 --> 00:17:46,560
Entonces tuvimos que añadir esa secuencia de disinculación para las peticiones.

221
00:17:46,560 --> 00:17:54,560
También otra esperanza que había a la hora de implementar los managers era que eran súper caóticos.

222
00:17:54,560 --> 00:18:01,560
De hecho había algunos métodos en los que tenías que llamar a Super, otros en los que no podías llamar a Super o el proceso se bloquearía.

223
00:18:01,560 --> 00:18:04,560
Otros en los que daba igual porque la llamada a Super no hacía nada.

224
00:18:04,560 --> 00:18:08,560
Y luego otros en los que tenías que usar la salida de Super.

225
00:18:08,560 --> 00:18:12,560
Y para arreglar este problema lo que hicimos fue simplemente quitar Super.

226
00:18:12,560 --> 00:18:14,560
Y ya está.

227
00:18:14,560 --> 00:18:22,560
Para esto tuvimos que rediseñar un poco el modelo que estábamos usando de lo que es un manager.

228
00:18:22,560 --> 00:18:27,560
Y esto es un poco el debate de herencia y contracomposición.

229
00:18:27,560 --> 00:18:30,560
Con la herencia teníamos que llamar a Super y todo eso.

230
00:18:30,560 --> 00:18:36,560
Sin embargo con la composición podíamos tratar del manager como si fuese un bloque que hace una función

231
00:18:36,560 --> 00:18:40,560
y no había que tener un padre.

232
00:18:40,560 --> 00:18:46,560
Me gusta pensar sobre la diferencia entre estos dos modelos.

233
00:18:46,560 --> 00:18:50,560
Las diferencias de cómo crece el programa y las clases.

234
00:18:50,560 --> 00:18:54,560
En el modelo de herencia las clases crecen como una cebolla.

235
00:18:54,560 --> 00:18:59,560
Por capas y del centro más abstracto a la parte de fuera más concreta.

236
00:18:59,560 --> 00:19:04,560
Mientras que en un modelo de composición todos los bloques son concretos

237
00:19:04,560 --> 00:19:08,560
y simplemente se ponen unos encima de otros, usan funciones de unos o de otros.

238
00:19:08,560 --> 00:19:13,560
Luego el programa crece en el modelo de composición simplemente añadiendo más bloques.

239
00:19:13,560 --> 00:19:15,560
Todo tiene la misma estructura.

240
00:19:15,560 --> 00:19:18,560
En el modelo de herencia crece añadiendo más cebollas.

241
00:19:18,560 --> 00:19:24,560
Es decir, hay una diferencia entre el contexto de dentro de una cebolla,

242
00:19:24,560 --> 00:19:30,560
que tienes una clase padre y ciertos invariantes, por ejemplo las llamadas a Super y todo ese tipo de cosas.

243
00:19:30,560 --> 00:19:34,560
Tienes que tener en cuenta el contexto de que alguien puede estar heredando de ti.

244
00:19:34,560 --> 00:19:40,560
Puede que tú no seas la clase más concreta, puede haber otra que cambie tu comportamiento.

245
00:19:40,560 --> 00:19:47,560
Luego fuera del contexto de la cebolla es igual que en el de composición.

246
00:19:47,560 --> 00:19:51,560
Simplemente tienes que interactuar con otras cebollas.

247
00:19:51,560 --> 00:20:10,560
En el modelo de composición al ser uniforme es mucho más fácil de tratar con los distintos bloques.

248
00:20:15,560 --> 00:20:20,560
A pesar de las diferencias entre orca-vagra y orca-robot, había algunas cosas que sí eran similares en los dos.

249
00:20:20,560 --> 00:20:28,560
Los dos tenían una integración con el Watchdog de SystemD, que es para que si el proceso se congela, se resetea el servicio.

250
00:20:28,560 --> 00:20:33,560
También los dos tenían una noción general de manejo de recursos, aunque era bastante distinta,

251
00:20:33,560 --> 00:20:43,560
porque en orca-robot eran dispositivos físicos y en orca-vagra era más RAM y cosas que son numéricas, en vez de valores discretos.

252
00:20:44,560 --> 00:20:52,560
A los dos nos interesaba tener un gráfico que nos dijese cuándo se inicia una petición y cuándo termina esa petición,

253
00:20:52,560 --> 00:21:00,560
y también cuándo se evalúan y por qué una ejecución se ha evaluado falso y ha tenido que esperar para ser encolada.

254
00:21:00,560 --> 00:21:23,560
Se nos ocurrió la idea de crear plugins para los plugins, que permitían meter código común a todos en un mismo sitio separado de los plugins.

255
00:21:23,560 --> 00:21:35,560
Este código no tenía sentido dentro de orca en sí, porque no trataba con la paralización o ejecución de las tareas.

256
00:21:35,560 --> 00:21:46,560
Como funcionaba antes en el sistema, este es el manager del Watchdog, que es el que hacía eso de enviar la señal a SystemD diciéndole por favor no me mates.

257
00:21:46,560 --> 00:21:57,560
Como podéis ver, hay bastante código, y es bastante complejo, tiene bastante boilerplate y bastante repetición con el manager.

258
00:21:57,560 --> 00:22:02,560
Luego con Plugable se queda en esto, mucho más sencillo, elegante y fácil de entender qué está haciendo.

259
00:22:02,560 --> 00:22:18,560
Como diseñamos una API que fuese suficientemente flexible para permitirnos hacer todos estos casos de uso del manejo de recursos, lo de SystemD,

260
00:22:18,560 --> 00:22:24,560
fue con este proceso de tres pasos. El primero fue analizar dónde queríamos meter código.

261
00:22:24,560 --> 00:22:32,560
En nuestro caso, la parte de procesamiento de peticiones orca la hace muy bien, entonces no nos interesaba demasiado modificar eso.

262
00:22:32,560 --> 00:22:37,560
Sin embargo, donde sí queremos meternos es en el manager la petición y el mensaje.

263
00:22:37,560 --> 00:22:49,560
Por ejemplo, cuando se recibe un mensaje, probablemente queremos registrarlo en algún sitio, en el manager queremos ver si la condición se ha evaluado a cierto falso, o la petición lo mejor queremos afectar a su ejecución.

264
00:22:49,560 --> 00:23:00,560
Lo siguiente que hicimos fue crear una estructura para la API que creamos, y la idea fue crear una estructura que fuese común a todos,

265
00:23:00,560 --> 00:23:06,560
y de esa forma es muy fácil saber qué estás buscando incluso si no sabes dónde encontrarlo.

266
00:23:06,560 --> 00:23:16,560
Por ejemplo, como estos hooks se ejecutan cuando le ocurre a alguno de estos elementos algo, decimos esta estructura para las funciones, onSubjectBird.

267
00:23:16,560 --> 00:23:29,560
Cuando un mensaje se recibe, cuando una petición se termina, y si queréis saber si quisierais hacer algo cuando una petición se inicia, sería onPetitionStart, se puede intuir.

268
00:23:29,560 --> 00:23:38,560
También añadimos una funcionalidad para que los pluggables pudiesen definir sus propios hooks, por ejemplo en el de SystemD que mostraba antes, es posible decirle,

269
00:23:38,560 --> 00:23:47,560
cámbiamelo el status de SystemD a uno concreto, y eso es un hook concreto de ese pluggable, pero que no está en ningún otro.

270
00:23:47,560 --> 00:23:56,560
Y lo último fue quitar todo lo que no fuese estrictamente necesario, con estas dos filosofías para ir eliminando cosas.

271
00:23:56,560 --> 00:24:01,560
Si no hay un caso de uso, no lo implementes, si no hay código, no hay bugs, muy sencillo.

272
00:24:01,560 --> 00:24:11,560
Y luego añadir un método nuevo, no rompe aplicaciones, pero quitarlo sí. Si alguien depende de tu API y tú quitas un método, puedes romper su aplicación, y eso es muy feo.

273
00:24:11,560 --> 00:24:21,560
Entonces, esta fue la metodología que seguimos, los posits que veis ahí abajo a la izquierda son estos tres elementos que queríamos modificar,

274
00:24:21,560 --> 00:24:34,560
y cada una de las líneas debajo son las cosas que podían ocurrir sobre cada uno de estos elementos, y todos los tachones son las cosas que o no tenían sentido o que no eran estrictamente necesarias.

275
00:24:34,560 --> 00:24:46,560
Si alguien encuentra un caso de uso para alguna de ellas, bueno, no creo que la veáis muy bien, pero si alguien encuentra un caso de uso para una de ellas, se puede añadir un método más, no hay problema.

276
00:24:46,560 --> 00:24:57,560
Entonces, en cuanto a lo que conseguimos con Orca, para Orca Vagrant, lo que nos interesaba era la velocidad de ejecución de las tareas,

277
00:24:57,560 --> 00:25:04,560
entonces, ahí tenemos unas nightlis con 40 escenarios por versión, unos smugs de 20 escenarios por versión, todos paralizados,

278
00:25:04,560 --> 00:25:12,560
y hemos visto en nuestro sistema de testeo que se han ejecutado hasta 200 en paralelo sin ningún problema.

279
00:25:12,560 --> 00:25:25,560
Para Orca Robot, lo que nos interesaba era sobre todo la flexibilidad, porque tenemos seis dispositivos físicos, y luego dos dispositivos auxiliares, por ejemplo un switch o el cliente de wifi que mencionaba antes,

280
00:25:25,560 --> 00:25:36,560
y además tenemos múltiples dispositivos de cada tipo, entonces, orquestrar la ejecución de todo esto de forma eficiente es bastante complejo y con Orca se puede hacer de forma bastante elegante.

281
00:25:36,560 --> 00:25:50,560
Y por último, lo más importante creo yo es la estabilidad que da Orca. Tenemos al menos 10 ejecuciones manuales al día, al menos 5 smugs al día, y no hemos visto ningún fallo en nuestro sistema de CI, es como una roca.

282
00:25:50,560 --> 00:25:54,560
En mi implementación del manager ha habido algún fallo, pero eso es culpa mía más que de Orca.

283
00:25:54,560 --> 00:26:13,560
Para el futuro, cosas que estamos mirando es Orca la versión 1.0, una versión estable y que no rompa la API que hemos creado, y eso va a ser pronto, como mencionaba estamos trabajando en ello en el vuelo, así que va a ser más pronto que tarde.

284
00:26:13,560 --> 00:26:26,560
Y luego atraer más usuarios, si tenéis algún tipo de tarea que requiere ejecución paralela y una complejidad de la ejecución de las tareas, a lo mejor Orca os encaja.

285
00:26:26,560 --> 00:26:47,560
Además de eso, queremos crear una librería de pluggables que pueda ser usada para todo el mundo con funcionalidad generalmente útil, por ejemplo un gráfico de carga de CPU y de memoria, o el manejo de recursos, o un timeline de ejecuciones para tener mejor introspección a lo que está haciendo Orca.

286
00:26:47,560 --> 00:26:55,560
Y con eso ya acabamos, tenéis ahí los links a todas nuestras cosas, a Orca por supuesto y al típico QR. Muchas gracias.

287
00:26:55,560 --> 00:26:56,560
Gracias.

288
00:27:05,560 --> 00:27:10,560
¿Alguna pregunta? ¿Por allá? ¿No? ¿Todo claro?

289
00:27:10,560 --> 00:27:11,560
¿Pero?

290
00:27:14,560 --> 00:27:15,560
Espera, espera, espera.

291
00:27:19,560 --> 00:27:20,560
Gracias.

292
00:27:20,560 --> 00:27:21,560
Hola.

293
00:27:22,560 --> 00:27:39,560
Yo soy muy ignorante de esos temas, pero sí me da curiosidad porque en lo que yo hago tenemos la necesidad de correr ciertas tareas en paralelo, en máquinas híbridas, un supercomputador o una GPU en un escritorio de algún tipo.

294
00:27:40,560 --> 00:27:49,560
Y no sé si ustedes han pensado si esto puede tener alguna aplicabilidad en ese tipo de escenarios.

295
00:27:49,560 --> 00:27:59,560
Lo que vi es que se maneja todo con la prioridad, entonces eso es muy simple, lo cual es muy bueno para un usuario, pero al mismo tiempo poco flexible.

296
00:27:59,560 --> 00:28:05,560
Porque solo es un número que me permite decir si lo corro o no lo corro, si lo corro primero o no.

297
00:28:05,560 --> 00:28:19,560
Pero no se puede optimizar mucho de acuerdo a otras métricas porque a veces no quiero optimizar solo que corro primero, sino dependencias, que tengan ciertas dependencias.

298
00:28:20,560 --> 00:28:24,560
Ese tipo de escenarios no sé si han pensado...

299
00:28:24,560 --> 00:28:33,560
Puede ser un stretch a lo que yo hago, pero no sé si tiene alguna aplicabilidad. Me da curiosidad.

300
00:28:33,560 --> 00:28:43,560
Yo creo que hay como dos circunstancias. Por una parte la prioridad en principio lo que define es el orden en el que se va a evaluar la petición que quieres ejecutar.

301
00:28:43,560 --> 00:28:48,560
Al final es cierto que es un número y dentro de todas sus limitaciones y alcances está restringido.

302
00:28:48,560 --> 00:28:56,560
Pero bueno, puede ser una función, es decir, en uno de los ejemplos la prioridad se asigna con el postinit de las data clases de Python.

303
00:28:56,560 --> 00:29:06,560
Lo que sugiere es del tema de recursos, lo interesante o lo potente es que se evaluaría durante la ejecución en la parte de la condición.

304
00:29:06,560 --> 00:29:10,560
Cuando una petición se extrae se evalúa una condición y una condición puede ser arbitraria.

305
00:29:10,560 --> 00:29:17,560
En este caso hemos hecho mucho encapiado en tener suficientes dispositivos, en tener la memoria RAM, en tener conectividad de red.

306
00:29:17,560 --> 00:29:23,560
Es como muy específico, pero una condición puede ser perfectamente cualquiera de las circunstancias que has planteado.

307
00:29:23,560 --> 00:29:31,560
Lo interesante es que si una condición por un casual se evaluara a falsa, también tienes la potestad de cambiar tu prioridad y podrías incrementarla.

308
00:29:31,560 --> 00:29:36,560
Porque sabes que la petición se va a volver a encolar de forma que se reasignarían todos los pesos,

309
00:29:36,560 --> 00:29:42,560
y en el caso de los pesos se reasignaría el orden y podrías tener mucho control sobre cuándo se evalúa cada petición.

310
00:29:42,560 --> 00:29:49,560
Si por ejemplo es un recurso que lo tienes muy poco, puedes decir, vale, han pasado ya cinco iteraciones en la que no tengo el recurso,

311
00:29:49,560 --> 00:29:52,560
aumento la prioridad para que se evalúe menos frecuentemente, por ejemplo.

312
00:29:52,560 --> 00:30:00,560
Y hasta que esa condición que se define a nivel de código no se cumple, la petición nunca se va a ejecutar.

313
00:30:00,560 --> 00:30:09,560
Y lo bueno es que además hay un mecanismo de no inanición en este caso, que permite que varias peticiones subsecuentes se evalúen simultáneamente.

314
00:30:09,560 --> 00:30:18,560
De forma que, por ejemplo, si las dos primeras no se pueden ejecutar, pero la tercera sí, se ejecutaría la tercera y se saltaría las dos primeras, para evitar escenarios de inanición.

315
00:30:18,560 --> 00:30:29,560
Sí, en Orca Robot tenemos ese problema de dependencias de que a lo mejor necesitas un cliente de wifi y luego un router físico, y a lo mejor no está disponible.

316
00:30:29,560 --> 00:30:38,560
Lo que se hace en ese caso es el mecanismo de LucaHead de mirar las siguientes peticiones, y eso se puede modificar desde el manager,

317
00:30:38,560 --> 00:30:43,560
y la idea es simplemente, vale, si yo no tengo este dispositivo, ¿puedo ejecutar la siguiente petición?

318
00:30:43,560 --> 00:30:49,560
Y si no, la siguiente, la siguiente, la siguiente, y en algún momento alguna podrás ejecutar o simplemente vuelves a empezar.

319
00:30:49,560 --> 00:30:56,560
Entonces, sí, Orca, para ese tipo de casos en los que tienes un recurso limitado, tanto discreto como continuo, la memoria de ramo...

320
00:30:56,560 --> 00:30:57,560
Un recurso variante.

321
00:30:57,560 --> 00:31:02,560
Sí, o que vaya variando a lo largo del tiempo. Tiene esa flexibilidad, sí.

322
00:31:04,560 --> 00:31:07,560
¿Alguna otra pregunta? ¿Todavía tenemos tiempo para una más?

323
00:31:07,560 --> 00:31:08,560
No.

324
00:31:08,560 --> 00:31:14,560
Sí. Si no hay nada más, entonces le damos las gracias de nuevo a los speakers.

325
00:31:14,560 --> 00:31:15,560
Gracias.

