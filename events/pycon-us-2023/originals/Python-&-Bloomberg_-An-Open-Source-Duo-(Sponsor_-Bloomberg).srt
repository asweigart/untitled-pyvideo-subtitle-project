1
00:00:00,000 --> 00:00:07,000
One, two, three.

2
00:00:07,000 --> 00:00:12,640
Yeah, it works.

3
00:00:12,640 --> 00:00:14,120
Welcome everyone.

4
00:00:14,120 --> 00:00:17,520
Welcome to Python and Bloomberg and Open Source Duo.

5
00:00:17,520 --> 00:00:22,500
We could just have titled this Python and Bloomberg, but nowadays you need catchy titles

6
00:00:22,500 --> 00:00:23,700
for talks, right?

7
00:00:23,700 --> 00:00:25,560
We were between Tango and Duo.

8
00:00:25,560 --> 00:00:28,400
I was more on the Tango side, but we ended up with Desnima.

9
00:00:28,400 --> 00:00:29,400
I hope you like the talk.

10
00:00:29,400 --> 00:00:33,800
Well, we're going to have multiple speakers today.

11
00:00:33,800 --> 00:00:38,720
We wanted to use this talk as, well, we want to do a similar talk every year.

12
00:00:38,720 --> 00:00:44,560
We want to use it as a yearly update on what we're doing on Bloomberg in case you're interested

13
00:00:44,560 --> 00:00:50,360
on the open source work that we are doing.

14
00:00:50,360 --> 00:00:57,440
In terms of the agenda, I'll start real quick with what's Bloomberg and how do we use Python

15
00:00:57,480 --> 00:01:01,600
and our relationship with the PSF and the Python community.

16
00:01:01,600 --> 00:01:03,720
And then we're going to have two main sections.

17
00:01:03,720 --> 00:01:05,800
One's going to be an open source update.

18
00:01:05,800 --> 00:01:07,840
This is some of the open source work that we do.

19
00:01:07,840 --> 00:01:09,360
It's a project we contribute to.

20
00:01:09,360 --> 00:01:15,160
Pablo is going to be speaking about the work that he does in C-Python as part of his Bloomberg

21
00:01:15,160 --> 00:01:16,160
work.

22
00:01:16,160 --> 00:01:21,080
Pradhan will be speaking about PIP and packaging in general.

23
00:01:21,080 --> 00:01:23,760
And there are some other Bloomberg employees that contribute to open source.

24
00:01:23,880 --> 00:01:29,080
If you see anyone, go and ask them because contributing to open source, we try to contribute

25
00:01:29,080 --> 00:01:32,000
to open source as much as possible.

26
00:01:32,000 --> 00:01:34,520
After that first section, we'll speak about some tools.

27
00:01:34,520 --> 00:01:38,680
We'll do a spotlight on some of the tools that either are already open source or that

28
00:01:38,680 --> 00:01:40,600
we are going to open source.

29
00:01:40,600 --> 00:01:48,200
I know many of you here came because you heard past the ML memory and you wish you had known

30
00:01:48,200 --> 00:01:49,200
about it better.

31
00:01:49,200 --> 00:01:53,400
So this is your opportunity because we just made yesterday PyStack open source and Pablo

32
00:01:53,400 --> 00:01:58,120
is going to speak about this tool that I know for sure many of you will be using in

33
00:01:58,120 --> 00:02:01,400
the future.

34
00:02:01,400 --> 00:02:06,240
In terms of what's Bloomberg, well what you see here is the Bloomberg terminal.

35
00:02:06,240 --> 00:02:07,640
It's kind of like the main product.

36
00:02:07,640 --> 00:02:12,440
It's basically what many of us work on.

37
00:02:12,440 --> 00:02:16,440
It's basically a software that allows you to get any kind of information related to

38
00:02:16,440 --> 00:02:17,440
finance.

39
00:02:17,440 --> 00:02:19,640
If you haven't seen it before, I would really recommend you.

40
00:02:19,640 --> 00:02:22,480
Yes, we are hiring, but that's not what I'm trying to do here.

41
00:02:22,520 --> 00:02:26,520
If you haven't seen it before, I recommend that you go to the booth and just try it out.

42
00:02:26,520 --> 00:02:31,760
Whether you have any interest on finance or not, there is really cool information that

43
00:02:31,760 --> 00:02:32,760
you can see in the terminal.

44
00:02:32,760 --> 00:02:35,560
If you just go through the booth, they can show it to you.

45
00:02:35,560 --> 00:02:38,360
You don't need to apply or anything like that.

46
00:02:38,360 --> 00:02:41,000
You can just see how it's being used.

47
00:02:41,000 --> 00:02:46,320
This is a tool that's used by many people across the globe.

48
00:02:46,320 --> 00:02:47,320
Just getting to see how it works.

49
00:02:47,320 --> 00:02:49,760
I think it's quite cool.

50
00:02:49,800 --> 00:02:56,200
Aside from the terminal, we also provide things like news, enterprise data, and some other

51
00:02:56,200 --> 00:02:57,840
enterprise program.

52
00:02:57,840 --> 00:03:02,360
That's kind of like what Bloomberg develops.

53
00:03:04,360 --> 00:03:10,760
About Bloomberg, it's a Bloomberg technology company that was founded before I was born.

54
00:03:10,760 --> 00:03:12,960
I'm not as young as I look like, I promise.

55
00:03:12,960 --> 00:03:15,720
If you want, we can speak about that afterwards.

56
00:03:15,720 --> 00:03:23,800
It was founded in 1991, which is a bit surprising given how we work.

57
00:03:23,800 --> 00:03:26,680
Many times, it feels like it's a small startup.

58
00:03:26,680 --> 00:03:30,920
We have hundreds of thousands of subscribers all around the world.

59
00:03:30,920 --> 00:03:35,840
There are approximately 20,000 employees spread across the globe.

60
00:03:35,840 --> 00:03:38,280
We have 7,000 engineers.

61
00:03:38,280 --> 00:03:41,200
Both the company and developers are quite distributed.

62
00:03:41,680 --> 00:03:46,360
The team I work on, we have a person in San Francisco, another one in Athens, multiple

63
00:03:46,360 --> 00:03:48,480
people in London, people in New York.

64
00:03:48,480 --> 00:03:51,560
I'm working from Madrid, living the dream.

65
00:03:51,560 --> 00:03:56,280
If you're interested in traveling through Spain, Philadelphia.

66
00:03:56,280 --> 00:03:57,280
We have a person from Philadelphia.

67
00:03:57,280 --> 00:03:58,920
I always include him in New York.

68
00:03:58,920 --> 00:03:59,920
Sorry, Matt.

69
00:03:59,920 --> 00:04:02,320
Big shout out to Philadelphia.

70
00:04:02,320 --> 00:04:04,720
He's also living the dream there.

71
00:04:04,720 --> 00:04:08,000
If you're interested in any travel trips on Spain, let me know.

72
00:04:08,000 --> 00:04:11,000
I can also help you with that.

73
00:04:11,000 --> 00:04:16,520
From Bloomberg, we are trying to contribute more and more to open source.

74
00:04:16,520 --> 00:04:18,760
We have been using it for quite a long time.

75
00:04:18,760 --> 00:04:23,440
Many open source projects, and we are getting more and more involved with open source.

76
00:04:23,440 --> 00:04:26,200
I'll use you in a bit.

77
00:04:26,200 --> 00:04:28,840
A big part of Bloomberg is philanthropy.

78
00:04:28,840 --> 00:04:35,800
We also take into account contributing back to open source as part of it.

79
00:04:35,800 --> 00:04:41,160
Many companies rely on open source just to survive.

80
00:04:41,160 --> 00:04:43,960
Open source is now the base of many software companies.

81
00:04:43,960 --> 00:04:49,320
I think it's more than fair to give back.

82
00:04:49,320 --> 00:04:51,640
We have increased in person in academic research.

83
00:04:51,640 --> 00:04:55,560
If you are into research, you might have seen some recent papers, especially in the ML area

84
00:04:55,560 --> 00:04:59,360
where Bloomberg is pushing the edge.

85
00:04:59,360 --> 00:05:03,400
In terms of Python and Bloomberg, I'm excited to say that developers at Bloomberg are able

86
00:05:03,440 --> 00:05:06,760
to use the latest version of Python from day one when it's released.

87
00:05:06,760 --> 00:05:12,880
If that doesn't lure you to work in Bloomberg, I don't know what would, because it makes

88
00:05:12,880 --> 00:05:13,880
me so excited.

89
00:05:13,880 --> 00:05:18,080
The team I work on works really hard to make sure that that happens, and we are already

90
00:05:18,080 --> 00:05:22,920
working on making sure that we can use Python 3.12 as soon as it's released.

91
00:05:22,920 --> 00:05:23,920
It's amazing.

92
00:05:23,920 --> 00:05:29,160
I don't know how you experience your companies, but the company I used to work before, we

93
00:05:29,160 --> 00:05:36,080
were anchored in a 10-year-old version of Python, and being able to leverage the latest

94
00:05:36,080 --> 00:05:38,240
feature of Python just feels great.

95
00:05:38,240 --> 00:05:45,560
We also think that it enables us to better contribute to upstream, because the fact that

96
00:05:45,560 --> 00:05:50,520
we are able to use the latest version of Python allows us to also hit some of the issues,

97
00:05:50,520 --> 00:05:54,320
but to be able to contribute in a meaningful way.

98
00:05:54,320 --> 00:05:55,320
Python is number one.

99
00:05:55,320 --> 00:05:57,920
The number one language in number of repositories.

100
00:05:57,920 --> 00:06:03,320
Again, if you know Bloomberg, many of you probably know it because it used to be a C++

101
00:06:03,320 --> 00:06:04,320
shop.

102
00:06:04,320 --> 00:06:10,120
We have transformed now to a point where Python is the most used language and kind of like

103
00:06:10,120 --> 00:06:14,520
the de facto language for any new application.

104
00:06:14,520 --> 00:06:15,840
I think that's amazing.

105
00:06:15,840 --> 00:06:19,760
There are still many people doing C++ because there are many time-sensitive software that

106
00:06:19,760 --> 00:06:22,080
we need to develop.

107
00:06:22,760 --> 00:06:28,240
If you know Python, now Bloomberg is an amazing company to work for.

108
00:06:28,240 --> 00:06:31,720
We have tens of thousands of Python applications running on the system.

109
00:06:31,720 --> 00:06:36,960
We handle a huge volume of data because Bloomberg basically aggregates data from a lot of financial

110
00:06:36,960 --> 00:06:40,600
sources and provides users a lot of volume.

111
00:06:40,600 --> 00:06:42,120
We have people in the steering council.

112
00:06:42,120 --> 00:06:44,160
Paolo is here, he's going to present.

113
00:06:44,160 --> 00:06:47,360
Two core developers, multiple committers in PyPA.

114
00:06:47,360 --> 00:06:50,080
We were heavily involved in the project Jupiter.

115
00:06:50,080 --> 00:06:52,040
You're not going to see anything about that today.

116
00:06:52,040 --> 00:06:55,920
We hope to bring someone from that's involved in the project that uses Jupiter and contributes

117
00:06:55,920 --> 00:06:58,160
upstream next year.

118
00:06:58,160 --> 00:07:02,960
We've been a PSF visionary sponsor from a long time.

119
00:07:02,960 --> 00:07:06,600
You're the first one to hear this.

120
00:07:06,600 --> 00:07:12,540
We are going to be sponsoring a deputy developer in residence with a PSF.

121
00:07:12,540 --> 00:07:18,520
That means that not only we will contribute with code upstream, but we also want to...

122
00:07:18,520 --> 00:07:21,000
We want to contribute with money as well.

123
00:07:21,000 --> 00:07:29,440
We think that many open source projects sometimes don't get the funding that they need.

124
00:07:29,440 --> 00:07:36,640
We've been in the past sponsoring the packaging community and we have raised our commitment

125
00:07:36,640 --> 00:07:42,160
and now we are able to sponsor the developing residence, something that came from the core

126
00:07:42,160 --> 00:07:43,640
dev saying that they need it.

127
00:07:43,640 --> 00:07:48,440
We're super excited because this will allow to run some projects that need a person full

128
00:07:48,440 --> 00:07:50,280
fully committed to a project.

129
00:07:50,280 --> 00:07:56,200
As you can imagine, it's super hard to drive a project like Python just based only on volunteer

130
00:07:56,200 --> 00:07:57,200
time.

131
00:07:57,200 --> 00:08:05,000
I know Paolo here can speak a lot about that and Gracie Knight.

132
00:08:05,000 --> 00:08:09,480
As I was saying, I'm hoping to be able to use this lot every year to give you an update

133
00:08:09,480 --> 00:08:13,880
of what have we done in the open source space, what have we contributed that is useful for

134
00:08:13,880 --> 00:08:14,880
everyone.

135
00:08:45,000 --> 00:08:46,000
Thank you.

136
00:08:46,000 --> 00:08:49,360
It's quite funny when you go to Starbucks and say Paolo.

137
00:08:49,360 --> 00:08:51,680
I have all the possible permutations.

138
00:08:51,680 --> 00:08:52,680
Cool.

139
00:08:52,680 --> 00:08:59,240
I'm trying to talk about my work in CPython, in particular what I have done this year.

140
00:08:59,240 --> 00:09:05,120
I'm lucky enough that I convinced my employer, Bloomberg, to give me some work time to work

141
00:09:05,120 --> 00:09:10,160
on CPython, particularly these days I'm spending 50% of my time on it.

142
00:09:10,160 --> 00:09:13,400
I'm going to talk about what we did past year.

143
00:09:13,400 --> 00:09:17,560
This was a collaboration with the Microsoft team that you probably know about, the faster

144
00:09:17,560 --> 00:09:18,560
CPython.

145
00:09:18,560 --> 00:09:24,760
I'm going to show you some slides that certainly I didn't reuse from a different talk.

146
00:09:24,760 --> 00:09:25,760
It's kind of exciting.

147
00:09:25,760 --> 00:09:28,920
Basically what I'm going to show you is the collaborative effort.

148
00:09:28,920 --> 00:09:31,080
This is not things that I have done on myself.

149
00:09:31,080 --> 00:09:34,560
It's things that I have done together with the Microsoft team.

150
00:09:34,560 --> 00:09:37,520
That went into 3.11.

151
00:09:37,520 --> 00:09:41,120
It's kind of a sneak peek into how we make Python 3.11 faster.

152
00:09:41,840 --> 00:09:44,840
Obviously because the talk is quite big, I'm not going to go into detail.

153
00:09:44,840 --> 00:09:49,360
I'm super happy to discuss things later if you have questions or if you want me to go

154
00:09:49,360 --> 00:09:50,360
into detail.

155
00:09:50,360 --> 00:09:54,000
But first, let me introduce the team.

156
00:09:54,000 --> 00:09:56,200
These are most Microsoft employees.

157
00:09:56,200 --> 00:10:01,200
Eric Snow, Edith Brand, and Mark Shannon, which is the technical leader, El and Guido, and

158
00:10:01,200 --> 00:10:02,200
that guy.

159
00:10:02,200 --> 00:10:08,560
Now I'm missing Michael, which is now the team leader of the team at Microsoft.

160
00:10:08,640 --> 00:10:13,400
I think they're still hiring or is going to grow, but I'm not sure.

161
00:10:13,400 --> 00:10:17,440
Let me show you some of the things that we did on Python 3.11, which are quite cool.

162
00:10:17,440 --> 00:10:19,680
The first one is frozen modules.

163
00:10:19,680 --> 00:10:25,880
The idea of this is that one of the things that we wanted for Python is that Python starts

164
00:10:25,880 --> 00:10:26,880
faster.

165
00:10:26,880 --> 00:10:31,280
This is quite important because most people think about improvements in Python as raw

166
00:10:31,280 --> 00:10:33,640
speed, like your application runs faster.

167
00:10:33,640 --> 00:10:35,400
That's quite important for everyone.

168
00:10:35,400 --> 00:10:39,040
It turns out that starting faster is quite important as well.

169
00:10:39,040 --> 00:10:42,360
Think about you have your command line application.

170
00:10:42,360 --> 00:10:46,680
If sewing the help of your command line applications takes two seconds, it's kind of like a good

171
00:10:46,680 --> 00:10:50,040
experience, right?

172
00:10:50,040 --> 00:10:54,280
Right now the startup time is not really bad, but compare with other applications that are

173
00:10:54,280 --> 00:10:57,200
written in C or Go or whatever, it was quite bad.

174
00:10:57,200 --> 00:11:01,480
We tried to find ways we could make startup faster.

175
00:11:01,480 --> 00:11:06,960
One of the things we realized is that if you use some tools like S-Trace to see what

176
00:11:06,960 --> 00:11:07,960
is going on when you start.

177
00:11:07,960 --> 00:11:12,480
S-Trace, if you don't know it, is a tool in Linux that will show you system calls.

178
00:11:12,480 --> 00:11:16,000
This is every single time your program goes to the kernel and asks for something, which

179
00:11:16,000 --> 00:11:18,680
normally is quite expensive and it takes time.

180
00:11:18,680 --> 00:11:22,440
If you check here in the top row, it's quite small, you cannot see it, but it's called

181
00:11:22,440 --> 00:11:23,440
new F-Stat.

182
00:11:23,440 --> 00:11:28,240
That's basically every time the interpreter when it starts goes to the kernel and basically

183
00:11:28,480 --> 00:11:33,240
tries to see what is the status of a file because it's going to read it.

184
00:11:33,240 --> 00:11:37,600
If you check the amount of seconds, it's basically the most expensive call and it's quite a lot.

185
00:11:37,600 --> 00:11:41,160
Basically what is going on here is that when you start the interpreter, the interpreter

186
00:11:41,160 --> 00:11:42,160
needs to load some files.

187
00:11:42,160 --> 00:11:44,000
Normally this is the standard library, right?

188
00:11:44,000 --> 00:11:48,160
Some PYC files with the code, think about, I don't know, the collections module or the

189
00:11:48,160 --> 00:11:49,800
records module or something like that.

190
00:11:49,800 --> 00:11:52,200
This happens every single time the interpreter starts.

191
00:11:52,200 --> 00:11:55,080
It's slow because it needs to touch disk.

192
00:11:55,280 --> 00:12:00,640
If you run this thing on a hard drive from the 80s, it's going to take a lot of time,

193
00:12:00,640 --> 00:12:01,640
right?

194
00:12:01,640 --> 00:12:05,800
And you know, Bloomberg is quite an old company, so no, no, no, we run cool hard drives.

195
00:12:05,800 --> 00:12:14,680
Anyway, so the idea is that we thought that we could actually remove those and the idea

196
00:12:14,680 --> 00:12:19,600
is that instead of letting the interpreter go to the disk and this is slow hard drives

197
00:12:19,600 --> 00:12:23,360
and read for those, the idea is that we can create an interpreter that already has these

198
00:12:23,400 --> 00:12:26,080
files like statically compiled inside.

199
00:12:26,080 --> 00:12:29,720
So when you compile Python, basically you grab this bytecode from this file and you

200
00:12:29,720 --> 00:12:31,200
include it inside the interpreter.

201
00:12:31,200 --> 00:12:35,400
And if you do the same experiment afterwards, you will see that, well, this system call

202
00:12:35,400 --> 00:12:39,360
now is not the first one, it's the second one and it went from 85 calls to 14.

203
00:12:39,360 --> 00:12:40,560
You still need to read some files.

204
00:12:40,560 --> 00:12:45,840
But most of them went down and if you actually check the time, it's almost like, I think,

205
00:12:45,840 --> 00:12:46,840
one-tenth of that.

206
00:12:46,840 --> 00:12:51,440
So right now Python 3.11 starts quite fast just because it has all these modules statically

207
00:12:51,480 --> 00:12:55,480
compiled and we are still looking into ways we can reduce this number.

208
00:12:55,480 --> 00:12:58,960
And the idea at the end is like when you run your common applications or even, I don't

209
00:12:58,960 --> 00:13:01,000
know, even your server, it starts quite fast.

210
00:13:01,000 --> 00:13:06,280
So running applications made in Python feels quite snappy and not like super slow.

211
00:13:06,280 --> 00:13:09,480
This is also quite important if you run it in Docker or something like that when you

212
00:13:09,480 --> 00:13:14,840
have limited resources because then this time basically grows like hell just because it

213
00:13:14,840 --> 00:13:19,080
needs to touch more stuff or it has less CPU or the disk is lower or whatever.

214
00:13:19,480 --> 00:13:23,560
Well, anyway, so that's why Python 3.11 starts faster, which is cool.

215
00:13:23,560 --> 00:13:26,760
So, this is another thing.

216
00:13:26,760 --> 00:13:29,360
These students say frozen modules, they say inline calls.

217
00:13:29,360 --> 00:13:30,840
Let me show you what this is.

218
00:13:30,840 --> 00:13:33,520
So imagine that you have a bunch of Python calls.

219
00:13:33,520 --> 00:13:39,160
In this particular example, we have food calls bar, bar calls bath, and then you call time.sleep.

220
00:13:39,160 --> 00:13:42,520
So the idea is that C.Python, surprise, is implemented in C.

221
00:13:42,520 --> 00:13:44,480
That's the C on C.Python.

222
00:13:44,480 --> 00:13:45,680
It doesn't stand for cap board.

223
00:13:45,680 --> 00:13:47,880
It starts from C to C language.

224
00:13:48,080 --> 00:13:52,440
Basically, if you check using a debugger or whatever, what is the C stack, so basically

225
00:13:52,440 --> 00:13:57,560
the calls that you do in C to execute this Python call, you're going to see a bunch of

226
00:13:57,560 --> 00:13:58,560
C calls.

227
00:13:58,560 --> 00:14:03,280
But the idea is that every one of these calls here basically takes a lot of C calls, in

228
00:14:03,280 --> 00:14:05,720
particular all of these over here.

229
00:14:05,720 --> 00:14:10,200
So basically the idea is that every time you call a Python function, then you are calling

230
00:14:10,200 --> 00:14:13,480
like one, two, three, four, five, six, seven C functions.

231
00:14:13,480 --> 00:14:15,320
And this is quite expensive.

232
00:14:15,320 --> 00:14:16,680
These functions don't do much.

233
00:14:16,680 --> 00:14:21,040
They basically pass arguments from one to the other, or maybe they prepare some functions.

234
00:14:21,040 --> 00:14:24,080
So basically it's a call that is common for several call paths.

235
00:14:24,080 --> 00:14:28,640
But the idea is that even if they are small, just preparing these calls is quite expensive

236
00:14:28,640 --> 00:14:31,880
because you need to prepare registers and things like that.

237
00:14:31,880 --> 00:14:35,280
And that is quite bad because this happens every time you call a Python function.

238
00:14:35,280 --> 00:14:36,280
You may notice this.

239
00:14:36,280 --> 00:14:39,640
The more Python functions you call, the more expensive it is.

240
00:14:39,640 --> 00:14:44,200
So the idea that we have is that the status right now is that we have this thing called

241
00:14:44,200 --> 00:14:45,200
the evaluation loop.

242
00:14:45,520 --> 00:14:47,960
So this is the thing that evaluates the Python bytecode.

243
00:14:47,960 --> 00:14:51,640
So I mean, if you don't know it, when you write your Python code, there is something

244
00:14:51,640 --> 00:14:55,040
that compiles this thing into bytecode, which is like a set of instructions.

245
00:14:55,040 --> 00:14:56,040
And then there is this loop.

246
00:14:56,040 --> 00:14:59,640
You can imagine this as a big function that just grabs one bytecode after the other and

247
00:14:59,640 --> 00:15:00,640
evaluates.

248
00:15:00,640 --> 00:15:02,920
And if you call three functions, basically what happens is that you have one of these

249
00:15:02,920 --> 00:15:04,480
evaluation loops.

250
00:15:04,480 --> 00:15:06,200
And this loop knows about your code object.

251
00:15:06,200 --> 00:15:08,300
So this is the code that you're going to execute.

252
00:15:08,300 --> 00:15:10,520
And basically it has another thing called a frame object.

253
00:15:10,520 --> 00:15:12,920
And this basically holds the value of your local variables.

254
00:15:13,400 --> 00:15:16,200
You have the same function, but some functions have some locals.

255
00:15:16,200 --> 00:15:20,160
And some of these calls, your local maybe have the value of one, and some other call

256
00:15:20,160 --> 00:15:21,160
it has the value of three.

257
00:15:21,160 --> 00:15:22,960
So you need to hold those values somewhere.

258
00:15:22,960 --> 00:15:25,320
And basically it ends on the frame object.

259
00:15:25,320 --> 00:15:28,280
So if you call another function, basically you will have another copy of this thing.

260
00:15:28,280 --> 00:15:32,520
So you will need to copy a bunch of C functions again, so it will pay a lot of time.

261
00:15:32,520 --> 00:15:35,640
And this will have, again, the two things, so the new function that you're calling and

262
00:15:35,640 --> 00:15:36,640
another frame object.

263
00:15:36,640 --> 00:15:39,240
And if you call another function, you will have another one.

264
00:15:39,240 --> 00:15:43,600
So every time you go in one of these evaluation loops, you will pay a lot of time.

265
00:15:43,600 --> 00:15:44,600
And this is kind of okay.

266
00:15:44,600 --> 00:15:49,320
It has obviously some drawbacks apart from memory, because this is using the C stack.

267
00:15:49,320 --> 00:15:52,360
And the more you use the C stack, the more you go into a stack overflow.

268
00:15:52,360 --> 00:15:56,040
That is why you have a recursion limit in Python, among other things.

269
00:15:56,040 --> 00:15:59,720
But we say, well, this is kind of bad, because this is quite expensive, even if these colors

270
00:15:59,720 --> 00:16:01,040
are not doing anything.

271
00:16:01,040 --> 00:16:04,880
So what we did is that we removed this schema, and we are using the following idea.

272
00:16:04,880 --> 00:16:07,400
And the idea is that it's going to be one of these only.

273
00:16:07,400 --> 00:16:10,280
So there is going to be one single function that evaluates the bytecode.

274
00:16:10,280 --> 00:16:12,680
And let's say you're calling a function.

275
00:16:12,680 --> 00:16:16,160
So the idea is that the frame object knows about what the code is executing.

276
00:16:16,160 --> 00:16:19,760
That is kind of normal, because you need to store these things somewhere.

277
00:16:19,760 --> 00:16:22,800
But the idea is that if you call a new function, instead of calling one of these loops, so

278
00:16:22,800 --> 00:16:29,080
you call like seven C functions again to just get a new copy of this, well, we are going

279
00:16:29,080 --> 00:16:31,200
to basically link this there.

280
00:16:31,200 --> 00:16:35,360
So instead of calling a new function, what we do is changing the local variables of this

281
00:16:36,000 --> 00:16:37,000
function just to point to the new object.

282
00:16:37,000 --> 00:16:38,000
And it's the same function.

283
00:16:38,000 --> 00:16:39,000
It's just the same while loop.

284
00:16:39,000 --> 00:16:42,040
It's just that we change the code that is being executed on the frame.

285
00:16:42,040 --> 00:16:43,720
Obviously, I'm making it sound very easy.

286
00:16:43,720 --> 00:16:46,440
This was months of work just to get this arrow here.

287
00:16:46,440 --> 00:16:49,440
But you know, like the magic of slides.

288
00:16:49,440 --> 00:16:51,480
Very easy, very easy.

289
00:16:51,480 --> 00:16:52,920
Anyone could do it.

290
00:16:52,920 --> 00:16:55,400
And then you call a third function, then you get another arrow.

291
00:16:55,400 --> 00:16:57,800
And let me not even talk about how exceptions work here.

292
00:16:57,800 --> 00:17:00,480
Let's imagine that they don't exist.

293
00:17:00,480 --> 00:17:03,960
So the idea here is that you can see instead of having three of these guys, we have only

294
00:17:03,960 --> 00:17:04,960
one.

295
00:17:05,000 --> 00:17:08,840
Instead of calling new functions and creating new versions of this guy, we basically link

296
00:17:08,840 --> 00:17:09,840
the data.

297
00:17:09,840 --> 00:17:12,560
So this is the old trick of changing the data instead of having a function.

298
00:17:12,560 --> 00:17:17,240
You have ever have a recursive function and transforming into iterative functions via

299
00:17:17,240 --> 00:17:19,040
a for loop, right, like factorial.

300
00:17:19,040 --> 00:17:23,040
Instead of calling factorial again and again and again, do it in a while loop, quite classic.

301
00:17:23,040 --> 00:17:27,280
You have done any code interviews at Bloomberg or other companies.

302
00:17:27,280 --> 00:17:28,280
Then you will know the trick.

303
00:17:28,280 --> 00:17:32,560
But it's just like a trick that is 600 times harder because the interpreter is quite old

304
00:17:32,560 --> 00:17:33,560
and you know it's harder.

305
00:17:34,200 --> 00:17:36,920
But the idea is that this makes the interpreter quite fast.

306
00:17:36,920 --> 00:17:39,960
Actually depending on your use case, it may be like 200 times faster.

307
00:17:39,960 --> 00:17:43,800
Obviously that only happens when you have a super recursive function and your only cost

308
00:17:43,800 --> 00:17:44,800
is calling functions.

309
00:17:44,800 --> 00:17:50,520
But in general I think it makes it like around 7% faster on average, which is quite cool.

310
00:17:50,520 --> 00:17:55,040
It depends on your workload obviously, but the idea is quite simple as you can see.

311
00:17:55,040 --> 00:17:58,560
We call this inline functions, which is kind of like not really what's going on, but we

312
00:17:58,560 --> 00:18:00,560
think the name is cool.

313
00:18:00,560 --> 00:18:01,560
That's what matters.

314
00:18:02,560 --> 00:18:04,560
This is something that Mark Shannon works a lot.

315
00:18:04,560 --> 00:18:06,560
It's called quickening and it's quite cool.

316
00:18:06,560 --> 00:18:07,560
Let me show you the idea here.

317
00:18:07,560 --> 00:18:12,560
So imagine that you have this function, like full of x and y, and then you add x and y.

318
00:18:12,560 --> 00:18:17,560
So the idea, like the course of Python is that when you have this function, Python doesn't

319
00:18:17,560 --> 00:18:18,560
know what you're adding here, right?

320
00:18:18,560 --> 00:18:21,560
It could be strings, it could be like elephants, I don't know, who knows?

321
00:18:21,560 --> 00:18:26,560
So the idea is that when you say, okay, Python, like kind of show me what's going on in this

322
00:18:26,560 --> 00:18:29,560
function, you will see the bytecode, so the instructions are going to be executed.

323
00:18:29,560 --> 00:18:30,560
And you will see that it's quite simple.

324
00:18:30,560 --> 00:18:31,560
It will have two load fast.

325
00:18:31,560 --> 00:18:34,560
This is basically loading x and y into the stack.

326
00:18:34,560 --> 00:18:37,560
And it has this binary op and return value.

327
00:18:37,560 --> 00:18:40,560
Obviously, because we don't know what's going on in that plus, this binary op could be anything.

328
00:18:40,560 --> 00:18:43,560
Literally, it could be the whole cost of addition.

329
00:18:43,560 --> 00:18:45,560
I don't know, you could even go to databases in that app.

330
00:18:45,560 --> 00:18:46,560
Please don't do that.

331
00:18:46,560 --> 00:18:50,560
But it can be quite expensive.

332
00:18:50,560 --> 00:18:55,560
And normally, same programmers, I mean, just by yourself if you are one of those, but normally

333
00:18:55,560 --> 00:18:58,560
they pass the same values to the same function, right?

334
00:18:58,560 --> 00:19:02,560
Maybe you use this function to add strings and numbers because you know why not.

335
00:19:02,560 --> 00:19:06,560
But normally you pass only strings or only numbers because it will have a better name

336
00:19:06,560 --> 00:19:07,560
than just full, right?

337
00:19:07,560 --> 00:19:12,560
And the idea is that with Python 3.11, Python will notice if you are passing all the time

338
00:19:12,560 --> 00:19:16,560
the same values, let's say you are passing integers, it will basically mutate the bytecode

339
00:19:16,560 --> 00:19:17,560
on the fly.

340
00:19:17,560 --> 00:19:21,560
So basically, instead of having those two load fasts and then the generic binary op,

341
00:19:21,560 --> 00:19:24,560
you will notice that you are passing only integers and it will basically create this

342
00:19:24,560 --> 00:19:26,560
adaptive bytecode.

343
00:19:26,560 --> 00:19:28,560
And here, there is a bunch of changes.

344
00:19:28,560 --> 00:19:30,560
So for instance, you have now this load fast load fast.

345
00:19:30,560 --> 00:19:34,560
So the idea here is that instead of loading one variable and then loading the other, it

346
00:19:34,560 --> 00:19:37,560
loads both in the same instruction and then it skips over the next one.

347
00:19:37,560 --> 00:19:41,560
The reason there is still a load fast is because we cannot change the number of instructions.

348
00:19:41,560 --> 00:19:43,560
The code cannot change the number of instructions.

349
00:19:43,560 --> 00:19:47,560
What we do is that basically we put both load fasts into the first one and then we skip over

350
00:19:47,560 --> 00:19:51,560
the next, which looks a bit confusing, but you know that's life.

351
00:19:51,560 --> 00:19:58,560
And then if you check the binary addition, now it's called binary op add integer.

352
00:19:58,560 --> 00:20:03,560
And basically, as you can imagine, rocket science, it adds integers because it knows

353
00:20:03,560 --> 00:20:04,560
that you are adding integers.

354
00:20:04,560 --> 00:20:05,560
It does a bit more.

355
00:20:05,560 --> 00:20:08,560
Basically, it checks that you are still passing integers because you may be deciding that

356
00:20:08,560 --> 00:20:11,560
you now suddenly are passing floats or something.

357
00:20:11,560 --> 00:20:12,560
If you do that, it will de-optimize.

358
00:20:12,560 --> 00:20:16,560
It will learn that you are passing floats and it will change to binary op add float.

359
00:20:16,560 --> 00:20:19,560
But the idea here is that because this knows that these are integers, they don't need to

360
00:20:19,560 --> 00:20:23,560
figure out what are those things, they don't need to figure out using a lot of C code what

361
00:20:23,560 --> 00:20:25,560
to do, and it will run faster.

362
00:20:25,560 --> 00:20:29,560
And it will still work if you start passing some other stuff.

363
00:20:29,560 --> 00:20:33,560
And the idea is that when that happens, if you start passing something else, it will

364
00:20:33,560 --> 00:20:36,560
change to this call binary op adaptive and this is the instruction that is basically

365
00:20:36,560 --> 00:20:37,560
learning.

366
00:20:37,560 --> 00:20:40,560
So basically, the way this works is that first it will go to the adaptive version.

367
00:20:40,560 --> 00:20:43,560
It will learn that you are passing integers and it will optimize to integers or floats

368
00:20:43,560 --> 00:20:44,560
or something else.

369
00:20:44,560 --> 00:20:47,560
We have a bunch of those, so it will not, like if you are passing numpy arrays, it will

370
00:20:47,560 --> 00:20:52,560
not basically change to binary add numpy array because we have only a certain amount

371
00:20:52,560 --> 00:20:53,560
of types.

372
00:20:53,560 --> 00:20:58,560
But in general, for types that are built in, we cover mostly all the important ones.

373
00:20:58,560 --> 00:21:04,560
And we plan to keep adding versions of these in Python 3.12 and beyond.

374
00:21:04,560 --> 00:21:07,560
So, well, this is the version with floats.

375
00:21:07,560 --> 00:21:08,560
That's it.

376
00:21:08,560 --> 00:21:11,560
And the idea is that you have a bunch of these operations that are adaptive.

377
00:21:11,560 --> 00:21:15,560
So here, for instance, this is like a chunk for the what's new on Python 3.11 and you

378
00:21:16,560 --> 00:21:19,560
can see that all these operations, additions, like, you know, subscripts, like, assignments,

379
00:21:19,560 --> 00:21:20,560
et cetera.

380
00:21:20,560 --> 00:21:24,560
So all of these operations learn about the types that you are passing.

381
00:21:24,560 --> 00:21:28,560
And the idea is that they will compile into a more specific and specialized version as

382
00:21:28,560 --> 00:21:29,560
it's running.

383
00:21:29,560 --> 00:21:34,560
And these are the ones that we have in Python 3.11 and how much faster it makes the operation

384
00:21:34,560 --> 00:21:35,560
itself.

385
00:21:35,560 --> 00:21:39,560
But we plan to keep adding these things because we already have kind of the framework to do

386
00:21:39,560 --> 00:21:40,560
it.

387
00:21:40,560 --> 00:21:41,560
So there is more.

388
00:21:41,560 --> 00:21:46,560
Obviously, this is just some of them that I'm very excited about and that I collaborated

389
00:21:46,560 --> 00:21:47,560
in some way or form.

390
00:21:47,560 --> 00:21:49,560
But let me show you kind of the results.

391
00:21:49,560 --> 00:21:56,560
So, for instance, this is kind of a public benchmark that we have in the speed.python.org

392
00:21:56,560 --> 00:22:01,560
website, sorry, when we track kind of the speed.

393
00:22:01,560 --> 00:22:06,560
This particular is a bloomer application, by the way, which the data is obviously, like,

394
00:22:06,560 --> 00:22:07,560
is real but it is salted.

395
00:22:08,560 --> 00:22:11,560
But the thing is you can see that more or less it goes like 20% faster just by using

396
00:22:11,560 --> 00:22:12,560
Python 3.11.

397
00:22:12,560 --> 00:22:16,560
And you can see, like, from, I don't know, like, what is this, like, March or something,

398
00:22:16,560 --> 00:22:20,560
like, how it went faster and faster as we had a new optimization.

399
00:22:20,560 --> 00:22:23,560
Sometimes we screw up, as you can see here.

400
00:22:23,560 --> 00:22:26,560
But we are not going to, let's call it outliers, right?

401
00:22:26,560 --> 00:22:29,560
It's not that we did something wrong, right?

402
00:22:29,560 --> 00:22:31,560
Certainly not me.

403
00:22:31,560 --> 00:22:32,560
Anyway, so it goes faster.

404
00:22:32,560 --> 00:22:35,560
This is just a bloomer application, so it was great for us.

405
00:22:35,560 --> 00:22:38,560
But like here are the rest of the Python benchmarks that we measured at the time.

406
00:22:38,560 --> 00:22:42,560
And you can see that in general some of them were, like, even twice as fast.

407
00:22:42,560 --> 00:22:45,560
But you see actually the slower one, it goes, like, 6% faster.

408
00:22:45,560 --> 00:22:50,560
So if your slow benchmark goes 6% faster, like, you probably deserve a raise, right?

409
00:22:50,560 --> 00:22:52,560
So hear that out.

410
00:22:52,560 --> 00:22:54,560
Anyway, so Python 3.11, quite cool.

411
00:22:54,560 --> 00:22:56,560
Really great improvements.

412
00:22:56,560 --> 00:22:57,560
It goes quite faster.

413
00:22:57,560 --> 00:22:58,560
Please use Python 3.11.

414
00:22:58,560 --> 00:23:00,560
It's a super cool interpreter.

415
00:23:00,560 --> 00:23:04,560
I'm certainly not saying this only because I own the release manager of 3.11.

416
00:23:04,560 --> 00:23:08,560
But, you know, like, we collaborated and we are quite happy with the improvements.

417
00:23:08,560 --> 00:23:12,560
And I'm looking forward to keep working on Python 3.12 and following.

418
00:23:12,560 --> 00:23:15,560
And that's everything I want to say for C Python.

419
00:23:15,560 --> 00:23:18,560
Obviously, if you want to talk more, I'm super happy to do afterwards.

420
00:23:18,560 --> 00:23:21,560
But I'm going to pass this to Priam, which is going to talk about PIP.

421
00:23:29,560 --> 00:23:31,560
Let's see if the clicker works.

422
00:23:31,560 --> 00:23:33,560
It does.

423
00:23:33,560 --> 00:23:34,560
Hello.

424
00:23:34,560 --> 00:23:35,560
I'm Pradyun.

425
00:23:35,560 --> 00:23:37,560
I wear many hats in open source spaces.

426
00:23:37,560 --> 00:23:42,560
But relevant to the next few minutes, I am a maintainer of PIP.

427
00:23:42,560 --> 00:23:46,560
And well, today I'm specifically going to talk about some of the cool things I've done in

428
00:23:46,560 --> 00:23:52,560
and around Python packaging that were a part of my day job effectively.

429
00:23:52,560 --> 00:23:56,560
I think most of the audience in the room knows what PIP is.

430
00:23:56,560 --> 00:24:02,560
It's one of the tools in the ecosystem that basically everybody has used, indirectly or directly.

431
00:24:02,560 --> 00:24:03,560
Probably directly.

432
00:24:03,560 --> 00:24:09,560
And the fact that nearly everyone who uses Python uses PIP is precisely why it's particularly

433
00:24:09,560 --> 00:24:11,560
valuable to make improvements to it.

434
00:24:11,560 --> 00:24:17,560
Like making the error messages just better, making it easier to understand what's happening

435
00:24:17,560 --> 00:24:21,560
when things go wrong and making it nicer to look at.

436
00:24:21,560 --> 00:24:28,560
It's nice to not get a wall of red text, especially when the alternative is a structured error

437
00:24:28,560 --> 00:24:32,560
that is easier to read, provides more context and guidance.

438
00:24:32,560 --> 00:24:34,560
Yeah, that's readable.

439
00:24:34,560 --> 00:24:37,560
And this is a significantly better error message.

440
00:24:37,560 --> 00:24:41,560
And these are the sorts of things that I've worked on.

441
00:24:41,560 --> 00:24:48,560
And it's a big piece of making that happen is that I was doing this as my day job.

442
00:24:48,560 --> 00:24:57,560
This is another giant wall of red that is one no longer a wall of red and explains in

443
00:24:57,560 --> 00:24:59,560
better detail what was happening.

444
00:24:59,560 --> 00:25:06,560
And it's easier to understand, well, what went wrong, what happened, and so on.

445
00:25:06,560 --> 00:25:10,560
These colorful error messages aren't the only thing I've worked on.

446
00:25:10,560 --> 00:25:15,560
One of the lowest-level pieces in the ecosystem is the parser for requirements, which are

447
00:25:15,560 --> 00:25:17,560
the things you write within the requirements files.

448
00:25:17,560 --> 00:25:19,560
Names are great.

449
00:25:19,560 --> 00:25:24,560
It didn't print nice error messages, as is probably self-evident there.

450
00:25:24,560 --> 00:25:30,560
And they've been really difficult to diagnose because the syntax is always slightly tricky.

451
00:25:30,560 --> 00:25:36,560
And here's a preview of what's coming in a version of PIP later this year.

452
00:25:36,560 --> 00:25:42,560
It's the same input as the last call, but now the error has more context, and it's pointing

453
00:25:42,560 --> 00:25:48,560
exactly what went wrong and why, and it's clearly stating what was expected.

454
00:25:48,560 --> 00:25:50,560
Generally, it's easier to understand.

455
00:25:50,560 --> 00:25:54,560
And there's more where that came from.

456
00:25:54,560 --> 00:25:59,560
This is my excuse to showcase a few of the cool ones that I found fun to work on.

457
00:25:59,560 --> 00:26:04,560
What enabled these was basically a parser rewrite, which actually happened as a contribution

458
00:26:04,560 --> 00:26:09,560
to the project that then needed to be picked up and driven to a completion.

459
00:26:09,560 --> 00:26:11,560
Well, open source.

460
00:26:11,560 --> 00:26:15,560
And the rewrite helped in more than one way.

461
00:26:15,560 --> 00:26:21,560
It removed a dependency in a foundational package, and the whole process is a lot faster.

462
00:26:21,560 --> 00:26:26,560
Turns out there was a lot of low-hanging fruit, and taking a closer look at things that have

463
00:26:26,560 --> 00:26:34,560
been there since before I started using Python helps make it easier to look at what can be faster.

464
00:26:34,560 --> 00:26:38,560
And it isn't just that as well.

465
00:26:38,560 --> 00:26:44,560
There's other pieces around Python that have Python's packaging tooling that have gotten quicker.

466
00:26:44,560 --> 00:26:51,560
PIP, for example, has gotten much quicker at certain parts of building your projects.

467
00:26:51,560 --> 00:26:57,560
This screenshot is probably my favorite PR out of the bunch, which utilized some import

468
00:26:57,560 --> 00:27:02,560
lib magic, and now local package installations are much quicker.

469
00:27:02,560 --> 00:27:09,560
The number is from a random test on my MacBook Air, 67 seconds outside of PIP's control,

470
00:27:09,560 --> 00:27:13,560
and it's a 12-second build, and now that shaves three seconds off of that.

471
00:27:13,560 --> 00:27:20,560
This improvement is visible for most builds, and it's especially drastic for things that involve NFS.

472
00:27:25,560 --> 00:27:32,560
One of the pieces that I'm most excited about that's actually just been released a couple of days ago

473
00:27:32,560 --> 00:27:37,560
as we were coming to Python is a significant improvement in PIP's dependency resolver,

474
00:27:37,560 --> 00:27:42,560
where it's ridiculously better.

475
00:27:42,560 --> 00:27:49,560
It started again as a contribution from someone in the community, and there's just been a lot of discussion

476
00:27:49,560 --> 00:27:54,560
around how do we make this happen, and lots of people providing input into that.

477
00:27:54,560 --> 00:28:02,560
And, well, this had multiple pieces in trying to go from, okay, this is theoretically what we can do,

478
00:28:02,560 --> 00:28:06,560
to actually having it implemented, rolled out, and all of those fun pieces.

479
00:28:06,560 --> 00:28:12,560
And all of that is made easier by being able to sit and work on it for a whole bunch of time.

480
00:28:12,560 --> 00:28:17,560
With that, I'm going to hand it over to Laszlo to talk about Shrinkwrap.

481
00:28:26,560 --> 00:28:31,560
Hi, everyone. I'm going to talk about Shrinkwrap, which is a packaging tool

482
00:28:31,560 --> 00:28:35,560
which powers literally thousands and thousands of application deployments in Bloomberg.

483
00:28:35,560 --> 00:28:38,560
In this talk, I'm going to talk a little bit about what is Shrinkwrap

484
00:28:38,560 --> 00:28:43,560
and why we felt the need to build something like this, a bit about the motivation,

485
00:28:43,560 --> 00:28:47,560
a little bit about how it works, really high level,

486
00:28:47,560 --> 00:28:52,560
and I want to showcase some of the sort of open source integrations

487
00:28:52,560 --> 00:28:55,560
and some of the tools we are using and leveraging in Shrinkwrap.

488
00:28:55,560 --> 00:29:01,560
So first of all, what is Shrinkwrap? It's a self-contained application bundler.

489
00:29:01,560 --> 00:29:08,560
There are other similar tools. What these tools tend to do is that they take everything

490
00:29:08,560 --> 00:29:12,560
which belongs to your application, including the application code, your dependencies,

491
00:29:12,560 --> 00:29:19,560
and the Python interpreter itself as well. I think this is slightly unique to Shrinkwrap.

492
00:29:19,560 --> 00:29:25,560
It takes all of these and creates an independent bundle, which then you can deploy to a machine

493
00:29:25,560 --> 00:29:27,560
which doesn't even have to have Python installed.

494
00:29:27,560 --> 00:29:33,560
So it basically pretty much decouples the runtime environment from your deployment environment,

495
00:29:33,560 --> 00:29:37,560
from your build environment. This enables rapid deployments.

496
00:29:37,560 --> 00:29:43,560
It enables you to upgrade Python and your dependencies whenever you want, very quickly, very rapidly.

497
00:29:43,560 --> 00:29:50,560
This tool at the moment is supported on Linux, Solaris, and AIX. Yeah, we use those platforms.

498
00:29:50,560 --> 00:29:54,560
It also works on Mac OS, but that's not a supported deployment platform in Bloomberg.

499
00:29:54,560 --> 00:30:03,560
To better understand why we built Shrinkwrap, I think it makes sense to talk a little bit about the past,

500
00:30:03,560 --> 00:30:06,560
how things worked before Shrinkwrap existed.

501
00:30:06,560 --> 00:30:13,560
So in general, packaging and deployment in Bloomberg for all application or all languages

502
00:30:13,560 --> 00:30:19,560
work through a semi-proprietary packaging standard.

503
00:30:19,560 --> 00:30:27,560
You can imagine something like a trunk-based development model where you have packages for C++, for Node.js, for Python.

504
00:30:27,560 --> 00:30:33,560
They will have generally one version at any given time of each of your packages you're going to use.

505
00:30:33,560 --> 00:30:36,560
And then you would deploy these in a system-wide deployment.

506
00:30:36,560 --> 00:30:45,560
This means that you usually can only have a single version of a package installed in any given machine at any given time.

507
00:30:45,560 --> 00:30:51,560
That means that all the applications on that machine, and pretty much across all of the machines,

508
00:30:51,560 --> 00:30:58,560
will share that single version of dependency, which is a significant drawback of this model, especially for something like Python.

509
00:30:58,560 --> 00:31:04,560
This works really well for something like C++ when you build everything statically.

510
00:31:04,560 --> 00:31:10,560
Then you don't really worry about this much, and especially if you don't have a lot of external dependencies.

511
00:31:10,560 --> 00:31:18,560
But for something like Python, where we really want to leverage this entire massive ecosystem on PyPI, it becomes an issue.

512
00:31:18,560 --> 00:31:26,560
This also means that dependency upgrades can become complicated because now you upgrade something like, I don't know, NumPy.

513
00:31:26,560 --> 00:31:32,560
You upgrade it to a new version, and it might affect a lot of applications which you don't want.

514
00:31:32,560 --> 00:31:36,560
This can slow down adoption of new versions of packages.

515
00:31:36,560 --> 00:31:43,560
It also means that every single Python package needs to be repackaged for this custom, semi-proprietary packaging standard,

516
00:31:43,560 --> 00:31:49,560
which again can inhibit adoption of newer versions and new PyPI packages.

517
00:31:49,560 --> 00:31:54,560
And lastly, it requires some amount of custom tooling in the development process.

518
00:31:54,560 --> 00:31:59,560
It can mean that you wouldn't be able to necessarily use something like VirtualM or PIP,

519
00:31:59,560 --> 00:32:05,560
or whatever is the latest and greatest open source tool you can plug into your development process,

520
00:32:05,560 --> 00:32:09,560
because you need some proprietary tooling in the place.

521
00:32:09,560 --> 00:32:18,560
How things are now with shrink wrap, how things have changed is that now developers can freely pick whatever dependency version they want.

522
00:32:18,560 --> 00:32:21,560
It will only affect their application because everything is self-contained.

523
00:32:21,560 --> 00:32:25,560
So they can easily perform package upgrades because it only affects their application.

524
00:32:25,560 --> 00:32:30,560
They can do this without impacting any other application.

525
00:32:30,560 --> 00:32:37,560
They can easily roll out and back out these applications whenever they want without impacting any other applications.

526
00:32:37,560 --> 00:32:48,560
And they can now leverage PyPI packages, basically use any standard OSS, Python tooling, PIP, VirtualM, whatever they really want,

527
00:32:48,560 --> 00:32:56,560
because this all leverages Python packaging standards, and you can just use PyPI packages.

528
00:32:56,560 --> 00:33:02,560
So a little bit really high level overview of what shrink wrap is, how it works.

529
00:33:02,560 --> 00:33:04,560
The blue rectangle is shrink wrap there.

530
00:33:04,560 --> 00:33:06,560
So it takes source code and configuration.

531
00:33:06,560 --> 00:33:08,560
The configuration is PyProjectorTuml.

532
00:33:08,560 --> 00:33:10,560
You feed this into shrink wrap.

533
00:33:10,560 --> 00:33:12,560
It will install the declared dependencies.

534
00:33:12,560 --> 00:33:17,560
This is one of the distinguishing features, and the reason why we didn't go with something like PyInstaller,

535
00:33:17,560 --> 00:33:31,560
because we wanted a completely declared sort of and dependent installation model where you can say these are the things I need and these are the things I want to bundle.

536
00:33:31,560 --> 00:33:37,560
So it will install all these declared dependencies using PIP into a staging environment, which is basically just VirtualM.

537
00:33:37,560 --> 00:33:46,560
And then it will start modeling all the library dependencies, your application code, the Python interpreter, all the native dependencies as well.

538
00:33:46,560 --> 00:33:50,560
So you can have extensions. You can have native extensions in your application.

539
00:33:50,560 --> 00:33:53,560
It will create custom hooks, runtime hooks.

540
00:33:53,560 --> 00:33:55,560
This is needed for certain things.

541
00:33:55,560 --> 00:34:08,560
For example, you don't want to use a certificate bundle shipped with some random PyPI package, because that can go out of date once you deploy that, or time zone databases.

542
00:34:08,560 --> 00:34:17,560
We always want to use those things from the machines, because we know that those machines will always have the canonical correct version of these things.

543
00:34:17,560 --> 00:34:31,560
And finally, it generates launcher scripts, which will ensure that whatever script you launch your application through will use the bundled interpreter and not something else on the machine, a different version of the interpreter.

544
00:34:31,560 --> 00:34:34,560
And then finally, this just creates an output bundle.

545
00:34:34,560 --> 00:34:40,560
It's really just a directory containing all of these dependencies and your application and the interpreter.

546
00:34:40,560 --> 00:34:44,560
And you can take the bundle, copy it to any machine you want.

547
00:34:44,560 --> 00:34:55,560
It only requires a minimum set of packages from the operating system and facilities, and then it should work.

548
00:34:55,560 --> 00:35:00,560
A few words about some of the open source tools we're using in ShrinkRAMP.

549
00:35:00,560 --> 00:35:05,560
Obviously, PIP, as I mentioned earlier, we use that to install these dependencies.

550
00:35:05,560 --> 00:35:10,560
We use virtual LAMP as the staging environment where we're installing these packages.

551
00:35:10,560 --> 00:35:13,560
We have a custom dependency resolver based on ResolveLib.

552
00:35:13,560 --> 00:35:20,560
This is necessary because we do offer integration between PyPI and our custom internal packaging format.

553
00:35:20,560 --> 00:35:23,560
So users can use that if they want.

554
00:35:23,560 --> 00:35:31,560
And just to make it possible to integrate these two and have two different package sources work together, we have a custom resolver.

555
00:35:31,560 --> 00:35:39,560
We use PyALF tools to inspect LLBinaries and use PatchALF to do dark magic with these LLF files to relocate things.

556
00:35:39,560 --> 00:35:41,560
And we are patching LLF files.

557
00:35:41,560 --> 00:35:49,560
We use different mechanisms on SolarCynX where this wouldn't work.

558
00:35:49,560 --> 00:36:02,560
And I think it's important to mention or highlight here that we either employ maintainers for almost all of these projects or at least we have significant contributions in these projects.

559
00:36:02,560 --> 00:36:12,560
And lastly, if you own a similar tool or you use a similar tool or you have a similar problem but you don't have a good tool to solve this problem,

560
00:36:12,560 --> 00:36:14,560
we would love to hear from you.

561
00:36:14,560 --> 00:36:20,560
Either catch us on this on PyCon or just shoot an email to one of these email addresses.

562
00:36:20,560 --> 00:36:24,560
We'd really love to hear from other folks and their experiences.

563
00:36:24,560 --> 00:36:27,560
There might be an open source feature for shrink up as well.

564
00:36:27,560 --> 00:36:28,560
Who knows?

565
00:36:28,560 --> 00:36:31,560
But we'd really love to hear from you.

566
00:36:31,560 --> 00:36:39,560
And with that, I'm going to hand it to Matt to talk about the most amazing Python memory profiler ever, MemRay.

567
00:36:45,560 --> 00:36:46,560
All right.

568
00:36:46,560 --> 00:36:48,560
So, yeah, I'm going to talk about MemRay.

569
00:36:48,560 --> 00:36:50,560
I am one of the maintainers.

570
00:36:50,560 --> 00:36:59,560
Although I think everyone you've heard talking today has contributed something to MemRay over time, pretty major contributions from everyone.

571
00:36:59,560 --> 00:37:01,560
I'm one of the current maintainers, though.

572
00:37:01,560 --> 00:37:04,560
As Laszlo said, it's a memory profiler.

573
00:37:04,560 --> 00:37:07,560
So what is a memory profiler?

574
00:37:07,560 --> 00:37:14,560
It's a debugging tool that lets you figure out where your process is spending all of its memory.

575
00:37:14,560 --> 00:37:25,560
That tends to be a major concern that people have when they have deployed application that turns out to be using much more memory than they expected.

576
00:37:25,560 --> 00:37:28,560
And they need to track down exactly why that's happening.

577
00:37:28,560 --> 00:37:46,560
So what MemRay does is for every allocation, it tracks the stack where that allocation happened at, the thread it happened on, the size of that allocation, the allocator that was used to make that allocation,

578
00:37:46,560 --> 00:37:52,560
and then when that allocation went on to be freed and if it went on to be freed.

579
00:37:52,560 --> 00:38:08,560
Given that, we're able to show you the full path to where that allocation happened from the Python functions that were called right through any C functions that were called.

580
00:38:08,560 --> 00:38:12,560
So you can see here that this is an allocation happening inside of NumPy.

581
00:38:12,560 --> 00:38:22,560
We're able to show you the full stack to that with the Python frame showing in green and the C frame showing there in gray.

582
00:38:22,560 --> 00:38:32,560
One cool thing that we do is we output one kind of capture file that can be then used to generate multiple kinds of reports.

583
00:38:32,560 --> 00:38:43,560
So here you see us generating a statistics report that's showing you a histogram of allocations by size.

584
00:38:43,560 --> 00:38:48,560
We have a table of just all of the allocations that were performed while tracking was active.

585
00:38:48,560 --> 00:38:54,560
We can show a tree showing where the biggest allocations that the process made occurred.

586
00:38:54,560 --> 00:39:03,560
We can show a flame graph that identifies the different allocations broken out by the size that they are,

587
00:39:03,560 --> 00:39:09,560
as well as the ability to switch between threads and show what different threads were doing at different times,

588
00:39:09,560 --> 00:39:23,560
and even a live view that is able to update as the program is running and tell you where the program is currently spending its memory as it runs.

589
00:39:23,560 --> 00:39:33,560
So we also, over the last year since we released this, we released this in last April, so right before PyCon 2022.

590
00:39:33,560 --> 00:39:44,560
Since then, one of the major features that we released is a tracking API that lets you integrate directly in your code the ability to turn on memory tracking,

591
00:39:44,560 --> 00:39:52,560
start capturing data, and turn it off when you exit a context manager.

592
00:39:52,560 --> 00:39:59,560
We also have integration with PyTest that is able to tell you which of your tests is allocating the most memory,

593
00:39:59,560 --> 00:40:05,560
what functions in those tests are performing the largest allocations.

594
00:40:05,560 --> 00:40:16,560
You can see this is enabled by just passing the dash dash memory option to your PyTest call while the PyTest memory plugin is installed.

595
00:40:16,560 --> 00:40:22,560
We also have the ability to limit the amount of memory being used by any of your tests,

596
00:40:22,560 --> 00:40:26,560
so you can say this test should never use more than 24 megabytes of memory.

597
00:40:26,560 --> 00:40:32,560
If it does, I would like the test to fail, and the plugin can do that for you.

598
00:40:32,560 --> 00:40:42,560
One feature that we've built since then is that for this marker to work, you no longer need to provide the dash dash memory command line argument.

599
00:40:42,560 --> 00:40:48,560
You used to need to provide that. Now you get that for free whenever PyTest.mark.limit memory is used.

600
00:40:54,560 --> 00:41:01,560
The other thing I want to talk about is what has changed in the last year since we originally released this.

601
00:41:01,560 --> 00:41:06,560
I didn't make slides for all of these things, so I'm just going to run through a whole bullet list for you. Sorry.

602
00:41:07,560 --> 00:41:17,560
One of the biggest complaints we got from actual users once we released this is that the environment they were testing in often involved long-running processes

603
00:41:17,560 --> 00:41:26,560
where they weren't able to isolate the part of the program that was interesting and figure out where they wanted to dive into.

604
00:41:26,560 --> 00:41:38,560
They just ran memory across an entire hour-long run of an LP job or something, and they found that the capture files that we generated

605
00:41:38,560 --> 00:41:42,560
when you left memory running for an hour were enormous.

606
00:41:42,560 --> 00:41:47,560
Since our initial release, our capture files have been reduced and reduced and reduced in size.

607
00:41:47,560 --> 00:41:53,560
We're now down to about 98 percent smaller for the same amount of data than our original release had.

608
00:41:54,560 --> 00:42:02,560
Our flame graphs now are the HTML that we pop out for our flame graphs is now about 85 percent smaller than it used to be.

609
00:42:02,560 --> 00:42:12,560
We have compression of all of our capture files by default now, so we automatically compress everything that we output in order to shrink it even further.

610
00:42:12,560 --> 00:42:21,560
We have just a brand new feature actually as of maybe two months ago, the ability to get an aggregated capture file

611
00:42:21,560 --> 00:42:33,560
where instead of dumping all of the raw data for every allocation that occurred, we summarize it by the location that it occurred at and just dump that to the capture file.

612
00:42:33,560 --> 00:42:38,560
There's a couple of types of reports that that won't work with, but for all of our most common reports,

613
00:42:38,560 --> 00:42:50,560
that gets you the information that you need to generate like a flame graph of the high watermark usage of the process or a flame graph of what leaks occurred from the process.

614
00:42:50,560 --> 00:42:55,560
It can get you all of that using drastically smaller capture files.

615
00:42:55,560 --> 00:43:02,560
Again, something like 95 percent smaller than our already 98 percent smaller capture files.

616
00:43:02,560 --> 00:43:06,560
It's ridiculous how much we managed to squeeze into those.

617
00:43:06,560 --> 00:43:14,560
We now have a Jupiter magic that you can use to profile a cell in Jupiter or in IPython.

618
00:43:14,560 --> 00:43:27,560
We've got a new type of report that captures temporary allocation, so it's designed to help you figure out if your program is allocating memory and then very quickly after it allocates it, removing it.

619
00:43:27,560 --> 00:43:34,560
You see this happen in places where say a list is being appended to over and over and over and over.

620
00:43:34,560 --> 00:43:44,560
The way a list works in Python is that it is a dynamic array, so it resizes to some every time you need to add a new element to an array.

621
00:43:44,560 --> 00:43:47,560
It checks if there's already capacity to add that.

622
00:43:47,560 --> 00:44:01,560
And if there's not, it allocates a new larger array under the hood and starts filling that in with the it copies over all of the existing data from your list and then adds your new item to it.

623
00:44:01,560 --> 00:44:13,560
That allocation pattern means that if you're repeatedly appending to a list, you wind up repeatedly copying over all of the beginning elements of the list and then copying them into a larger array over and over and over.

624
00:44:13,560 --> 00:44:15,560
And that can be really inefficient.

625
00:44:15,560 --> 00:44:27,560
Our temporary allocations mode can help you figure out when you're doing that by figuring out when you have allocated something and then just immediately throw it away in favor of another larger thing or something like that.

626
00:44:27,560 --> 00:44:32,560
We have the ability to attach to a running process now.

627
00:44:32,560 --> 00:44:43,560
You can ask us to inject ourselves into some process that's already running and start capturing the allocations that it performs from that point forward.

628
00:44:43,560 --> 00:44:47,560
PyTest memory lets you preserve the capture files that it records.

629
00:44:47,560 --> 00:44:54,560
So in addition to giving you a report, you can ask it to save the capture files so that you can investigate them yourself after the fact.

630
00:44:54,560 --> 00:44:57,560
We have support for debug info D now.

631
00:44:57,560 --> 00:45:06,560
So if you are running in a distro where they don't ship debugging symbols for their interpreters or something like that, you may be able to download debugging symbols on the fly.

632
00:45:06,560 --> 00:45:16,560
Memory integrates with that and is able to fetch all of those symbols for the C calls, even if your interpreter doesn't have symbols built in.

633
00:45:16,560 --> 00:45:24,560
We can hide frames for imports that are happening in the flame graphs, which lets us generate much cleaner looking flame graphs.

634
00:45:24,560 --> 00:45:26,560
And we built a ton of new features.

635
00:45:26,560 --> 00:45:30,560
Probably the biggest is support for Mac OS.

636
00:45:30,560 --> 00:45:32,560
When we released it, we only had support for Linux.

637
00:45:32,560 --> 00:45:34,560
Mac OS is a huge new feature.

638
00:45:34,560 --> 00:45:41,560
Support for 3.11 turned out to be surprisingly involved because of the...

639
00:45:41,560 --> 00:45:44,560
Thanks, Pablo.

640
00:45:44,560 --> 00:45:57,560
Because of things like the inlining that Pablo talked about, the fact that now we no longer can map just one Python function call to one C call, it turned out to be surprisingly difficult to integrate with that.

641
00:45:57,560 --> 00:46:06,560
We support muscle libc now, which you might not think is a big deal, but again turned out to be way more difficult under the hood than you might expect.

642
00:46:06,560 --> 00:46:08,560
We have some amount of support for greenlit.

643
00:46:08,560 --> 00:46:18,560
So if you are using something like gevent and you have multiple threads of execution that aren't really separate operating system threads, we can tell you what's happening in each of those.

644
00:46:18,560 --> 00:46:21,560
We support tracing the Python allocators as well now.

645
00:46:21,560 --> 00:46:30,560
Previously, we only really traced the C level allocators, and we could tell you the Python stack that led to a call to malloc happening under the hood.

646
00:46:30,560 --> 00:46:40,560
But now we're able to give you the stack up to a point where space is requested from one of the Python memory pools.

647
00:46:40,560 --> 00:46:55,560
Under the hood, Python pulls memory for efficiency, and it only refills those pools using malloc when it needs more space than it is available or if you're trying to allocate something so large that it thinks that it's not worth pooling.

648
00:46:55,560 --> 00:47:12,560
We're now able to, at the expense of extra time spent tracking and larger capture files, we're able to tell you every time any Python object gets allocated, basically, instead of just the point at which the system gets asked to give more memory to Python using malloc.

649
00:47:12,560 --> 00:47:18,560
Let's see. That's almost everything. We've got a couple of little small things.

650
00:47:18,560 --> 00:47:23,560
We've got the dash C command line argument to memory run that lets you profile a single command.

651
00:47:23,560 --> 00:47:29,560
That was something that someone asked for. And we've got progress bars now is a huge one.

652
00:47:29,560 --> 00:47:39,560
One of the problems with our giant capture files that we used to have is that you didn't get any transparency into what was happening while memory was chugging along and trying to produce a report.

653
00:47:39,560 --> 00:47:47,560
And now you get at least some information about what's happening and how long it's going to take, which is hugely helpful.

654
00:47:47,560 --> 00:47:59,560
And coming up very soon, we have PRs in flight. One has landed and one is still underway to try to get the ability to do temporal allocation reports.

655
00:47:59,560 --> 00:48:13,560
So one of the things that that's really unique about memory compared to other memory profilers is we capture we capture a tremendous amount of information, much more than what other memory profilers capture.

656
00:48:13,560 --> 00:48:20,560
We have the ability to tell you where things happened in time so we can show you.

657
00:48:20,560 --> 00:48:28,560
Well, we can't today show you, but we soon will be able to show you something like if you.

658
00:48:28,560 --> 00:48:31,560
Backing up a little.

659
00:48:31,560 --> 00:48:43,560
One of the one of the types of reports that we have tells you what the high watermark is from your entire run of your process, like at what point your process had the most memory allocated.

660
00:48:43,560 --> 00:48:50,560
One of the reports that we have is the ability to tell you what was leaked from your process. So anything that was allocated and not freed.

661
00:48:50,560 --> 00:48:56,560
We have a ton more information than that and we've never really figured out a good way to show it and we think we have now.

662
00:48:56,560 --> 00:49:17,560
So we're going to give you the ability to select a range of time and figure out what the high watermark point between two points is on your flame graph or the ability to select a range of time and tell you what was allocated within that time window and not freed by the end of that time window, which lets you sort of drill down into your process as it's running.

663
00:49:17,560 --> 00:49:29,560
And you can recognize, OK, there was this big peak of memory and at that point we know that we finished all of our setup and we've imported everything that we're importing and we have built the arrays that we're going to work with for the rest of our process.

664
00:49:29,560 --> 00:49:34,560
But then there's this other little peak over here and I want to know what caused that little peak.

665
00:49:34,560 --> 00:49:43,560
We're going to give you the ability to sort of drill down into that specific area and figure out what was happening there coming very soon.

666
00:49:44,560 --> 00:49:59,560
And I think that's all I have to say about memory. So I'm going to hand it over to I think this is Pablo. Is it Pablo.

667
00:49:59,560 --> 00:50:02,560
What's up this is people.

668
00:50:02,560 --> 00:50:03,560
Cool.

669
00:50:03,560 --> 00:50:06,560
So that's the last thing. So just hang with me a bit.

670
00:50:06,560 --> 00:50:14,560
So I'm going to present the new cool stuff. It's called Pies. It has this cool logo because it's like a stack of pancakes.

671
00:50:14,560 --> 00:50:24,560
We have pancakes actually as merchandising so you can go to the Bloomberg booth and have pancakes with the logo, which is quite wild as you make like a program and then this ends in pancakes.

672
00:50:24,560 --> 00:50:27,560
Kind of a life changing event.

673
00:50:27,560 --> 00:50:32,560
Anyway, let me take a look at this. So imagine kind of like this figure this out right like you're a Bloomberg employee.

674
00:50:32,560 --> 00:50:35,560
So this is why this was created. Your Bloomberg employee is 9 a.m.

675
00:50:35,560 --> 00:50:38,560
You come to work or something if you come in 9 a.m.

676
00:50:38,560 --> 00:50:47,560
Which probably you don't but like you know figure out and then you see that there is a bug report and it's because like one Python application that has like insane amount of C extensions and whatnot.

677
00:50:47,560 --> 00:50:49,560
It just crashed.

678
00:50:49,560 --> 00:50:54,560
And because you know you work in a large company we create core files right because it crashes.

679
00:50:54,560 --> 00:51:03,560
It's like false something underneath. So you have a core file and now you know like everything is red and the world is burning and it's your task to solve the problem.

680
00:51:03,560 --> 00:51:06,560
Right. And then you say OK let me let me attach a problem.

681
00:51:06,560 --> 00:51:10,560
So let's say this is your application. It's a super enterprise ready slip.

682
00:51:10,560 --> 00:51:13,560
One thousand. It creates a lot of money.

683
00:51:13,560 --> 00:51:23,560
This this thing. So you know it crashes because you know who knows what happened maybe this lip is hardcore sleeping and then you attach a debugger right.

684
00:51:23,560 --> 00:51:32,560
And what can you attach to a core file. Well you can attach GDB for instance and it gives you this which is kind of like a lot of C code and it's not very useful right.

685
00:51:32,560 --> 00:51:37,560
And then you cry in a corner. That's the situation. That was bad. We don't like our developers to cry.

686
00:51:37,560 --> 00:51:40,560
I mean we respect that we are a very inclusive company.

687
00:51:40,560 --> 00:51:52,560
But we don't we you know we recognize that this is not useful. Why is not useful. Well because like every one of these functions actually is mapped by as you saw before anyone like six or seven of these things.

688
00:51:52,560 --> 00:52:03,560
And what is worse when you see this thing on GDB doesn't tell you which one it is. So you are only seeing this code and you don't know like actually that what's happening is that you're calling a slip and then you're calling this function and you're calling this function.

689
00:52:03,560 --> 00:52:12,560
So basically the technical term for this I think it appears on several of the notebooks is called useless is useless computer science for you anyway.

690
00:52:12,560 --> 00:52:20,560
So what what by stack does so instead of using GDB you use by stack almost the same vocation by stack minus C decor and then you get this.

691
00:52:20,560 --> 00:52:28,560
Wow incredible. It works also even if you don't have the Python binary around because like we do a bunch of dark magic and whatnot.

692
00:52:28,560 --> 00:52:43,560
That was certainly not insane to go but yeah the idea is that you get this and you get the Python stack from the core files but you can also get them from Python processes so imagine that instead of like this is scenario I just described to you when the process crashes and then you need to analyze the core file.

693
00:52:43,560 --> 00:52:49,560
Imagine that what happens is that you have a Python process that is a stack is just hung. You don't know what's going on or maybe just too slow.

694
00:52:49,560 --> 00:52:58,560
So you can use by stack as well over that process and you can attach to it from the outside and say well by stack tell me what this this process is doing and basically will bring you this.

695
00:52:58,560 --> 00:53:05,560
You will say well this process is kind of a sleeping and basically this function is calling this function is calling this function which is quite cool.

696
00:53:05,560 --> 00:53:12,560
But it was cool about this is like you know as I described before it turns out that in our imaginary story certainly not real.

697
00:53:12,560 --> 00:53:27,560
Like normally you don't use just Python. You use a lot of C and C++ and RAS and whatnot and that is kind of important right because like if you have some complicated for instance machine learning applications what you're going to have is not just this super simple enterprise ready code.

698
00:53:27,560 --> 00:53:29,560
What you're going to have is.

699
00:53:29,560 --> 00:53:37,560
I'm going back into the C++ etc etc and this is quite hardcore and the thing gets super super messy when you when you have like several levels of this.

700
00:53:37,560 --> 00:53:45,560
So sometimes this is actually not useful because like maybe between these two calls there is like some C++ code and maybe you're a Python only developer and that tells you nothing.

701
00:53:45,560 --> 00:53:54,560
But most of the time you are you know a bit of C or C++ or maybe you are even using the library or you are creating a bug report and those C and C++ calls actually tells you a lot.

702
00:53:54,560 --> 00:54:03,560
So one of the things that you can do with PyStack as well is that you can transform this madness into you remove those calls and then you can transform into that madness.

703
00:54:03,560 --> 00:54:09,560
So you can see here that you know like there is a function a bunch of C functions and then you go into your Python functions and then you again into your C functions.

704
00:54:09,560 --> 00:54:16,560
So we call this the hybrid stack and the idea here is that you can see how Python and C++ intertwine and you can see like some Python code C++.

705
00:54:16,560 --> 00:54:20,560
You can see here like let me show you one of these reports a little bit better.

706
00:54:20,560 --> 00:54:27,560
So you can see here that for instance you see that the program starts on start and it calls like start main and a bunch of like Python functions.

707
00:54:27,560 --> 00:54:32,560
So you have the C here which is C code and then it goes into your first Python function which is first function.

708
00:54:32,560 --> 00:54:34,560
It goes into a bunch of more C Python functions etc.

709
00:54:34,560 --> 00:54:39,560
Third function the sleep call and you can actually see that that sleep call doesn't end there.

710
00:54:39,560 --> 00:54:45,560
What happens is that it goes into libc and you can see that calls like into the time module in Python and finally it goes into select.

711
00:54:45,560 --> 00:54:48,560
Surprise when you call sleep on Python you are actually doing a network call.

712
00:54:48,560 --> 00:54:55,560
What about that? Well it's not a network call but it's using the select primitive to sleep because it wants to interrupt all like control C.

713
00:54:55,560 --> 00:54:58,560
Now you know something. It's not calling a sleep underneath. It's called select.

714
00:54:58,560 --> 00:55:01,560
So you can know that by using pystack. How cool is that?

715
00:55:01,560 --> 00:55:04,560
Now you're supposed to clap.

716
00:55:04,560 --> 00:55:12,560
Awesome. So yeah this is kind of the thing. It doesn't do as many things as memory but this is kind of cool enough and it's difficult enough to maintain.

717
00:55:12,560 --> 00:55:15,560
Because it needs to know all the versions of all interpreters and all of that thing.

718
00:55:15,560 --> 00:55:18,560
We support like all of them. We support even Python 3.11.

719
00:55:18,560 --> 00:55:29,560
Actually one cool thing about Python 3.11 is that when it shows you these things because of the work that I did in Python 3.11 with the cool error reporting that basically tells you not only the line where the error happens but also the columns.

720
00:55:29,560 --> 00:55:35,560
We also highlight that information here. So we show you sadly like what part of the function call the program is running.

721
00:55:35,560 --> 00:55:39,560
So I mean there is a bunch of things that I'm going to read here but the idea is that that's what it is.

722
00:55:39,560 --> 00:55:44,560
It's basically a program that is attached to a core file or a process and shows you the stack of what's going on.

723
00:55:44,560 --> 00:55:50,560
And the main idea compared to other programs that do similar things because like this is not just new in the Python ecosystem.

724
00:55:50,560 --> 00:55:53,560
You have other things that can do similar things like PySpy and Austin and things like that.

725
00:55:53,560 --> 00:56:02,560
Normally profilers. But this is a debugger. And the idea of a debugger is that it puts a huge amount of effort or gives you the information at the cost of being a bit slow because you're not profiling your program.

726
00:56:02,560 --> 00:56:07,560
You want to know what's going on. And also it works on core file which none of the profilers do at the time.

727
00:56:08,560 --> 00:56:16,560
So the idea is that for instance when you have a core file even if you don't have the Python executable or even if you don't have the libraries we're able to still show you most of the time the Python stack.

728
00:56:16,560 --> 00:56:23,560
This works even if you move the core file to a different machine. So think about that. I don't know. Some clients send you the core file and say like hey man figure this out.

729
00:56:23,560 --> 00:56:28,560
And you can use it secretly this cool tool while you are eating a pancake. So that's it. That's PyStack.

730
00:56:28,560 --> 00:56:33,560
You can learn more about in Bloomberg PyStack and you can like start the thing if you want.

731
00:56:33,560 --> 00:56:44,560
And we plan to add super exciting new features. And you can also help us with our booth and grab a free pancake I think which is cool and get you the logo as well.

732
00:56:44,560 --> 00:56:49,560
I think that's everything we have here. So thank you very much for coming. We're super excited to have you here.

733
00:56:54,560 --> 00:57:01,560
And any of us are super happy to talk about our open source involvement or Bloomberg if you are interested in Bloomberg.

734
00:57:01,560 --> 00:57:04,560
But yeah that's all. Thank you very much for coming and see you around.

