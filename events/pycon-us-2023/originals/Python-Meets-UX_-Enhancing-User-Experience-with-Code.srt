1
00:00:00,000 --> 00:00:02,000
Good morning again, everyone.

2
00:00:02,000 --> 00:00:04,000
So now we have Neeraj Pandey with us.

3
00:00:04,000 --> 00:00:06,000
So he's going to deliver a talk on

4
00:00:06,000 --> 00:00:08,000
Python meets UX

5
00:00:08,000 --> 00:00:10,000
and enhancing user experience with code.

6
00:00:10,000 --> 00:00:12,000
What do you think?

7
00:00:12,000 --> 00:00:14,000
Thank you so much. Good morning, guys.

8
00:00:14,000 --> 00:00:18,000
So this talk is about the intersection of

9
00:00:18,000 --> 00:00:21,000
AI and the user experience.

10
00:00:21,000 --> 00:00:24,000
So we're going to be discovering how we can use

11
00:00:24,000 --> 00:00:27,000
Python, which is like a very powerful tool in the AI landscape,

12
00:00:27,000 --> 00:00:30,000
and use it to enhance a user experience.

13
00:00:30,000 --> 00:00:33,000
So throughout the presentation, we will be discussing

14
00:00:33,000 --> 00:00:36,000
the importance of user experience in the digital landscape

15
00:00:36,000 --> 00:00:39,000
of any product, be it any online space

16
00:00:39,000 --> 00:00:42,000
or the offsite or offline space.

17
00:00:42,000 --> 00:00:45,000
So later on, we'll see how Python can be

18
00:00:45,000 --> 00:00:48,000
incorporated on those aspects,

19
00:00:48,000 --> 00:00:51,000
including data mining and automation

20
00:00:51,000 --> 00:00:54,000
to personalization for any product.

21
00:00:54,000 --> 00:00:57,000
So to begin with, we see

22
00:00:57,000 --> 00:01:00,000
the importance of UX, because UX

23
00:01:00,000 --> 00:01:03,000
is very crucial to understand any user

24
00:01:03,000 --> 00:01:06,000
needs and to satisfy the end user goals.

25
00:01:06,000 --> 00:01:09,000
So it not only helps in making the product

26
00:01:09,000 --> 00:01:12,000
more personalized, but also increases the

27
00:01:12,000 --> 00:01:15,000
revenue for the organization, for the companies,

28
00:01:15,000 --> 00:01:18,000
and also helps them retain the users.

29
00:01:18,000 --> 00:01:21,000
So to begin with, we need to understand

30
00:01:21,000 --> 00:01:24,000
the process, a typical process that a UX designer

31
00:01:24,000 --> 00:01:27,000
works with in its everyday life.

32
00:01:27,000 --> 00:01:30,000
So we start with our discover process, where

33
00:01:30,000 --> 00:01:33,000
all the information is being gathered from different

34
00:01:33,000 --> 00:01:36,000
multiple sources, be it a data engineering task or something.

35
00:01:36,000 --> 00:01:39,000
And then once we have a lot of data with us,

36
00:01:39,000 --> 00:01:42,000
we can define the problems for any businesses.

37
00:01:42,000 --> 00:01:45,000
So we have a concrete example now that

38
00:01:45,000 --> 00:01:48,000
we need this kind of problem to solve, we need to find a solution

39
00:01:48,000 --> 00:01:51,000
for this issue now. So then we go to the

40
00:01:51,000 --> 00:01:54,000
ideation phase, which is a more experimental

41
00:01:54,000 --> 00:01:57,000
phase where the UX designer is continuously

42
00:01:57,000 --> 00:02:00,000
brainstorming over different kinds of iterations for the product.

43
00:02:00,000 --> 00:02:03,000
And then it creates some sort of designs,

44
00:02:03,000 --> 00:02:06,000
be it a digital product or some sort of

45
00:02:06,000 --> 00:02:09,000
prototype with its own normal product.

46
00:02:09,000 --> 00:02:12,000
And then they test it with the real users

47
00:02:12,000 --> 00:02:15,000
or the test users and reiterate accordingly.

48
00:02:15,000 --> 00:02:19,000
So if we go deeper in the AI and UX space,

49
00:02:19,000 --> 00:02:22,000
we'll see there is a common train between

50
00:02:22,000 --> 00:02:25,000
the AI and UX, both goes with

51
00:02:25,000 --> 00:02:28,000
gathering the information and then finding their insights.

52
00:02:28,000 --> 00:02:31,000
And if you see more than 70% of the

53
00:02:31,000 --> 00:02:34,000
devices we use has some sort of AI services being built in.

54
00:02:34,000 --> 00:02:37,000
And when you see

55
00:02:37,000 --> 00:02:40,000
if a typical UX person is going to do all these

56
00:02:40,000 --> 00:02:43,000
in-depth processes for the product,

57
00:02:43,000 --> 00:02:46,000
for gathering information and finding insights,

58
00:02:46,000 --> 00:02:49,000
all these AI models can just do those things

59
00:02:49,000 --> 00:02:52,000
in like a fraction of minutes and so.

60
00:02:52,000 --> 00:02:55,000
So with this intersection, we can make more user-friendly products

61
00:02:55,000 --> 00:02:58,000
and help them make personalized for every specific user.

62
00:02:58,000 --> 00:03:01,000
So to begin with or to understand

63
00:03:01,000 --> 00:03:04,000
this intersection, we will be starting with

64
00:03:04,000 --> 00:03:07,000
data processing. And we see like data is

65
00:03:07,000 --> 00:03:10,000
code, we always hear this term, that data is code. Similarly for the UX

66
00:03:10,000 --> 00:03:13,000
processes, data is very crucial because it

67
00:03:13,000 --> 00:03:16,000
helps identify the insights about a specific

68
00:03:16,000 --> 00:03:19,000
user and how they are behaving to the product

69
00:03:19,000 --> 00:03:22,000
and their feedbacks. And then we can use those feedbacks

70
00:03:22,000 --> 00:03:25,000
to create new metrics and make

71
00:03:25,000 --> 00:03:28,000
decisions which are not just based on presumptions, they are like data-driven

72
00:03:28,000 --> 00:03:31,000
decisions. And the second thing we see

73
00:03:31,000 --> 00:03:34,000
is like traditionally when you see UX designers need to like spend

74
00:03:34,000 --> 00:03:37,000
hours to analyze some processes and

75
00:03:37,000 --> 00:03:40,000
gain insights. So Python can be utilized to analyze these

76
00:03:40,000 --> 00:03:43,000
kind of data from multiple resources, be it in

77
00:03:43,000 --> 00:03:46,000
forms or historical data like e-commerce,

78
00:03:46,000 --> 00:03:49,000
purchase history or buying

79
00:03:49,000 --> 00:03:52,000
studio. And then we can find insights

80
00:03:52,000 --> 00:03:55,000
which are more useful for the users.

81
00:03:55,000 --> 00:03:58,000
And the third thing is the patterns,

82
00:03:58,000 --> 00:04:01,000
the UX patterns, like every single user

83
00:04:01,000 --> 00:04:04,000
or every single pool of users have a similar pattern.

84
00:04:04,000 --> 00:04:07,000
So we can use those similar user behaviors

85
00:04:07,000 --> 00:04:10,000
to filter out those patterns

86
00:04:10,000 --> 00:04:13,000
and then make decisions without arguing data-driven.

87
00:04:13,000 --> 00:04:16,000
And a good example would be clickstream data, which helps us finding

88
00:04:16,000 --> 00:04:19,000
the navigation patterns of a user.

89
00:04:19,000 --> 00:04:22,000
This can be used to get the knowledge of

90
00:04:22,000 --> 00:04:25,000
how the user is behaving to your product

91
00:04:25,000 --> 00:04:28,000
and then we can make some decisions on like how

92
00:04:28,000 --> 00:04:31,000
and where the bottlenecks are and how we can like sufficiently

93
00:04:31,000 --> 00:04:34,000
efficiently fix those issues.

94
00:04:34,000 --> 00:04:37,000
So, and yes, so we can also like,

95
00:04:37,000 --> 00:04:40,000
you know, whenever there is a UX designer involved,

96
00:04:40,000 --> 00:04:43,000
it's not just one UX designer, there are a couple of people

97
00:04:43,000 --> 00:04:46,000
involved in a UX experience process and those processes

98
00:04:46,000 --> 00:04:49,000
get way too tedious, especially when

99
00:04:49,000 --> 00:04:52,000
the end has to be done by an engineer

100
00:04:52,000 --> 00:04:55,000
and all the workload has been like thrown up to the engineer

101
00:04:55,000 --> 00:04:58,000
and those lead to very increased latency and all.

102
00:04:58,000 --> 00:05:01,000
So these kind of specific tasks like collection of data,

103
00:05:01,000 --> 00:05:04,000
creating user personas, pain points, can be directly utilized

104
00:05:04,000 --> 00:05:07,000
and created by Python and the AI models.

105
00:05:07,000 --> 00:05:10,000
And in conclusion, we can say like

106
00:05:10,000 --> 00:05:13,000
AI can help you ease your work and then make

107
00:05:13,000 --> 00:05:16,000
products which are more user friendly.

108
00:05:16,000 --> 00:05:19,000
The second thing before we go directly to examples,

109
00:05:19,000 --> 00:05:22,000
like real life examples, we can say like if we look at the user's behavior,

110
00:05:22,000 --> 00:05:25,000
no one wants to go with a genetic kind of experience,

111
00:05:25,000 --> 00:05:28,000
they are all looking for something that is more personalized to them.

112
00:05:28,000 --> 00:05:31,000
And things like

113
00:05:31,000 --> 00:05:34,000
AI can offer these personalized

114
00:05:34,000 --> 00:05:37,000
experiences to the designers and make

115
00:05:37,000 --> 00:05:40,000
their work more accessible and efficient, which can be

116
00:05:40,000 --> 00:05:43,000
used to create like an art personalized experience

117
00:05:43,000 --> 00:05:46,000
using lots of auto data. So a good example would be like

118
00:05:46,000 --> 00:05:49,000
normal user streaming apps, you can use normally every single day

119
00:05:49,000 --> 00:05:52,000
and based on your listening performance

120
00:05:52,000 --> 00:05:55,000
the user, the system just like gives you

121
00:05:55,000 --> 00:05:58,000
the recommendations on what to listen for that.

122
00:05:58,000 --> 00:06:01,000
You don't lose customers and you are just the team of people

123
00:06:01,000 --> 00:06:04,000
who are just like listening to some specific kind of music.

124
00:06:04,000 --> 00:06:07,000
The second thing is behavioral data analysis.

125
00:06:07,000 --> 00:06:10,000
So AI can help you identify these patterns in user's behavior

126
00:06:10,000 --> 00:06:13,000
and make predictions about it.

127
00:06:13,000 --> 00:06:16,000
So for instance, we will be using this use case

128
00:06:16,000 --> 00:06:19,000
in the slides, the presentation.

129
00:06:19,000 --> 00:06:22,000
So on the customer's website, we can use those purchasing

130
00:06:22,000 --> 00:06:25,000
purchase history and their buying history

131
00:06:25,000 --> 00:06:28,000
to create some sort of

132
00:06:28,000 --> 00:06:31,000
recommendations on how we can associate items together,

133
00:06:31,000 --> 00:06:34,000
like alt text sets, and make the best use of the data we have.

134
00:06:34,000 --> 00:06:37,000
So that we, as an organization, I can improve

135
00:06:37,000 --> 00:06:40,000
and increase my users

136
00:06:40,000 --> 00:06:43,000
and everything.

137
00:06:43,000 --> 00:06:46,000
The third thing is like, again, the predictive analytics.

138
00:06:46,000 --> 00:06:49,000
We can say like the quick stream data, which we already discussed

139
00:06:49,000 --> 00:06:52,000
in the A-B test, could be a great help here to find

140
00:06:52,000 --> 00:06:55,000
the predictions about a specific user set.

141
00:06:55,000 --> 00:06:58,000
And the last thing is the real-time data targeting, which is like

142
00:06:58,000 --> 00:07:01,000
we can based on locations and diversity,

143
00:07:01,000 --> 00:07:04,000
we can make some necessary immediate changes for the users

144
00:07:04,000 --> 00:07:07,000
and which again enhances the user experience.

145
00:07:07,000 --> 00:07:10,000
So to conclude, this extension does not only

146
00:07:10,000 --> 00:07:13,000
enhance the user experience, but also provide personalized

147
00:07:13,000 --> 00:07:16,000
content to the UX designers

148
00:07:16,000 --> 00:07:19,000
so that they can make those predictions

149
00:07:19,000 --> 00:07:22,000
for the users and streamline the process.

150
00:07:22,000 --> 00:07:25,000
And the last thing we want to discuss before

151
00:07:25,000 --> 00:07:28,000
going to the example is like the automating tasks.

152
00:07:28,000 --> 00:07:31,000
Traditionally, UX designers are known to use

153
00:07:31,000 --> 00:07:34,000
these kind of manual tasks to

154
00:07:34,000 --> 00:07:37,000
complete their processes.

155
00:07:37,000 --> 00:07:40,000
And this is first of all, very time consuming.

156
00:07:40,000 --> 00:07:43,000
It's very tool-as-efficient and also leads to like

157
00:07:43,000 --> 00:07:46,000
engineering delays.

158
00:07:46,000 --> 00:07:49,000
And with the automated AI to this

159
00:07:49,000 --> 00:07:52,000
systems, we can at least not fully automate

160
00:07:52,000 --> 00:07:55,000
and at least we can do semi-automate these kind of processes,

161
00:07:55,000 --> 00:07:58,000
which again enhances the accuracy.

162
00:07:58,000 --> 00:08:01,000
And processes like A-B testing,

163
00:08:01,000 --> 00:08:04,000
wide framing can be completely changed

164
00:08:04,000 --> 00:08:07,000
when your unit has a data driven decision to make.

165
00:08:07,000 --> 00:08:10,000
And we also have to save our costs

166
00:08:10,000 --> 00:08:13,000
and engineering failures so we can also find

167
00:08:13,000 --> 00:08:16,000
early design failures into the process so that like

168
00:08:16,000 --> 00:08:19,000
the country is not losing money while not losing

169
00:08:19,000 --> 00:08:22,000
while returning the users.

170
00:08:22,000 --> 00:08:25,000
So let's put this learning into an actual case study.

171
00:08:25,000 --> 00:08:28,000
So we have something like a problem set, which is like

172
00:08:28,000 --> 00:08:31,000
enhancing user experience for a hybrid e-commerce platform.

173
00:08:31,000 --> 00:08:34,000
And here we assume a few challenges like

174
00:08:34,000 --> 00:08:37,000
we need to make this platform more personalized for the user

175
00:08:37,000 --> 00:08:40,000
and we need to understand the user behavior.

176
00:08:40,000 --> 00:08:43,000
And we also take into account that we don't want to just like, you know,

177
00:08:43,000 --> 00:08:46,000
make the UX better for the online visual space.

178
00:08:46,000 --> 00:08:49,000
We can also take care of the offline store experience.

179
00:08:49,000 --> 00:08:52,000
So let's say we have, we just opened up

180
00:08:52,000 --> 00:08:55,000
a store called Pashions for all the Pashions staff there.

181
00:08:55,000 --> 00:08:58,000
So we try to solve these issues. We already just

182
00:08:58,000 --> 00:09:01,000
assumed using a data driven strategy.

183
00:09:01,000 --> 00:09:04,000
And we also see like how this intersection can be most useful in this.

184
00:09:04,000 --> 00:09:07,000
Because this will help us find the bottlenecks

185
00:09:07,000 --> 00:09:10,000
and fix them in real time, which will be much more

186
00:09:10,000 --> 00:09:13,000
accurate than the normal traditional UX processes.

187
00:09:13,000 --> 00:09:16,000
So just to give you a little insight,

188
00:09:16,000 --> 00:09:19,000
like we don't know what data mining is,

189
00:09:19,000 --> 00:09:22,000
it's like a process of finding digital insights

190
00:09:22,000 --> 00:09:25,000
on the larger data sets. And when we have to

191
00:09:25,000 --> 00:09:28,000
put it up into the UX perspective, we can say like

192
00:09:28,000 --> 00:09:31,000
we want to find some crucial insights about the user's behavior.

193
00:09:31,000 --> 00:09:34,000
And specifically in our case, it's going to be

194
00:09:34,000 --> 00:09:37,000
a purchasing history of the user, and then

195
00:09:37,000 --> 00:09:40,000
thereby we'll be able to cater the users in a better way.

196
00:09:40,000 --> 00:09:43,000
And some specific things we're going to look at,

197
00:09:43,000 --> 00:09:46,000
it's going to be user segmentation, where we're going to include those

198
00:09:46,000 --> 00:09:49,000
specific users based on their purchasing

199
00:09:49,000 --> 00:09:52,000
history or browsing experience.

200
00:09:52,000 --> 00:09:55,000
And we're also going to analyze the feedback, which is a very crucial thing

201
00:09:55,000 --> 00:09:58,000
when you're working with the e-commerce kind of a domain,

202
00:09:58,000 --> 00:10:01,000
because the use affects a lot of things, specifically

203
00:10:01,000 --> 00:10:04,000
to find the pinpoints for a UX person

204
00:10:04,000 --> 00:10:07,000
in order to identify some details in the next iteration.

205
00:10:07,000 --> 00:10:10,000
And again, there's one more thing, which is

206
00:10:10,000 --> 00:10:13,000
the clickstream, which is a very crucial thing throughout

207
00:10:13,000 --> 00:10:16,000
the processes for the UX to understand how the users

208
00:10:16,000 --> 00:10:19,000
actually behave into the product.

209
00:10:19,000 --> 00:10:22,000
So if you see the user segmentation, we can like

210
00:10:22,000 --> 00:10:25,000
provide some personalized experience so that we can cater different

211
00:10:25,000 --> 00:10:28,000
user pools with their needs. Here we can

212
00:10:28,000 --> 00:10:31,000
make use of something like a cluster, k-means

213
00:10:31,000 --> 00:10:34,000
clustering, which is a very novel unsupervised

214
00:10:34,000 --> 00:10:37,000
learning algorithm. And then we're going to segment users

215
00:10:37,000 --> 00:10:40,000
based on their purchasing history and browsing

216
00:10:40,000 --> 00:10:43,000
history, which is basically, we'll be getting this data

217
00:10:43,000 --> 00:10:46,000
from the transaction history of a digital e-commerce

218
00:10:46,000 --> 00:10:49,000
system. And then we're going to segment those users

219
00:10:49,000 --> 00:10:52,000
into specific clusters based on their

220
00:10:52,000 --> 00:10:55,000
vector and feature similarity,

221
00:10:55,000 --> 00:10:58,000
like the browsing history, as we already discussed.

222
00:10:58,000 --> 00:11:01,000
And for example, here we are just simply

223
00:11:01,000 --> 00:11:04,000
creating whole clusters for users based on the transaction

224
00:11:04,000 --> 00:11:07,000
history and putting them to have a feature

225
00:11:07,000 --> 00:11:10,000
similarity. So with this, we can say

226
00:11:10,000 --> 00:11:13,000
we're going to get some personalized content

227
00:11:13,000 --> 00:11:16,000
created for every single user. And even

228
00:11:16,000 --> 00:11:19,000
the early training thing is like getting some

229
00:11:19,000 --> 00:11:22,000
promotions to some specific pool of users, because we don't

230
00:11:22,000 --> 00:11:25,000
want something that has been selling super fast.

231
00:11:25,000 --> 00:11:28,000
We don't want to get like most discounts on to those products,

232
00:11:28,000 --> 00:11:31,000
but we want to get something, we want to sell,

233
00:11:31,000 --> 00:11:34,000
we want to make the things without being in the

234
00:11:34,000 --> 00:11:37,000
inventory for a long time, we want to sell off those products by

235
00:11:37,000 --> 00:11:40,000
getting some sort of promotions. And the second

236
00:11:40,000 --> 00:11:43,000
thing is we're going to, you know, UX designers

237
00:11:43,000 --> 00:11:46,000
usually take into account the feedback or the reviews on

238
00:11:46,000 --> 00:11:49,000
their website, which helps them identify the

239
00:11:49,000 --> 00:11:52,000
recurring beliefs about the problems and the bottlenecks which people

240
00:11:52,000 --> 00:11:55,000
are usually facing. And it also helps like

241
00:11:55,000 --> 00:11:58,000
gain, you know, like the valuable insights about some sort of product.

242
00:11:58,000 --> 00:12:01,000
And the most important thing here is

243
00:12:01,000 --> 00:12:04,000
per user pain points, because this is something

244
00:12:04,000 --> 00:12:07,000
UX designers are used to make, like

245
00:12:07,000 --> 00:12:10,000
by getting some information manually and then making them one by one,

246
00:12:10,000 --> 00:12:13,000
and then creating, putting them into a pain point prioritizer, and then

247
00:12:13,000 --> 00:12:16,000
finding some specific pain points, and

248
00:12:16,000 --> 00:12:19,000
then using those to ideate into the next process.

249
00:12:19,000 --> 00:12:22,000
So here we can simply use something like very basic

250
00:12:22,000 --> 00:12:25,000
sentimental analysis to understand the sentiments of the

251
00:12:25,000 --> 00:12:28,000
reviews that users are getting onto this platform.

252
00:12:28,000 --> 00:12:31,000
And it could be positive, negative, or neutral,

253
00:12:31,000 --> 00:12:34,000
and therefore they can help them create a pain point so that

254
00:12:34,000 --> 00:12:37,000
it can streamline the process for the ideation phase.

255
00:12:37,000 --> 00:12:40,000
And in this example, if you see like, we just like simply

256
00:12:40,000 --> 00:12:43,000
calculating the polarity score of the

257
00:12:43,000 --> 00:12:46,000
text we're getting of the reviews.

258
00:12:46,000 --> 00:12:49,000
And if you see like, for example, if you think that a large number of

259
00:12:49,000 --> 00:12:52,000
reviews has a mention called like

260
00:12:52,000 --> 00:12:55,000
floor shipping, and they have a negative sentiment, so we can

261
00:12:55,000 --> 00:12:59,000
consider like, it's a clear indication that the shipping needs to be better.

262
00:12:59,000 --> 00:13:02,000
So instead of just like going through a tedious process, we simply do that

263
00:13:02,000 --> 00:13:05,000
for the UX designers so that they can simply

264
00:13:05,000 --> 00:13:08,000
ideate this thing, this step into the next iteration.

265
00:13:08,000 --> 00:13:11,000
And the clickstream analysis for navigation patterns,

266
00:13:11,000 --> 00:13:14,000
so as we said, it's a very powerful thing to understand

267
00:13:14,000 --> 00:13:17,000
for the navigation patterns of a user onto a platform.

268
00:13:17,000 --> 00:13:20,000
And it gives you insights and bottlenecks

269
00:13:20,000 --> 00:13:23,000
for losing customers, and it also detects

270
00:13:23,000 --> 00:13:26,000
potential pain points, which

271
00:13:26,000 --> 00:13:29,000
tells them the areas of confusion where the user is actually

272
00:13:29,000 --> 00:13:32,000
getting stuck are the bottlenecks, and

273
00:13:32,000 --> 00:13:35,000
thereby using this information, we can fix the needs of the user,

274
00:13:35,000 --> 00:13:38,000
which we'll be seeing later in the A.V. testing with this thing,

275
00:13:38,000 --> 00:13:41,000
like how we can automate those tests.

276
00:13:41,000 --> 00:13:44,000
And one thing which we discussed earlier is like,

277
00:13:44,000 --> 00:13:47,000
the store is not going to be an online database, based on like,

278
00:13:47,000 --> 00:13:50,000
it's going to be in the offline setting as well. So it's a new,

279
00:13:50,000 --> 00:13:53,000
we just made the store open up into the offline setting,

280
00:13:53,000 --> 00:13:56,000
and we have a lot of issues now. We don't know what the user

281
00:13:56,000 --> 00:13:59,000
experience will be, but we're going to be able to do that.

282
00:13:59,000 --> 00:14:02,000
We have a lot of issues now. We don't know what the user experience will be,

283
00:14:02,000 --> 00:14:05,000
how the arguments will be placed, and

284
00:14:05,000 --> 00:14:08,000
there will be stock outs which we need to figure out

285
00:14:08,000 --> 00:14:11,000
so that our utility doesn't go down. And we also want to

286
00:14:11,000 --> 00:14:14,000
cater the users by providing some specific

287
00:14:14,000 --> 00:14:17,000
promotions during discounts on some specific products,

288
00:14:17,000 --> 00:14:20,000
thereby increasing everybody's utility. So we'll make use of

289
00:14:20,000 --> 00:14:23,000
the online store insights which we gathered in the

290
00:14:23,000 --> 00:14:26,000
previous slides, and put them in use

291
00:14:26,000 --> 00:14:29,000
into the local setting. So we basically make use of the data

292
00:14:29,000 --> 00:14:32,000
mining algorithms here. So mostly we have

293
00:14:32,000 --> 00:14:35,000
four which are the most used in the e-commerce space.

294
00:14:35,000 --> 00:14:38,000
One is the market basket analysis.

295
00:14:38,000 --> 00:14:41,000
So it basically analyzes the transactions,

296
00:14:41,000 --> 00:14:44,000
and helps you find the frequently bought items

297
00:14:44,000 --> 00:14:47,000
together so that you can play those items together

298
00:14:47,000 --> 00:14:50,000
to increase the revenue. For example, if you go to a

299
00:14:50,000 --> 00:14:53,000
supermarket and you see like milk and milk bread and butter

300
00:14:53,000 --> 00:14:56,000
all together, so you're more likely to buy all these three items together

301
00:14:56,000 --> 00:14:59,000
instead of just like being put up into different sections

302
00:14:59,000 --> 00:15:02,000
of the supermarket. So the second thing, as we already discussed,

303
00:15:02,000 --> 00:15:05,000
is the customer segmentation, where we can

304
00:15:05,000 --> 00:15:08,000
just like segment them on the basis of some specific

305
00:15:08,000 --> 00:15:11,000
user behavior. And optimizing store layout.

306
00:15:11,000 --> 00:15:14,000
This is a very important thing which you must have seen in stores

307
00:15:14,000 --> 00:15:17,000
like IKEA, that some specific items

308
00:15:17,000 --> 00:15:20,000
are always put up next to each other.

309
00:15:20,000 --> 00:15:23,000
So it basically helps you navigate to the store

310
00:15:23,000 --> 00:15:26,000
and helps you increase the revenue because you are

311
00:15:26,000 --> 00:15:29,000
most likely to purchase those products because you're getting those

312
00:15:29,000 --> 00:15:32,000
historical data based on the previous transactions of the users.

313
00:15:32,000 --> 00:15:35,000
And the last thing is the inventory management.

314
00:15:35,000 --> 00:15:38,000
Again, we use the historical data here

315
00:15:38,000 --> 00:15:41,000
and we figure out what the trends are, specifically

316
00:15:41,000 --> 00:15:44,000
in some seasonal events like if it's Christmas,

317
00:15:44,000 --> 00:15:47,000
we know some specific items will be sold out

318
00:15:47,000 --> 00:15:50,000
much sooner than the others. So we make sure like

319
00:15:50,000 --> 00:15:53,000
we can improve the inventory accordingly so that we don't go into

320
00:15:53,000 --> 00:15:56,000
stockouts because, for example, if you are more tend to buy

321
00:15:56,000 --> 00:15:59,000
milk and butter, and if you go to a store you find

322
00:15:59,000 --> 00:16:02,000
milk only and if you want butter and bread as well,

323
00:16:02,000 --> 00:16:05,000
and if you don't find that in the store, you're more likely

324
00:16:05,000 --> 00:16:08,000
to go to another store where you can find all these three items together.

325
00:16:08,000 --> 00:16:11,000
So reducing stockouts is another aspect of data mining.

326
00:16:11,000 --> 00:16:14,000
So if you see this example,

327
00:16:14,000 --> 00:16:17,000
we are trying to find the more purchased items together,

328
00:16:17,000 --> 00:16:20,000
the item sets.

329
00:16:20,000 --> 00:16:23,000
So what you're doing is we are pre-processing the data

330
00:16:23,000 --> 00:16:26,000
with one-hot encoding, which is basically

331
00:16:26,000 --> 00:16:29,000
we create in binary columns for each unique item,

332
00:16:29,000 --> 00:16:32,000
one being the items being present in the data set.

333
00:16:32,000 --> 00:16:35,000
And then we complete the frequent item sets

334
00:16:35,000 --> 00:16:38,000
by using the A-priori algorithm, which is

335
00:16:38,000 --> 00:16:41,000
a very common algorithm in the data set.

336
00:16:41,000 --> 00:16:44,000
We are using the A-priori algorithm, which is

337
00:16:44,000 --> 00:16:47,000
a very old algorithm, but we're not using it from scratch.

338
00:16:47,000 --> 00:16:50,000
We're just using the email exchange things, and we just like calling the algorithm,

339
00:16:50,000 --> 00:16:53,000
which creates a set of items that are bought together.

340
00:16:53,000 --> 00:16:56,000
And we also provide a minimum support to this function,

341
00:16:56,000 --> 00:16:59,000
which is at least

342
00:16:59,000 --> 00:17:02,000
this x percent of the items should appear together.

343
00:17:02,000 --> 00:17:05,000
So that's the first step in creating the high-frequency item sets.

344
00:17:05,000 --> 00:17:08,000
Then we generate the associative rules,

345
00:17:08,000 --> 00:17:11,000
which is basically how these item sets

346
00:17:11,000 --> 00:17:14,000
are frequent to each other, and we discuss some sort of

347
00:17:14,000 --> 00:17:17,000
rules for likelihood of these items being purchased.

348
00:17:17,000 --> 00:17:20,000
So while the rules,

349
00:17:20,000 --> 00:17:23,000
as we said, these are the likelihood, and the confidence

350
00:17:23,000 --> 00:17:26,000
which we are also passing, is representing

351
00:17:26,000 --> 00:17:29,000
the probability of the rule being leased, like it's been true or not.

352
00:17:29,000 --> 00:17:32,000
Then we simply filter the rules

353
00:17:32,000 --> 00:17:35,000
based on the lift and confidence, which we already discussed,

354
00:17:35,000 --> 00:17:38,000
and create the item sets with the higher lift.

355
00:17:38,000 --> 00:17:41,000
If you see in the next slide, that

356
00:17:41,000 --> 00:17:44,000
these are the items, and if you see the lift,

357
00:17:44,000 --> 00:17:47,000
these are the items with the highest lift, and these are the items which are more

358
00:17:47,000 --> 00:17:50,000
likely to be purchased together. So as a store owner,

359
00:17:50,000 --> 00:17:53,000
I'll place those items more closely into the store,

360
00:17:53,000 --> 00:17:56,000
so that it's as a user behavior, they are more likely to purchase those

361
00:17:56,000 --> 00:17:59,000
items into the store.

362
00:17:59,000 --> 00:18:02,000
And another aspect is like,

363
00:18:02,000 --> 00:18:05,000
we use Python to automate tests, and this can include

364
00:18:05,000 --> 00:18:08,000
something like web scraping, scraping the data

365
00:18:08,000 --> 00:18:11,000
from different sources, because for example,

366
00:18:11,000 --> 00:18:14,000
on a website, you're just getting this

367
00:18:14,000 --> 00:18:17,000
information about the reviews on a website, but there are so many

368
00:18:17,000 --> 00:18:20,000
different third party websites, which are also, people are just putting

369
00:18:20,000 --> 00:18:23,000
their reviews about our company or our product, so we can scrape

370
00:18:23,000 --> 00:18:26,000
their data from these different sources, and we can also

371
00:18:26,000 --> 00:18:29,000
automate multiple tools, like human efforts.

372
00:18:29,000 --> 00:18:32,000
For example, if you are a subletting company, like a broker kind of thing,

373
00:18:32,000 --> 00:18:35,000
you have so many tedious processes, like you have to go to

374
00:18:35,000 --> 00:18:38,000
some exchange kind of thing, you have to put the email first of a user,

375
00:18:38,000 --> 00:18:41,000
who's just going to be coming to your house,

376
00:18:41,000 --> 00:18:44,000
and you have to go to some messaging platform, like a text matcher,

377
00:18:44,000 --> 00:18:47,000
you put the phone number of the user, and so on.

378
00:18:47,000 --> 00:18:50,000
So you can always use Python to automate

379
00:18:50,000 --> 00:18:53,000
these processes, so that you can streamline the process, and

380
00:18:53,000 --> 00:18:56,000
reduce the human efforts and human errors.

381
00:18:56,000 --> 00:18:59,000
So, in this example of the case study, we are saying,

382
00:18:59,000 --> 00:19:02,000
we want to automate external review analysis,

383
00:19:02,000 --> 00:19:05,000
as we already discussed, as I said,

384
00:19:05,000 --> 00:19:08,000
instead of just the reviews coming on onto a platform from

385
00:19:08,000 --> 00:19:11,000
our own comments and reviews, there must be

386
00:19:11,000 --> 00:19:14,000
third party websites where these reviews are being put up by the users.

387
00:19:14,000 --> 00:19:17,000
And for example, we say we have a couple of reviews,

388
00:19:17,000 --> 00:19:20,000
customer reviews over here, and what we're going to do is

389
00:19:20,000 --> 00:19:23,000
we simply can scrape the reviews

390
00:19:23,000 --> 00:19:26,000
and what we're going to do is we simply can scrape,

391
00:19:26,000 --> 00:19:29,000
as useful as the web scraping thing from multiple websites,

392
00:19:29,000 --> 00:19:32,000
anything can be used, like we did with Soup,

393
00:19:32,000 --> 00:19:35,000
here we used Excelanium, and we can use that information

394
00:19:35,000 --> 00:19:38,000
to get more data insights, we do

395
00:19:38,000 --> 00:19:41,000
sentiment analysis or the common trends using NLP,

396
00:19:41,000 --> 00:19:44,000
and once we have the keyed-nosed data

397
00:19:44,000 --> 00:19:47,000
from some text website, from a third party website,

398
00:19:47,000 --> 00:19:50,000
we can then synthesize that data into a single

399
00:19:50,000 --> 00:19:53,000
data warehouse or a data lake so that we can just

400
00:19:53,000 --> 00:19:56,000
now start working on it. So as a typical

401
00:19:56,000 --> 00:19:59,000
user experience designer, we're going to

402
00:19:59,000 --> 00:20:02,000
create some sort of affinity time-gaming, which is

403
00:20:02,000 --> 00:20:05,000
gathering those similar groups all together, and we can make

404
00:20:05,000 --> 00:20:08,000
use of something like another user segmentation

405
00:20:08,000 --> 00:20:11,000
like K-Rings Clustering to do so, so that we don't have to

406
00:20:11,000 --> 00:20:14,000
go through this tedious process of affinity time-gaming.

407
00:20:14,000 --> 00:20:17,000
And this

408
00:20:17,000 --> 00:20:20,000
helps us, again, finding the pinpoints, because pinpoints

409
00:20:20,000 --> 00:20:23,000
are always important for the user at every single step of the

410
00:20:23,000 --> 00:20:26,000
user journey, and we will find the required things,

411
00:20:26,000 --> 00:20:29,000
also positively used. We can then visualize

412
00:20:29,000 --> 00:20:32,000
these results and draw the conclusions.

413
00:20:32,000 --> 00:20:35,000
Second last thing is the user

414
00:20:35,000 --> 00:20:38,000
testing, user analytics and A-B testing.

415
00:20:38,000 --> 00:20:41,000
So A-B testing is basically a method of comparing

416
00:20:41,000 --> 00:20:44,000
two or more different variations, and so

417
00:20:44,000 --> 00:20:47,000
for example, we have to figure out which design pattern

418
00:20:47,000 --> 00:20:50,000
performs better. So we can make use of

419
00:20:50,000 --> 00:20:53,000
different A-B tests, and how this

420
00:20:53,000 --> 00:20:56,000
typically works is like we can use some sort of an analytics service

421
00:20:56,000 --> 00:20:59,000
like Google Analytics and the Python API. We can gather

422
00:20:59,000 --> 00:21:02,000
information. It could be page clicks, time spent on the

423
00:21:02,000 --> 00:21:05,000
website, and you can see a simple

424
00:21:05,000 --> 00:21:08,000
example where we are just like, we have two groups of data,

425
00:21:08,000 --> 00:21:11,000
the control groups A and B, with the conversion rates for each

426
00:21:11,000 --> 00:21:14,000
group represented by an array of data. Maybe, for example,

427
00:21:14,000 --> 00:21:17,000
we can say there are five pages altogether. Then we perform

428
00:21:17,000 --> 00:21:20,000
a test, and we use the TTTest underscore IUD

429
00:21:20,000 --> 00:21:23,000
functions of the site library, which helps you

430
00:21:23,000 --> 00:21:26,000
calculate the T statistic and the T value

431
00:21:26,000 --> 00:21:29,000
for the given samples. And we have

432
00:21:29,000 --> 00:21:32,000
put up alpha value as 0.05, which says like

433
00:21:32,000 --> 00:21:35,000
if we have the T value less than

434
00:21:35,000 --> 00:21:38,000
0.05, then there is definitely a significant

435
00:21:38,000 --> 00:21:41,000
change in the T value, which is not by chance, and it is happening

436
00:21:41,000 --> 00:21:44,000
because of some specific reason. To understand this

437
00:21:44,000 --> 00:21:47,000
more comfortably, we can go for something like a button

438
00:21:47,000 --> 00:21:50,000
color test. For example, we have two buttons on the website,

439
00:21:50,000 --> 00:21:53,000
like the CTA call to action buttons, and one is red, one is

440
00:21:53,000 --> 00:21:56,000
blue. So we can provide

441
00:21:56,000 --> 00:21:59,000
a basic picture of it, just two buttons,

442
00:21:59,000 --> 00:22:02,000
for instance. And what we are doing is like

443
00:22:02,000 --> 00:22:05,000
we are taking two groups again. One is

444
00:22:05,000 --> 00:22:08,000
clicks, and the second one is the non-clicks happening

445
00:22:08,000 --> 00:22:11,000
on the website, on the specific page. And then

446
00:22:11,000 --> 00:22:14,000
we are creating a contingency table, which is basically

447
00:22:14,000 --> 00:22:17,000
the interrelation between those two or more variables

448
00:22:17,000 --> 00:22:20,000
on the groups here, like group A and group B.

449
00:22:20,000 --> 00:22:23,000
Then we are going with the car square test,

450
00:22:23,000 --> 00:22:26,000
which is a statistical method used to determine

451
00:22:26,000 --> 00:22:29,000
whether there is a significant association between

452
00:22:29,000 --> 00:22:32,000
those two categorical values.

453
00:22:32,000 --> 00:22:35,000
And once we are done with this, we are going to be finding

454
00:22:35,000 --> 00:22:38,000
the T values. If you don't know, I think I can explain in the last slide.

455
00:22:38,000 --> 00:22:41,000
T values and statistics is basically

456
00:22:41,000 --> 00:22:44,000
the evidence of the null hypothesis. And in our

457
00:22:44,000 --> 00:22:47,000
example, if you see, the null hypothesis

458
00:22:47,000 --> 00:22:50,000
is basically the difference between the conversion

459
00:22:50,000 --> 00:22:53,000
rates by clicking those buttons and like

460
00:22:53,000 --> 00:22:56,000
non-clicks. And if we just like try

461
00:22:56,000 --> 00:22:59,000
to print out the values, the significance

462
00:22:59,000 --> 00:23:02,000
values, we'll see the group A has a CTR of 10 percent,

463
00:23:02,000 --> 00:23:05,000
while the group B, which is the green button,

464
00:23:05,000 --> 00:23:08,000
is having a CTR of 12 percent, and there is a

465
00:23:08,000 --> 00:23:11,000
relative increase in the CTR by 20 percent,

466
00:23:11,000 --> 00:23:14,000
and the P3 value is 0.268, which is

467
00:23:14,000 --> 00:23:17,000
greater than 0.05, which shows that there is a significant

468
00:23:17,000 --> 00:23:20,000
change, which is not by chance and due to some different

469
00:23:20,000 --> 00:23:23,000
user's behavior. Similarly, we can also

470
00:23:23,000 --> 00:23:26,000
go with something like a navigation mini test, which

471
00:23:26,000 --> 00:23:29,000
helps you consider like reordering of the

472
00:23:29,000 --> 00:23:32,000
libraries. Does they have any effect on the user's behavior

473
00:23:32,000 --> 00:23:35,000
or the kinds they are spending on your website?

474
00:23:35,000 --> 00:23:38,000
And we can make use of an algorithm, a test

475
00:23:38,000 --> 00:23:41,000
basically, a map mini test, which determines

476
00:23:41,000 --> 00:23:44,000
like whether the two groups are

477
00:23:44,000 --> 00:23:47,000
coming from the same population or not. And in our

478
00:23:47,000 --> 00:23:50,000
perspective, if you see, we have two groups A and B, which

479
00:23:50,000 --> 00:23:53,000
has the page view, the bounce rate, and the average

480
00:23:53,000 --> 00:23:56,000
from one page. And we're going to check like whether the differences

481
00:23:56,000 --> 00:23:59,000
between the average size spent on the page is

482
00:23:59,000 --> 00:24:02,000
happening because of a chance or they are statistically

483
00:24:02,000 --> 00:24:06,000
significant. And if you see, what we're doing is like

484
00:24:06,000 --> 00:24:09,000
we just like simply calling the man with the

485
00:24:09,000 --> 00:24:12,000
geofunctions from the game recycle library, which is just

486
00:24:12,000 --> 00:24:15,000
giving us the P value and the T values of the use statistics.

487
00:24:15,000 --> 00:24:18,000
And if you see the use statistics, the more the

488
00:24:18,000 --> 00:24:21,000
use statistics is, we can consider like this is not

489
00:24:21,000 --> 00:24:24,000
because of, this is more likely to happen because of chance, but

490
00:24:24,000 --> 00:24:27,000
if the use statistics vary too less, then we can conclude

491
00:24:27,000 --> 00:24:30,000
that this is not happening because of a chance and has

492
00:24:30,000 --> 00:24:33,000
statistically significant values associated with it.

493
00:24:33,000 --> 00:24:36,000
And the last thing I want to discuss is like

494
00:24:36,000 --> 00:24:39,000
developing the personalized user experience. So we can

495
00:24:39,000 --> 00:24:42,000
use some algorithms like user-based collaborative filtering

496
00:24:42,000 --> 00:24:45,000
or the content-based collaborative filtering

497
00:24:45,000 --> 00:24:48,000
to provide personalized user product suggestions

498
00:24:48,000 --> 00:24:51,000
or recommendations to the user. We can also provide

499
00:24:51,000 --> 00:24:54,000
some user experiences like data promotions to a specific

500
00:24:54,000 --> 00:24:57,000
set of users and which again, will start to help those

501
00:24:57,000 --> 00:25:00,000
conversion rates more.

502
00:25:00,000 --> 00:25:03,000
And what we're doing here is like we are just like

503
00:25:03,000 --> 00:25:06,000
going to the user-based collaborative filtering, which is

504
00:25:06,000 --> 00:25:09,000
what we're doing is like we're creating a sample data

505
00:25:09,000 --> 00:25:12,000
space for the users, ratings for various products

506
00:25:12,000 --> 00:25:16,000
represented as dictionary. And then we basically

507
00:25:16,000 --> 00:25:19,000
pivoting the data frame to create a user product matrix

508
00:25:19,000 --> 00:25:22,000
that is also representing the users in the column

509
00:25:22,000 --> 00:25:25,000
which represent the product and the values are the corresponding

510
00:25:25,000 --> 00:25:29,000
ratings of those products. We then simply compute the cosine

511
00:25:29,000 --> 00:25:32,000
similarity between those two vectors in a space.

512
00:25:32,000 --> 00:25:35,000
And then we find the target user, which is two in our case,

513
00:25:35,000 --> 00:25:38,000
and we find the most similar user by using the NG ArcMaps

514
00:25:38,000 --> 00:25:41,000
on the user similarity matrix.

515
00:25:41,000 --> 00:25:44,000
And then we find the target user, which is two in our case,

516
00:25:44,000 --> 00:25:47,000
and we find the most similar user by using the NG ArcMaps

517
00:25:47,000 --> 00:25:50,000
on the user similarity matrix, which is going to give us

518
00:25:50,000 --> 00:25:53,000
back a value and that you can send those values back

519
00:25:53,000 --> 00:25:56,000
to the, to identify the product rated by the most similar

520
00:25:56,000 --> 00:25:59,000
user on the group. And once we are done, we can just like

521
00:25:59,000 --> 00:26:02,000
find the difference between the sets of the rated products

522
00:26:02,000 --> 00:26:05,000
for both the users. And if we just like run this

523
00:26:05,000 --> 00:26:08,000
and if you are about to look for the outcome for the most

524
00:26:08,000 --> 00:26:11,000
recommended product for the user two, will be our BNC.

525
00:26:12,000 --> 00:26:15,000
And similarly, we can also make use of the TTF-IDF

526
00:26:15,000 --> 00:26:18,000
vectorizer for the quantum based

527
00:26:18,000 --> 00:26:21,000
collaborative filtering, where we basically convert

528
00:26:21,000 --> 00:26:24,000
the object and transform the descriptions into a TTF-IDF

529
00:26:24,000 --> 00:26:27,000
matrix. And then this matrix holds the importance of

530
00:26:27,000 --> 00:26:30,000
all the input, which is there in the descriptions of the disk.

531
00:26:30,000 --> 00:26:33,000
which is there in the descriptions of the disk.

532
00:26:33,000 --> 00:26:36,000
If you see the object has a product and a description,

533
00:26:36,000 --> 00:26:39,000
and then we take into account the frequency of the object

534
00:26:39,000 --> 00:26:42,000
and then we take into account the frequency of the words in the description.

535
00:26:42,000 --> 00:26:45,000
And again, we do the same thing.

536
00:26:45,000 --> 00:26:48,000
We compute the cosine similarity, but now

537
00:26:48,000 --> 00:26:51,000
we are not using the, the functions we used earlier.

538
00:26:51,000 --> 00:26:54,000
We are using the linear kernel, which takes into TTF-IDF

539
00:26:54,000 --> 00:26:57,000
matrix. And then this matrix basically

540
00:26:57,000 --> 00:27:00,000
represents the similarity between all pairs of

541
00:27:00,000 --> 00:27:03,000
clues based on their descriptions.

542
00:27:03,000 --> 00:27:06,000
And then we simply define a recognition function,

543
00:27:06,000 --> 00:27:09,000
which is the most common one, and it takes the product index

544
00:27:09,000 --> 00:27:12,000
and the similarity matrix, which returns the most similar products.

545
00:27:12,000 --> 00:27:15,000
And in our case, if you go with the

546
00:27:15,000 --> 00:27:18,000
product A, which is the linear cache-based clues,

547
00:27:18,000 --> 00:27:21,000
the products, the most similar products

548
00:27:21,000 --> 00:27:24,000
with the cosine similarity are the stickers and the format keys.

549
00:27:24,000 --> 00:27:27,000
And all together,

550
00:27:27,000 --> 00:27:30,000
we can create the user personas.

551
00:27:30,000 --> 00:27:33,000
More recently, when we

552
00:27:33,000 --> 00:27:36,000
consider the large learning models, creating user personas is much more simpler,

553
00:27:36,000 --> 00:27:39,000
especially when we have all those information from

554
00:27:39,000 --> 00:27:42,000
the previous information we had,

555
00:27:42,000 --> 00:27:45,000
like the information we gathered from

556
00:27:45,000 --> 00:27:48,000
the user segmentations and all.

557
00:27:48,000 --> 00:27:51,000
And here we can say, like, there is a user, say, Jared Thompson.

558
00:27:51,000 --> 00:27:54,000
He's 28 years old. His occupation is a software engineer,

559
00:27:54,000 --> 00:27:57,000
and he is in the specific, like, the Inca range.

560
00:27:57,000 --> 00:28:00,000
And he's a tech savvy,

561
00:28:00,000 --> 00:28:03,000
and he prefers to shop online, and he has a specific personality.

562
00:28:03,000 --> 00:28:06,000
And he has some goals and needs,

563
00:28:06,000 --> 00:28:09,000
which we have figured out from the previous examples.

564
00:28:09,000 --> 00:28:12,000
And he has some frustrations, which we again figured out from the reviews

565
00:28:12,000 --> 00:28:15,000
and feedback. And then these are the solutions,

566
00:28:15,000 --> 00:28:18,000
which us as a UX designer has provided,

567
00:28:18,000 --> 00:28:21,000
which are data-driven. We can provide them more personalized

568
00:28:21,000 --> 00:28:24,000
communication based on the browsing and the process industry,

569
00:28:24,000 --> 00:28:27,000
and we can also make use of the AI-powered A-B testing,

570
00:28:27,000 --> 00:28:30,000
which might lead to the better user experience

571
00:28:30,000 --> 00:28:33,000
and so on. And yes, that's pretty much it.

572
00:28:33,000 --> 00:28:36,000
Thank you so much for joining,

573
00:28:36,000 --> 00:28:39,000
and if you have any questions, please let me know.

