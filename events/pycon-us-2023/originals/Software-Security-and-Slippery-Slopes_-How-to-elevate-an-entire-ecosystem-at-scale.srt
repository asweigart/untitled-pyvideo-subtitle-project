1
00:00:00,000 --> 00:00:03,000
All right, I hope everyone settled on.

2
00:00:03,000 --> 00:00:05,000
Our speaker for this session will talk about

3
00:00:05,000 --> 00:00:07,000
how secure security and security slopes

4
00:00:07,000 --> 00:00:10,000
help to elevate an entire ecosystem at scale.

5
00:00:10,000 --> 00:00:12,000
Let us welcome Dustin Ingram.

6
00:00:12,000 --> 00:00:14,000
Thank you.

7
00:00:14,000 --> 00:00:16,000
Thank you.

8
00:00:16,000 --> 00:00:18,000
Thank you.

9
00:00:18,000 --> 00:00:19,000
Thank you.

10
00:00:19,000 --> 00:00:20,000
Yeah, so I'm Dustin.

11
00:00:20,000 --> 00:00:23,000
I'm on the Google Open Source Security Team.

12
00:00:23,000 --> 00:00:27,000
So the mission of that team is to make open source software

13
00:00:27,000 --> 00:00:31,000
software that Google and everyone else uses more secure.

14
00:00:31,000 --> 00:00:34,000
Also on the Python Software Foundation Board of Directors

15
00:00:34,000 --> 00:00:36,000
where I help ensure the long-term success

16
00:00:36,000 --> 00:00:38,000
of one very big open source Python project

17
00:00:38,000 --> 00:00:40,000
you've probably heard of, Python.

18
00:00:40,000 --> 00:00:43,000
I'm also one of the Python package maintainers

19
00:00:43,000 --> 00:00:45,000
where I help ensure the long-term success

20
00:00:45,000 --> 00:00:48,000
of hundreds of thousands of teeny tiny Python projects,

21
00:00:48,000 --> 00:00:50,000
many of which you've never heard of.

22
00:00:50,000 --> 00:00:53,000
And also I'm on the Technical Advisory Committee

23
00:00:53,000 --> 00:00:55,000
for the Open Software Security Foundation,

24
00:00:55,000 --> 00:00:58,000
which is new, where I help guide the success

25
00:00:58,000 --> 00:01:02,000
of the next generation of open source security technologies.

26
00:01:02,000 --> 00:01:06,000
So this talk is about ecosystem security.

27
00:01:06,000 --> 00:01:09,000
My focus at Google is to kind of think about

28
00:01:09,000 --> 00:01:12,000
how to increase adoption of security improvements

29
00:01:12,000 --> 00:01:14,000
across many open source ecosystems,

30
00:01:14,000 --> 00:01:17,000
including Python but not just Python.

31
00:01:17,000 --> 00:01:19,000
And as a maintainer of PyGI,

32
00:01:19,000 --> 00:01:22,000
I also really deeply care about making sure

33
00:01:22,000 --> 00:01:25,000
that the Python ecosystem is safe and secure

34
00:01:25,000 --> 00:01:26,000
for all our users.

35
00:01:26,000 --> 00:01:30,000
So I definitely care about how one would go

36
00:01:30,000 --> 00:01:33,000
about elevating the security of an entire ecosystem

37
00:01:33,000 --> 00:01:36,000
at scale, but you probably don't have

38
00:01:36,000 --> 00:01:38,000
those same responsibilities.

39
00:01:38,000 --> 00:01:40,000
You probably don't have that job either.

40
00:01:40,000 --> 00:01:41,000
That's fine.

41
00:01:41,000 --> 00:01:45,000
But why do I think you should care about this?

42
00:01:45,000 --> 00:01:49,000
So here's what I want to get from this talk.

43
00:01:49,000 --> 00:01:51,000
First, why is this hard?

44
00:01:51,000 --> 00:01:54,000
I want you to understand what are the actual challenges

45
00:01:54,000 --> 00:01:56,000
that make this hard to do.

46
00:01:56,000 --> 00:01:59,000
Second, I want you to understand why some turn is necessary.

47
00:01:59,000 --> 00:02:01,000
And I was playing with what I mean by turn,

48
00:02:01,000 --> 00:02:03,000
why I think it's necessary,

49
00:02:03,000 --> 00:02:06,000
and how I think we should deal with it.

50
00:02:06,000 --> 00:02:08,000
Also, I want you to understand how security

51
00:02:08,000 --> 00:02:11,000
can increase sustainability.

52
00:02:11,000 --> 00:02:13,000
So there's more of a relationship between these two things

53
00:02:13,000 --> 00:02:15,000
than you might actually expect.

54
00:02:15,000 --> 00:02:18,000
And finally, I want you to understand how you can help.

55
00:02:18,000 --> 00:02:20,000
And this isn't just, let's pitch in on some

56
00:02:20,000 --> 00:02:21,000
of the projects together.

57
00:02:21,000 --> 00:02:24,000
There are actual ways for you to help when you think about security.

58
00:02:24,000 --> 00:02:28,000
Overall, I want to give you an insight into how these things

59
00:02:28,000 --> 00:02:32,000
change, how we think about security in our ecosystem,

60
00:02:32,000 --> 00:02:35,000
and all the trade-offs that exist at each point.

61
00:02:35,000 --> 00:02:38,000
Because everything is a trade-off, right?

62
00:02:38,000 --> 00:02:40,000
There are trade-offs in every decision.

63
00:02:40,000 --> 00:02:42,000
There is never just one best move.

64
00:02:42,000 --> 00:02:46,000
They're always balancing between the two choices.

65
00:02:46,000 --> 00:02:49,000
So part one, why is this hard?

66
00:02:50,000 --> 00:02:54,000
Security is hard enough on a small scale.

67
00:02:54,000 --> 00:02:57,000
So some of you might be familiar with implementing

68
00:02:57,000 --> 00:03:00,000
or adopting security changes, procedures, practices

69
00:03:00,000 --> 00:03:03,000
on your teams or maybe in your organizations.

70
00:03:03,000 --> 00:03:06,000
And my guess is that those were

71
00:03:06,000 --> 00:03:08,000
pretty somewhat challenging.

72
00:03:08,000 --> 00:03:10,000
If you were implementing those changes,

73
00:03:10,000 --> 00:03:12,000
maybe you had to articulate the problem,

74
00:03:12,000 --> 00:03:15,000
get the sign-off, that's what I will make a claim on,

75
00:03:15,000 --> 00:03:17,000
check it twice, get everyone on board,

76
00:03:17,000 --> 00:03:19,000
and bring forward all the holdouts,

77
00:03:19,000 --> 00:03:21,000
announce it, roll it out,

78
00:03:21,000 --> 00:03:23,000
then you can clean the holdouts.

79
00:03:23,000 --> 00:03:25,000
It's not easy, right?

80
00:03:25,000 --> 00:03:27,000
If you were the person adopting the change,

81
00:03:27,000 --> 00:03:29,000
you probably had to deal with a really confusing change

82
00:03:29,000 --> 00:03:31,000
that nobody told you about

83
00:03:31,000 --> 00:03:34,000
and probably wrote something that you're responsible for.

84
00:03:34,000 --> 00:03:37,000
So these things are hard on a small scale.

85
00:03:37,000 --> 00:03:40,000
These things are extremely hard on a large scale,

86
00:03:40,000 --> 00:03:42,000
especially on the ecosystem scale.

87
00:03:42,000 --> 00:03:44,000
Why is that?

88
00:03:44,000 --> 00:03:48,000
So one reason is that users don't ask for security.

89
00:03:48,000 --> 00:03:50,000
They need a student.

90
00:03:50,000 --> 00:03:53,000
So, for example, GitHub is requiring all developers

91
00:03:53,000 --> 00:03:56,000
who contribute code on github.com

92
00:03:56,000 --> 00:03:59,000
to enable one or more forms of two-factor authentication

93
00:03:59,000 --> 00:04:02,000
by the end of this year, by the end of 2023.

94
00:04:02,000 --> 00:04:04,000
The number of people I've seen

95
00:04:04,000 --> 00:04:07,000
asking, cleaning GitHub to make this change

96
00:04:07,000 --> 00:04:10,000
is basically zero, no one's asking for this.

97
00:04:10,000 --> 00:04:13,000
However, the number of people that are assuming

98
00:04:13,000 --> 00:04:15,000
that all code pushed to GitHub

99
00:04:15,000 --> 00:04:18,000
is pushed by the actual owners of the accounts

100
00:04:18,000 --> 00:04:20,000
that push it is basically everywhere, right?

101
00:04:20,000 --> 00:04:23,000
That's like the basis of your GitHub.

102
00:04:23,000 --> 00:04:27,000
So, so many times I've seen this lack of user requests

103
00:04:27,000 --> 00:04:31,000
held up, or lack of user requests for a security feature

104
00:04:31,000 --> 00:04:35,000
held up as a reason for why it's just not worth doing.

105
00:04:35,000 --> 00:04:38,000
The lack of requests is because users assume

106
00:04:38,000 --> 00:04:41,000
that corrective security is just happening behind the scenes,

107
00:04:41,000 --> 00:04:44,000
that they don't need to ask for it.

108
00:04:44,000 --> 00:04:46,000
Additionally, for those users,

109
00:04:46,000 --> 00:04:49,000
it's hard to work towards hypotheticals.

110
00:04:49,000 --> 00:04:51,000
So, I think that security as a field

111
00:04:51,000 --> 00:04:54,000
generally is an exercise in professional anxiety

112
00:04:54,000 --> 00:04:56,000
and care and learning.

113
00:04:56,000 --> 00:05:00,000
It's preemptive work to mitigate some potential outcome

114
00:05:00,000 --> 00:05:02,000
of varying levels of bad

115
00:05:02,000 --> 00:05:06,000
that has some varying level likelihood of happening.

116
00:05:06,000 --> 00:05:07,000
And users are reactive.

117
00:05:07,000 --> 00:05:10,000
The reason they don't ask for security features

118
00:05:10,000 --> 00:05:13,000
is because a bad outcome is of sufficient impact

119
00:05:13,000 --> 00:05:15,000
hasn't happened to them yet.

120
00:05:15,000 --> 00:05:18,000
So, we can sort of think of this as a quadrant, right?

121
00:05:18,000 --> 00:05:21,000
The range from low likelihood to high likelihood,

122
00:05:21,000 --> 00:05:23,000
low impact to high impact.

123
00:05:23,000 --> 00:05:25,000
So, low impact, low likelihood,

124
00:05:25,000 --> 00:05:28,000
these things are nothing to worry about.

125
00:05:28,000 --> 00:05:30,000
Low impact, high likelihood,

126
00:05:30,000 --> 00:05:32,000
this might be something like password reuse,

127
00:05:32,000 --> 00:05:34,000
credential stuff, a decision.

128
00:05:34,000 --> 00:05:36,000
So, something where like, you know,

129
00:05:36,000 --> 00:05:38,000
it's very likely it's gonna happen.

130
00:05:38,000 --> 00:05:40,000
You're just gonna stick your password somewhere

131
00:05:40,000 --> 00:05:41,000
that doesn't belong.

132
00:05:41,000 --> 00:05:43,000
But usually there's enough of a blast

133
00:05:43,000 --> 00:05:45,000
where you just have a single person

134
00:05:45,000 --> 00:05:47,000
that it's not the end of the world.

135
00:05:47,000 --> 00:05:49,000
There's some things like high impact,

136
00:05:49,000 --> 00:05:51,000
high likelihood stuff like leading us

137
00:05:51,000 --> 00:05:53,000
to a bucket world readable.

138
00:05:53,000 --> 00:05:55,000
That happens all the time.

139
00:05:55,000 --> 00:05:57,000
You can lose a lot of information,

140
00:05:57,000 --> 00:05:59,000
like secrets, leaks, and the like,

141
00:05:59,000 --> 00:06:01,000
taking you to the east of your kingdom.

142
00:06:01,000 --> 00:06:03,000
These are extremely high impact,

143
00:06:03,000 --> 00:06:05,000
but also very incredibly frequent.

144
00:06:05,000 --> 00:06:07,000
And in the last quadrant is stuff

145
00:06:07,000 --> 00:06:10,000
that's just like getting hacked by an agent state, right?

146
00:06:10,000 --> 00:06:12,000
For the same reason that some of these

147
00:06:12,000 --> 00:06:14,000
are harder to address than others,

148
00:06:14,000 --> 00:06:16,000
they're also harder to load.

149
00:06:16,000 --> 00:06:19,000
It's not really just the users though.

150
00:06:19,000 --> 00:06:23,000
Here's a refrain I see from time to time from maintainers.

151
00:06:23,000 --> 00:06:25,000
But we haven't made any mistakes yet.

152
00:06:25,000 --> 00:06:29,000
Where the ask is to adopt some security tool or feature.

153
00:06:29,000 --> 00:06:32,000
Like, I haven't been phished yet.

154
00:06:32,000 --> 00:06:34,000
I haven't leaked any API tokens yet.

155
00:06:34,000 --> 00:06:36,000
I haven't included things I shouldn't

156
00:06:36,000 --> 00:06:38,000
and released yet.

157
00:06:38,000 --> 00:06:41,000
I haven't unknownly created a vulnerability yet.

158
00:06:41,000 --> 00:06:44,000
Here's a real paraphrase quote

159
00:06:44,000 --> 00:06:46,000
that I got from a maintainer,

160
00:06:46,000 --> 00:06:48,000
and this is in response to a request to automate

161
00:06:48,000 --> 00:06:50,000
the build and release process for a library.

162
00:06:50,000 --> 00:06:52,000
They said, I've made more than 200 releases to date,

163
00:06:52,000 --> 00:06:54,000
and I've never made a mistake.

164
00:06:54,000 --> 00:06:57,000
So first of all, who knows this may take a look.

165
00:06:57,000 --> 00:06:59,000
That is actually impressive.

166
00:06:59,000 --> 00:07:01,000
I have not done it myself.

167
00:07:01,000 --> 00:07:03,000
I break Prykion all the time.

168
00:07:04,000 --> 00:07:07,000
But is this actually a good mindset to have

169
00:07:07,000 --> 00:07:09,000
as an open source maintainer?

170
00:07:09,000 --> 00:07:11,000
Like, what are they actually saying?

171
00:07:11,000 --> 00:07:14,000
They're saying that they don't want to change their behavior

172
00:07:14,000 --> 00:07:16,000
and that even if the risk is high enough,

173
00:07:16,000 --> 00:07:19,000
an impact is reasonably unlikely.

174
00:07:19,000 --> 00:07:21,000
And they're probably also a little offended

175
00:07:21,000 --> 00:07:23,000
that we don't consider that a foul word

176
00:07:23,000 --> 00:07:25,000
that they might make a mistake.

177
00:07:25,000 --> 00:07:28,000
This also kind of fails to consider

178
00:07:28,000 --> 00:07:30,000
because really often our problems

179
00:07:30,000 --> 00:07:32,000
are not directly caused by us,

180
00:07:32,000 --> 00:07:34,000
but we don't work in decades.

181
00:07:34,000 --> 00:07:36,000
A lot of things are caused by other people

182
00:07:36,000 --> 00:07:37,000
or other systems.

183
00:07:37,000 --> 00:07:39,000
Other things around us that are

184
00:07:39,000 --> 00:07:41,000
entirely out of our control perhaps,

185
00:07:41,000 --> 00:07:43,000
that can also go wrong.

186
00:07:44,000 --> 00:07:46,000
High uncertainty components in complex systems.

187
00:07:46,000 --> 00:07:48,000
This is another reason why this is hard.

188
00:07:48,000 --> 00:07:50,000
Security often requires changing

189
00:07:50,000 --> 00:07:52,000
high uncertainty components in complex systems.

190
00:07:52,000 --> 00:07:54,000
Who is it that's key to that question?

191
00:07:54,000 --> 00:07:56,000
Do you know what I mean by that?

192
00:07:56,000 --> 00:07:58,000
People.

193
00:07:58,000 --> 00:08:00,000
It means changing people's behavior,

194
00:08:00,000 --> 00:08:02,000
especially their behavior, in security.

195
00:08:02,000 --> 00:08:04,000
There's really, I think,

196
00:08:04,000 --> 00:08:06,000
two ways to interpret

197
00:08:06,000 --> 00:08:08,000
how to handle a technical problem.

198
00:08:08,000 --> 00:08:10,000
One is that the problem is easy

199
00:08:10,000 --> 00:08:12,000
because people can just do X, right?

200
00:08:12,000 --> 00:08:14,000
We can build a technical solution to our problem,

201
00:08:14,000 --> 00:08:16,000
and that solution might work perfectly,

202
00:08:16,000 --> 00:08:18,000
and then people can just use that system.

203
00:08:20,000 --> 00:08:22,000
The other interpretation is that

204
00:08:22,000 --> 00:08:24,000
the problem is actually incredibly hard

205
00:08:24,000 --> 00:08:26,000
because it requires people to do X.

206
00:08:26,000 --> 00:08:28,000
So often that the hardest part

207
00:08:28,000 --> 00:08:30,000
of solving the technical problem

208
00:08:30,000 --> 00:08:32,000
is not the technical problem itself,

209
00:08:32,000 --> 00:08:34,000
but changing the behavior of people

210
00:08:34,000 --> 00:08:36,000
around that technical problem.

211
00:08:36,000 --> 00:08:38,000
So, for example,

212
00:08:38,000 --> 00:08:40,000
I still use Code Editor

213
00:08:40,000 --> 00:08:42,000
that I first used almost 20 years ago,

214
00:08:42,000 --> 00:08:44,000
which itself was first released

215
00:08:44,000 --> 00:08:46,000
almost 50 years ago.

216
00:08:46,000 --> 00:08:48,000
And to be fair, Code Editors are

217
00:08:48,000 --> 00:08:50,000
incredibly personal tools.

218
00:08:50,000 --> 00:08:52,000
But for me, all of the benefit

219
00:08:52,000 --> 00:08:54,000
of a modern IDE

220
00:08:54,000 --> 00:08:56,000
has still not outweighed

221
00:08:56,000 --> 00:08:58,000
across the meaning changing our behavior.

222
00:08:58,000 --> 00:09:00,000
I'm extremely resistant to it.

223
00:09:00,000 --> 00:09:02,000
So, there is

224
00:09:02,000 --> 00:09:04,000
a third thing that happens sometimes.

225
00:09:04,000 --> 00:09:06,000
Sometimes it's very easy.

226
00:09:06,000 --> 00:09:08,000
It doesn't require people at all.

227
00:09:10,000 --> 00:09:12,000
This almost never happens.

228
00:09:12,000 --> 00:09:14,000
Improvements that don't need humans

229
00:09:14,000 --> 00:09:16,000
to change their behavior are so

230
00:09:16,000 --> 00:09:18,000
few and far between.

231
00:09:18,000 --> 00:09:20,000
Even still, when we have the opportunity

232
00:09:20,000 --> 00:09:22,000
to make such an improvement

233
00:09:22,000 --> 00:09:24,000
that might not require humans to change their behavior,

234
00:09:24,000 --> 00:09:26,000
it often doesn't happen

235
00:09:26,000 --> 00:09:28,000
because it ends up

236
00:09:28,000 --> 00:09:30,000
being technically more talented.

237
00:09:30,000 --> 00:09:32,000
And so we fail to take into account

238
00:09:32,000 --> 00:09:34,000
the cost of changing human behavior

239
00:09:34,000 --> 00:09:36,000
and adding that up all the time

240
00:09:36,000 --> 00:09:38,000
that it goes into a change.

241
00:09:38,000 --> 00:09:40,000
So there's almost always a requirement

242
00:09:40,000 --> 00:09:42,000
that people do X.

243
00:09:42,000 --> 00:09:44,000
The need to change human behavior is basically

244
00:09:44,000 --> 00:09:46,000
inevitable for almost

245
00:09:46,000 --> 00:09:48,000
every technical problem.

246
00:09:48,000 --> 00:09:50,000
And I think this is especially true

247
00:09:50,000 --> 00:09:52,000
for software security because

248
00:09:52,000 --> 00:09:54,000
let's be real.

249
00:09:54,000 --> 00:09:56,000
Most software in security

250
00:09:56,000 --> 00:09:58,000
comes from those best in humans.

251
00:09:58,000 --> 00:10:00,000
This is called

252
00:10:00,000 --> 00:10:02,000
churn. This is the part where we

253
00:10:02,000 --> 00:10:04,000
require humans to change and make a change.

254
00:10:04,000 --> 00:10:06,000
Meaning any work that you

255
00:10:06,000 --> 00:10:08,000
have to do that isn't directly

256
00:10:08,000 --> 00:10:10,000
related to your outcome.

257
00:10:10,000 --> 00:10:12,000
So an example of churn would be

258
00:10:12,000 --> 00:10:14,000
adopting a convention updating tool

259
00:10:14,000 --> 00:10:16,000
on your ProBit and merging version

260
00:10:16,000 --> 00:10:18,000
bumps in your releases.

261
00:10:18,000 --> 00:10:20,000
Or adopting a vulnerability scanner to run

262
00:10:20,000 --> 00:10:22,000
across all your dependencies and a lot of your vulnerabilities

263
00:10:22,000 --> 00:10:24,000
when they happen.

264
00:10:24,000 --> 00:10:26,000
Or maybe churn is like

265
00:10:26,000 --> 00:10:28,000
a change on your keyboard for just a second

266
00:10:28,000 --> 00:10:30,000
to pop a little address in your

267
00:10:30,000 --> 00:10:32,000
toggle window.

268
00:10:32,000 --> 00:10:34,000
Okay, so part two. Why is some churn

269
00:10:34,000 --> 00:10:36,000
necessary? So we've established that

270
00:10:36,000 --> 00:10:38,000
churn exists, but let's figure out

271
00:10:38,000 --> 00:10:40,000
why it's necessary.

272
00:10:40,000 --> 00:10:42,000
So sometimes security

273
00:10:42,000 --> 00:10:44,000
can happen by default.

274
00:10:44,000 --> 00:10:46,000
So one example

275
00:10:46,000 --> 00:10:48,000
of this might be like auto-revoking

276
00:10:48,000 --> 00:10:50,000
credentials from a token leak.

277
00:10:50,000 --> 00:10:52,000
So right now if you put your

278
00:10:52,000 --> 00:10:54,000
IPI or API token

279
00:10:54,000 --> 00:10:56,000
inside of a PID and you push out

280
00:10:56,000 --> 00:10:58,000
to GitHub, GitHub would

281
00:10:58,000 --> 00:11:00,000
immediately detect it,

282
00:11:00,000 --> 00:11:02,000
alert IPI, and PID would immediately

283
00:11:02,000 --> 00:11:04,000
revert back to the control. Probably within

284
00:11:04,000 --> 00:11:06,000
milliseconds of you pushing that PID.

285
00:11:06,000 --> 00:11:08,000
And this requires zero

286
00:11:08,000 --> 00:11:10,000
change to human behavior.

287
00:11:10,000 --> 00:11:12,000
To get this increased security,

288
00:11:12,000 --> 00:11:14,000
humans can and do

289
00:11:14,000 --> 00:11:16,000
continue to happily share in those

290
00:11:16,000 --> 00:11:18,000
handshakes, secrets, and so on, but they're protected.

291
00:11:18,000 --> 00:11:20,000
There's just significantly

292
00:11:20,000 --> 00:11:22,000
less fallout to deal with and so

293
00:11:22,000 --> 00:11:24,000
they don't have to do anything to get that benefit.

294
00:11:24,000 --> 00:11:26,000
But usually security

295
00:11:26,000 --> 00:11:28,000
does require some change. Usually you gotta

296
00:11:28,000 --> 00:11:30,000
do something to give something in return.

297
00:11:32,000 --> 00:11:34,000
So yeah, okay, if churn is inevitable,

298
00:11:34,000 --> 00:11:36,000
what should we do about it?

299
00:11:36,000 --> 00:11:38,000
So first, here's some things that I don't

300
00:11:38,000 --> 00:11:40,000
think we should do.

301
00:11:40,000 --> 00:11:42,000
So one of these things I think

302
00:11:42,000 --> 00:11:44,000
we should do is assume we can just

303
00:11:44,000 --> 00:11:46,000
magically give people to do things that are

304
00:11:46,000 --> 00:11:48,000
impossible.

305
00:11:48,000 --> 00:11:50,000
We can't just like wave our hand and get humans

306
00:11:50,000 --> 00:11:52,000
to act. So maybe

307
00:11:52,000 --> 00:11:54,000
an example here is PGP in a

308
00:11:54,000 --> 00:11:56,000
anatomical sense, okay, maybe it's a decent solution

309
00:11:56,000 --> 00:11:58,000
to the problem. In a practical sense,

310
00:11:58,000 --> 00:12:00,000
I think it's kind of absolutely nonsense.

311
00:12:00,000 --> 00:12:02,000
In order to use

312
00:12:02,000 --> 00:12:04,000
this project, you get a bunch of humans

313
00:12:04,000 --> 00:12:06,000
in a room and have them show each other their drivers licenses

314
00:12:06,000 --> 00:12:08,000
and special random characters

315
00:12:08,000 --> 00:12:10,000
to verify their identities. And then also

316
00:12:10,000 --> 00:12:12,000
those humans can never lose those stuff of random

317
00:12:12,000 --> 00:12:14,000
things or characters, ever.

318
00:12:14,000 --> 00:12:16,000
Like, god.

319
00:12:16,000 --> 00:12:18,000
Everything we shouldn't do is

320
00:12:18,000 --> 00:12:20,000
shift the complexity on the

321
00:12:20,000 --> 00:12:22,000
people. And I'm talking about stuff that's not

322
00:12:22,000 --> 00:12:24,000
strictly possible for humans to do,

323
00:12:24,000 --> 00:12:26,000
but it's not always the most

324
00:12:26,000 --> 00:12:28,000
ideal thing for humans to do.

325
00:12:28,000 --> 00:12:30,000
So, for example,

326
00:12:30,000 --> 00:12:32,000
reading CDE does for vulnerabilities.

327
00:12:32,000 --> 00:12:34,000
And trying to determine if

328
00:12:34,000 --> 00:12:36,000
they're effective. So

329
00:12:36,000 --> 00:12:38,000
I have defined the vulnerability

330
00:12:38,000 --> 00:12:40,000
in some computer thing, and

331
00:12:40,000 --> 00:12:42,000
then the way to fix it is that I write

332
00:12:42,000 --> 00:12:44,000
as humans some long document,

333
00:12:44,000 --> 00:12:46,000
and then I have to have other humans

334
00:12:46,000 --> 00:12:48,000
read that long document, and then

335
00:12:48,000 --> 00:12:50,000
they have to try to figure out if they're trying to find their own computer

336
00:12:50,000 --> 00:12:52,000
changes. That's madness, right?

337
00:12:52,000 --> 00:12:54,000
We should cut out the middle bit of that.

338
00:12:54,000 --> 00:12:56,000
There's better ways to do that.

339
00:12:56,000 --> 00:12:58,000
The other thing we

340
00:12:58,000 --> 00:13:00,000
shouldn't do is nothing.

341
00:13:00,000 --> 00:13:02,000
We could all just give up and become

342
00:13:02,000 --> 00:13:04,000
farmers, but we shouldn't do that.

343
00:13:04,000 --> 00:13:06,000
So one

344
00:13:06,000 --> 00:13:08,000
thing maybe we could do

345
00:13:08,000 --> 00:13:10,000
is force a change. The other thing.

346
00:13:10,000 --> 00:13:12,000
So, I work at Google,

347
00:13:12,000 --> 00:13:14,000
which I often describe as having

348
00:13:14,000 --> 00:13:16,000
like this black key upside down,

349
00:13:16,000 --> 00:13:18,000
which is a universe of software tools.

350
00:13:18,000 --> 00:13:20,000
And in this ecosystem,

351
00:13:20,000 --> 00:13:22,000
adoption is not owned by

352
00:13:22,000 --> 00:13:24,000
many little small ones that can get

353
00:13:24,000 --> 00:13:26,000
some individuals. Because if you

354
00:13:26,000 --> 00:13:28,000
can just convince a couple people,

355
00:13:28,000 --> 00:13:30,000
very special people, you can just

356
00:13:30,000 --> 00:13:32,000
demand that everybody does the same thing.

357
00:13:32,000 --> 00:13:34,000
So overnight your software

358
00:13:34,000 --> 00:13:36,000
pool can go from like zero users to

359
00:13:36,000 --> 00:13:38,000
9,000 users just by hand.

360
00:13:38,000 --> 00:13:40,000
And this works for Google, because

361
00:13:40,000 --> 00:13:42,000
there is authority

362
00:13:42,000 --> 00:13:44,000
for anybody to change. So

363
00:13:44,000 --> 00:13:46,000
it's possible that could work for your

364
00:13:46,000 --> 00:13:48,000
organization as well, but it might

365
00:13:48,000 --> 00:13:50,000
not work because it's also really

366
00:13:50,000 --> 00:13:52,000
expensive, right? It requires

367
00:13:52,000 --> 00:13:54,000
paying people money to beat the cost

368
00:13:54,000 --> 00:13:56,000
of adoption to do all the turn.

369
00:13:56,000 --> 00:13:58,000
And I think usually this

370
00:13:58,000 --> 00:14:00,000
doesn't work in open source ecosystems.

371
00:14:00,000 --> 00:14:02,000
Usually because there isn't

372
00:14:02,000 --> 00:14:04,000
a central authority. So in this context,

373
00:14:04,000 --> 00:14:06,000
something like the Python steering

374
00:14:06,000 --> 00:14:08,000
capsule or core dev team, right?

375
00:14:08,000 --> 00:14:10,000
They have some authority to

376
00:14:10,000 --> 00:14:12,000
make changes, but they have very little authority

377
00:14:12,000 --> 00:14:14,000
to make users use those changes

378
00:14:14,000 --> 00:14:16,000
to adopt their changes, right?

379
00:14:16,000 --> 00:14:18,000
It works for using M197.

380
00:14:18,000 --> 00:14:20,000
But also, because of

381
00:14:20,000 --> 00:14:22,000
the expense, right? If those

382
00:14:22,000 --> 00:14:24,000
mandatings are not the ones who are

383
00:14:24,000 --> 00:14:26,000
paying the expensive curve

384
00:14:26,000 --> 00:14:28,000
of mandates,

385
00:14:28,000 --> 00:14:30,000
that becomes problematic, and I'll talk more about that

386
00:14:30,000 --> 00:14:32,000
in a second. And that's

387
00:14:32,000 --> 00:14:34,000
the other thing that I do think there are times when

388
00:14:34,000 --> 00:14:36,000
mandates are necessary, and I'll talk more

389
00:14:36,000 --> 00:14:38,000
about that in a second.

390
00:14:38,000 --> 00:14:40,000
Alright, so turn is inevitable.

391
00:14:40,000 --> 00:14:42,000
What should we do?

392
00:14:42,000 --> 00:14:44,000
First, as much as possible,

393
00:14:44,000 --> 00:14:46,000
we should minimize. This means

394
00:14:46,000 --> 00:14:48,000
putting the hard work in to shift

395
00:14:48,000 --> 00:14:50,000
as much complexity

396
00:14:50,000 --> 00:14:52,000
from people into

397
00:14:52,000 --> 00:14:54,000
the technology to pull the

398
00:14:54,000 --> 00:14:56,000
solution. And there's a few types of

399
00:14:56,000 --> 00:14:58,000
minimizations that are possible here.

400
00:14:58,000 --> 00:15:00,000
So one, we can minimize human

401
00:15:00,000 --> 00:15:02,000
effort through local automation. So like,

402
00:15:02,000 --> 00:15:04,000
these are like the bots that

403
00:15:04,000 --> 00:15:06,000
vary. The dependencies for new engines.

404
00:15:06,000 --> 00:15:08,000
Humans don't have to be there.

405
00:15:08,000 --> 00:15:10,000
Humans don't have to make the PRs, the bots do that too.

406
00:15:10,000 --> 00:15:12,000
All the maintainer has to do is click work.

407
00:15:12,000 --> 00:15:14,000
It's still a little churn there, but we've really

408
00:15:14,000 --> 00:15:16,000
minimized it. Another

409
00:15:16,000 --> 00:15:18,000
one is minimizing the costs

410
00:15:18,000 --> 00:15:20,000
through efficiency and simplification. I'm not

411
00:15:20,000 --> 00:15:22,000
talking about like cost to cost of life,

412
00:15:22,000 --> 00:15:24,000
I'm talking about people's time.

413
00:15:24,000 --> 00:15:26,000
As long as we can use

414
00:15:26,000 --> 00:15:28,000
these to minimize the effort necessary

415
00:15:28,000 --> 00:15:30,000
to adopt some new thing or to change,

416
00:15:30,000 --> 00:15:32,000
that counts.

417
00:15:32,000 --> 00:15:34,000
The other thing is minimizing

418
00:15:34,000 --> 00:15:36,000
human time through education. If

419
00:15:36,000 --> 00:15:38,000
the human already has a base level

420
00:15:38,000 --> 00:15:40,000
of understanding about why a change

421
00:15:40,000 --> 00:15:42,000
is necessary, that's not just coming out of the blue.

422
00:15:42,000 --> 00:15:44,000
There's a lot less context that

423
00:15:44,000 --> 00:15:46,000
you need to build in your mind

424
00:15:46,000 --> 00:15:48,000
to adopt a change.

425
00:15:48,000 --> 00:15:50,000
I'll highlight a specific example

426
00:15:50,000 --> 00:15:52,000
on this if you don't want to get

427
00:15:52,000 --> 00:15:54,000
clued by the foreign firms thinking. So

428
00:15:54,000 --> 00:15:56,000
we just launched trusted publishers for

429
00:15:56,000 --> 00:15:58,000
API application. So this is

430
00:15:58,000 --> 00:16:00,000
a new way for maintainers to publish

431
00:16:00,000 --> 00:16:02,000
to API

432
00:16:02,000 --> 00:16:04,000
from GitHub Actions that does

433
00:16:04,000 --> 00:16:06,000
not require long-lived passwords or

434
00:16:06,000 --> 00:16:08,000
API tokens.

435
00:16:08,000 --> 00:16:10,000
So this is something that requires

436
00:16:10,000 --> 00:16:12,000
maintainers to make a change,

437
00:16:12,000 --> 00:16:14,000
but we put a lot of effort into

438
00:16:14,000 --> 00:16:16,000
minimizing how much churn is necessary

439
00:16:16,000 --> 00:16:18,000
to adopt this.

440
00:16:18,000 --> 00:16:20,000
So, the first thing we do is

441
00:16:20,000 --> 00:16:22,000
we minimize effort through automation.

442
00:16:22,000 --> 00:16:24,000
So this requires a new authentication

443
00:16:24,000 --> 00:16:26,000
that involves a somewhat complex

444
00:16:26,000 --> 00:16:28,000
token exchange. You can't use it

445
00:16:28,000 --> 00:16:30,000
because it might see your eyes, nose, and nose

446
00:16:30,000 --> 00:16:32,000
already. Don't worry. It's all

447
00:16:32,000 --> 00:16:34,000
really nicely abstracted away

448
00:16:34,000 --> 00:16:36,000
from the user if you use

449
00:16:36,000 --> 00:16:38,000
the IP address GitHub Actions. You don't have to think about it.

450
00:16:40,000 --> 00:16:42,000
We also minimize the cost of adoption

451
00:16:42,000 --> 00:16:44,000
significantly. To adopt this,

452
00:16:44,000 --> 00:16:46,000
all you need to do is really change

453
00:16:46,000 --> 00:16:48,000
the different lines that you're going to have work with.

454
00:16:48,000 --> 00:16:50,000
And one of those is to comment

455
00:16:50,000 --> 00:16:52,000
two over windows such as building an entire

456
00:16:52,000 --> 00:16:54,000
house or building a building.

457
00:16:54,000 --> 00:16:56,000
And the other thing we did

458
00:16:56,000 --> 00:16:58,000
is we educated. I

459
00:16:58,000 --> 00:17:00,000
just told you everything you need

460
00:17:00,000 --> 00:17:02,000
to know about this. So you should now

461
00:17:02,000 --> 00:17:04,000
feel empowered to go and adopt it.

462
00:17:04,000 --> 00:17:06,000
You'll be accepted.

463
00:17:06,000 --> 00:17:08,000
Okay, getting back to the ways that we can

464
00:17:08,000 --> 00:17:10,000
minimize churn.

465
00:17:10,000 --> 00:17:12,000
Another thing I can do,

466
00:17:12,000 --> 00:17:14,000
another thing you can do is a regular response.

467
00:17:14,000 --> 00:17:16,000
So what I mean by this is

468
00:17:16,000 --> 00:17:18,000
is to develop a change that is so compelling

469
00:17:18,000 --> 00:17:20,000
in a way that it always

470
00:17:20,000 --> 00:17:22,000
makes sense to adopt a

471
00:17:22,000 --> 00:17:24,000
buy or a novelty.

472
00:17:24,000 --> 00:17:26,000
And an example here that I like to have

473
00:17:26,000 --> 00:17:28,000
is Sixer. I think Sixer is a really

474
00:17:28,000 --> 00:17:30,000
good example of this. So full disclosure,

475
00:17:30,000 --> 00:17:32,000
I am very biased because I work on

476
00:17:32,000 --> 00:17:34,000
this project. I created

477
00:17:34,000 --> 00:17:36,000
the Python client for Sixer. Sixer was

478
00:17:36,000 --> 00:17:38,000
support and I spent on the

479
00:17:38,000 --> 00:17:40,000
technical committee that

480
00:17:40,000 --> 00:17:42,000
is the whole of Sixer.

481
00:17:42,000 --> 00:17:44,000
But the reason I decided

482
00:17:44,000 --> 00:17:46,000
to do all these things is because I found

483
00:17:46,000 --> 00:17:48,000
Sixer to be such a novel

484
00:17:48,000 --> 00:17:50,000
compelling solution to the problem of code

485
00:17:50,000 --> 00:17:52,000
signing, which solves many

486
00:17:52,000 --> 00:17:54,000
existing problems in our ecosystem.

487
00:17:54,000 --> 00:17:56,000
So I'm not going to go into full detail

488
00:17:56,000 --> 00:17:58,000
about what really compelling

489
00:17:58,000 --> 00:18:00,000
reasons are and how Sixer works.

490
00:18:00,000 --> 00:18:02,000
If you want that you can watch one of my videos

491
00:18:02,000 --> 00:18:04,000
to talk ergonomic code signing

492
00:18:04,000 --> 00:18:06,000
for Python, especially with Sixer.

493
00:18:06,000 --> 00:18:08,000
He goes into a very deep dive about this.

494
00:18:08,000 --> 00:18:10,000
But suffice it to say that Sixer

495
00:18:10,000 --> 00:18:12,000
is a brand new way to sign

496
00:18:12,000 --> 00:18:14,000
and verify just without any

497
00:18:14,000 --> 00:18:16,000
help. But this comes at a really high

498
00:18:16,000 --> 00:18:18,000
price point. You need to adopt an entirely new

499
00:18:18,000 --> 00:18:20,000
set of tools first time and verify.

500
00:18:20,000 --> 00:18:22,000
Or about the set of

501
00:18:22,000 --> 00:18:24,000
tools in the first place.

502
00:18:24,000 --> 00:18:26,000
So in this case I would say that

503
00:18:26,000 --> 00:18:28,000
Sixer is revolutionary because

504
00:18:28,000 --> 00:18:30,000
the cost is worth it and

505
00:18:30,000 --> 00:18:32,000
continuing to invest in this as a solution

506
00:18:32,000 --> 00:18:34,000
is still worth it.

507
00:18:36,000 --> 00:18:38,000
And if all else fails we can't

508
00:18:38,000 --> 00:18:40,000
accept the lies. So one, maybe

509
00:18:40,000 --> 00:18:42,000
this solution is really expensive

510
00:18:42,000 --> 00:18:44,000
to adopt or maybe it's not

511
00:18:44,000 --> 00:18:46,000
particularly revolutionary

512
00:18:46,000 --> 00:18:48,000
or maybe it may need

513
00:18:48,000 --> 00:18:50,000
fields related to it. So

514
00:18:50,000 --> 00:18:52,000
instead we can sort of put our finger

515
00:18:52,000 --> 00:18:54,000
on the scale in various ways.

516
00:18:54,000 --> 00:18:56,000
So sometimes this comes naturally

517
00:18:56,000 --> 00:18:58,000
like with trusted publishers

518
00:18:58,000 --> 00:19:00,000
the presence of this feature unlocks

519
00:19:00,000 --> 00:19:02,000
many other things that we could do like

520
00:19:02,000 --> 00:19:04,000
we'll be able to show

521
00:19:04,000 --> 00:19:06,000
and verify a link between a Python

522
00:19:06,000 --> 00:19:08,000
project and the GitHub repository that created it.

523
00:19:08,000 --> 00:19:10,000
It's not something we could verify before.

524
00:19:10,000 --> 00:19:12,000
But this will only be possible

525
00:19:12,000 --> 00:19:14,000
to projects that have adopted this feature.

526
00:19:14,000 --> 00:19:16,000
So in that way it sort of helps

527
00:19:16,000 --> 00:19:18,000
to incentivize our adoption.

528
00:19:18,000 --> 00:19:20,000
Sometimes we can add

529
00:19:20,000 --> 00:19:22,000
other related benefits.

530
00:19:22,000 --> 00:19:24,000
Like for example the PyQ 2.0

531
00:19:24,000 --> 00:19:26,000
giveaway and

532
00:19:26,000 --> 00:19:28,000
security key giveaway.

533
00:19:28,000 --> 00:19:30,000
Sometimes this works, sometimes it doesn't.

534
00:19:30,000 --> 00:19:32,000
I know there is some drama

535
00:19:32,000 --> 00:19:34,000
about the security key giveaway.

536
00:19:34,000 --> 00:19:36,000
I know. But in this example

537
00:19:36,000 --> 00:19:38,000
I think it worked because

538
00:19:38,000 --> 00:19:40,000
we gave away

539
00:19:40,000 --> 00:19:42,000
thousands of keys

540
00:19:42,000 --> 00:19:44,000
and

541
00:19:44,000 --> 00:19:46,000
more than 35,000

542
00:19:46,000 --> 00:19:48,000
users now have two factor enable a giveaway.

543
00:19:50,000 --> 00:19:52,000
So with the example of

544
00:19:52,000 --> 00:19:54,000
two factor authentication

545
00:19:54,000 --> 00:19:56,000
why would we even do that?

546
00:19:56,000 --> 00:19:58,000
What's the point?

547
00:19:58,000 --> 00:20:00,000
It's perfectly how security

548
00:20:00,000 --> 00:20:02,000
increases sustainability.

549
00:20:02,000 --> 00:20:04,000
I think a lot of it already means

550
00:20:04,000 --> 00:20:06,000
four security improvements usually

551
00:20:06,000 --> 00:20:08,000
amount to because hackers.

552
00:20:08,000 --> 00:20:10,000
It's preventative care to make sure

553
00:20:10,000 --> 00:20:12,000
that bad things don't happen

554
00:20:12,000 --> 00:20:14,000
someday or bad things are like

555
00:20:14,000 --> 00:20:16,000
compromised execution or whatever.

556
00:20:16,000 --> 00:20:18,000
There's also

557
00:20:18,000 --> 00:20:20,000
common criticism for this though that security

558
00:20:20,000 --> 00:20:22,000
is like a slippery slope. Especially

559
00:20:22,000 --> 00:20:24,000
things like security decades.

560
00:20:24,000 --> 00:20:26,000
The idea is that if we keep increasing

561
00:20:26,000 --> 00:20:28,000
security requirements we'll end up

562
00:20:28,000 --> 00:20:30,000
in some pit of maximum security where

563
00:20:30,000 --> 00:20:32,000
innovation is completely stifled

564
00:20:32,000 --> 00:20:34,000
and those maintainers and consumers

565
00:20:34,000 --> 00:20:36,000
just can't do anything.

566
00:20:38,000 --> 00:20:40,000
Ultimately I think

567
00:20:40,000 --> 00:20:42,000
our goal is increasing

568
00:20:42,000 --> 00:20:44,000
the sustainability of security. There is a cost

569
00:20:44,000 --> 00:20:46,000
to the life of security.

570
00:20:46,000 --> 00:20:48,000
We just don't pay it when we choose

571
00:20:48,000 --> 00:20:50,000
not to adopt. We pay it months

572
00:20:50,000 --> 00:20:52,000
and years later when things happen

573
00:20:52,000 --> 00:20:54,000
and we pay it with interest.

574
00:20:54,000 --> 00:20:56,000
So here's an example.

575
00:20:56,000 --> 00:20:58,000
Think about what has to happen

576
00:20:58,000 --> 00:21:00,000
when just one maintainer on a pipe

577
00:21:00,000 --> 00:21:02,000
attic gets fished and their project

578
00:21:02,000 --> 00:21:04,000
gets compromised and now it gets published.

579
00:21:04,000 --> 00:21:06,000
So it's up to

580
00:21:06,000 --> 00:21:08,000
to create reports, that can do investigation,

581
00:21:08,000 --> 00:21:10,000
that can do public communications,

582
00:21:10,000 --> 00:21:12,000
get down to malware,

583
00:21:12,000 --> 00:21:14,000
do a top cover for the maintainer,

584
00:21:14,000 --> 00:21:16,000
talk to the press, talk to

585
00:21:16,000 --> 00:21:18,000
other folks. Like it gets really complicated.

586
00:21:18,000 --> 00:21:20,000
It eats up a lot of time. So

587
00:21:20,000 --> 00:21:22,000
at the expense of maybe one person

588
00:21:22,000 --> 00:21:24,000
not taking a few seconds to be used

589
00:21:24,000 --> 00:21:26,000
to fact-count a login, this seems like

590
00:21:26,000 --> 00:21:28,000
hours of maintainer time that

591
00:21:28,000 --> 00:21:30,000
could be used for other things. And also

592
00:21:30,000 --> 00:21:32,000
potentially downstream users who use

593
00:21:32,000 --> 00:21:34,000
this up their time as well.

594
00:21:34,000 --> 00:21:36,000
So in this case we want PyPy to be

595
00:21:36,000 --> 00:21:38,000
secure and useful for

596
00:21:38,000 --> 00:21:40,000
all users. The goal isn't to make

597
00:21:40,000 --> 00:21:42,000
PyPy impossible to use. That would be

598
00:21:42,000 --> 00:21:44,000
kind of a two-bit thing.

599
00:21:44,000 --> 00:21:46,000
But if these are the kind of things we have to

600
00:21:46,000 --> 00:21:48,000
spend all our time on, PyPy will

601
00:21:48,000 --> 00:21:50,000
become impossible to use. Like if I spend

602
00:21:50,000 --> 00:21:52,000
my entire day responding to

603
00:21:52,000 --> 00:21:54,000
malware, then responding to

604
00:21:54,000 --> 00:21:56,000
people getting fished and their

605
00:21:56,000 --> 00:21:58,000
accounts compromised, I don't have time to build

606
00:21:58,000 --> 00:22:00,000
cool features.

607
00:22:00,000 --> 00:22:02,000
I don't disagree that software security

608
00:22:02,000 --> 00:22:04,000
is a slippery slope, but

609
00:22:04,000 --> 00:22:06,000
I think the slope is actually upwards,

610
00:22:06,000 --> 00:22:08,000
not downwards. Like really

611
00:22:08,000 --> 00:22:10,000
deep, honey, upwards.

612
00:22:10,000 --> 00:22:12,000
And it's because people, like people, all

613
00:22:12,000 --> 00:22:14,000
people, they're resistant to change.

614
00:22:14,000 --> 00:22:16,000
And because this upfront

615
00:22:16,000 --> 00:22:18,000
investment in security is really

616
00:22:18,000 --> 00:22:20,000
hard to show impact for

617
00:22:20,000 --> 00:22:22,000
when the result is something that doesn't

618
00:22:22,000 --> 00:22:24,000
happen. The effect

619
00:22:24,000 --> 00:22:26,000
of all of this is that it's

620
00:22:26,000 --> 00:22:28,000
often really hard to adopt

621
00:22:28,000 --> 00:22:30,000
insecure features.

622
00:22:30,000 --> 00:22:32,000
Here's another example. So maybe you've heard

623
00:22:32,000 --> 00:22:34,000
about the 3CX

624
00:22:34,000 --> 00:22:36,000
blockchain attack. So this just happened last month.

625
00:22:36,000 --> 00:22:38,000
What happened was

626
00:22:38,000 --> 00:22:40,000
attackers exploited a bug in the way

627
00:22:40,000 --> 00:22:42,000
that when those executables are signed,

628
00:22:42,000 --> 00:22:44,000
it compromises a communications company

629
00:22:44,000 --> 00:22:46,000
called 3CX.

630
00:22:46,000 --> 00:22:48,000
So this bug allows an attacker to modify

631
00:22:48,000 --> 00:22:50,000
any sign of an executable

632
00:22:50,000 --> 00:22:52,000
to include malware.

633
00:22:52,000 --> 00:22:54,000
And for verification to frame,

634
00:22:54,000 --> 00:22:56,000
to still claim that it's

635
00:22:56,000 --> 00:22:58,000
originally inside.

636
00:22:58,000 --> 00:23:00,000
This bug was known for 10 years.

637
00:23:00,000 --> 00:23:02,000
And yeah, it sounds like a pretty big

638
00:23:02,000 --> 00:23:04,000
deal. It was fixed, actually,

639
00:23:04,000 --> 00:23:06,000
but then it was only fixed as

640
00:23:06,000 --> 00:23:08,000
an opt-in.

641
00:23:08,000 --> 00:23:10,000
Why was it opted?

642
00:23:10,000 --> 00:23:12,000
Because many people were shipping

643
00:23:12,000 --> 00:23:14,000
Windows executables

644
00:23:14,000 --> 00:23:16,000
using this bug as a feature.

645
00:23:16,000 --> 00:23:18,000
So, for example, Google Chrome,

646
00:23:18,000 --> 00:23:20,000
the installer for Google Chrome,

647
00:23:20,000 --> 00:23:22,000
using this bug to provide

648
00:23:22,000 --> 00:23:24,000
a slightly modified binary if you opted

649
00:23:24,000 --> 00:23:26,000
into sending you statistics

650
00:23:26,000 --> 00:23:28,000
and crash reports to Google.

651
00:23:28,000 --> 00:23:30,000
So when you downloaded Chrome, and when the executable

652
00:23:30,000 --> 00:23:32,000
was installed, it uses

653
00:23:32,000 --> 00:23:34,000
this bug to determine if diagnostic

654
00:23:34,000 --> 00:23:36,000
reports should be enabled.

655
00:23:36,000 --> 00:23:38,000
It wasn't just Chrome. Lots of people were using this.

656
00:23:38,000 --> 00:23:40,000
So because fixing this

657
00:23:40,000 --> 00:23:42,000
bug would break so many

658
00:23:42,000 --> 00:23:44,000
workflows, it didn't happen.

659
00:23:44,000 --> 00:23:46,000
For 10 years, it was opt-in.

660
00:23:46,000 --> 00:23:48,000
Engines made things worse.

661
00:23:48,000 --> 00:23:50,000
When you upgrade to any version

662
00:23:50,000 --> 00:23:52,000
of Windows, it reverts the opt-in.

663
00:23:52,000 --> 00:23:54,000
So remember, users

664
00:23:54,000 --> 00:23:56,000
don't ask for security. No one was petitioning

665
00:23:56,000 --> 00:23:58,000
Microsoft to fix this.

666
00:23:58,000 --> 00:24:00,000
And sometimes users tend to take

667
00:24:00,000 --> 00:24:02,000
it to pregnancy. No one asked for security.

668
00:24:04,000 --> 00:24:06,000
Okay. That's enough writing for me.

669
00:24:06,000 --> 00:24:08,000
Let me get to my point.

670
00:24:08,000 --> 00:24:10,000
So I hope that you now have an understanding about

671
00:24:10,000 --> 00:24:12,000
why this is hard,

672
00:24:12,000 --> 00:24:14,000
about why some turn is necessary,

673
00:24:14,000 --> 00:24:16,000
about how security

674
00:24:16,000 --> 00:24:18,000
increases the system in building.

675
00:24:18,000 --> 00:24:20,000
What do I want you to do?

676
00:24:20,000 --> 00:24:22,000
First, please don't depend

677
00:24:22,000 --> 00:24:24,000
on others.

678
00:24:24,000 --> 00:24:26,000
This may be a rare case, but don't take

679
00:24:26,000 --> 00:24:28,000
dependencies on the presence of bugs,

680
00:24:28,000 --> 00:24:30,000
especially security bugs.

681
00:24:30,000 --> 00:24:32,000
While we're at it, don't take dependencies on

682
00:24:32,000 --> 00:24:34,000
internal APIs that way.

683
00:24:34,000 --> 00:24:36,000
When you're breaking changes, it's a difficult

684
00:24:36,000 --> 00:24:38,000
program. The users should be able

685
00:24:38,000 --> 00:24:40,000
to ship bug fixes,

686
00:24:40,000 --> 00:24:42,000
ship security fixes, and improvements,

687
00:24:42,000 --> 00:24:44,000
and have them happen by default,

688
00:24:44,000 --> 00:24:46,000
especially when they address security

689
00:24:46,000 --> 00:24:48,000
vulnerabilities.

690
00:24:48,000 --> 00:24:50,000
I think what I'd like you to do is

691
00:24:50,000 --> 00:24:52,000
look for opportunities to proactively

692
00:24:52,000 --> 00:24:54,000
invest in security.

693
00:24:54,000 --> 00:24:56,000
So weigh the cost of adoption

694
00:24:56,000 --> 00:24:58,000
with the cost of if things

695
00:24:58,000 --> 00:25:00,000
go well. So for example,

696
00:25:00,000 --> 00:25:02,000
with trusted publishers,

697
00:25:02,000 --> 00:25:04,000
what would you rather do? A few minutes

698
00:25:04,000 --> 00:25:06,000
to change your developer

699
00:25:06,000 --> 00:25:08,000
for now, or a dental clapper bias

700
00:25:08,000 --> 00:25:10,000
if your long-term security

701
00:25:10,000 --> 00:25:12,000
can't be there for some reason.

702
00:25:12,000 --> 00:25:14,000
Maybe take a couple minutes to pay the check.

703
00:25:16,000 --> 00:25:18,000
Last thing is I want you to support

704
00:25:18,000 --> 00:25:20,000
sustainability.

705
00:25:20,000 --> 00:25:22,000
Be willing to accept a little

706
00:25:22,000 --> 00:25:24,000
turn when it's reasonable,

707
00:25:24,000 --> 00:25:26,000
and when the end result is

708
00:25:26,000 --> 00:25:28,000
a better overall ecosystem.

709
00:25:28,000 --> 00:25:30,000
Maybe also trust

710
00:25:30,000 --> 00:25:32,000
the maintainers of your ecosystem to make

711
00:25:32,000 --> 00:25:34,000
decisions that have your overall

712
00:25:34,000 --> 00:25:36,000
long-term best interests

713
00:25:36,000 --> 00:25:38,000
in mind, even if they might

714
00:25:38,000 --> 00:25:40,000
fail to still learn the inconvenience.

715
00:25:42,000 --> 00:25:44,000
Okay, let me make some quick shout-outs and

716
00:25:44,000 --> 00:25:46,000
acknowledgements. First, I want

717
00:25:46,000 --> 00:25:48,000
to thank William Woodruff and the Team

718
00:25:48,000 --> 00:25:50,000
Control Bits. We've done a lot of work on

719
00:25:50,000 --> 00:25:52,000
podcast specifically around trusted publishers

720
00:25:52,000 --> 00:25:54,000
and quality of good work on how

721
00:25:54,000 --> 00:25:56,000
it was doing. I want to thank

722
00:25:56,000 --> 00:25:58,000
Stas Los-Sitarenko, who's the maintainer

723
00:25:58,000 --> 00:26:00,000
of the podcast publisher,

724
00:26:00,000 --> 00:26:02,000
She Had Action, for also doing a lot of good

725
00:26:02,000 --> 00:26:04,000
work and making it very easy to use

726
00:26:04,000 --> 00:26:06,000
and adopt. I want to shout-out

727
00:26:06,000 --> 00:26:08,000
to everyone who enabled two-factor authentication

728
00:26:08,000 --> 00:26:10,000
on PyGI. Thank you.

729
00:26:10,000 --> 00:26:12,000
Again, go ahead.

730
00:26:12,000 --> 00:26:14,000
I want to shout-out to the

731
00:26:14,000 --> 00:26:15,760
Google

732
00:26:15,760 --> 00:26:17,760
Social Security, who funded much of this work,

733
00:26:17,760 --> 00:26:19,760
funded this creative giveaway, funded their trusted publishers' work.

734
00:26:19,760 --> 00:26:21,760
And finally,

735
00:26:21,760 --> 00:26:23,760
I want to give a huge thank you to

736
00:26:23,760 --> 00:26:25,760
podcast staff and everyone else

737
00:26:25,760 --> 00:26:27,760
as usual. They really deserve their thanks as well.

738
00:26:27,760 --> 00:26:29,760
So please look forward to and appreciate

739
00:26:29,760 --> 00:26:31,760
what they're doing. I thank you to all of you

740
00:26:31,760 --> 00:26:33,760
as well. Thanks.

741
00:26:33,760 --> 00:26:35,760
Thank you.

742
00:26:35,760 --> 00:26:37,760
Thank you.

743
00:26:37,760 --> 00:26:39,760
Thank you so much

744
00:26:39,760 --> 00:26:41,760
for being so thoughtful. In the same sense,

745
00:26:41,760 --> 00:26:43,760
we still have a couple of minutes

746
00:26:43,760 --> 00:26:45,760
left. If you have any questions, you can come up here

747
00:26:45,760 --> 00:26:47,760
and we can go next to you.

748
00:26:47,760 --> 00:26:49,760
Let's do a...I'm going to cut it off now and we can do

749
00:26:49,760 --> 00:26:51,760
a follow-up track. And then also, I should say

750
00:26:51,760 --> 00:26:53,760
there is an open space at

751
00:26:53,760 --> 00:26:55,760
4 o'clock downstairs

752
00:26:55,760 --> 00:26:57,760
on Black Sheep Security for maintainers.

753
00:26:57,760 --> 00:26:59,760
If you're using my archive,

754
00:26:59,760 --> 00:27:01,760
I'm going to take this as well and show what we talked about

755
00:27:01,760 --> 00:27:03,760
along the way and stuff. So please

756
00:27:03,760 --> 00:27:05,760
go check it out. Count with me. Nothing

757
00:27:05,760 --> 00:27:07,760
less for all I have. Go see the stickers

758
00:27:07,760 --> 00:27:09,760
if you want to. It's all down in the sticker.

759
00:27:09,760 --> 00:27:11,760
Thanks.

760
00:27:11,760 --> 00:27:13,760
Thanks.

