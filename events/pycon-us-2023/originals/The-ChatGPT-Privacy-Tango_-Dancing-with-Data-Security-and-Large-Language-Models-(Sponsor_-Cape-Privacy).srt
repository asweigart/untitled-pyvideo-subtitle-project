1
00:00:00,000 --> 00:00:01,000
Hello.

2
00:00:01,000 --> 00:00:04,920
Thanks for being here.

3
00:00:04,920 --> 00:00:09,200
All right.

4
00:00:09,200 --> 00:00:26,200
So, you are at the chat GPT privacy tango, dancing with data security and large language

5
00:00:26,200 --> 00:00:27,400
models.

6
00:00:27,400 --> 00:00:29,440
My name is Jason Mancuso.

7
00:00:29,440 --> 00:00:30,440
This is Mike Gardner.

8
00:00:30,440 --> 00:00:32,320
We are from Cape Privacy.

9
00:00:32,320 --> 00:00:35,640
So, yeah, let's get started.

10
00:00:35,640 --> 00:00:40,320
Yeah, so I've been working with Cape since its inception about five years ago.

11
00:00:40,320 --> 00:00:44,400
The whole time we've been focused on enabling privacy preserving machine learning.

12
00:00:44,400 --> 00:00:46,440
We've done it a few different ways.

13
00:00:46,440 --> 00:00:48,680
I'll talk a little bit about that.

14
00:00:48,680 --> 00:00:53,560
In the past I worked on applied data science and machine learning research.

15
00:00:53,560 --> 00:00:58,440
And we are here at booth 411 for the future.

16
00:00:58,440 --> 00:01:01,440
Mike is also with Cape.

17
00:01:01,440 --> 00:01:04,880
So, yeah, a little bit about Cape.

18
00:01:04,880 --> 00:01:09,960
We met working on open source privacy preserving machine learning around five years ago.

19
00:01:09,960 --> 00:01:14,280
We have a long history of contributions to open source as a result because that's kind

20
00:01:14,280 --> 00:01:16,800
of how the company started.

21
00:01:16,800 --> 00:01:20,000
Here are some projects we've worked on in the past.

22
00:01:20,000 --> 00:01:25,360
PySift is probably the largest tool, largest single tool that is used.

23
00:01:25,360 --> 00:01:29,320
It's mostly used for education, but it has been used for actual deployments.

24
00:01:29,320 --> 00:01:33,240
It's by a community called Open Mind.

25
00:01:33,240 --> 00:01:37,160
And then we're also the creators of a library called TF encrypted.

26
00:01:37,160 --> 00:01:38,560
About three or four years ago we wrote this.

27
00:01:38,560 --> 00:01:42,320
It's a tool for doing encrypted machine learning in TensorFlow.

28
00:01:42,320 --> 00:01:47,480
So, yeah, Python front end uses TensorFlow for the back end.

29
00:01:47,480 --> 00:01:52,680
And then TensorFlow is not really secure, so it's not great for privacy preserving machine

30
00:01:52,680 --> 00:01:53,680
learning.

31
00:01:54,000 --> 00:01:57,640
The servers themselves are kind of expected to trust each other, which is a no go for

32
00:01:57,640 --> 00:01:58,640
us.

33
00:01:58,640 --> 00:02:03,680
So we did a complete rewrite of that in Rust for a more recent product based on multi-party

34
00:02:03,680 --> 00:02:07,120
computation.

35
00:02:07,120 --> 00:02:11,400
But since then we've also worked with a few other technologies.

36
00:02:11,400 --> 00:02:17,320
We have Nitrogen, which is a open source tool just for working with nitro enclaves, AWS nitro

37
00:02:17,320 --> 00:02:18,560
enclaves.

38
00:02:18,640 --> 00:02:26,280
And then we have hybrid PKE, which is a Python exposure to the hybrid HPK protocol, which

39
00:02:26,280 --> 00:02:29,800
is a recently standardized public key cryptography protocol.

40
00:02:29,800 --> 00:02:32,880
Kind of one protocol to rule them all, hopefully.

41
00:02:32,880 --> 00:02:38,160
But yeah, that's kind of the gist of it is that we love ML, we love privacy, we love

42
00:02:38,160 --> 00:02:40,000
Python, and we love open source.

43
00:02:40,000 --> 00:02:41,000
That's our company.

44
00:02:41,000 --> 00:02:44,920
That's kind of what we've been since day one.

45
00:02:44,920 --> 00:02:49,920
So machine learning has changed a lot in the past five years.

46
00:02:49,920 --> 00:02:53,080
This talk is about large language models, which are all the rage currently.

47
00:02:53,080 --> 00:02:57,320
For us it's a big departure from where our company came from.

48
00:02:57,320 --> 00:03:01,080
So we've kind of had to get our heads around it fairly quickly, I think just like everybody

49
00:03:01,080 --> 00:03:02,080
else.

50
00:03:02,080 --> 00:03:06,200
So just an overview of the talk is that we'll talk a little bit about large language models,

51
00:03:06,200 --> 00:03:11,080
what they are, how they relate to data privacy, can you even have data privacy in a world

52
00:03:12,080 --> 00:03:16,520
models are the main form of machine learning that gets done.

53
00:03:16,520 --> 00:03:20,400
Talk a bit about this idea of the privacy tango, this metaphor.

54
00:03:20,400 --> 00:03:26,560
And also discuss some privacy primitives to prepare us to talk about one way that we've

55
00:03:26,560 --> 00:03:31,080
tried to solve this kind of paradox.

56
00:03:31,080 --> 00:03:34,800
And then we'll go with a demo of that system, talk about how we designed it, analyze if

57
00:03:34,800 --> 00:03:39,160
it's actually preserving privacy and if it still maintains utility.

58
00:03:39,160 --> 00:03:42,960
And then talk about some alternatives, conclude and yeah.

59
00:03:42,960 --> 00:03:48,400
And hopefully we'll have time to have Q&A if there's any questions or conversation to

60
00:03:48,400 --> 00:03:49,400
have.

61
00:03:49,400 --> 00:03:55,680
So what are large language models?

62
00:03:55,680 --> 00:04:00,840
Essentially what they are is just a giant model trained on internet scale data and internet

63
00:04:00,840 --> 00:04:04,800
scale data set where it's a language model.

64
00:04:04,800 --> 00:04:09,960
So you can put in some input or previous tokens in a phrase, let's say, predict the

65
00:04:09,960 --> 00:04:13,800
next token and you just do this on a massive scale.

66
00:04:13,800 --> 00:04:19,240
Tokens themselves are kind of an overloaded term, they could be letters, morphemes or

67
00:04:19,240 --> 00:04:21,480
full words.

68
00:04:21,480 --> 00:04:27,200
Most recently I think it's kind of like, it's not really morphemes but it's just pairs of

69
00:04:27,200 --> 00:04:30,960
letters.

70
00:04:31,120 --> 00:04:36,560
So yeah, the main feature why we call them large is because obviously they're very big

71
00:04:36,560 --> 00:04:38,040
in the context of machine learning.

72
00:04:38,040 --> 00:04:43,480
In the past maybe we've had a million parameter models, now these things are billions to hundreds

73
00:04:43,480 --> 00:04:48,040
of billions of parameters large, which means normally they can't actually fit on one computer

74
00:04:48,040 --> 00:04:54,560
for inference unless you do some real clever hacks and stuff like that.

75
00:04:54,560 --> 00:04:55,880
So that's one reason they're large.

76
00:04:55,880 --> 00:04:58,400
Another reason they're large is because of the scale of data they're trained on, which

77
00:04:58,400 --> 00:04:59,600
is generally internet scale.

78
00:04:59,600 --> 00:05:03,160
That's not necessarily all public data, although a lot of the open source models are trained

79
00:05:03,160 --> 00:05:04,520
on just public data.

80
00:05:04,520 --> 00:05:08,920
I think OpenAI at least has admitted that they've used data partnerships and stuff like

81
00:05:08,920 --> 00:05:12,200
that to get higher quality data.

82
00:05:12,200 --> 00:05:16,560
So that's one aspect that's also large is the amount of data they're trained on.

83
00:05:16,560 --> 00:05:22,440
But really the interesting thing about them is that at that scale these models show emergent

84
00:05:22,440 --> 00:05:25,120
behavior that's useful for a lot of different tasks.

85
00:05:25,120 --> 00:05:27,040
That's actually kind of always been the case for language models.

86
00:05:27,480 --> 00:05:33,840
I have two papers here cited that are from 27 and 2018, kind of before anyone was talking

87
00:05:33,840 --> 00:05:38,920
about LLMs, but when I read this paper in 2017 I remember thinking that is crazy.

88
00:05:38,920 --> 00:05:44,080
This is a paper basically saying that a language model just trained on a large data set of

89
00:05:44,080 --> 00:05:49,320
Amazon reviews, one of the neurons in its representation ended up being a state of the

90
00:05:49,320 --> 00:05:51,840
art sentiment analysis detector.

91
00:05:51,840 --> 00:05:55,960
It was better than all models trained specifically for sentiment analysis, but it was just one

92
00:05:55,960 --> 00:06:01,160
neuron that it had found in its internal representation and its job was just to predict

93
00:06:01,160 --> 00:06:02,160
the next token just like this.

94
00:06:02,160 --> 00:06:06,320
So this was like an early precursor saying language models are useful for a variety of

95
00:06:06,320 --> 00:06:07,600
other tasks.

96
00:06:07,600 --> 00:06:16,720
The next year Jeremy Howard, who's a big open source ML guy, him and Sebastian Ruter published

97
00:06:16,720 --> 00:06:20,720
this paper talking about how actually it's not just sentiment analysis, basically all

98
00:06:21,480 --> 00:06:27,480
classification problems can be solved, or maybe not solved, but you can get state of

99
00:06:27,480 --> 00:06:31,120
the art performance just by pre-training a pretty good language model and then fine tuning

100
00:06:31,120 --> 00:06:33,000
for whatever task you want.

101
00:06:33,000 --> 00:06:37,680
So this is kind of like the beginnings of it and now with GPT-3, et cetera, we find

102
00:06:37,680 --> 00:06:41,680
out that actually if you just go big enough you don't even need to do the fine tuning,

103
00:06:41,680 --> 00:06:46,760
you can just fit whatever you want into the prompt and as long as you structure the problem

104
00:06:46,760 --> 00:06:51,560
that you care about in the prompt well enough, generally language models can get smart enough

105
00:06:51,560 --> 00:06:54,720
to just do the task for you.

106
00:06:54,720 --> 00:06:59,960
Obviously there's some caveats there, but, and yeah, so some general modern examples

107
00:06:59,960 --> 00:07:01,840
of language models are below.

108
00:07:01,840 --> 00:07:07,200
Hopefully you've heard of one, if not several of these because, I mean, essentially it's

109
00:07:07,200 --> 00:07:11,760
open AIs, it's Googles, and then it's the open, it's Facebooks, which is now kind of

110
00:07:12,000 --> 00:07:19,000
open source, and then it's all the open source versions that have been trained as a result.

111
00:07:19,880 --> 00:07:24,520
Chat GPT, I'm sure everyone's heard of this.

112
00:07:24,520 --> 00:07:27,960
It's just, it's probably the most famous language, large language model because it actually hit

113
00:07:27,960 --> 00:07:32,560
the mainstream and people are using it in their everyday lives, at least people that

114
00:07:32,560 --> 00:07:34,720
I know.

115
00:07:34,720 --> 00:07:39,120
What it is really is just a large language model and then fine tuned with human feedback,

116
00:07:39,120 --> 00:07:44,440
human interaction using a technique that was published by OpenAI a while ago called

117
00:07:44,440 --> 00:07:47,560
Reinforcement Learning with Human Preferences.

118
00:07:47,560 --> 00:07:52,520
And then they fine tune it, not just for safety reasons, so that like the language is kind

119
00:07:52,520 --> 00:07:57,640
of, I mean, okay, that's one reason that it gets fine tuned is to prevent it from spitting

120
00:07:57,640 --> 00:08:01,520
out things that you may not want it to say, because obviously internet scale data is going

121
00:08:01,520 --> 00:08:07,140
to include a lot of stuff that you don't necessarily want in your chat bot, but also to just actually

122
00:08:07,180 --> 00:08:11,700
make it so that you can have a conversation with it as opposed to provide it like the

123
00:08:11,700 --> 00:08:14,780
first half of the sentence that you wanted to complete, right?

124
00:08:14,780 --> 00:08:20,460
Previous language models, like you actually had to figure out how to start the text that

125
00:08:20,460 --> 00:08:23,300
you wanted and then have the model fill it in.

126
00:08:23,300 --> 00:08:26,660
The fine tuning actually allows you to interact with it as if it's another person you're having

127
00:08:26,660 --> 00:08:27,660
a conversation.

128
00:08:27,660 --> 00:08:31,980
But the jury's kind of out if that's the best way to use these things, but that's kind of

129
00:08:31,980 --> 00:08:38,660
the way that OpenAI and then as a result, everyone else has kind of done it.

130
00:08:38,660 --> 00:08:43,700
So there are a number of privacy concerns when we think about large language models.

131
00:08:43,700 --> 00:08:47,940
The first and most basic is that, is the question of how do you handle sensitive information

132
00:08:47,940 --> 00:08:52,580
when you actually want to solve a task that requires that sensitive information?

133
00:08:52,580 --> 00:09:00,020
This could be PII, like health information, or just generally like sensitive personal

134
00:09:00,100 --> 00:09:01,820
information about yourself or other people.

135
00:09:01,820 --> 00:09:03,580
It could be confidential business data.

136
00:09:03,580 --> 00:09:06,260
It could be trade secrets, stuff like this.

137
00:09:06,260 --> 00:09:08,780
These are the things that you don't necessarily want to just give to OpenAI so that they can

138
00:09:08,780 --> 00:09:11,700
run it through the language model because you don't really know.

139
00:09:11,700 --> 00:09:15,740
We can all take OpenAI as where that they're deleting data after 30 days and they're not

140
00:09:15,740 --> 00:09:19,740
using your data for retraining, but we can't really know unless they were to implement

141
00:09:19,740 --> 00:09:24,580
some mechanism that would prove that to us and they haven't yet.

142
00:09:24,580 --> 00:09:29,220
And so you have this risk of leaks from those large language providers like OpenAI or Coheer,

143
00:09:29,220 --> 00:09:30,220
or whoever else.

144
00:09:30,220 --> 00:09:33,140
You have data leakage risks outside of that.

145
00:09:33,140 --> 00:09:38,620
So this is more about the training data where the large language models, they see so much

146
00:09:38,620 --> 00:09:42,100
data they actually memorize a lot of it.

147
00:09:42,100 --> 00:09:47,340
And if you really get detailed with your prompt engineering, you can actually extract that

148
00:09:47,340 --> 00:09:49,940
training data from the model pretty easily.

149
00:09:49,940 --> 00:09:51,860
This is true of other machine learning models.

150
00:09:51,860 --> 00:09:57,860
It just seems to be the fact that the scale of these models makes it actually kind of

151
00:09:58,260 --> 00:10:01,380
more of a concern because it's been trained on massive amount of both private and public

152
00:10:01,380 --> 00:10:02,380
data.

153
00:10:02,380 --> 00:10:06,940
And then there's kind of another privacy issue, which I'm not even sure if it's a privacy

154
00:10:06,940 --> 00:10:08,740
issue, but I included it here.

155
00:10:08,740 --> 00:10:10,940
It's this idea of hallucinations about individuals.

156
00:10:10,940 --> 00:10:17,420
The fact that it has any knowledge of individuals means it's, and that paired with the idea

157
00:10:17,420 --> 00:10:21,820
or paired with the fact that these language models tend to hallucinate if you kind of

158
00:10:21,820 --> 00:10:25,540
lead them into the hallucination, it's kind of a bad recipe, right?

159
00:10:26,020 --> 00:10:31,140
You have maybe lesser known people where it's seen the name, it's seen some things about

160
00:10:31,140 --> 00:10:37,620
them, knows enough to sound smart about the person, and then you lead it into hallucination

161
00:10:37,620 --> 00:10:41,500
and it just spits out garbage about that person that's not actually true.

162
00:10:41,500 --> 00:10:43,460
That is definitely possible.

163
00:10:43,460 --> 00:10:45,260
And I would say that's kind of a privacy concern, right?

164
00:10:45,260 --> 00:10:48,780
Because your data or information might be in this model, it might think it knows about

165
00:10:48,780 --> 00:10:51,220
you and then say something that's false.

166
00:10:51,220 --> 00:10:54,460
And I would say that's a risk.

167
00:10:54,460 --> 00:10:59,180
And then obviously there's some data that can only be used in very constrained ways.

168
00:10:59,180 --> 00:11:02,900
Data that's covered by GDPR, CCPA, HIPAA, et cetera.

169
00:11:02,900 --> 00:11:07,900
There's a lot of laws like this around data usage, so you have to consider that whenever

170
00:11:07,900 --> 00:11:13,340
you use any data with, or whenever you move any data outside of any organization or even

171
00:11:13,340 --> 00:11:14,580
within an organization.

172
00:11:14,580 --> 00:11:16,180
These are things you have to consider.

173
00:11:16,180 --> 00:11:20,820
And most businesses have practices for dealing with this internally, right?

174
00:11:20,940 --> 00:11:25,700
So these are the two, I think, that are most relevant for this talk.

175
00:11:25,700 --> 00:11:27,180
Data leak address and hallucinations.

176
00:11:27,180 --> 00:11:31,860
I think of these as more of like operator problems or provider problems where OpenAI

177
00:11:31,860 --> 00:11:34,500
better be figuring this out, Coheer better be figuring this out.

178
00:11:34,500 --> 00:11:37,980
But like as an app developer, there's not much I can do about these, there's much more

179
00:11:37,980 --> 00:11:41,900
I can do about the first and the last, or at least I would hope so.

180
00:11:41,900 --> 00:11:50,540
But that's actually the question of this talk is like, well, I'll say that is there any

181
00:11:50,580 --> 00:11:58,020
way for us to use these large language models without exposing all of our sensitive data

182
00:11:58,020 --> 00:12:03,140
to large language model providers?

183
00:12:03,140 --> 00:12:07,540
And we use this, actually GPT came up with this metaphor basically when I was talking

184
00:12:07,540 --> 00:12:08,540
to him about this talk.

185
00:12:08,540 --> 00:12:16,660
He was saying, why don't you use a dance because, I mean, I guess one way to think of it is

186
00:12:16,700 --> 00:12:20,460
a tango is hard to do and it's apparently also hard not to feed sensitive data to chat

187
00:12:20,460 --> 00:12:26,900
GPT based on all these headlines where people are due to some errors on OpenAI's part, chat

188
00:12:26,900 --> 00:12:33,900
GPT actually leaked some sensitive information including some Samsung source code.

189
00:12:33,900 --> 00:12:39,100
Sam Altman, the CEO of OpenAI is there apologizing for that and then you see in response a bunch

190
00:12:39,100 --> 00:12:43,500
of organizations basically say, oh, that's it, no chat GPT for us.

191
00:12:43,540 --> 00:12:47,940
So we're going to cut that off right now, which is sensible I think.

192
00:12:47,940 --> 00:12:52,460
But I mean, I don't think that that has to be the end of the story.

193
00:12:52,460 --> 00:12:59,220
Because really I think like we can describe this situation as kind of a dance between

194
00:12:59,220 --> 00:13:00,220
privacy and utility.

195
00:13:00,220 --> 00:13:05,460
If you have full privacy, you can't really use these models the way they are because

196
00:13:05,460 --> 00:13:11,660
you can't actually send sensitive data to them.

197
00:13:11,700 --> 00:13:17,460
I mean, okay, one option which I'll talk briefly about is self-deploying in LLM.

198
00:13:17,460 --> 00:13:21,340
It's hard because these models are very big, they don't fit on the machine and actually

199
00:13:21,340 --> 00:13:25,180
operational like standing it up and being able to hit it with requests is one thing

200
00:13:25,180 --> 00:13:31,340
but actually operationalizing it, maintaining it, etc., completely other thing.

201
00:13:31,340 --> 00:13:35,620
I'll also say that the reason that it's so cheap from OpenAI or Coheer is because of

202
00:13:35,620 --> 00:13:39,220
economies of scale and when you're self-deploying, you don't get that.

203
00:13:39,260 --> 00:13:43,060
So it's always going to be more expensive no matter what.

204
00:13:43,060 --> 00:13:49,540
Even if you do have the expertise and the budget, you're still always arguing with that

205
00:13:49,540 --> 00:13:52,540
price gap.

206
00:13:52,540 --> 00:13:57,140
But, and I mean, so that's like one, that's like, and also the open source models, the

207
00:13:57,140 --> 00:14:00,500
models that are available to us are just not as good as the ones that are being developed

208
00:14:00,500 --> 00:14:02,140
by these LLM providers, right?

209
00:14:02,140 --> 00:14:09,060
GPT-4 is generally used to evaluate open source models against GPT-3, meaning GPT-4 is like

210
00:14:09,460 --> 00:14:13,660
if GPT-4 can evaluate that, obviously it's clearly much stronger than any of these individual

211
00:14:13,660 --> 00:14:16,180
models.

212
00:14:16,180 --> 00:14:21,620
And so as a result, you're kind of going to lose utility no matter what by not using an

213
00:14:21,620 --> 00:14:25,460
LLM provider and the fact that you gain on privacy, let's say it's perfectly private

214
00:14:25,460 --> 00:14:32,540
because it's within your organization, it's kind of along with your compliance constraints,

215
00:14:32,540 --> 00:14:38,980
etc., in your organization, you're never going to reach that same level of utility and obviously

216
00:14:38,980 --> 00:14:40,820
the cost factors into utility as well.

217
00:14:40,820 --> 00:14:44,820
It's not just efficacy, it's also cost.

218
00:14:44,820 --> 00:14:50,540
And so you kind of have like complete privacy and like half utility or like no privacy and

219
00:14:50,540 --> 00:14:53,460
100% utility by going to an LLM provider.

220
00:14:53,460 --> 00:14:58,660
And the question is, can you do something in between there?

221
00:14:58,660 --> 00:15:03,060
And having worked on privacy for a while, I would say yes, we can.

222
00:15:03,060 --> 00:15:06,220
The other reason that I actually thought this metaphor was interesting was because to me

223
00:15:06,340 --> 00:15:12,220
at least, when you build systems that can solve for both this data privacy problem and

224
00:15:12,220 --> 00:15:15,820
without sacrificing utility, to me these are always very elegant solutions.

225
00:15:15,820 --> 00:15:20,140
It feels pretty elegant once you see it in action and you pull it off, knowing that the

226
00:15:20,140 --> 00:15:22,580
data is private.

227
00:15:22,580 --> 00:15:25,180
So that's why I thought the metaphor was cool.

228
00:15:25,180 --> 00:15:29,300
When I spit it back out to me, I was like, all right, I'm going to go with this.

229
00:15:29,300 --> 00:15:32,060
But before we can actually talk about how we would build a system like that, we have

230
00:15:32,100 --> 00:15:36,220
to talk about some of the primitives that are available to us to build privacy preserving

231
00:15:36,220 --> 00:15:38,500
technology.

232
00:15:38,500 --> 00:15:40,780
So the first one I'll talk about is confidential computing.

233
00:15:40,780 --> 00:15:47,380
This is kind of like an industry buzzword, but like the real core technology is called

234
00:15:47,380 --> 00:15:48,380
the secure enclave.

235
00:15:48,380 --> 00:15:54,820
The idea of it is it's just an isolated and hardened constrained computing environment

236
00:15:54,820 --> 00:15:56,860
where you run security critical applications.

237
00:15:56,860 --> 00:15:57,860
Your phone has this.

238
00:15:57,860 --> 00:15:59,740
You can run it on an iPhone.

239
00:15:59,740 --> 00:16:01,580
It's where they keep all of your biometric data.

240
00:16:01,580 --> 00:16:04,020
That biometric data can never leave the enclave.

241
00:16:04,020 --> 00:16:10,460
And they take it very seriously what services and programs can actually interact with that,

242
00:16:10,460 --> 00:16:13,860
with any data that's in the enclave.

243
00:16:13,860 --> 00:16:18,020
It's basically as close to a black box as a computer can get while still being somewhat

244
00:16:18,020 --> 00:16:19,020
useful.

245
00:16:19,020 --> 00:16:25,060
And the main feature of it that protects it is that the enclave has a separate CPU and

246
00:16:25,060 --> 00:16:28,860
memory from the host machine.

247
00:16:28,860 --> 00:16:32,180
And it's very like there are very strict rules about how the host machine can interact with

248
00:16:32,180 --> 00:16:35,020
that CPU and memory.

249
00:16:35,020 --> 00:16:41,980
One main feature of enclaves that allows you to use them securely is this idea of attestation.

250
00:16:41,980 --> 00:16:45,940
Essentially what it does is it allows enclaves to verify themselves and the code they're

251
00:16:45,940 --> 00:16:49,580
running to someone interacting with it from the outside world.

252
00:16:49,580 --> 00:16:53,700
So as a client, you can kind of make an attestation request to an enclave.

253
00:16:53,700 --> 00:16:57,460
The enclave can say here are all the boot statistics and measurements that I made as

254
00:16:57,460 --> 00:17:03,220
soon as I booted, as well as kind of like let's say a hash of the entire chain of software

255
00:17:03,220 --> 00:17:06,700
that enabled me to run some application code.

256
00:17:06,700 --> 00:17:09,400
And here's also a hash for the application code itself.

257
00:17:09,400 --> 00:17:13,780
So you can know A, that the device has not been tampered with at all, and then B, that

258
00:17:13,780 --> 00:17:17,180
you're running a particular application inside this box.

259
00:17:17,180 --> 00:17:21,740
So that at least if you know what that application is ahead of time, you can be reasonably sure

260
00:17:21,900 --> 00:17:25,460
that there are no other pieces of code in there that are splitting data off, shooting

261
00:17:25,460 --> 00:17:26,860
it somewhere else, et cetera.

262
00:17:26,860 --> 00:17:30,860
If you know the functionality of the application, then you can, after you've attested, you can

263
00:17:30,860 --> 00:17:36,260
be sure that any data you send in is only going to be used according to however that

264
00:17:36,260 --> 00:17:39,620
app has been developed.

265
00:17:39,620 --> 00:17:43,780
And one thing that you can get with this, these two primitives, is you can have, I mean,

266
00:17:43,780 --> 00:17:48,180
you could set this up to do more complicated services architecture, like microservices.

267
00:17:48,180 --> 00:17:51,220
You can have several services and enclaves that can only talk to each other, and they

268
00:17:51,220 --> 00:17:53,620
can be sure that they're talking to the right service.

269
00:17:53,620 --> 00:17:58,060
And as long as each service has a clear definition, you can do stuff that way.

270
00:17:58,060 --> 00:18:01,140
You can also do distributed computing, where they're all running the same program, and

271
00:18:01,140 --> 00:18:07,140
you're just trying to scale out and do stuff that way.

272
00:18:07,140 --> 00:18:14,140
So yeah, it's kind of like a building block to incorporate security-critical code into

273
00:18:14,140 --> 00:18:17,020
your larger applications or ecosystem or whatever.

274
00:18:17,020 --> 00:18:19,100
So that's the first primitive.

275
00:18:19,100 --> 00:18:24,700
The second primitive is PIR removal and text de-identification.

276
00:18:24,700 --> 00:18:29,700
This is basically just removing sensitive information from natural language documents.

277
00:18:29,700 --> 00:18:34,980
For example, names, places, organizations.

278
00:18:34,980 --> 00:18:39,180
There are a lot of other categories of text that you would consider private.

279
00:18:39,180 --> 00:18:44,820
Some of these can even be linked all the way up to HIPAA or GDPR requirements.

280
00:18:44,980 --> 00:18:47,860
There's essentially like a...

281
00:18:47,860 --> 00:18:50,620
If you have a list of things that you wanted to classify, you can train a machine learning

282
00:18:50,620 --> 00:18:56,780
model to identify those categories and redact the terms in the document.

283
00:18:56,780 --> 00:19:00,500
Essentially, what you do is it's named entity recognition, and then you do a masking step

284
00:19:00,500 --> 00:19:03,300
to redact the data.

285
00:19:03,300 --> 00:19:06,940
And so that's an example of what it looks like after you've redacted data.

286
00:19:06,940 --> 00:19:12,660
Obviously, there's problems with this, right?

287
00:19:12,700 --> 00:19:16,380
Obviously, it's based on machine learning, which means that there's always going to be

288
00:19:16,380 --> 00:19:23,020
some error bar, whether it's 10%, 1%, or 0.1%, like errors still matter and you need to plan

289
00:19:23,020 --> 00:19:26,140
for those.

290
00:19:26,140 --> 00:19:31,820
And the other idea is that this might remove sensitive terms in PII, but not all...

291
00:19:31,820 --> 00:19:35,980
Let me think about this.

292
00:19:35,980 --> 00:19:38,580
Redaction is not equivalent to privacy, right?

293
00:19:38,620 --> 00:19:43,820
You can have concepts be discussed in very plain language without sensitive terms, but

294
00:19:43,820 --> 00:19:48,380
still be leaking that concept after the text has been redacted, right?

295
00:19:48,380 --> 00:19:53,900
So it's not a perfect definition of privacy, but it is what it is.

296
00:19:53,900 --> 00:19:58,260
For a lot of businesses, this has been good enough in terms of privacy, but for some,

297
00:19:58,260 --> 00:19:59,260
it's not.

298
00:19:59,260 --> 00:20:01,300
So that's something to keep in mind.

299
00:20:01,300 --> 00:20:05,540
Okay, so I've talked about these primitives, and I wanted to talk a bit about how we built

300
00:20:05,580 --> 00:20:12,580
a system to try to take advantage of LLM providers like OpenAI while trying to give back some

301
00:20:12,580 --> 00:20:22,100
privacy, so it's not this 100 privacy, 50 utility versus 0 privacy, 100 utility tradeoff.

302
00:20:22,100 --> 00:20:27,580
Try to get some maybe better numbers in there for us to work with.

303
00:20:27,580 --> 00:20:29,620
So did you want to discuss this stuff?

304
00:20:29,620 --> 00:20:30,620
You're doing great.

305
00:20:30,620 --> 00:20:31,620
All right.

306
00:20:31,620 --> 00:20:40,620
Okay, so I'd like to show you a demo of what Jason's talking about.

307
00:20:40,620 --> 00:20:46,780
And so this came about through the popularity of LLM's, OpenAI in particular, and in the

308
00:20:46,780 --> 00:20:53,380
context of finding utility in a business environment, a company came to us and said, listen, we

309
00:20:53,380 --> 00:20:57,940
think this could be transformative, but the business we're in, there's a lot of sensitive

310
00:20:57,940 --> 00:21:00,660
data, proprietary data.

311
00:21:00,700 --> 00:21:04,100
So to set the stage here, that's what we're dealing with here.

312
00:21:04,100 --> 00:21:08,940
Let's say you own a business or a startup, and the customers you work with have information

313
00:21:08,940 --> 00:21:14,980
that they'd like to leverage OpenAI or a public LLM for, but of course you don't want to send

314
00:21:14,980 --> 00:21:15,980
it out.

315
00:21:15,980 --> 00:21:19,060
We heard about Samsung already.

316
00:21:19,060 --> 00:21:20,900
So that's the setup here.

317
00:21:20,900 --> 00:21:25,300
The idea of what we'll see here, in order to answer questions though, you need that context.

318
00:21:25,300 --> 00:21:30,620
So we will upload information that's sensitive, and then we'll ask some questions.

319
00:21:30,620 --> 00:21:33,020
And the idea is that we'll get answers.

320
00:21:33,020 --> 00:21:38,060
So nothing special except for everything Jason said, which is we'll do the redaction, we'll

321
00:21:38,060 --> 00:21:44,300
cleanse the data in a way that we can still use OpenAI and get that back.

322
00:21:44,300 --> 00:21:48,400
So we'll show you what that looks like, and then Jason will continue to go under the covers

323
00:21:48,400 --> 00:21:52,040
with how that's happening.

324
00:21:52,040 --> 00:21:53,980
So two things to point out here.

325
00:21:53,980 --> 00:21:58,320
I mentioned in the documents and the user queries, both have sensitive, potentially

326
00:21:59,320 --> 00:22:02,040
So we want to redact both.

327
00:22:02,040 --> 00:22:05,760
And of course we don't want to send any of that to the LLM.

328
00:22:05,760 --> 00:22:11,200
So those are the two challenges that we'll make sure we're addressing.

329
00:22:11,200 --> 00:22:14,560
Real quickly, this is a picture of what I just said, and to start from the left here,

330
00:22:14,560 --> 00:22:18,440
we're going to start with a question, and the question involves information that is

331
00:22:18,440 --> 00:22:22,520
sensitive or particular to our customers.

332
00:22:22,520 --> 00:22:28,800
The question, I'm sorry, that information is going to be encrypted so that even the

333
00:22:28,800 --> 00:22:32,520
cloud provider, CAPE, can't see that.

334
00:22:32,520 --> 00:22:36,600
Once it's redacted, we're going to send it out to Chad GPT.

335
00:22:36,600 --> 00:22:42,320
So the question along with that uploaded information, the additional context, are going to be sent

336
00:22:42,320 --> 00:22:43,680
across redacted.

337
00:22:43,680 --> 00:22:48,400
We're going to leverage the capabilities of the LLM to get an answer back relative to

338
00:22:48,400 --> 00:22:50,920
the redacted versions.

339
00:22:50,920 --> 00:22:57,800
Then we're going to re-identify the things that we de-identified so that the response

340
00:22:57,800 --> 00:23:01,120
is going to be useful to the caller.

341
00:23:01,120 --> 00:23:06,200
And that as well is going to be encrypted on the way out as well as sent over TLS.

342
00:23:06,200 --> 00:23:10,120
So this is sort of the system.

343
00:23:10,120 --> 00:23:13,120
Let's take a look.

344
00:23:13,120 --> 00:23:18,120
Here we go.

345
00:23:18,120 --> 00:23:19,720
Okay we have a chat pod here.

346
00:23:19,720 --> 00:23:23,080
Well, actually I have a question first before we jump into this.

347
00:23:23,080 --> 00:23:24,880
We're at Kaikon.

348
00:23:24,880 --> 00:23:29,920
Does anyone know who the creator of the Python language is?

349
00:23:29,920 --> 00:23:35,280
I heard Guido.

350
00:23:35,280 --> 00:23:36,280
Anyone else?

351
00:23:36,280 --> 00:23:41,600
Okay, I was going to use Guido, but I think it's actually Dutch.

352
00:23:41,600 --> 00:23:43,240
Hedo might be more appropriate.

353
00:23:43,240 --> 00:23:45,480
So I'll use Hedo.

354
00:23:45,480 --> 00:23:47,800
I wouldn't have anything any better.

355
00:23:47,800 --> 00:23:49,800
Thanks for not making me use the accent.

356
00:23:49,800 --> 00:23:50,800
Hedo.

357
00:23:50,800 --> 00:23:52,480
So let's use that as an example.

358
00:23:52,480 --> 00:23:53,640
That's our sensitive information.

359
00:23:53,640 --> 00:24:00,280
Let's pretend that's data security information, how we protect our web servers or who your

360
00:24:00,280 --> 00:24:01,600
clients are, financial information.

361
00:24:01,600 --> 00:24:06,620
It could be anything, but for fun we'll use Hedo.

362
00:24:06,620 --> 00:24:13,480
So the first thing that we'll do here is provide some context.

363
00:24:13,480 --> 00:24:19,760
So I'm uploading information and this could be PDFs, it could be sensitive financial documentation,

364
00:24:19,760 --> 00:24:24,440
whatever it is appropriate for the use case, but we're uploading this additional sensitive

365
00:24:24,440 --> 00:24:30,640
information.

366
00:24:30,640 --> 00:24:34,240
For those that understand what's going on, I'm going to change my index for the...

367
00:24:34,240 --> 00:24:38,080
Oh, I should have done that first.

368
00:24:38,080 --> 00:24:39,080
Sorry.

369
00:24:39,080 --> 00:24:40,080
Did that in the wrong order.

370
00:24:40,680 --> 00:24:45,880
I changed my index and then after I uploaded my data, so we have to re-upload so that we

371
00:24:45,880 --> 00:24:49,120
actually have it in our vector database.

372
00:24:49,120 --> 00:24:51,280
Okay, so that's done.

373
00:24:51,280 --> 00:24:56,240
Again, hand waving a little bit in the demo, but Jason's going to go through step by step,

374
00:24:56,240 --> 00:25:00,400
but the point here is that we've uploaded some sensitive information.

375
00:25:00,400 --> 00:25:03,360
Under the covers it was encrypted and sent across.

376
00:25:03,360 --> 00:25:09,680
Now let's ask some questions here.

377
00:25:09,680 --> 00:25:14,840
We'll start with where does Hedo currently live?

378
00:25:14,840 --> 00:25:18,120
This could be, let's say you're going for compliance...

379
00:25:18,120 --> 00:25:19,120
Oh, we got a question.

380
00:25:19,120 --> 00:25:20,120
Yep.

381
00:25:20,120 --> 00:25:29,120
We got to do over here first.

382
00:25:29,120 --> 00:25:31,360
Better?

383
00:25:31,360 --> 00:25:32,880
Okay.

384
00:25:32,880 --> 00:25:39,040
So again, we're using Hedo and some information about him, but maybe we're doing a question

385
00:25:39,040 --> 00:25:45,920
and answer about some regulatory compliance, a PCI or something, and we've uploaded a

386
00:25:45,920 --> 00:25:52,920
bunch of policy docs, knowledge-based information from a customer, and now you can interrogate

387
00:25:52,920 --> 00:25:57,040
if you're meeting the policies that are required for certification.

388
00:25:57,040 --> 00:26:00,720
But again, we're just using Hedo for fun, and we asked a question and it looks like

389
00:26:00,720 --> 00:26:02,560
we got an answer.

390
00:26:02,560 --> 00:26:04,240
That's great because that's what we expect.

391
00:26:04,240 --> 00:26:08,200
The idea is that it was privacy preserved and we'll talk about that in just a second,

392
00:26:08,200 --> 00:26:13,480
so let's just ask it a couple more questions for fun.

393
00:26:13,480 --> 00:26:19,920
If you don't know this, Hedo worked for Dropbox.

394
00:26:19,920 --> 00:26:26,200
So we'll ask when.

395
00:26:26,200 --> 00:26:30,640
So the power is not in these questions and answers, of course, it's that we can leverage

396
00:26:30,640 --> 00:26:37,280
OpenAI's LLM for getting us good answers, and it looks like it's working, so fantastic.

397
00:26:37,280 --> 00:26:39,360
I'm going to stop there because you get the point.

398
00:26:39,360 --> 00:26:43,520
We could continue to ask questions about whatever context we provided through the upload, and

399
00:26:43,520 --> 00:26:49,320
now we're in essence using chat GPT, just like you would with information that's not

400
00:26:49,320 --> 00:26:54,320
sensitive, but now you can with proprietary or sensitive information.

401
00:26:54,320 --> 00:26:55,320
Yes?

402
00:26:55,320 --> 00:27:12,680
Yeah, we provided context through the uploaded document, which has Hedo, and so it's going

403
00:27:12,680 --> 00:27:13,680
to overindex toward that.

404
00:27:13,680 --> 00:27:14,680
Go ahead, Jason.

405
00:27:14,680 --> 00:27:17,880
We basically just fed it his Wikipedia page.

406
00:27:17,880 --> 00:27:22,880
So it has essentially all these sentences about his life in context with his name.

407
00:27:23,680 --> 00:27:28,840
Now, what's interesting is actually this is what the model sees.

408
00:27:28,840 --> 00:27:31,360
This is what we pass to them.

409
00:27:31,360 --> 00:27:32,360
We don't actually...

410
00:27:32,360 --> 00:27:33,360
Oh, question.

411
00:27:33,360 --> 00:27:40,360
That's a good question.

412
00:27:40,360 --> 00:27:46,080
I actually haven't tried that, so I think it might...

413
00:27:46,080 --> 00:27:48,720
Oh, actually, no, it would not work.

414
00:27:48,720 --> 00:27:51,720
It would actually just say, sorry, I don't have the right context.

415
00:27:51,720 --> 00:27:55,880
I mean, if you really want...

416
00:27:55,880 --> 00:28:00,880
When did Gary work for Dropbox?

417
00:28:00,880 --> 00:28:03,880
Yeah, I think it'll basically just error.

418
00:28:03,880 --> 00:28:09,880
It'll just say there's not enough information in this context to answer the question, I

419
00:28:09,880 --> 00:28:10,880
hope.

420
00:28:10,880 --> 00:28:11,880
Oh, no.

421
00:28:11,880 --> 00:28:16,880
So do you want a job?

422
00:28:16,880 --> 00:28:17,880
No more questions.

423
00:28:17,880 --> 00:28:18,880
Yeah, don't...

424
00:28:19,040 --> 00:28:22,040
Okay, so...

425
00:28:22,040 --> 00:28:25,040
Just joking, by the way.

426
00:28:25,040 --> 00:28:28,040
We have questions.

427
00:28:28,040 --> 00:28:29,040
Yeah.

428
00:28:29,040 --> 00:28:34,040
So, yeah, obviously, this is very new for us.

429
00:28:34,040 --> 00:28:39,040
We've built machine learning tooling for a long time, but using a machine learning model

430
00:28:39,040 --> 00:28:44,040
provider is pretty alien to us because that's always been very counterintuitive for us in

431
00:28:44,040 --> 00:28:45,040
terms of privacy.

432
00:28:45,200 --> 00:28:51,200
I think the economic argument, or just at least for now, LLMs are kind of stuck in these

433
00:28:51,200 --> 00:29:00,200
providers, so unless they get cheaper to deploy significantly or the technology just becomes

434
00:29:00,200 --> 00:29:05,560
more widely available, I think this is one of the best solutions that we've come up with

435
00:29:05,560 --> 00:29:08,560
outside of deploying for yourself.

436
00:29:09,080 --> 00:29:15,080
Anyways, I wanted to dive in a little bit about how the system is designed because I

437
00:29:15,080 --> 00:29:18,080
think there's some interesting things about how we do this.

438
00:29:18,080 --> 00:29:24,080
I mean, our two goals here are, A, that OpenAI doesn't see sensitive information, but also,

439
00:29:24,080 --> 00:29:29,080
B, because we're deploying this ourselves, we also don't want to ever be able to see

440
00:29:29,080 --> 00:29:31,080
anybody's sensitive data.

441
00:29:31,080 --> 00:29:38,080
So, essentially, we want not only a secure channel from the client to OpenAI where we

442
00:29:38,600 --> 00:29:42,600
can't snoop on any of the intermediate results or intermediate values.

443
00:29:42,600 --> 00:29:47,600
We also, we want it not only to be a secure channel, but also to be like a redacted channel

444
00:29:47,600 --> 00:29:51,600
as well, where the text gets redacted out of the way, while also still giving relevant

445
00:29:51,600 --> 00:29:52,600
context.

446
00:29:52,600 --> 00:29:58,600
And we do this through essentially three, this is kind of simplified, but for the diagram

447
00:29:58,600 --> 00:30:01,600
it works, essentially three services.

448
00:30:02,120 --> 00:30:09,120
So, the way we make sure that this is all secure is we first start with the setup phase.

449
00:30:09,120 --> 00:30:10,620
Two things are happening here.

450
00:30:10,620 --> 00:30:15,620
One is that, oh, I should say, we have three services, database service, which is kind

451
00:30:15,620 --> 00:30:20,620
of a normal, just database layer, which includes a vector store database, a server, which is

452
00:30:20,620 --> 00:30:24,620
just like your general fast API web server.

453
00:30:24,620 --> 00:30:30,620
However, it's inside of an enclave, so you have some additional authentication abilities

454
00:30:31,140 --> 00:30:35,140
with that, like that attestation I was talking about.

455
00:30:35,140 --> 00:30:36,640
And then we have a de-identification service.

456
00:30:36,640 --> 00:30:41,140
This is another server running a machine learning model, waiting for prediction requests.

457
00:30:41,140 --> 00:30:43,140
It's also running in an enclave.

458
00:30:43,140 --> 00:30:47,140
What this allows is for these two enclaves to do mutual attestation, meaning that they

459
00:30:47,140 --> 00:30:54,140
know each other, more or less, and that data between them can be sent while knowing that

460
00:30:54,660 --> 00:31:00,660
no outside, nobody's going to be able to snoop on that data, just based on, because they

461
00:31:00,660 --> 00:31:07,660
both know that they're enclaves and they both know that all of the software is in expected

462
00:31:08,660 --> 00:31:11,660
bounds, I guess.

463
00:31:11,660 --> 00:31:17,660
And yeah, so the client can, what this allows is essentially any data that the client sends.

464
00:31:17,660 --> 00:31:22,460
The client can be sure that the server is not going to leak that data to anybody else,

465
00:31:22,780 --> 00:31:23,780
and that's not going to be a problem.

466
00:31:23,780 --> 00:31:26,660
So, you can set up for the de-identification service, but because those are mutually attested,

467
00:31:26,660 --> 00:31:31,940
you can essentially assume that the client has attested to the third service as well.

468
00:31:31,940 --> 00:31:36,500
So the client can know that as long as data is moving through those two services, it's

469
00:31:36,500 --> 00:31:40,940
in a box, nobody can touch it.

470
00:31:40,940 --> 00:31:44,540
So this setup phase is just doing that mutual attestation, and then the client attesting

471
00:31:44,540 --> 00:31:45,940
to the server.

472
00:31:45,940 --> 00:31:51,620
As part of that, they get a public key, which they're going to use to encrypt their data.

473
00:31:51,780 --> 00:31:57,260
So this is an example of a client uploading data to the knowledge base.

474
00:31:57,260 --> 00:32:03,340
They want to save this document, they encrypt it, they send it to the server.

475
00:32:03,340 --> 00:32:06,980
The server, actually, before it does anything, it's going to decrypt this data, it's in a

476
00:32:06,980 --> 00:32:10,460
black box, remember, so we can still kind of consider this to be encrypted.

477
00:32:10,460 --> 00:32:17,460
It's going to decrypt it, it's going to create embeddings, and it's going to take that document

478
00:32:18,300 --> 00:32:23,140
and the embedding, re-encrypt it, and then insert it into the database service.

479
00:32:23,140 --> 00:32:26,500
So at this point, this is a normal database, because all the data inside of it is essentially

480
00:32:26,500 --> 00:32:32,860
AES encrypted, and the only instance of service with that key is going to be the server.

481
00:32:32,860 --> 00:32:37,540
So the only way that this data is ever useful to anyone is if this server inside of an enclave

482
00:32:37,540 --> 00:32:40,980
pulls it back, it has the AES key and decrypt it only inside the enclave.

483
00:32:40,980 --> 00:32:43,900
So this kind of just gives you some external storage to work with.

484
00:32:43,900 --> 00:32:47,340
In theory, we could put this in the server as well, but in practice, it's just easier

485
00:32:47,340 --> 00:32:50,580
to do this, I would say.

486
00:32:50,580 --> 00:32:55,100
So let's say we've uploaded a few documents, they're all encrypted in that database service.

487
00:32:55,100 --> 00:32:57,620
Now that's how we populate the knowledge base.

488
00:32:57,620 --> 00:33:02,300
How do we actually satisfy a user query?

489
00:33:02,300 --> 00:33:08,420
For example, let's say that this is some data about your user base, and you want to ask

490
00:33:08,420 --> 00:33:10,420
how many of your users are being billed through Stripe.

491
00:33:11,060 --> 00:33:17,940
First thing you do is encrypt that prompt, that request, send it to the server.

492
00:33:17,940 --> 00:33:23,780
It gets decrypted, and then using the vector store in this database service, you're able

493
00:33:23,780 --> 00:33:29,860
to essentially get relevant documents from the knowledge base, because essentially what

494
00:33:29,860 --> 00:33:35,860
you do is you embed the prompt in this database as a bunch of documents with their embeddings.

495
00:33:35,860 --> 00:33:37,260
You do some math between those embeddings.

496
00:33:37,260 --> 00:33:42,780
By embedding, I mean it's essentially just another term for a dense vector that semantically

497
00:33:42,780 --> 00:33:46,780
represents the text that you are talking about.

498
00:33:46,780 --> 00:33:49,420
You can do operations with those embeddings.

499
00:33:49,420 --> 00:33:54,580
You can add them together, and it preserves the semantic of this plus this, or you can

500
00:33:54,580 --> 00:34:00,460
subtract and say I want a zebra without stripes, and you get a horse, for example.

501
00:34:00,460 --> 00:34:02,460
That's what an embedding is.

502
00:34:03,180 --> 00:34:09,500
It allows you to just operate in the context of dense vectors instead of discrete text

503
00:34:09,500 --> 00:34:11,500
space.

504
00:34:32,460 --> 00:34:50,540
So if the attacker got access to the embedding, but not the raw text, and they also had access

505
00:34:50,540 --> 00:34:54,780
to the model that was used to generate the embedding, then yeah, they definitely could

506
00:34:54,780 --> 00:34:55,780
do that.

507
00:34:56,100 --> 00:35:04,580
For this system at least, anything in these boxes is considered kind of secure.

508
00:35:04,580 --> 00:35:10,100
So if they were to get these embeddings, we'd have a bigger problem than that, I think.

509
00:35:10,100 --> 00:35:15,620
But yeah, in general, embeddings are reversible.

510
00:35:15,620 --> 00:35:18,340
Well I guess it depends on how you do the embedding, but that's a little bit beyond

511
00:35:18,340 --> 00:35:19,340
the scope of this, I think.

512
00:35:19,340 --> 00:35:23,580
At least for us, so for example, you can use OpenAI or Coheer to generate embeddings as

513
00:35:23,580 --> 00:35:25,540
well, but you have to send the text to them.

514
00:35:25,540 --> 00:35:31,540
So for us, we use our own model inside of the server to do the embedding with the vector

515
00:35:31,540 --> 00:35:32,540
database.

516
00:35:32,540 --> 00:35:36,980
You can still, because you can use your own embeddings with the vector database, do all

517
00:35:36,980 --> 00:35:41,780
this context retrieval stuff, and then we're good to go.

518
00:35:41,780 --> 00:35:44,020
That's actually, that's how we do this.

519
00:35:44,020 --> 00:35:52,580
And what we have here at the end of it is a prompt, relevant documents, sorry, a query,

520
00:35:52,580 --> 00:35:54,700
user query, relevant documents.

521
00:35:54,700 --> 00:35:59,460
We stuff them together, that forms the prompt, the prompt that we want to send to OpenAI.

522
00:35:59,460 --> 00:36:03,380
However, the sensitive information could still be in this prompt.

523
00:36:03,380 --> 00:36:06,420
So this is the part where we do the actual redaction.

524
00:36:06,420 --> 00:36:11,020
We send it to our model, it does the redaction.

525
00:36:11,020 --> 00:36:14,700
For example here, Stripe was redacted out in the prompt.

526
00:36:14,700 --> 00:36:17,500
You could also imagine in the document, there's a bunch of sensitive terms that would also

527
00:36:17,500 --> 00:36:22,460
be redacted.

528
00:36:22,460 --> 00:36:27,140
The reason we do all this at once, the documents haven't been redacted yet, the prompt hasn't

529
00:36:27,140 --> 00:36:32,900
been redacted, it's because we want the redaction to be consistent across documents and prompts.

530
00:36:32,900 --> 00:36:37,300
And if we were to do all the redaction up front on the documents, we wouldn't necessarily

531
00:36:37,300 --> 00:36:44,500
be able to recognize sensitive terms in queries the same way.

532
00:36:44,500 --> 00:36:48,980
But yeah, once that's been done, it can go back to the main server.

533
00:36:49,660 --> 00:36:54,980
At this point we can actually send it to OpenAI because it's been, in theory, redacted.

534
00:36:54,980 --> 00:36:59,620
We get some answer back, for example, 10,000 users are billed by organization.

535
00:36:59,620 --> 00:37:05,540
And because the server, so as part of this de-identification service, you also get back

536
00:37:05,540 --> 00:37:08,540
a kind of the mapping that you used to do the redaction.

537
00:37:08,540 --> 00:37:13,660
So it'll just show like, let's say, in this case, organizations mapped to Stripe.

538
00:37:13,660 --> 00:37:17,420
So when you get the response back from the LLM, it should be referencing this organization

539
00:37:17,420 --> 00:37:22,260
mask, and then we can undo that mask with the mapping that we saved from the de-identification

540
00:37:22,260 --> 00:37:26,620
service to say, OK, it's Stripe in the response.

541
00:37:26,620 --> 00:37:29,220
At that point, we're ready to return it back to the user.

542
00:37:29,220 --> 00:37:31,540
And we do that over TLS.

543
00:37:31,540 --> 00:37:38,420
The idea here is that the data, any sense of data that the user supplies is always locked

544
00:37:38,420 --> 00:37:40,620
to the point where we cannot see it as CAPE.

545
00:37:40,620 --> 00:37:45,220
We're just acting as an operator for these services.

546
00:37:45,260 --> 00:37:50,140
And we're kind of acting as like a de-identifying proxy between them and OpenAI or any other

547
00:37:50,140 --> 00:37:51,140
LLM provider.

548
00:37:51,140 --> 00:37:52,980
This is not specific to OpenAI at all.

549
00:37:52,980 --> 00:37:58,380
It could be coherent or I actually don't know if there are any others besides those two.

550
00:37:58,380 --> 00:38:01,980
Those are the only ones I know about.

551
00:38:01,980 --> 00:38:08,260
OK, so what's cool about this, in my opinion, is we, the service provider, can host the

552
00:38:08,260 --> 00:38:15,020
system without deploying an entire LLM as long as those secure enclaves are accessible.

553
00:38:15,020 --> 00:38:18,260
There's definitely a learning curve when it comes to working with these enclaves, but

554
00:38:18,260 --> 00:38:21,700
it's not so bad that we can't do it.

555
00:38:21,700 --> 00:38:25,620
It's also a lot cheaper than hosting your own LLM.

556
00:38:25,620 --> 00:38:30,140
Yeah, the fact that these documents and prompts get end-to-end encrypted so that we never

557
00:38:30,140 --> 00:38:32,140
see it, I think, is pretty good.

558
00:38:32,140 --> 00:38:37,780
I would say that's a plus from the zero privacy we had to like, I don't know, 60, 70, 80,

559
00:38:37,780 --> 00:38:38,780
let's say, hopefully.

560
00:38:39,780 --> 00:38:46,540
I mean, in practice, the de-identification model that we use is actually very, very accurate.

561
00:38:46,540 --> 00:38:49,940
It's above 99% accurate.

562
00:38:49,940 --> 00:38:54,340
There will be a small number of cases where we miss and we actually feed data.

563
00:38:54,340 --> 00:38:58,300
There will be a sensitive term in there that wasn't caught, but for the most part, we get

564
00:38:58,300 --> 00:38:59,300
a lot of them.

565
00:38:59,300 --> 00:39:05,020
And yeah, I would say this improves the privacy of the overall system.

566
00:39:05,100 --> 00:39:10,500
However, so the question I guess that remains is like, oh, sorry, and the LLM providers

567
00:39:10,500 --> 00:39:13,140
can still be used for their economies of scale, which is kind of the nice thing about this

568
00:39:13,140 --> 00:39:17,020
is that you get a lot of cost-benefit.

569
00:39:17,020 --> 00:39:18,020
Yes?

570
00:39:35,020 --> 00:39:40,020
Yeah.

571
00:39:40,020 --> 00:39:49,620
We don't have data on it because we have yet to like do a full comprehensive evaluation.

572
00:39:49,620 --> 00:39:53,380
I think also in the context of LLMs, evaluation is really hard.

573
00:39:53,380 --> 00:39:56,100
I think like it's an open question how you evaluate those.

574
00:39:56,100 --> 00:39:59,980
I will say that for that model, we're using something that's as close to the state of

575
00:40:00,060 --> 00:40:06,820
the art as we could find, or that we could build, or that we could just use.

576
00:40:06,820 --> 00:40:09,420
And I'm not sure how it compares to humans.

577
00:40:09,420 --> 00:40:11,780
I would have to look into that.

578
00:40:11,780 --> 00:40:15,620
But yeah, so these are all the benefits.

579
00:40:15,620 --> 00:40:16,620
I think it's...

580
00:40:16,620 --> 00:40:17,620
You have a question?

581
00:40:17,620 --> 00:40:18,620
Yeah.

582
00:40:18,620 --> 00:40:27,620
So is it useful to use on a digitalized model to do the retraction to catch things like

583
00:40:27,620 --> 00:40:34,620
the news, for example, CBS News got the incentive to dispatch.

584
00:40:34,620 --> 00:40:43,620
We were taken for a ride in the context of the pulp-do journalism on the side of the

585
00:40:43,620 --> 00:40:57,500
world.

586
00:40:57,500 --> 00:41:00,320
So the LLM isn't actually being used for redaction at all.

587
00:41:00,320 --> 00:41:02,180
It's a separate model that does that.

588
00:41:02,180 --> 00:41:05,220
You could fine tune an LLM to do that.

589
00:41:05,220 --> 00:41:06,880
We have not had a chance to do that.

590
00:41:06,880 --> 00:41:08,900
But that would be possible.

591
00:41:09,500 --> 00:41:14,460
It's possible that you could improve the efficacy of the model that way.

592
00:41:14,460 --> 00:41:15,460
I think idioms...

593
00:41:15,460 --> 00:41:16,460
Yeah.

594
00:41:16,460 --> 00:41:20,980
Maybe another example of this is if you were, let's say, redacting information about Warren

595
00:41:20,980 --> 00:41:23,100
Buffett, and I think he has a nickname.

596
00:41:23,100 --> 00:41:26,500
I don't remember what it is, but it's like the something of something.

597
00:41:26,500 --> 00:41:27,500
I don't know.

598
00:41:27,500 --> 00:41:28,500
Do you know?

599
00:41:28,500 --> 00:41:29,500
Oracle of Omaha.

600
00:41:29,500 --> 00:41:30,500
Yes.

601
00:41:30,500 --> 00:41:31,500
Thank you.

602
00:41:31,500 --> 00:41:35,140
So if you were to pass this through the redaction system, it would probably say Warren Buffett,

603
00:41:35,140 --> 00:41:36,140
and that would be redacted.

604
00:41:36,140 --> 00:41:39,060
And then it would be like Oracle of Omaha, and it would be like Oracle of Location.

605
00:41:39,060 --> 00:41:42,860
But then you're like, okay, who...

606
00:41:42,860 --> 00:41:43,860
You know?

607
00:41:43,860 --> 00:41:48,660
That would probably be a failure on our part, even though the model worked the way it was

608
00:41:48,660 --> 00:41:49,660
supposed to.

609
00:41:49,660 --> 00:41:54,460
So yeah, I think ultimately rewriting text to remove sensitive information is probably

610
00:41:54,460 --> 00:41:57,060
a better way of doing this.

611
00:41:57,060 --> 00:41:58,060
But yeah, we haven't tried it.

612
00:41:58,060 --> 00:41:59,060
So...

613
00:41:59,060 --> 00:42:00,060
Yes?

614
00:42:30,740 --> 00:42:34,980
Yeah, that's an interesting idea, because I'm sure that, for example, GPT-4, if you

615
00:42:34,980 --> 00:42:40,740
gave it that Oracle of Omaha example, it might be able to actually unredact the text on its

616
00:42:40,740 --> 00:42:41,740
own.

617
00:42:41,740 --> 00:42:45,740
Yeah, that's another kind of attack vector that I hadn't thought of yet.

618
00:42:45,740 --> 00:42:46,740
Yeah.

619
00:42:59,700 --> 00:43:03,340
I have not, but honestly, that is probably the next thing I'm going to do next week.

620
00:43:03,340 --> 00:43:06,340
So thanks.

621
00:43:06,340 --> 00:43:08,340
Sure.

622
00:43:15,740 --> 00:43:22,580
So we are not the...

623
00:43:22,580 --> 00:43:27,420
While we can operate the enclaves, the idea of an enclave is that the operator can't see

624
00:43:27,420 --> 00:43:28,660
what's inside of it.

625
00:43:28,660 --> 00:43:30,620
That's like the whole reason it exists.

626
00:43:30,620 --> 00:43:31,940
You're like, it's...

627
00:43:31,940 --> 00:43:34,220
I mean, technically the security model is the host instance.

628
00:43:34,220 --> 00:43:39,300
The host instance cannot view, like, look at anything inside of the enclave.

629
00:43:39,300 --> 00:43:45,100
Here in the diagrams, I have these bars, which is kind of meant to say like the outer square

630
00:43:45,100 --> 00:43:49,540
is the host instance and like the inner square is the enclave.

631
00:43:49,540 --> 00:43:54,040
And really we don't use the host instance for anything except as a proxy.

632
00:43:54,040 --> 00:43:59,540
So when you send a request to the server, technically you're sending a request to a

633
00:43:59,540 --> 00:44:00,720
proxy.

634
00:44:00,720 --> 00:44:03,540
That request is encrypted, so it can't snoop on the data.

635
00:44:03,540 --> 00:44:10,100
Once it makes its way into the enclave, that's when you're free to decrypt it, because when

636
00:44:10,100 --> 00:44:12,980
you're inside that enclave, the host can't actually access it.

637
00:44:12,980 --> 00:44:18,580
Because the CPU memory are isolated, the CPU and memory of the host machine doesn't have

638
00:44:18,580 --> 00:44:23,380
access to that memory or that CPU processing.

639
00:44:23,380 --> 00:44:26,180
So yeah, that's how you generally keep...

640
00:44:26,180 --> 00:44:28,260
That's how enclaves work.

641
00:44:28,260 --> 00:44:32,980
So I mean, the real question for enclaves is like, how strong is that separation?

642
00:44:32,980 --> 00:44:37,920
Like, yeah, you can say it's like error gapped, but that doesn't necessarily mean that it's

643
00:44:37,920 --> 00:44:38,920
perfectly secure.

644
00:44:38,920 --> 00:44:41,980
In fact, enclaves in the past have been broken.

645
00:44:41,980 --> 00:44:46,940
However, the cost of pulling off an attack like that is extremely high.

646
00:44:46,940 --> 00:44:49,020
So yeah, there's that little end of it as well.

647
00:44:49,020 --> 00:44:51,660
But I mean, I would say for...

648
00:44:51,660 --> 00:44:56,700
As long as you're not worried about nation state level actors hacking these systems,

649
00:44:56,700 --> 00:45:00,660
it's generally a pretty secure way of processing in an encrypted fashion.

650
00:45:00,660 --> 00:45:02,620
There are other techniques for doing this.

651
00:45:02,620 --> 00:45:07,260
Actually I was going to go to the disadvantages section and talk about some of this stuff.

652
00:45:07,260 --> 00:45:08,980
Oh, maybe it's the next section.

653
00:45:08,980 --> 00:45:10,940
Well, anyways, there are other technologies.

654
00:45:10,940 --> 00:45:12,540
Formorph encryption is one.

655
00:45:12,540 --> 00:45:13,980
Multi-party computation is another.

656
00:45:13,980 --> 00:45:18,460
These are more software-based cryptographic approaches.

657
00:45:18,460 --> 00:45:25,460
Yeah, you can say that they have different trade-offs, different levels of security,

658
00:45:25,460 --> 00:45:27,780
different performance levels.

659
00:45:27,780 --> 00:45:31,460
And with enclaves, you can just run any software in them, whereas with each of those, you have

660
00:45:31,460 --> 00:45:35,820
to rewrite the software from scratch in a compiler or a framework that allows you to

661
00:45:35,820 --> 00:45:36,820
use those tools.

662
00:45:36,820 --> 00:45:40,300
That's what we worked on at the company for the first several years.

663
00:45:40,300 --> 00:45:45,740
So I'm happy to talk about that after.

664
00:45:45,740 --> 00:45:47,180
So we talked about the benefits.

665
00:45:47,180 --> 00:45:53,540
I would say here are some disadvantages of the system as we see them.

666
00:45:53,540 --> 00:45:59,860
So one issue that we found is that our de-identification model, while it's good at what it does, it

667
00:45:59,860 --> 00:46:01,160
lacks more context.

668
00:46:01,160 --> 00:46:07,180
So it will not do entity co-reference resolution, which is an NLP term for...

669
00:46:07,220 --> 00:46:10,980
I can't recognize that Warren Edward Buffett and Warren and Buffett and Warren Buffett

670
00:46:10,980 --> 00:46:14,140
are all the same person.

671
00:46:14,140 --> 00:46:18,940
So this is an NLP problem, which is almost as hard as any other NLP problem, maybe except

672
00:46:18,940 --> 00:46:19,940
for language modeling.

673
00:46:19,940 --> 00:46:22,940
But it is a hard one to solve.

674
00:46:22,940 --> 00:46:29,660
We have some basic ideas how we do this, but in general, I think it's like you could argue

675
00:46:29,660 --> 00:46:35,740
that you need an LLM to do this step for you as well, which is kind of like a chicken and

676
00:46:35,820 --> 00:46:36,820
chicken and egg situation.

677
00:46:36,820 --> 00:46:37,820
I don't think so completely.

678
00:46:37,820 --> 00:46:42,740
I think there are solutions here that are much easier, but I will say it's not a very

679
00:46:42,740 --> 00:46:45,540
one-step solution.

680
00:46:45,540 --> 00:46:49,020
And then, yeah, the two things I mentioned before, which is that accuracy, even though

681
00:46:49,020 --> 00:46:51,140
it's above 99%, is still a problem.

682
00:46:51,140 --> 00:46:55,020
You still have to plan for the fact that you're going to have errors here, and customers have

683
00:46:55,020 --> 00:46:59,180
to know that and opt into that.

684
00:46:59,180 --> 00:47:01,980
And then de-identification isn't equivalent to privacy.

685
00:47:01,980 --> 00:47:08,420
So yeah, differential privacy is the best formal way we have of describing privacy,

686
00:47:08,420 --> 00:47:13,060
but it's not clear how that would fit into this kind of a system at all.

687
00:47:13,060 --> 00:47:15,820
So that's that.

688
00:47:15,820 --> 00:47:21,260
Some alternatives we considered are, I mentioned, homework for encryption and multi-party computation.

689
00:47:21,260 --> 00:47:26,900
These are two other technologies for doing secure processing of data.

690
00:47:26,900 --> 00:47:30,780
Like I said, enclaves are the fastest, most general purpose, and most cost-effective solution

691
00:47:31,300 --> 00:47:35,780
for this problem, which is why we use them.

692
00:47:35,780 --> 00:47:42,140
So actually, another kind of fun fact is that the NVIDIA H100, which is what OpenAI trains

693
00:47:42,140 --> 00:47:48,820
GPT-4 on and what they use for inference as well, likely, no one really knows, but they

694
00:47:48,820 --> 00:47:51,580
are actually able to do confidential computing on their own.

695
00:47:51,580 --> 00:47:57,740
They have enclaves as part of their offering.

696
00:47:57,740 --> 00:48:00,180
And I'm guessing that OpenAI takes advantage of that.

697
00:48:00,220 --> 00:48:03,820
The fact is that they haven't actually exposed any of that functionality to users, though,

698
00:48:03,820 --> 00:48:08,860
which still means that there's this issue of how can we trust OpenAI that they're keeping

699
00:48:08,860 --> 00:48:09,860
data secure?

700
00:48:09,860 --> 00:48:10,860
Like yeah, sure.

701
00:48:10,860 --> 00:48:16,500
Because I mean, from their perspective, what's probably most valuable to protect is the model

702
00:48:16,500 --> 00:48:17,980
itself.

703
00:48:17,980 --> 00:48:23,260
So yeah, they may do inference in an enclave, but the data up until the point where it hits

704
00:48:23,260 --> 00:48:26,600
the model is probably still in kind of normal servers.

705
00:48:26,600 --> 00:48:28,340
So I'm sure they have access to that data.

706
00:48:28,580 --> 00:48:32,100
I mean, they definitely would because they want to be able to improve on their mistakes, right?

707
00:48:32,100 --> 00:48:35,860
But yeah, I mean, this is all just guesswork because we don't really know what the provider's

708
00:48:35,860 --> 00:48:36,860
ever doing.

709
00:48:38,060 --> 00:48:41,740
But yeah, it would be much more straightforward if they just exposed some of those features

710
00:48:41,740 --> 00:48:42,740
to us.

711
00:48:42,740 --> 00:48:44,100
That way we wouldn't even have to be there.

712
00:48:44,100 --> 00:48:47,860
We could just use those features of the LLM and build cool stuff on top of it.

713
00:48:50,380 --> 00:48:54,020
And yeah, in the future, this kind of system might not be necessary as well because of

714
00:48:54,020 --> 00:48:57,220
the cost issue might not be as strong as it is today.

715
00:48:58,500 --> 00:49:01,220
Although the economies of scale argument, I think always will apply.

716
00:49:02,980 --> 00:49:04,980
So yeah, that's it.

717
00:49:04,980 --> 00:49:06,580
Maybe as a quick recap.

718
00:49:08,460 --> 00:49:12,740
Currently, data privacy prevents enterprises from using LLM providers.

719
00:49:14,060 --> 00:49:16,580
Self-deploying LLMs can solve this, but there are other issues there.

720
00:49:19,060 --> 00:49:24,140
Primitives for privacy or computing can be used to improve this trade off, along with a

721
00:49:24,140 --> 00:49:25,500
lot of other cool things, honestly.

722
00:49:25,820 --> 00:49:31,260
This is one application, but we have done a lot of other cool things with these privacy

723
00:49:31,260 --> 00:49:32,260
preserving technologies.

724
00:49:32,580 --> 00:49:36,180
And hopefully you learned about it today and are interested as well.

725
00:49:37,020 --> 00:49:41,460
But yeah, if you're interested in any other applications of this kind of technology or

726
00:49:41,580 --> 00:49:44,660
if you want to play with the demo a little bit, we'll be at booth for 11.

727
00:49:44,660 --> 00:49:45,740
So please come say hi.

728
00:49:46,140 --> 00:49:51,020
Happy to talk more about this stuff there or now if we have some time.

729
00:49:51,300 --> 00:49:53,900
Yeah. So thanks.

