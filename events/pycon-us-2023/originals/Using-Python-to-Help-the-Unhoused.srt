1
00:00:00,000 --> 00:00:05,000
Welcome to the final talk of Python 2023.

2
00:00:05,000 --> 00:00:07,000
You have made it. Congratulations.

3
00:00:07,000 --> 00:00:10,000
Yeah, that's better. A round of applause.

4
00:00:10,000 --> 00:00:12,000
Yeah. Okay.

5
00:00:12,000 --> 00:00:19,000
So our final presentation today will be using Python to help the unhoused by Josh Weisbach and Sheila Flood.

6
00:00:19,000 --> 00:00:25,000
They just asked that we do a Q&A for them after the session to catch them outside in the hallway.

7
00:00:25,000 --> 00:00:28,000
This is going to run pretty close to the end of the time here.

8
00:00:28,000 --> 00:00:35,000
So without any further ado, please join me in giving a round of applause to our two speakers for the final talk for Python 2023.

9
00:00:35,000 --> 00:00:43,000
Good afternoon, everyone. Thanks for joining us for your last talk here at Python.

10
00:00:43,000 --> 00:00:50,000
So yeah, a favor to talk about how to use Python to support an NGO to help the unhoused in Victoria, BC.

11
00:00:50,000 --> 00:00:54,000
Quick introductions. I'm Josh Weisbach. I've been using Python since you had a six.

12
00:00:54,000 --> 00:01:00,000
I'm currently living in Ottawa, Ontario, Canada. I'm actually from Victoria, so this is a hit post to home for me.

13
00:01:00,000 --> 00:01:05,000
Right now I work within the Canadian Federal Government doing data science.

14
00:01:05,000 --> 00:01:09,000
I also have a PhD this fall at Cross University in data science.

15
00:01:09,000 --> 00:01:14,000
And with me on my team, or for this team for this project, is my shield, Sheila Flood.

16
00:01:14,000 --> 00:01:19,000
Hi everyone. My name is Sheila Flood. I'm a data scientist and sort of cross-medialist.

17
00:01:19,000 --> 00:01:25,000
I have a master's in cross statistics and a background in risk analytics and strategy.

18
00:01:25,000 --> 00:01:29,000
Let's see. Let's get into it.

19
00:01:29,000 --> 00:01:35,000
So currently there are over 1,500 people experiencing homelessness in Victoria, BC.

20
00:01:35,000 --> 00:01:40,000
To kick our perspective, that is the largest percent of population anywhere in Canada.

21
00:01:40,000 --> 00:01:46,000
The second and third largest cities are Vancouver, who have twice the many people experiencing homelessness,

22
00:01:46,000 --> 00:01:52,000
with a population seven times larger, and Toronto with a population 30 few times larger,

23
00:01:52,000 --> 00:01:56,000
but seven times as many people as three times less.

24
00:01:56,000 --> 00:02:01,000
So this is a big issue within the city of Victoria, BC.

25
00:02:01,000 --> 00:02:08,000
So those of you who aren't familiar with where Victoria is, it's the capital of British Columbia from the west coast of Canada.

26
00:02:08,000 --> 00:02:14,000
It's on the southern tip of Vancouver Island, which we're going to stick with now,

27
00:02:14,000 --> 00:02:17,000
which is the main city of Vancouver.

28
00:02:17,000 --> 00:02:21,000
And for this talk, we need to reference Victoria. We're talking about the capital region district,

29
00:02:21,000 --> 00:02:25,000
which is actually 13 different cities in the southern tip of Vancouver Island.

30
00:02:25,000 --> 00:02:31,000
The city of Victoria, plus 12 other cities, such as New Toson, Duke, Esquimalt, Langford.

31
00:02:31,000 --> 00:02:38,000
But when we say Victoria, we're talking about all of them combined into one area.

32
00:02:38,000 --> 00:02:46,000
So trying to help the people experiencing homelessness in Victoria is an NGO called the Greater Victoria Coalition on Homelessness.

33
00:02:46,000 --> 00:02:48,000
This is what they're known plus for, this project.

34
00:02:48,000 --> 00:02:53,000
They actually just presented themselves to the Alliance to End Homelessness in the capital region.

35
00:02:53,000 --> 00:02:59,000
They were paid back in 2007, when the mayor of the day started a task force to address funding solutions

36
00:02:59,000 --> 00:03:03,000
to help people experience homelessness in the community.

37
00:03:03,000 --> 00:03:10,000
This evolved into what became the NGO, and there's a group of people in the community trying to find a cage of institutions

38
00:03:10,000 --> 00:03:13,000
to help people who need it the most.

39
00:03:13,000 --> 00:03:19,000
It comprises of social workers, healthcare workers, government, businesses, religious organizations,

40
00:03:19,000 --> 00:03:22,000
people who are experiencing homelessness who have a sense of homelessness,

41
00:03:22,000 --> 00:03:28,000
thinking that anyone in the community who has some integral piece to the solution

42
00:03:28,000 --> 00:03:33,000
are working together to try to find a solution.

43
00:03:33,000 --> 00:03:39,000
Now, part of the NGO is they would often go online and look on Twitter, they would look on Reddit,

44
00:03:39,000 --> 00:03:44,000
they would look at the CDC and the news articles, and try and figure out what the current sense is about people

45
00:03:44,000 --> 00:03:48,000
experiencing homelessness in the community, and also to know where it would be if they had any problems

46
00:03:48,000 --> 00:03:51,000
or a problem area, or a street problem area.

47
00:03:51,000 --> 00:03:57,000
They could then use this information to know where to direct them in their resources to try and help the community.

48
00:03:57,000 --> 00:04:02,000
It's taken a lot of people hours, so they decided they needed a tool to help do it.

49
00:04:02,000 --> 00:04:08,000
So with that, they went to statistics and web borders in order to come up with a tool to help monitor

50
00:04:08,000 --> 00:04:11,000
the sense of homelessness within the community.

51
00:04:11,000 --> 00:04:16,000
Yep, so this is how we got involved with the project, which is Fixed Without Borders.

52
00:04:16,000 --> 00:04:21,000
SWB is a volunteer outreach group that's part of the American Statistical Association,

53
00:04:21,000 --> 00:04:26,000
and they provide poor borders with the support of data science services to NGOs and different global organizations

54
00:04:26,000 --> 00:04:30,000
specifically who wouldn't have access to such resources.

55
00:04:30,000 --> 00:04:35,000
So GBC and EH reached out to SWB with their project proposal.

56
00:04:35,000 --> 00:04:40,000
SWB reviewed it, it was accepted, and then together they formed the State and the Work,

57
00:04:40,000 --> 00:04:47,000
which as Josh was mentioning, involves reporting on online social media posts by citizens

58
00:04:48,000 --> 00:04:53,000
that are related to homelessness in the Greater Victoria area.

59
00:04:53,000 --> 00:04:58,000
So in January 2022, we each got this email.

60
00:04:58,000 --> 00:05:05,000
This is about people calling for volunteers, and this is pretty standard for SWB if anyone is interested in looking to get involved.

61
00:05:05,000 --> 00:05:10,000
Just go onto the specific website where you spoke to them, you fill out an online form of information,

62
00:05:10,000 --> 00:05:13,000
you're getting an email super quick.

63
00:05:13,000 --> 00:05:20,000
And then I say that if you decide to come times a year, you'll get an email such as this form calling for volunteers.

64
00:05:20,000 --> 00:05:26,000
If you're interested, all you have to do is respond, especially if you're interested in the project, you can get such a resume,

65
00:05:26,000 --> 00:05:30,000
explain your credentials, and then if you're supposed to keep up with the project,

66
00:05:30,000 --> 00:05:36,000
there will usually be an interview step or otherwise you'll get an email saying that enough volunteers have already been called

67
00:05:36,000 --> 00:05:41,000
and the call is being closed.

68
00:05:41,000 --> 00:05:45,000
So our team consisted of seven individuals from five different countries.

69
00:05:45,000 --> 00:05:50,000
We had a three team mates and then four of us were given different components of the application.

70
00:05:50,000 --> 00:05:54,000
There was the integration, the modeling, the dashboard, and the spacing component.

71
00:05:54,000 --> 00:06:00,000
And then we also met by twice a month over the past year to work on this project.

72
00:06:00,000 --> 00:06:09,000
We used free communication tools such as Slack, Google to meet and communicate with women.

73
00:06:09,000 --> 00:06:12,000
And then the special guests were really awesome.

74
00:06:12,000 --> 00:06:22,000
Yeah, so the team leads were from SWED and the NGO came together and ultimately came up with a statement of work to help define what we needed to do.

75
00:06:22,000 --> 00:06:32,000
The very first step was that was creating, was a 27th analysis on tweets about the subject of people's trans and homelessness in Victoria.

76
00:06:32,000 --> 00:06:35,000
So the team lead and the NGO put these together.

77
00:06:35,000 --> 00:06:41,000
So they gave us a key to finally list the keywords they wanted us to start looking at to help identify the relevant tweets.

78
00:06:41,000 --> 00:06:46,000
As a team, on the more technical side, we put our heads together and decided we would come up with just a straightforward pipeline.

79
00:06:46,000 --> 00:06:49,000
We're going to do it all in Python and we're going to do it all free.

80
00:06:49,000 --> 00:06:56,000
The NGO is going to continue to run it at whatever cost it may involve, additional resources.

81
00:06:56,000 --> 00:06:59,000
It's a pipeline, pretty straightforward, exactly what we expect.

82
00:06:59,000 --> 00:07:01,000
We're going to build a scraper, collecting information.

83
00:07:01,000 --> 00:07:07,000
We're going to put the collected data to a sentiment model, which will apply the sentiment on the tweets.

84
00:07:07,000 --> 00:07:18,000
And then we have a dashboard for the non-technical users to be able to actually look at the collected data, come up with their own analysis of the subject, and be able to make their own decisions within the NGO.

85
00:07:18,000 --> 00:07:27,000
And underline all that, we had an integration layout which helped connect the different pieces so they could talk to each other.

86
00:07:27,000 --> 00:07:30,000
So right away, we started working on the scraper.

87
00:07:30,000 --> 00:07:32,000
Scraper was my team.

88
00:07:32,000 --> 00:07:37,000
As we said, the statement of work gave us a predefined list of keywords to collect relevant tweets,

89
00:07:37,000 --> 00:07:43,000
to help identify which tweets are actually relevant to the subject of people, students, and home staff, and in the region of the target audience.

90
00:07:43,000 --> 00:07:45,000
These are the capital of the region of the scraper.

91
00:07:45,000 --> 00:07:51,000
From a Python perspective, we just used TPUs, a simple package that wraps the query API.

92
00:07:51,000 --> 00:07:54,000
These panels help collect the most data.

93
00:07:54,000 --> 00:08:01,000
We collected additional metadata on the tweets that were collected, so it's like we're developing a Lights 3T quote tweet.

94
00:08:01,000 --> 00:08:11,000
We tried looking at geodata on tweets to actually identify tweets that were coming from this partner, which would be a huge problem because less than 1% of tweets actually had geodata.

95
00:08:11,000 --> 00:08:16,000
We also experimented with user-provided profile locations, which had some help.

96
00:08:16,000 --> 00:08:22,000
And also, this paper corrected what we thought were the relevant tweets and stored in the CSVs.

97
00:08:24,000 --> 00:08:33,000
Getting a little bit more perspective on what that looked like, like I said, the NGO and the statement of work provided us with keywords to help identify the site that the tweets were looking for.

98
00:08:33,000 --> 00:08:37,000
So the first part was tweets about people's experience in homelessness.

99
00:08:37,000 --> 00:08:41,000
So that was divided into words that were related to the subjects.

100
00:08:41,000 --> 00:08:45,000
We used in the story typical words of poor, crime, poverty, sex, camping.

101
00:08:45,000 --> 00:08:59,000
We talked about sectors, so larger groups related to people from homelessness, so frontline workers, pay to supply, car reduction, and then actual organizations in the local community who were involved in this,

102
00:08:59,000 --> 00:09:03,000
whether it was the districts, different cities, the businesses those cities.

103
00:09:07,000 --> 00:09:12,000
And the second part was words related to identified tweets that were actually coming from Victoria, DC.

104
00:09:12,000 --> 00:09:19,000
So a letter was the neighborhoods, a prior better word is cities, the affected call with, life, or the clothing and street.

105
00:09:19,000 --> 00:09:29,000
Region identifiers, so again, the Greater Victoria, Victoria, DC, hashtag YYJ, the local airport, or influencers.

106
00:09:29,000 --> 00:09:37,000
So in this case influencers began your local politicians, the mayors, organizations began helping people from homelessness in the community.

107
00:09:42,000 --> 00:09:48,000
So as it was great for it wasn't difficult on its own, the underlying issues started rising to start to come up.

108
00:09:48,000 --> 00:09:54,000
First was limited Twitter support, API support, even back in 2012.

109
00:09:54,000 --> 00:09:58,000
There's no way to really get hold of anything on Twitter, no way to verify and ask questions.

110
00:09:58,000 --> 00:10:02,000
We had to go into the feed and Discord a lot for actual technical advice.

111
00:10:02,000 --> 00:10:06,000
There's a lot of issues around version 2 versus version 1.1.

112
00:10:06,000 --> 00:10:12,000
And we were also even trying to hope to get a, sorry, after-miss account.

113
00:10:12,000 --> 00:10:22,000
So we started looking at a historical purpose of feed to look at how feeds from Victoria were forming fast.

114
00:10:23,000 --> 00:10:29,000
Of course we started blocking out different API restrictions, so we couldn't get that API, or the accurate API.

115
00:10:29,000 --> 00:10:33,000
We couldn't get ahold of some Twitter to actually help us figure out how to get that space.

116
00:10:33,000 --> 00:10:36,000
We had to go forward with the basic limitations.

117
00:10:36,000 --> 00:10:43,000
So we ended up having to be only look at seven days of history of feed-fitted cards, which meant that we had to establish

118
00:10:43,000 --> 00:10:47,000
some sort of routine collection that looked at some period of human life seven days.

119
00:10:47,000 --> 00:10:54,000
That ended up affecting our feature decisions with the inspiration we were going to have with the dashboard map.

120
00:10:54,000 --> 00:11:01,000
Another interesting issue with the Twitter API is you're limited on a number of tweets that you can actually make in a series.

121
00:11:01,000 --> 00:11:04,000
You can make for tweets within a 15-minute window.

122
00:11:04,000 --> 00:11:11,000
First you start with a batch scraping and break down that into X number of tweets for 15 minutes, over 15 minutes to fill in the scraper.

123
00:11:11,000 --> 00:11:14,000
It's going to be difficult with our under-applied integration issues.

124
00:11:14,000 --> 00:11:19,000
We started with some blue-nated keyword logic, sometimes adding some of the keywords together, which actually

125
00:11:19,000 --> 00:11:23,000
for a search has some pretty powerful blue-nated keyword logic.

126
00:11:23,000 --> 00:11:27,000
That did reduce the number of tweets you had, but still went over the limit.

127
00:11:27,000 --> 00:11:33,000
And we ended up finding out that the limit was actually a little window, so to size things by applying a very simple

128
00:11:33,000 --> 00:11:39,000
sleeve of 10-minute sections between our requests, we could actually keep the number of requests the limit of the API limit.

129
00:11:40,000 --> 00:11:46,000
So with that, we started actually collecting tweets, first of all, about that word about the subject we're looking for.

130
00:11:46,000 --> 00:11:49,000
Here's a couple examples of ones we found.

131
00:11:49,000 --> 00:11:57,000
The very first one, at City of Toria, a Cormac houseless encampment needing attention, located waterside at Galveston Blues,

132
00:11:57,000 --> 00:12:01,000
near Polar City and Dockside Green, featuring a 12-ton dam in the bridge.

133
00:12:01,000 --> 00:12:04,000
So if you're familiar with this region, you'll know exactly what it is.

134
00:12:04,000 --> 00:12:09,000
You can tell it's about the Toria, it's about the subject of the people with trans and homelessness.

135
00:12:09,000 --> 00:12:14,000
So we see that tweets are starting to come in that are relevant to a subject.

136
00:12:14,000 --> 00:12:18,000
And the other two tweets are saying things that are about Toria and about trans and actually.

137
00:12:18,000 --> 00:12:20,000
There's one being about Toria, being about homeless.

138
00:12:20,000 --> 00:12:25,000
So these two were brand tweets. Now at this point, certain things are relevant to the NGO.

139
00:12:25,000 --> 00:12:29,000
So with these tweets collected, we needed to pass on the sentiment model.

140
00:12:30,000 --> 00:12:33,000
So the next page was the sentiment model.

141
00:12:33,000 --> 00:12:39,000
We ended up using a pre-trained official model, a human-based model, developed by Cardiff University.

142
00:12:39,000 --> 00:12:46,000
Like I said, it was pre-framed. It was pre-framed onto their data, so it landed itself really well to our use case.

143
00:12:46,000 --> 00:12:51,000
If anyone's not familiar with the sentiment model, it's simply assigned two positives,

144
00:12:51,000 --> 00:12:56,000
negative, or neutral category to each piece that's passed through it.

145
00:12:56,000 --> 00:13:00,000
There were some pre-processing instructions that came along with the model.

146
00:13:00,000 --> 00:13:04,000
So it shows remove URLs, cache tags, these things.

147
00:13:04,000 --> 00:13:06,000
We ended up not actually doing that.

148
00:13:06,000 --> 00:13:10,000
We needed to get the information later on for the relevancy component, which we'll get to.

149
00:13:10,000 --> 00:13:13,000
But we did verify the results and the accuracy was really good,

150
00:13:13,000 --> 00:13:18,000
as though we didn't have that many pre-processing instructions at the end.

151
00:13:19,000 --> 00:13:21,000
This is us actually downloading the model.

152
00:13:21,000 --> 00:13:23,000
Like I said, it's super straightforward.

153
00:13:23,000 --> 00:13:25,000
It's really easy to use.

154
00:13:25,000 --> 00:13:27,000
I'm happy with it.

155
00:13:29,000 --> 00:13:38,000
The veteran, we wanted to give the NGO the ability to interact with the data outside of the raw data format.

156
00:13:38,000 --> 00:13:41,000
So we built a dashboard using the Streamlite for them.

157
00:13:41,000 --> 00:13:44,000
The dashboard was also hosted using the Streamlite Cloud.

158
00:13:44,000 --> 00:13:49,000
If anyone, if you guys aren't familiar, it's a completely free, open source,

159
00:13:49,000 --> 00:13:50,000
platform-based framework.

160
00:13:50,000 --> 00:13:52,000
And it's honestly very awesome.

161
00:13:52,000 --> 00:13:59,000
If you do data science projects, it's super easy to build that beautiful layer in between your project and the user.

162
00:13:59,000 --> 00:14:01,000
It's specifically designed for data science.

163
00:14:01,000 --> 00:14:04,000
So I recommend you guys check it out if you're interested.

164
00:14:04,000 --> 00:14:06,000
There were three main functions for the dashboard.

165
00:14:06,000 --> 00:14:10,000
We wanted it to have the ability to fetch and summarize data.

166
00:14:10,000 --> 00:14:13,000
We wanted to get the users to use their speech filtering abilities.

167
00:14:13,000 --> 00:14:20,000
And we also wanted to give the user the ability to download the raw data in case they wanted to further analyze it in Excel.

168
00:14:26,000 --> 00:14:27,000
All right.

169
00:14:27,000 --> 00:14:29,000
So this is our pre-recorded demo.

170
00:14:29,000 --> 00:14:31,000
On the left-hand side, we've got some folders.

171
00:14:31,000 --> 00:14:34,000
So data tweet was created.

172
00:14:34,000 --> 00:14:41,000
We've got basic time series charts in the dashboard using the data source, so the quantity of tweets over time.

173
00:14:41,000 --> 00:14:47,000
Down at the bottom, we give the user the ability to search for tweets by sentiment category.

174
00:14:47,000 --> 00:14:54,000
And then one of the filters on the left is the ability to search by specific agencies or organizations.

175
00:14:54,000 --> 00:15:04,000
So if you know that some politician in the area has been quite vocal on Twitter recently, they can go in here, search for that creator,

176
00:15:04,000 --> 00:15:09,000
pull up the tweets, quickly get a sense of is that person the most tweeting?

177
00:15:09,000 --> 00:15:11,000
Maybe there's a positive tweet.

178
00:15:11,000 --> 00:15:16,000
And then how many over the last whatever time period that they choose.

179
00:15:23,000 --> 00:15:24,000
Okay.

180
00:15:24,000 --> 00:15:25,000
That's good.

181
00:15:25,000 --> 00:15:44,000
So, painting it all together is the integration layer.

182
00:15:44,000 --> 00:15:49,000
I worked on this part, and we ended up electing to use four tools for this.

183
00:15:49,000 --> 00:15:51,000
So the first was the Twitter API.

184
00:15:51,000 --> 00:15:53,000
The second was GitHub.

185
00:15:53,000 --> 00:15:58,000
The third was setting phase, and the fourth was Streamline and Streamline Cloud for actually hosting the application.

186
00:15:58,000 --> 00:16:06,000
So the way this works is every time that ATM specific and GitHub action can drop, it kicks off.

187
00:16:06,000 --> 00:16:09,000
And it expects a full virtual environment.

188
00:16:09,000 --> 00:16:19,000
So, we ended up taking a pipeline script connected to the Twitter API, and we created our collectable that hit on the keywords that Josh was just discussing.

189
00:16:19,000 --> 00:16:27,000
We download the pre-setting supplement models from the heavy-face repo to that virtual environment, run the new tweets through it,

190
00:16:27,000 --> 00:16:34,000
and then as the devising process occurs, it gives a historical face, which our story in our GitHub repository.

191
00:16:34,000 --> 00:16:42,000
We ended up selecting to have a 48-hour look back period for the spray face.

192
00:16:42,000 --> 00:16:50,000
So even though it runs every night, it pulls for us two days worth of tweets, so we're always doing about 24 hours of overlapping data.

193
00:16:50,000 --> 00:16:55,000
We chose to do this because we noticed that there was a little bit of variability with when the claim talks kicked off.

194
00:16:55,000 --> 00:17:00,000
It's pretty good that it did fluctuate by about 30 minutes.

195
00:17:00,000 --> 00:17:07,000
So, we ended up spending 48 hours to just be extra safe so that we're not missing any data.

196
00:17:07,000 --> 00:17:15,000
So that's all on the back end, and then on the front end we have the Streamlight app, which again is hosted by Streamlight Cloud.

197
00:17:15,000 --> 00:17:24,000
It connects directly to a Python script that is in our GitHub repository, and it's just pretty nice because anytime we need to make updates for the application,

198
00:17:24,000 --> 00:17:31,000
we just push those updates through GitHub repo and you have automatically updates.

199
00:17:31,000 --> 00:17:40,000
So a few decisions that went into this was whether or not we wanted it to be automated or if we wanted to allow the user to manually trigger this workflow.

200
00:17:40,000 --> 00:17:43,000
We ended up going with the automated claim job for a few different reasons.

201
00:17:43,000 --> 00:17:47,000
One was to ensure that new tweets were being captured daily.

202
00:17:47,000 --> 00:17:53,000
We wanted to have almost no stuck-to-date data whenever the user logged into the application.

203
00:17:53,000 --> 00:18:01,000
The biggest one was we didn't want to miss any data, so again we had that same little structure based on our limited Twitter account.

204
00:18:01,000 --> 00:18:09,000
So if a user only logged in once a month and they were manually triggering that workflow, we'd end up missing a lot of data.

205
00:18:09,000 --> 00:18:13,000
And then also just warning that the automated decided it was more user-friendly.

206
00:18:13,000 --> 00:18:19,000
All this happens behind the scenes in the evening and there's no wait time when they log in in the morning.

207
00:18:19,000 --> 00:18:25,000
Second was public repos. We ended up using both a public GitHub and a public hub in case we felt.

208
00:18:25,000 --> 00:18:29,000
If they're public, they're free, which was a big factor for us.

209
00:18:29,000 --> 00:18:37,000
And then assuming the file size is going to change for those new tweets that we're posting every evening, we won't reach the soft GitHub file limit for about 30 years.

210
00:18:39,000 --> 00:18:40,000
So some challenges.

211
00:18:40,000 --> 00:18:48,000
Biggest challenge was when running these scripts locally versus through GitHub Actions when we were calling those environment variables.

212
00:18:48,000 --> 00:18:50,000
Certainly the syntax was required.

213
00:18:50,000 --> 00:18:58,000
Biggest challenge in my opinion was just reading and writing data from the virtual environment to the GitHub repo.

214
00:18:58,000 --> 00:19:09,000
We ended up using the 5Github library, which overall is pretty good, but you do need to use additional specific APIs if file size is exceeding 1 MB.

215
00:19:09,000 --> 00:19:15,000
And then just the actual code, the code and encoded data correctly in Francis Transfer CSVs back and forth.

216
00:19:15,000 --> 00:19:20,000
Just a little bit of time to actually implement it. It's not great documentation online for it.

217
00:19:20,000 --> 00:19:22,000
So overall, it worked pretty well.

218
00:19:24,000 --> 00:19:29,000
So this brings us to our biggest challenge of the project, which was relevancy.

219
00:19:29,000 --> 00:19:35,000
So the app at this point was working according to the statement of work, but it was too broad.

220
00:19:35,000 --> 00:19:39,000
We're pulling in a lot of false positive tweets with the keyword-based approach.

221
00:19:39,000 --> 00:19:41,000
So a few examples here.

222
00:19:41,000 --> 00:19:51,000
In the first few tweets you can see these are actually about Victoria or we are joined as a unit about Victoria based on the tweets in the tweet text itself.

223
00:19:51,000 --> 00:19:53,000
But it's not actually about homelessness.

224
00:19:53,000 --> 00:19:59,000
So poor wasn't the keyword, but it's not specific enough to homelessness.

225
00:19:59,000 --> 00:20:01,000
It can have different meanings and different context.

226
00:20:02,000 --> 00:20:06,000
The second few tweets are not actually about Victoria.

227
00:20:06,000 --> 00:20:14,000
They do hit on the keyword camping and go into Central Park, but they're not actually referring to the greater Victoria area.

228
00:20:14,000 --> 00:20:16,000
I'll pass it over.

229
00:20:16,000 --> 00:20:17,000
Thanks, thank you.

230
00:20:17,000 --> 00:20:22,000
Yeah, at this point we did, like you said, we had a practically working pipeline.

231
00:20:22,000 --> 00:20:26,000
We were able to collect the tweets, we were able to file size, we were able to make them work for the dashboard.

232
00:20:26,000 --> 00:20:31,000
But there wasn't a whole lot that was actually becoming useful for the engineering use.

233
00:20:31,000 --> 00:20:34,000
So the first question I asked myself was, well, why is it happening?

234
00:20:34,000 --> 00:20:36,000
Words have multiple meanings.

235
00:20:36,000 --> 00:20:43,000
Last example is perfect where you saw camp and camping actually applied to the actual sports areas, summer camps and camping.

