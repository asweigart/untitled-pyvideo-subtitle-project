1
00:00:00,000 --> 00:00:03,000
Hi, everyone. My name is William.

2
00:00:03,000 --> 00:00:06,000
This talk is Cuda in your Python.

3
00:00:06,000 --> 00:00:11,000
So I'm going to talk to you a little bit about how we can start programming on the GPU

4
00:00:11,000 --> 00:00:16,000
from the safety of our Python program in the language we're familiar with.

5
00:00:16,000 --> 00:00:20,000
So I want to start off with some bad news. Moore's Law is dead.

6
00:00:20,000 --> 00:00:23,000
This is kind of a tough one.

7
00:00:23,000 --> 00:00:25,000
So Moore's Law, what was it?

8
00:00:25,000 --> 00:00:29,000
It's basically an idea that the number of transistors that you can fit on an integrated circuit

9
00:00:29,000 --> 00:00:31,000
would double every two years.

10
00:00:31,000 --> 00:00:33,000
Initially, it was every one year, actually.

11
00:00:33,000 --> 00:00:35,000
He had to revise that.

12
00:00:35,000 --> 00:00:39,000
And this is Gordon Moore, namesake of the law.

13
00:00:39,000 --> 00:00:41,000
And so he based it on this data.

14
00:00:41,000 --> 00:00:43,000
So you can see this graph.

15
00:00:43,000 --> 00:00:45,000
On the left side, it's a log scale.

16
00:00:45,000 --> 00:00:48,000
So the linear nature of it says that it's doubling.

17
00:00:48,000 --> 00:00:53,000
And you can see this held from like the 50s, 60s, 70s.

18
00:00:53,000 --> 00:00:56,000
This actually went up all the way until 2016.

19
00:00:56,000 --> 00:00:58,000
This is with some actual data.

20
00:00:58,000 --> 00:01:01,000
And so you can see that relationship pretty much holds true.

21
00:01:01,000 --> 00:01:05,000
I mean, it's doubling and doubling up until 2016.

22
00:01:05,000 --> 00:01:08,000
But honestly, we're starting to hit a plateau.

23
00:01:08,000 --> 00:01:10,000
A lot of people like to write about it.

24
00:01:10,000 --> 00:01:12,000
There's a lot of headlines popping up.

25
00:01:12,000 --> 00:01:14,000
What are we doing now? Moore's Law is dead.

26
00:01:14,000 --> 00:01:16,000
All these things.

27
00:01:16,000 --> 00:01:19,000
And you might ask, is this really true?

28
00:01:19,000 --> 00:01:22,000
And I'll say, I trust this guy.

29
00:01:22,000 --> 00:01:26,000
This guy says, I guess I see Moore's Law dying here in the next decade or so.

30
00:01:26,000 --> 00:01:28,000
But that's not surprising.

31
00:01:28,000 --> 00:01:30,000
And who is this guy?

32
00:01:30,000 --> 00:01:31,000
This is Gordon Moore.

33
00:01:31,000 --> 00:01:33,000
He said that in 2015.

34
00:01:33,000 --> 00:01:37,000
So if he said it, I think we can start to buy into this idea

35
00:01:37,000 --> 00:01:40,000
that at least over the next, I mean, he said it in 2015.

36
00:01:40,000 --> 00:01:44,000
So over the next five years, we're going to start to see a plateau

37
00:01:44,000 --> 00:01:48,000
in getting more and more transistors onto these chips.

38
00:01:48,000 --> 00:01:52,000
And so this leads us to the question, why am I up here talking to you about GPUs

39
00:01:52,000 --> 00:01:57,000
and how can they help us kind of combat this problem that we're facing?

40
00:01:57,000 --> 00:02:00,000
So a little bit about the history of the GPU.

41
00:02:00,000 --> 00:02:02,000
It's a graphics processing unit.

42
00:02:02,000 --> 00:02:05,000
It was originally developed for gaming uses.

43
00:02:05,000 --> 00:02:10,000
The typical workload involves a lot of arithmetic on like a lot of pixels,

44
00:02:10,000 --> 00:02:15,000
you might imagine, or a lot of objects that are in a frame to do rendering and shading.

45
00:02:15,000 --> 00:02:19,000
And it's specialized for matrix operations because if you can imagine

46
00:02:19,000 --> 00:02:24,000
a representation of a scene in a game could be a matrix, a 2D matrix of pixels

47
00:02:24,000 --> 00:02:28,000
or even a 3D matrix, and then you're performing some transformations on them,

48
00:02:28,000 --> 00:02:30,000
operations like that.

49
00:02:30,000 --> 00:02:34,000
So that's kind of the background of why did people start manufacturing

50
00:02:34,000 --> 00:02:36,000
these devices in the first place.

51
00:02:36,000 --> 00:02:39,000
And so to start to understand even more the differences

52
00:02:39,000 --> 00:02:43,000
and how we might get some benefit from computation on the GPU,

53
00:02:44,000 --> 00:02:46,000
we can look at the specs.

54
00:02:46,000 --> 00:02:51,000
And these are two kind of top tier consumer grade GPU versus CPU.

55
00:02:51,000 --> 00:02:57,000
So on one side, we have Nvidia's 2080 Ti, which came out pretty recently.

56
00:02:57,000 --> 00:03:03,000
And then on the other side, this is Intel's i9-9900K.

57
00:03:03,000 --> 00:03:05,000
So this is what they are.

58
00:03:05,000 --> 00:03:07,000
We can look at the specs.

59
00:03:07,000 --> 00:03:11,000
And so on the one hand, you can see the GPU has a ton more cores

60
00:03:11,000 --> 00:03:13,000
and even more processors.

61
00:03:13,000 --> 00:03:19,000
So that's over 4,000 CUDA cores across 68 streaming multiprocessors

62
00:03:19,000 --> 00:03:22,000
versus the Intel CPU has eight.

63
00:03:22,000 --> 00:03:27,000
It can get up to 16 hyper threads, but that still is very small

64
00:03:27,000 --> 00:03:29,000
in comparison to the GPU.

65
00:03:29,000 --> 00:03:33,000
But on the other hand, you look at the base clock and the boost clock,

66
00:03:33,000 --> 00:03:37,000
and you can see that the CPU in terms of the clock speed

67
00:03:37,000 --> 00:03:43,000
is achieving up to three times better performance than the GPU.

68
00:03:43,000 --> 00:03:47,000
So if we look at this diagram of the architecture,

69
00:03:47,000 --> 00:03:52,000
we can start to understand kind of why these things are the way they are

70
00:03:52,000 --> 00:03:56,000
and how the GPU can start to help us with certain workloads.

71
00:03:56,000 --> 00:03:59,000
So basically, you have the CPU architecture.

72
00:03:59,000 --> 00:04:03,000
You can see a large amount of it is related to control and caching.

73
00:04:03,000 --> 00:04:06,000
So these are two important pieces of how a CPU operates.

74
00:04:06,000 --> 00:04:10,000
You want it to interpret your if statements, your while loops,

75
00:04:10,000 --> 00:04:13,000
and also to be able to cache memory close to it

76
00:04:13,000 --> 00:04:16,000
so it doesn't have to go into RAM.

77
00:04:16,000 --> 00:04:20,000
And you can see relatively fewer of the transistors are allocated

78
00:04:20,000 --> 00:04:23,000
to arithmetic units, which is the green.

79
00:04:23,000 --> 00:04:28,000
In the GPU diagram, you can see relatively little of the transistors

80
00:04:28,000 --> 00:04:31,000
are dedicated to control and caching,

81
00:04:31,000 --> 00:04:36,000
and so more and more can be dedicated to arithmetic operations,

82
00:04:36,000 --> 00:04:39,000
which is why for simple arithmetic operations

83
00:04:39,000 --> 00:04:42,000
that don't involve a lot of control flow,

84
00:04:42,000 --> 00:04:45,000
the GPU can end up running a lot faster.

85
00:04:45,000 --> 00:04:53,000
And so kind of all those factors have led to the rise of what you call GPGPU,

86
00:04:53,000 --> 00:04:58,000
which is the idea that we can do general purpose computing on a GPU

87
00:04:58,000 --> 00:05:02,000
and not just use it for specialized purposes like video games, graphics.

88
00:05:02,000 --> 00:05:06,000
And so this quote is from a paper published by Nvidia in 2014

89
00:05:06,000 --> 00:05:09,000
and basically was saying that, that in the past we thought

90
00:05:09,000 --> 00:05:13,000
these devices are only good for gaming, for graphics,

91
00:05:13,000 --> 00:05:16,000
but now we can start to think of them as parallel processors,

92
00:05:16,000 --> 00:05:18,000
which is what they really are.

93
00:05:18,000 --> 00:05:22,000
And a bunch of the GPU companies started making their own kind of models

94
00:05:22,000 --> 00:05:24,000
of how to program these things.

95
00:05:24,000 --> 00:05:26,000
So CUDA, which is what I'm going to talk about today,

96
00:05:26,000 --> 00:05:29,000
was developed by Nvidia, but also AMD had AP

97
00:05:29,000 --> 00:05:33,000
and OpenCL is kind of a growing open standard.

98
00:05:33,000 --> 00:05:36,000
And so these are different ways people are coming up with

99
00:05:36,000 --> 00:05:41,000
how we can turn this device that we have that has certain desirable properties

100
00:05:41,000 --> 00:05:45,000
into something that we can run general purpose workloads on.

101
00:05:45,000 --> 00:05:48,000
And so this kind of gets into, well, why am I interested in this?

102
00:05:48,000 --> 00:05:51,000
How did I come to be giving this talk up here?

103
00:05:51,000 --> 00:05:53,000
And so I'll start by talking a little bit about my work.

104
00:05:53,000 --> 00:05:56,000
So I work as an engineer on the data team at Compass.

105
00:05:56,000 --> 00:05:59,000
Compass is a real estate technology platform,

106
00:05:59,000 --> 00:06:02,000
basically bringing together the real estate agent

107
00:06:02,000 --> 00:06:05,000
with engineers who are building technology to empower them.

108
00:06:05,000 --> 00:06:08,000
And what I specifically work on is bringing in listing data

109
00:06:08,000 --> 00:06:11,000
from a bunch of different geographies, and then we transform it.

110
00:06:11,000 --> 00:06:14,000
We perform a lot of conversions and normalization.

111
00:06:14,000 --> 00:06:17,000
And so we use a lot of common tools you might see

112
00:06:17,000 --> 00:06:19,000
in data pipelines and data processing.

113
00:06:20,000 --> 00:06:23,000
So this includes Spark, Kafka, Airflow.

114
00:06:23,000 --> 00:06:26,000
But I also like to keep my ear to the ground

115
00:06:26,000 --> 00:06:30,000
in terms of what's going on in the industry around this tooling.

116
00:06:30,000 --> 00:06:33,000
And you start to read some interesting things.

117
00:06:33,000 --> 00:06:36,000
This is Rapids AI, which is partnered with Nvidia,

118
00:06:36,000 --> 00:06:41,000
and their concept is that the GPUs might start to be used

119
00:06:41,000 --> 00:06:44,000
for this entire data processing workflow

120
00:06:44,000 --> 00:06:46,000
and not just for, say, model training,

121
00:06:46,000 --> 00:06:48,000
which is a place where they've started to be used.

122
00:06:48,000 --> 00:06:50,000
So you might actually be able to do your preprocessing,

123
00:06:50,000 --> 00:06:53,000
even visualization, using the power of GPUs.

124
00:06:53,000 --> 00:06:55,000
So this is something that's in development.

125
00:06:55,000 --> 00:06:57,000
We might have GPU databases.

126
00:06:57,000 --> 00:06:59,000
This is just from this year.

127
00:06:59,000 --> 00:07:04,000
Uber built kind of a GPU-powered database for doing analytics.

128
00:07:04,000 --> 00:07:07,000
So even in terms of what I work on professionally,

129
00:07:07,000 --> 00:07:12,000
there's a couple kind of advancements that might be, you know,

130
00:07:12,000 --> 00:07:16,000
putting GPUs into use for these kinds of data processing workflows.

131
00:07:16,000 --> 00:07:21,000
But it also relates to my hobbies, which include deep learning.

132
00:07:21,000 --> 00:07:26,000
So I kind of got hooked in it through the Fast AI deep learning course.

133
00:07:26,000 --> 00:07:30,000
I started doing some competitions on Kaggle.

134
00:07:30,000 --> 00:07:34,000
And this is kind of what got me hooked into thinking about these hardware questions.

135
00:07:34,000 --> 00:07:38,000
So this is my computer at home that I put together.

136
00:07:38,000 --> 00:07:42,000
Basically, after leaving a GPU running on AWS for one weekend,

137
00:07:42,000 --> 00:07:44,000
you get a pretty big bill, and you're like,

138
00:07:44,000 --> 00:07:48,000
okay, maybe I should start to think about building one myself.

139
00:07:48,000 --> 00:07:50,000
Maybe some people have been there before.

140
00:07:50,000 --> 00:07:54,000
And so this is kind of what got me thinking about GPUs and CUDA

141
00:07:54,000 --> 00:07:58,000
on kind of a more general level, not just related to deep learning.

142
00:07:58,000 --> 00:08:05,000
And specifically, you know, I made a small pull request to PyTorch last year.

143
00:08:05,000 --> 00:08:09,000
And that's kind of what inspired this talk was this was moving one function

144
00:08:09,000 --> 00:08:12,000
basically from their Python into the C++ implementation.

145
00:08:12,000 --> 00:08:16,000
But just seeing kind of how that library was all put together

146
00:08:16,000 --> 00:08:19,000
and how they were able to kind of merge, you know,

147
00:08:19,000 --> 00:08:23,000
the Python API with C++ and also CUDA programming,

148
00:08:23,000 --> 00:08:25,000
it was really fascinating to me.

149
00:08:25,000 --> 00:08:27,000
And so I wanted to kind of dive in and learn more.

150
00:08:27,000 --> 00:08:30,000
So that was part of the origin of this talk.

151
00:08:30,000 --> 00:08:35,000
So finally, I get to the question that everybody probably wanted to know,

152
00:08:35,000 --> 00:08:38,000
which is how can you start programming on the GPU?

153
00:08:38,000 --> 00:08:42,000
And so for an example, I've started out with NumPy.

154
00:08:42,000 --> 00:08:45,000
So this would run on the CPU.

155
00:08:45,000 --> 00:08:48,000
And those of you who are familiar kind of with NumPy might see

156
00:08:48,000 --> 00:08:53,000
we're taking a random basic vector of 10 million numbers.

157
00:08:53,000 --> 00:08:56,000
We're taking two of them and just adding them together, right?

158
00:08:56,000 --> 00:09:01,000
And so this is something that, you know, you could run.

159
00:09:01,000 --> 00:09:03,000
And NumPy is pretty good at this.

160
00:09:03,000 --> 00:09:05,000
Like when I benchmarked it, it wasn't too large.

161
00:09:05,000 --> 00:09:06,000
So let's see.

162
00:09:06,000 --> 00:09:09,000
What would the code look like, the equivalent of this on the GPU?

163
00:09:09,000 --> 00:09:11,000
Whoa, that's not much change, right?

164
00:09:11,000 --> 00:09:13,000
I'll go back to the other slide.

165
00:09:13,000 --> 00:09:15,000
And there's the other slide.

166
00:09:15,000 --> 00:09:16,000
And you're like, whoa.

167
00:09:16,000 --> 00:09:20,000
So this is from a library called QPy, basically.

168
00:09:20,000 --> 00:09:23,000
And it tries to mirror the NumPy API,

169
00:09:23,000 --> 00:09:27,000
and it lets you start to take advantage of GPU processing

170
00:09:27,000 --> 00:09:29,000
in a pretty straightforward way.

171
00:09:29,000 --> 00:09:31,000
Like I said, just one more time.

172
00:09:31,000 --> 00:09:33,000
Here's NumPy, QPy.

173
00:09:33,000 --> 00:09:38,000
But when you benchmark it, you can see that actually

174
00:09:38,000 --> 00:09:41,000
just that switch gives you a 30-time speedup.

175
00:09:41,000 --> 00:09:45,000
And that's accounting for things like, you know,

176
00:09:45,000 --> 00:09:47,000
the GPU executes asynchronously,

177
00:09:47,000 --> 00:09:49,000
but you want to make sure you synchronize it.

178
00:09:49,000 --> 00:09:52,000
So you're seeing about a 30-time speedup, again,

179
00:09:52,000 --> 00:09:54,000
just from this change.

180
00:09:54,000 --> 00:09:56,000
And so that's part of why I think this is so cool,

181
00:09:56,000 --> 00:09:58,000
because I think, you know, how many people out there

182
00:09:58,000 --> 00:10:01,000
use NumPy for some things, right?

183
00:10:01,000 --> 00:10:03,000
You know, it's a pretty common tool in Python.

184
00:10:03,000 --> 00:10:06,000
And so, you know, if there's this drop-in replacement

185
00:10:06,000 --> 00:10:08,000
that would let you get these kinds of speedups,

186
00:10:08,000 --> 00:10:11,000
like that might really help your workflow.

187
00:10:11,000 --> 00:10:13,000
And so that brings me into talking about kind of

188
00:10:13,000 --> 00:10:15,000
the outline of this talk.

189
00:10:15,000 --> 00:10:17,000
And I'm going to talk about different approaches

190
00:10:17,000 --> 00:10:20,000
to doing CUDA programming from your Python application.

191
00:10:20,000 --> 00:10:24,000
One, which I just showed you is as a drop-in replacement.

192
00:10:24,000 --> 00:10:28,000
Two, is basically taking CUDA strings

193
00:10:28,000 --> 00:10:31,000
and compiling them in a Python program.

194
00:10:31,000 --> 00:10:34,000
And then the third one, which is kind of the most complex,

195
00:10:34,000 --> 00:10:38,000
is actually building it as a C++ extension to Python,

196
00:10:38,000 --> 00:10:41,000
which Python allows you to do.

197
00:10:41,000 --> 00:10:44,000
And, you know, as with a lot of things, I think,

198
00:10:44,000 --> 00:10:46,000
in programming, software development,

199
00:10:46,000 --> 00:10:49,000
these are kind of increasing levels of complexity.

200
00:10:49,000 --> 00:10:51,000
But what comes along with that complexity

201
00:10:51,000 --> 00:10:53,000
and kind of initial setup is

202
00:10:53,000 --> 00:10:55,000
it gives you additional flexibility.

203
00:10:55,000 --> 00:10:57,000
It might unlock features of the CUDA platform

204
00:10:57,000 --> 00:10:59,000
that aren't necessarily available

205
00:10:59,000 --> 00:11:01,000
in, say, a drop-in replacement,

206
00:11:01,000 --> 00:11:03,000
but that you might be able to access

207
00:11:03,000 --> 00:11:05,000
by kind of rolling your own.

208
00:11:05,000 --> 00:11:07,000
So to talk about drop-in replacement,

209
00:11:07,000 --> 00:11:10,000
the library I showed was, like I said, QPy,

210
00:11:10,000 --> 00:11:12,000
which is built as a drop-in replacement.

211
00:11:12,000 --> 00:11:15,000
It was originally developed for a deep learning framework

212
00:11:15,000 --> 00:11:17,000
called Chainer.

213
00:11:17,000 --> 00:11:19,000
It supports a lot of NumBy features,

214
00:11:19,000 --> 00:11:21,000
some of which are pretty, you know, complex.

215
00:11:21,000 --> 00:11:23,000
Like the whole indexing system,

216
00:11:23,000 --> 00:11:26,000
I'm pretty sure they have it working just like NumPy,

217
00:11:26,000 --> 00:11:28,000
and NumPy does a lot of crazy things with that.

218
00:11:28,000 --> 00:11:30,000
Supports a bunch of different data types,

219
00:11:30,000 --> 00:11:32,000
broadcasting.

220
00:11:32,000 --> 00:11:34,000
There are a couple gotchas,

221
00:11:34,000 --> 00:11:36,000
so those of you who raised your hands

222
00:11:36,000 --> 00:11:38,000
who might want to think about dropping this into your program,

223
00:11:38,000 --> 00:11:40,000
these are things to watch out for.

224
00:11:40,000 --> 00:11:43,000
So it can't use data types that are strings or objects.

225
00:11:43,000 --> 00:11:45,000
This makes sense if you think about

226
00:11:45,000 --> 00:11:47,000
the kind of diagram I showed you

227
00:11:47,000 --> 00:11:50,000
that there's a ton of arithmetic units on the GPU,

228
00:11:50,000 --> 00:11:52,000
so they're going to be working on numbers,

229
00:11:52,000 --> 00:11:54,000
which is good for these other kinds of data.

230
00:11:54,000 --> 00:11:56,000
The other thing is array creation.

231
00:11:56,000 --> 00:12:01,000
So in NumPy, you can do numpy.array over a list.

232
00:12:01,000 --> 00:12:03,000
It'll turn it into a for you. You can't do that.

233
00:12:03,000 --> 00:12:06,000
And then the last one, which really might trip you up,

234
00:12:06,000 --> 00:12:09,000
is if you sum a NumPy array,

235
00:12:09,000 --> 00:12:11,000
it's going to return a scalar.

236
00:12:11,000 --> 00:12:13,000
It's just one value,

237
00:12:13,000 --> 00:12:18,000
but in QPy, it returns a zero-dimensional array,

238
00:12:18,000 --> 00:12:21,000
which is going to be a little bit different.

239
00:12:21,000 --> 00:12:24,000
So that is to say that these libraries are good,

240
00:12:24,000 --> 00:12:26,000
and I would definitely encourage you to try them out,

241
00:12:26,000 --> 00:12:28,000
but definitely keep in mind

242
00:12:28,000 --> 00:12:30,000
that there are certain things that might trip you up.

243
00:12:30,000 --> 00:12:34,000
It's not quite going to be a total drop-in replacement.

244
00:12:34,000 --> 00:12:39,000
So the second way to do it, like I said,

245
00:12:39,000 --> 00:12:42,000
is that you can compile CUDA strings

246
00:12:42,000 --> 00:12:44,000
in your Python application.

247
00:12:44,000 --> 00:12:47,000
And so before we talk about that,

248
00:12:47,000 --> 00:12:49,000
then we have to talk about the CUDA API,

249
00:12:49,000 --> 00:12:51,000
because compared to the drop-in,

250
00:12:51,000 --> 00:12:54,000
you're going to actually have to write some CUDA code.

251
00:12:54,000 --> 00:12:59,000
And so this is a diagram of the basic kind of building blocks

252
00:12:59,000 --> 00:13:02,000
of how CUDA programming is done.

253
00:13:02,000 --> 00:13:04,000
So on the top, you've got a grid.

254
00:13:04,000 --> 00:13:06,000
You've got blocks inside that,

255
00:13:06,000 --> 00:13:08,000
and then you've got threads inside your blocks.

256
00:13:08,000 --> 00:13:10,000
So to break it down a little bit,

257
00:13:10,000 --> 00:13:14,000
threads are the things that actually execute CUDA kernels,

258
00:13:14,000 --> 00:13:16,000
and they have a thread index,

259
00:13:16,000 --> 00:13:19,000
and what that's used for is basically to specify

260
00:13:19,000 --> 00:13:23,000
which part of the data that thread is meant to work on.

261
00:13:23,000 --> 00:13:26,000
And so this is from the body of a particular CUDA function,

262
00:13:26,000 --> 00:13:28,000
but you can see here,

263
00:13:28,000 --> 00:13:32,000
if you're executing over a two-dimensional matrix,

264
00:13:32,000 --> 00:13:37,000
you can basically have a thread index in the X and Y axes,

265
00:13:37,000 --> 00:13:39,000
and that lets you specify, like,

266
00:13:39,000 --> 00:13:44,000
this thread is meant to take this element of the matrix

267
00:13:44,000 --> 00:13:46,000
and add it together.

268
00:13:46,000 --> 00:13:49,000
And you can imagine that this kind of simplifies the logic

269
00:13:49,000 --> 00:13:52,000
if you've got 1,000 threads.

270
00:13:52,000 --> 00:13:55,000
And I guess if you've ever tried to write some, like, parallel code,

271
00:13:55,000 --> 00:13:58,000
like, there's kind of a lot of housekeeping that goes around,

272
00:13:58,000 --> 00:14:00,000
like, keeping track of these thread indexes

273
00:14:00,000 --> 00:14:03,000
and which thread touches which part of the matrix,

274
00:14:03,000 --> 00:14:07,000
and a lot of that is built into this CUDA paradigm for you.

275
00:14:07,000 --> 00:14:09,000
So then blocks.

276
00:14:09,000 --> 00:14:11,000
Blocks is the next level.

277
00:14:11,000 --> 00:14:13,000
A block is a group of threads.

278
00:14:13,000 --> 00:14:15,000
And the important thing is the blocks are required

279
00:14:15,000 --> 00:14:17,000
to be able to execute independently,

280
00:14:17,000 --> 00:14:20,000
but threads within a block can share data.

281
00:14:20,000 --> 00:14:23,000
There's kind of block-level shared memory.

282
00:14:23,000 --> 00:14:26,000
And so if you do need to do some synchronization between your threads,

283
00:14:26,000 --> 00:14:28,000
that is possible on the block level.

284
00:14:28,000 --> 00:14:30,000
And just like the threads,

285
00:14:30,000 --> 00:14:34,000
the block also has dimensions and indexing.

286
00:14:34,000 --> 00:14:37,000
So you can do block index as well as the size of the block.

287
00:14:37,000 --> 00:14:41,000
And this is if you need to do kind of more complex computations

288
00:14:41,000 --> 00:14:45,000
of, like, what specific bit of this matrix is this thread

289
00:14:45,000 --> 00:14:47,000
supposed to operate on.

290
00:14:47,000 --> 00:14:50,000
Grids, there's nothing too fancy as a group of blocks.

291
00:14:50,000 --> 00:14:52,000
So going back to the diagram, then,

292
00:14:52,000 --> 00:14:55,000
we kind of have these different levels and layers

293
00:14:55,000 --> 00:14:57,000
of being able to achieve parallelism.

294
00:14:57,000 --> 00:15:01,000
So from the lowest to the top, we've got threads,

295
00:15:01,000 --> 00:15:05,000
which are executing the CUDA code organized into these blocks,

296
00:15:05,000 --> 00:15:09,000
which, again, can also be arranged in this two-dimensional

297
00:15:09,000 --> 00:15:11,000
or three-dimensional space.

298
00:15:11,000 --> 00:15:14,000
So you see, like, the 0, 0 there is the index.

299
00:15:14,000 --> 00:15:17,000
So that block is indexed at 0, 0, and then 1, 0.

300
00:15:17,000 --> 00:15:22,000
So the grid itself is also a two-dimensional group of blocks.

301
00:15:22,000 --> 00:15:27,000
So that's kind of threads, blocks, and grids.

302
00:15:27,000 --> 00:15:29,000
So the other kind of piece of it,

303
00:15:29,000 --> 00:15:33,000
when you get around to actually executing CUDA code, is kernels.

304
00:15:33,000 --> 00:15:36,000
And that's basically C or C++ code

305
00:15:36,000 --> 00:15:39,000
that they've added a bit of extra syntax to.

306
00:15:39,000 --> 00:15:43,000
So specifically, there's this global kind of identifier

307
00:15:43,000 --> 00:15:46,000
that you're used to specify your kernel function.

308
00:15:46,000 --> 00:15:49,000
And then there's this angle bracket syntax

309
00:15:49,000 --> 00:15:52,000
that lets you specify the grid size and the block size.

310
00:15:52,000 --> 00:15:54,000
So if we go back to here, obviously,

311
00:15:54,000 --> 00:15:57,000
this is a parameter that you have control over.

312
00:15:57,000 --> 00:16:00,000
So you can say how many blocks go in my grid

313
00:16:00,000 --> 00:16:02,000
and then how many threads go in my block,

314
00:16:02,000 --> 00:16:05,000
and that's kind of one way to tune the performance.

315
00:16:05,000 --> 00:16:07,000
And so by using this syntax,

316
00:16:07,000 --> 00:16:10,000
you can kind of play around with that and see,

317
00:16:10,000 --> 00:16:13,000
well, what's going to be the most effective.

318
00:16:13,000 --> 00:16:16,000
So this is one example of what a kernel might look like.

319
00:16:16,000 --> 00:16:19,000
So this global identifier at the top here

320
00:16:19,000 --> 00:16:21,000
is saying this is the kernel.

321
00:16:21,000 --> 00:16:23,000
So this is what's going to execute on the GPU,

322
00:16:23,000 --> 00:16:26,000
whereas this main function is what executes on the CPU.

323
00:16:26,000 --> 00:16:29,000
And that's kind of the relationship in a CUDA program

324
00:16:29,000 --> 00:16:31,000
is you have your host, which is the CPU,

325
00:16:31,000 --> 00:16:34,000
you have your device, which is the term for the GPU.

326
00:16:34,000 --> 00:16:37,000
And basically, your program starts executing on a CPU,

327
00:16:37,000 --> 00:16:40,000
and then as soon as it gets to one of these kernels

328
00:16:40,000 --> 00:16:42,000
that's going to be executed on device,

329
00:16:42,000 --> 00:16:45,000
it basically calls that and communicates

330
00:16:45,000 --> 00:16:49,000
and does data transfer across from the CPU to the GPU

331
00:16:49,000 --> 00:16:51,000
to be able to run this.

332
00:16:51,000 --> 00:16:53,000
And you can see here that at the top,

333
00:16:53,000 --> 00:16:55,000
the kernel is making use of these kind of terms

334
00:16:55,000 --> 00:16:58,000
I was talking about, so block index, block dimension,

335
00:16:58,000 --> 00:17:01,000
thread index, to figure out, in this case,

336
00:17:01,000 --> 00:17:04,000
it's adding two two-dimensional matrices,

337
00:17:04,000 --> 00:17:06,000
like figure out which piece of it

338
00:17:06,000 --> 00:17:09,000
the thread is actually meant to do work on.

339
00:17:09,000 --> 00:17:15,000
So that's kind of the high-level overview of the CUDA API.

340
00:17:15,000 --> 00:17:18,000
So let me get to PyCuda.

341
00:17:18,000 --> 00:17:21,000
Basically, this was built by a researcher.

342
00:17:21,000 --> 00:17:24,000
It's used for a lot of scientific and research projects.

343
00:17:24,000 --> 00:17:27,000
There's even a research paper about this library,

344
00:17:27,000 --> 00:17:29,000
which is not necessarily something you see

345
00:17:29,000 --> 00:17:31,000
for all the code you find on GitHub.

346
00:17:31,000 --> 00:17:33,000
I was a little bit surprised.

347
00:17:33,000 --> 00:17:35,000
So what does it do?

348
00:17:35,000 --> 00:17:37,000
Basically, this is example code

349
00:17:37,000 --> 00:17:39,000
that kind of gets to the main thing

350
00:17:39,000 --> 00:17:41,000
that it gives to you as a programmer,

351
00:17:41,000 --> 00:17:45,000
which is you can take the CUDA kernel code.

352
00:17:45,000 --> 00:17:47,000
So if we go back here,

353
00:17:47,000 --> 00:17:49,000
this is what a kernel looks like,

354
00:17:49,000 --> 00:17:51,000
and this global function is what's going to execute

355
00:17:51,000 --> 00:17:54,000
on the device, that's what's going to execute on the GPU.

356
00:17:54,000 --> 00:17:56,000
You can basically pull that out,

357
00:17:56,000 --> 00:17:58,000
and within your Python program,

358
00:17:58,000 --> 00:18:02,000
supply that as a string to this source module object,

359
00:18:02,000 --> 00:18:06,000
and then there's an additional, this get function call,

360
00:18:06,000 --> 00:18:10,000
and this basically compiles it as a GPU kernel

361
00:18:10,000 --> 00:18:13,000
and then pulls it into your Python program

362
00:18:13,000 --> 00:18:17,000
to allow you to be able to call it over your objects.

363
00:18:17,000 --> 00:18:22,000
So this is one way that lets you start to write CUDA code,

364
00:18:22,000 --> 00:18:26,000
but you don't have to step out of a Python environment

365
00:18:26,000 --> 00:18:28,000
because all the objects and whatnot

366
00:18:28,000 --> 00:18:33,000
are still going to be just like in a Python program.

367
00:18:33,000 --> 00:18:35,000
And so this is one of the nice things,

368
00:18:35,000 --> 00:18:39,000
is in PyCuda you get automatic memory management.

369
00:18:39,000 --> 00:18:42,000
So basically, once something goes out of scope

370
00:18:42,000 --> 00:18:44,000
or once you delete something,

371
00:18:44,000 --> 00:18:47,000
it's actually going to free the allocated memory on the GPU.

372
00:18:47,000 --> 00:18:49,000
And when I get to describing C extensions

373
00:18:49,000 --> 00:18:51,000
and kind of the most manual way to do it,

374
00:18:51,000 --> 00:18:55,000
you'll see that that can actually save you a lot of hassle.

375
00:18:55,000 --> 00:18:57,000
These are other kind of classes it provides,

376
00:18:57,000 --> 00:19:00,000
so it's an in-out and in-out classes

377
00:19:00,000 --> 00:19:04,000
to describe your arrays or matrices,

378
00:19:04,000 --> 00:19:08,000
and these handle memory transfer between the CPU and GPU.

379
00:19:08,000 --> 00:19:11,000
Basically, there's a lot of steps you'd have to do

380
00:19:11,000 --> 00:19:12,000
in order to perform,

381
00:19:12,000 --> 00:19:14,000
let's say you have a NumPy array on your CPU,

382
00:19:14,000 --> 00:19:17,000
you want to like double all the elements.

383
00:19:17,000 --> 00:19:19,000
These are all the steps you'd have to do

384
00:19:19,000 --> 00:19:20,000
to be able to accomplish that,

385
00:19:20,000 --> 00:19:22,000
and there's a lot of them in terms of creating memory,

386
00:19:22,000 --> 00:19:24,000
moving data across,

387
00:19:24,000 --> 00:19:27,000
and so PyCuda also has certain higher level abstractions

388
00:19:27,000 --> 00:19:32,000
where basically in means this array is meant to go to the GPU,

389
00:19:32,000 --> 00:19:33,000
and it handles that for you.

390
00:19:33,000 --> 00:19:36,000
And in-out is even a little bit more complicated.

391
00:19:36,000 --> 00:19:39,000
We're just saying this NumPy array, say,

392
00:19:39,000 --> 00:19:41,000
is meant to go into the GPU,

393
00:19:41,000 --> 00:19:44,000
be processed in your CUDA kernel, and then come back out.

394
00:19:44,000 --> 00:19:47,000
And as long as you're okay with that happening automatically,

395
00:19:47,000 --> 00:19:49,000
PyCuda can handle that behind the scenes.

396
00:19:49,000 --> 00:19:51,000
And the last thing, which I think is really useful,

397
00:19:51,000 --> 00:19:53,000
is automatic error checking.

398
00:19:53,000 --> 00:19:55,000
So because CUDA is...

399
00:19:55,000 --> 00:19:58,000
some of the operations execute asynchronously,

400
00:19:58,000 --> 00:20:01,000
and so collecting and surfacing the errors

401
00:20:01,000 --> 00:20:02,000
can be really challenging.

402
00:20:02,000 --> 00:20:04,000
And including this, this is from the documentation.

403
00:20:04,000 --> 00:20:06,000
If an asynchronous error occurs,

404
00:20:06,000 --> 00:20:08,000
it will be reported by some subsequent,

405
00:20:08,000 --> 00:20:11,000
unrelated runtime function call.

406
00:20:11,000 --> 00:20:13,000
And that sounded pretty confusing to me.

407
00:20:13,000 --> 00:20:15,000
It's like, okay, well, if an error happens in one thing,

408
00:20:15,000 --> 00:20:18,000
basically the next function you call is going to error,

409
00:20:18,000 --> 00:20:21,000
and so PyCuda also handles that for you

410
00:20:21,000 --> 00:20:24,000
and raises them as kind of specific Python exceptions

411
00:20:24,000 --> 00:20:27,000
in the paradigm you might be more familiar with.

412
00:20:27,000 --> 00:20:29,000
And finally, another interesting bit

413
00:20:29,000 --> 00:20:30,000
is you can do metaprogramming.

414
00:20:30,000 --> 00:20:33,000
So basically, when I was talking about threads, blocks, grids,

415
00:20:33,000 --> 00:20:38,000
basically you have to tune those parameters,

416
00:20:38,000 --> 00:20:41,000
like how many threads in a block, how many blocks in a grid,

417
00:20:41,000 --> 00:20:45,000
just like you might do with parallel programming on a CPU.

418
00:20:45,000 --> 00:20:47,000
And it's often done with heuristics,

419
00:20:47,000 --> 00:20:50,000
but PyCuda basically says, forget heuristics.

420
00:20:50,000 --> 00:20:53,000
We can do this by running things,

421
00:20:53,000 --> 00:20:55,000
and we can empirically determine

422
00:20:55,000 --> 00:20:58,000
how to actually set these parameters.

423
00:20:58,000 --> 00:21:00,000
So this is one example,

424
00:21:00,000 --> 00:21:02,000
which is actually using Jinja templating.

425
00:21:02,000 --> 00:21:06,000
So like I said, because the module is a string in Python,

426
00:21:06,000 --> 00:21:09,000
it kind of like follows that any way

427
00:21:09,000 --> 00:21:13,000
you can create a Python string, you can pass and compile it.

428
00:21:13,000 --> 00:21:16,000
So this is kind of a clever, kind of crazy way of doing it,

429
00:21:16,000 --> 00:21:19,000
but you can basically pass in different parameters

430
00:21:19,000 --> 00:21:20,000
in terms of types.

431
00:21:20,000 --> 00:21:24,000
So you can see it's parameterizing over the float type.

432
00:21:24,000 --> 00:21:26,000
You can pass in a different block size,

433
00:21:26,000 --> 00:21:28,000
a different thread block size.

434
00:21:28,000 --> 00:21:29,000
And it basically...

435
00:21:29,000 --> 00:21:31,000
So first templates your string,

436
00:21:31,000 --> 00:21:36,000
and then kind of just in time compiles it into a PyCuda kernel.

437
00:21:36,000 --> 00:21:37,000
I mean, that's really cool,

438
00:21:37,000 --> 00:21:40,000
because especially as you're trying to play around with this,

439
00:21:40,000 --> 00:21:43,000
and like this week I was trying to compile different things

440
00:21:43,000 --> 00:21:44,000
and see how they ran,

441
00:21:44,000 --> 00:21:47,000
but this step would be a lot faster,

442
00:21:47,000 --> 00:21:49,000
just to be able to basically loop

443
00:21:49,000 --> 00:21:52,000
through different configurations in Python

444
00:21:52,000 --> 00:21:55,000
and be able to see the results empirically.

445
00:22:01,000 --> 00:22:03,000
Okay, so then the last bit, which like I said,

446
00:22:03,000 --> 00:22:04,000
is the most manual,

447
00:22:04,000 --> 00:22:06,000
but it might give you the most flexibility,

448
00:22:06,000 --> 00:22:08,000
is CUDA has a C extension.

449
00:22:08,000 --> 00:22:11,000
And so first, let's talk about Python C extensions.

450
00:22:12,000 --> 00:22:18,000
So Python, because the interpreter is C-based,

451
00:22:18,000 --> 00:22:23,000
you can extend it with C and C++ programs.

452
00:22:23,000 --> 00:22:26,000
You can add new modules of your own design.

453
00:22:26,000 --> 00:22:29,000
And that's used in even a lot of programs

454
00:22:29,000 --> 00:22:33,000
that want to achieve better performance on the CPU.

455
00:22:33,000 --> 00:22:35,000
So, for example, NumPy.

456
00:22:35,000 --> 00:22:39,000
NumPy links into certain CPU system,

457
00:22:39,000 --> 00:22:41,000
or yeah, C system calls rather,

458
00:22:41,000 --> 00:22:44,000
to be able to achieve the kind of performance it does.

459
00:22:44,000 --> 00:22:47,000
So this is already a paradigm being used

460
00:22:47,000 --> 00:22:50,000
in kind of high performance computing settings.

461
00:22:50,000 --> 00:22:51,000
And so the question is, okay,

462
00:22:51,000 --> 00:22:55,000
so now I can get my C or C++ program into Python

463
00:22:55,000 --> 00:22:56,000
as an extension.

464
00:22:56,000 --> 00:23:01,000
So then how do I get my CUDA program to a C or C++?

465
00:23:01,000 --> 00:23:03,000
And basically CUDA,

466
00:23:03,000 --> 00:23:05,000
well, there's different forms of it,

467
00:23:05,000 --> 00:23:08,000
but what I've been talking about is referred to as CUDA C.

468
00:23:08,000 --> 00:23:10,000
And so this is basically C,

469
00:23:10,000 --> 00:23:13,000
but with a special syntax.

470
00:23:13,000 --> 00:23:18,000
And so NVIDIA provides you with this NVCC compiler,

471
00:23:18,000 --> 00:23:20,000
which basically takes your CUDA C source code

472
00:23:20,000 --> 00:23:22,000
and it does a couple things.

473
00:23:22,000 --> 00:23:25,000
One, it turns the kernel into like assembly

474
00:23:25,000 --> 00:23:28,000
or binary for operations on the GPU.

475
00:23:28,000 --> 00:23:31,000
It takes one of these special syntaxes

476
00:23:32,000 --> 00:23:35,000
and replaces it with runtime calls.

477
00:23:35,000 --> 00:23:37,000
And then finally, you can also have it

478
00:23:37,000 --> 00:23:40,000
just compile the host like CPU code.

479
00:23:40,000 --> 00:23:45,000
And so NVCC can do all of it like itself.

480
00:23:45,000 --> 00:23:47,000
So there's just one compilation step

481
00:23:47,000 --> 00:23:49,000
that basically generates output in CUDA,

482
00:23:49,000 --> 00:23:52,000
as well as the code you would need to run on the CPU

483
00:23:52,000 --> 00:23:54,000
to launch the kernel.

484
00:23:54,000 --> 00:23:57,000
And so on the Python side, it can get very complex.

485
00:23:57,000 --> 00:24:00,000
This is a very good GitHub repo.

486
00:24:00,000 --> 00:24:03,000
It cheats a little bit and uses Scython,

487
00:24:03,000 --> 00:24:05,000
but there's a couple options you can use

488
00:24:05,000 --> 00:24:08,000
when you're kind of going between C++ and Python.

489
00:24:08,000 --> 00:24:09,000
So Scython is one of them.

490
00:24:09,000 --> 00:24:11,000
Swig is another one that's also,

491
00:24:11,000 --> 00:24:15,000
there's an example in this repo.

492
00:24:15,000 --> 00:24:17,000
So basically, once you find some way

493
00:24:17,000 --> 00:24:18,000
of creating your extension,

494
00:24:18,000 --> 00:24:20,000
then you need to use setup tools

495
00:24:20,000 --> 00:24:22,000
to compile it and link it together.

496
00:24:22,000 --> 00:24:27,000
And this code is also from that repository.

497
00:24:27,000 --> 00:24:29,000
So basically, in setup tools,

498
00:24:29,000 --> 00:24:31,000
in your basically setup.py,

499
00:24:31,000 --> 00:24:33,000
you create an extension.

500
00:24:33,000 --> 00:24:35,000
You can specify the sources,

501
00:24:35,000 --> 00:24:37,000
as well as how to compile it.

502
00:24:37,000 --> 00:24:40,000
And so you can see here

503
00:24:40,000 --> 00:24:44,000
that we're using the CUDA libraries.

504
00:24:44,000 --> 00:24:47,000
And you can see NVCC as well as GCC

505
00:24:47,000 --> 00:24:49,000
to basically compile all this together

506
00:24:49,000 --> 00:24:51,000
and package it up.

507
00:24:51,000 --> 00:24:55,000
And then that will basically link into our Python program.

508
00:24:55,000 --> 00:24:57,000
But I would encourage you,

509
00:24:57,000 --> 00:24:59,000
this part gets very complex.

510
00:24:59,000 --> 00:25:00,000
If you're interested in this,

511
00:25:00,000 --> 00:25:02,000
I would encourage you to check out that code,

512
00:25:02,000 --> 00:25:04,000
because those people put together

513
00:25:04,000 --> 00:25:06,000
some very complex tricks

514
00:25:06,000 --> 00:25:10,000
to actually be able to get all this to come together.

515
00:25:10,000 --> 00:25:12,000
So why would you want to do that?

516
00:25:12,000 --> 00:25:15,000
I just said it's really complex.

517
00:25:15,000 --> 00:25:16,000
One is manual memory management,

518
00:25:16,000 --> 00:25:18,000
which you might see as a downside.

519
00:25:18,000 --> 00:25:19,000
You're like, we're using Python.

520
00:25:19,000 --> 00:25:21,000
Why do I want to manage my own memory?

521
00:25:21,000 --> 00:25:23,000
And this is kind of the constructor

522
00:25:23,000 --> 00:25:27,000
of that GPU adder class.

523
00:25:27,000 --> 00:25:28,000
You see mallocs.

524
00:25:28,000 --> 00:25:31,000
You're like, whoa.

525
00:25:31,000 --> 00:25:33,000
And here's a destructor.

526
00:25:33,000 --> 00:25:35,000
So you have to call free.

527
00:25:35,000 --> 00:25:37,000
And heaven forbid you don't.

528
00:25:37,000 --> 00:25:39,000
But basically, why would you want this?

529
00:25:39,000 --> 00:25:41,000
But I think there is a benefit,

530
00:25:41,000 --> 00:25:44,000
because there's certain manual memory management features

531
00:25:44,000 --> 00:25:45,000
in CUDA.

532
00:25:45,000 --> 00:25:47,000
So some advanced things.

533
00:25:47,000 --> 00:25:49,000
Mapped memory, I think, is the most interesting,

534
00:25:49,000 --> 00:25:51,000
which is basically like you can map memory

535
00:25:51,000 --> 00:25:54,000
between your host, which is CPU again, and device,

536
00:25:54,000 --> 00:25:57,000
and basically be able to access it

537
00:25:57,000 --> 00:26:00,000
without having to do an explicit data transfer.

538
00:26:00,000 --> 00:26:01,000
And so that can be pretty cool.

539
00:26:01,000 --> 00:26:03,000
But basically, there's a couple features

540
00:26:03,000 --> 00:26:05,000
that might only be accessible

541
00:26:05,000 --> 00:26:08,000
once you get down to the level of doing this kind of stuff.

542
00:26:08,000 --> 00:26:09,000
The other thing is,

543
00:26:09,000 --> 00:26:10,000
and I know this might be heresy,

544
00:26:10,000 --> 00:26:11,000
because it's a Python conference,

545
00:26:11,000 --> 00:26:13,000
but you do get a compiler.

546
00:26:13,000 --> 00:26:15,000
And I would say the nice thing about that

547
00:26:15,000 --> 00:26:17,000
is you're writing in a language

548
00:26:17,000 --> 00:26:19,000
that's unfamiliar to you, right?

549
00:26:20,000 --> 00:26:21,000
At least for me,

550
00:26:21,000 --> 00:26:23,000
I don't do a ton of CUDA programming.

551
00:26:23,000 --> 00:26:26,000
Even when I use it, I'm mostly using Python.

552
00:26:26,000 --> 00:26:28,000
And so being able to have the compiler tell you

553
00:26:28,000 --> 00:26:30,000
this went wrong on these lines,

554
00:26:30,000 --> 00:26:32,000
I would say when I was working on tweaking

555
00:26:32,000 --> 00:26:34,000
this C extension stuff,

556
00:26:34,000 --> 00:26:37,000
I actually felt kind of grateful for the compiler.

557
00:26:37,000 --> 00:26:40,000
So that can be kind of nice.

558
00:26:40,000 --> 00:26:42,000
So to conclude,

559
00:26:42,000 --> 00:26:45,000
one, I want to talk a little bit about accessing a GPU.

560
00:26:45,000 --> 00:26:47,000
So you might be interested in playing around

561
00:26:47,000 --> 00:26:48,000
with some of this stuff yourself.

562
00:26:48,000 --> 00:26:50,000
One, Google Colab is really awesome.

563
00:26:50,000 --> 00:26:53,000
It's a browser-based notebook interface,

564
00:26:53,000 --> 00:26:54,000
and it's free,

565
00:26:54,000 --> 00:26:56,000
and it gives you access to a GPU.

566
00:26:56,000 --> 00:26:59,000
And so some of the more custom stuff,

567
00:26:59,000 --> 00:27:02,000
I don't think you'll be able to link extensions on it,

568
00:27:02,000 --> 00:27:04,000
but you can definitely download QPy

569
00:27:04,000 --> 00:27:06,000
and play around with that.

570
00:27:06,000 --> 00:27:09,000
And a Pi CUDA should also be able to install,

571
00:27:09,000 --> 00:27:11,000
and so that's free.

572
00:27:11,000 --> 00:27:13,000
And then even cloud GPU instances

573
00:27:13,000 --> 00:27:15,000
are starting to become accessible.

574
00:27:15,000 --> 00:27:18,000
So on AWS, you can get it for less than a dollar an hour

575
00:27:18,000 --> 00:27:20,000
if you just want to spend, you know,

576
00:27:20,000 --> 00:27:22,000
two or three or four hours playing around with that,

577
00:27:22,000 --> 00:27:24,000
or Google Cloud.

578
00:27:24,000 --> 00:27:25,000
Their pricing is a little bit different.

579
00:27:25,000 --> 00:27:26,000
This is for a GPU.

580
00:27:26,000 --> 00:27:28,000
I think you also need an instance to attach it to,

581
00:27:28,000 --> 00:27:31,000
but it still should be under a dollar an hour

582
00:27:31,000 --> 00:27:33,000
if this stuff kind of interests you

583
00:27:33,000 --> 00:27:36,000
and you want to play around with it.

584
00:27:36,000 --> 00:27:38,000
So then the last question I want to talk about

585
00:27:38,000 --> 00:27:39,000
is where do you go next?

586
00:27:39,000 --> 00:27:43,000
So you kind of have gone through this talk.

587
00:27:43,000 --> 00:27:46,000
Talk. I've submitted you to this talk.

588
00:27:46,000 --> 00:27:48,000
And what can you do with it?

589
00:27:48,000 --> 00:27:50,000
So one, I would say you could start to think about

590
00:27:50,000 --> 00:27:53,000
how you can apply CUDA and GPU programming to your workflow.

591
00:27:53,000 --> 00:27:57,000
Like I said, that's like a really active area of development.

592
00:27:57,000 --> 00:27:59,000
A lot of things coming out.

593
00:27:59,000 --> 00:28:02,000
And so especially if your work has anything to do

594
00:28:02,000 --> 00:28:04,000
with data pipelines, processing,

595
00:28:04,000 --> 00:28:06,000
or obviously machine learning and deep learning,

596
00:28:06,000 --> 00:28:09,000
but that's kind of already been done.

597
00:28:09,000 --> 00:28:11,000
So that's something to start thinking about.

598
00:28:11,000 --> 00:28:14,000
The other thing is now you have access to 4,000 cores.

599
00:28:14,000 --> 00:28:15,000
What do you do with it?

600
00:28:15,000 --> 00:28:16,000
So you can start learning a little bit more

601
00:28:16,000 --> 00:28:18,000
about parallel programming algorithms,

602
00:28:18,000 --> 00:28:21,000
how to make use of that most effectively,

603
00:28:21,000 --> 00:28:23,000
because you can do cooler things with it

604
00:28:23,000 --> 00:28:27,000
than just adding 10 million numbers to each other.

605
00:28:27,000 --> 00:28:30,000
And then the other thing is if this stuff kind of excites you,

606
00:28:30,000 --> 00:28:32,000
there's also a bunch of other kinds of devices

607
00:28:32,000 --> 00:28:33,000
to start thinking about.

608
00:28:33,000 --> 00:28:36,000
So people talk about the XPU concepts.

609
00:28:36,000 --> 00:28:39,000
So Google has TPUs, which are specifically for deep learning.

610
00:28:39,000 --> 00:28:41,000
People are starting to come up with all sorts of devices.

611
00:28:41,000 --> 00:28:44,000
And then you can kind of go a different direction too

612
00:28:44,000 --> 00:28:46,000
and start doing FPGA,

613
00:28:46,000 --> 00:28:48,000
which basically is building hardware

614
00:28:48,000 --> 00:28:51,000
to execute like certain algorithms.

615
00:28:51,000 --> 00:28:54,000
So there's a whole device side of it too

616
00:28:54,000 --> 00:28:56,000
and kind of getting beyond the CPU paradigm

617
00:28:56,000 --> 00:29:00,000
and figuring out, well, can I come up with a specialized device

618
00:29:00,000 --> 00:29:04,000
that's better than this kind of one single general smart thing?

619
00:29:04,000 --> 00:29:08,000
So that's what I would suggest if you found this to be interesting.

620
00:29:08,000 --> 00:29:09,000
Thank you so much.

621
00:29:09,000 --> 00:29:10,000
There's my Twitter.

622
00:29:10,000 --> 00:29:11,000
I'll probably post...

623
00:29:11,000 --> 00:29:14,000
I'll definitely post the slides and related code on there,

624
00:29:14,000 --> 00:29:17,000
so if you have any more questions.

625
00:29:17,000 --> 00:29:19,000
Thank you so much.

