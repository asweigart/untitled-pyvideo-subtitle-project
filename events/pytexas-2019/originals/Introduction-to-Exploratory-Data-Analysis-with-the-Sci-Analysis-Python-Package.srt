1
00:00:00,000 --> 00:00:07,000
And we have Chris here, who's a senior software engineer at Silicon Labs.

2
00:00:07,000 --> 00:00:09,000
He's also an instructor.

3
00:00:09,000 --> 00:00:15,000
So make sure you take notes, because if you want lunch, you've got to pass the quiz at the end of the talk.

4
00:00:15,000 --> 00:00:18,000
That's right. Thank you, Paul.

5
00:00:18,000 --> 00:00:21,000
So as Paul said, my name is Chris Morrow.

6
00:00:21,000 --> 00:00:27,000
I am here to talk to you today about a Python package I developed called SciAnalysis.

7
00:00:27,000 --> 00:00:32,000
So a little bit about me. I am a software developer at Silicon Labs.

8
00:00:32,000 --> 00:00:37,000
There's how you can get hold of me if you have questions afterwards.

9
00:00:37,000 --> 00:00:43,000
And then just a quick show of hands, who here has worked with data in any capacity?

10
00:00:43,000 --> 00:00:47,000
Even if it's not in Python. Okay, so quite a few people here.

11
00:00:47,000 --> 00:00:54,000
Okay, so usually working with data, at some point, you need to analyze that data, right?

12
00:00:54,000 --> 00:00:59,000
And if that's the case, then I think SciAnalysis might be able to help you out there.

13
00:00:59,000 --> 00:01:01,000
So what is SciAnalysis?

14
00:01:01,000 --> 00:01:09,000
It is a Python package I developed for quickly and easily performing data analysis.

15
00:01:09,000 --> 00:01:21,000
And it is a high-level wrapper around part of the PyData stack, such as Pandas, SciPy, and Matplotlib.

16
00:01:21,000 --> 00:01:27,000
And so you might be thinking, you know, these tools are great, why do we need one more, right?

17
00:01:27,000 --> 00:01:37,000
And I think really what differentiates SciAnalysis from these tools is that they are very general-focused,

18
00:01:37,000 --> 00:01:46,000
whereas SciAnalysis is very focused in terms of the analysis types it performs.

19
00:01:46,000 --> 00:01:55,000
It exposes a single function called analyze that you use to do all the analysis.

20
00:01:55,000 --> 00:02:03,000
Also, part of my motivation for creating this was to tackle three problems that I incurred

21
00:02:03,000 --> 00:02:08,000
and probably other people have incurred when working with the PyData stack.

22
00:02:08,000 --> 00:02:22,000
So these analysis types that SciAnalysis currently handles is the distribution analysis of continuous numeric data.

23
00:02:22,000 --> 00:02:29,000
It can also provide a bar chart and frequency analysis of categorical data,

24
00:02:29,000 --> 00:02:36,000
as well as bivariate or correlation between two numeric vectors.

25
00:02:36,000 --> 00:02:45,000
And then lastly, location testing and distribution between different groups of numeric data.

26
00:02:45,000 --> 00:02:48,000
Okay, so those problems I mentioned.

27
00:02:48,000 --> 00:02:58,000
Problem one, when I learned Python, it was initially to do data analysis based off of the recommendation of some friends.

28
00:02:58,000 --> 00:03:07,000
So the first thing I did was I went out and I bought the Wes McKinney Python for data analysis book.

29
00:03:07,000 --> 00:03:16,000
And I was coming from working in Scala, and so Python was actually a breath of fresh air in that sense.

30
00:03:16,000 --> 00:03:27,000
But working with Pandas and Matplotlib was tricky at first, and so I found myself referring back to the book quite a bit.

31
00:03:27,000 --> 00:03:37,000
So one thing that I wanted to accomplish here was I provide the analyze function,

32
00:03:37,000 --> 00:03:42,000
which is a single entry point into performing data analysis here.

33
00:03:42,000 --> 00:03:53,000
It is handling or rather abstracting away a lot of the functions that are being called by SciPy and Matplotlib

34
00:03:53,000 --> 00:04:01,000
and taking care of that for you so you don't have to remember or constantly go back to and refer to these functions.

35
00:04:01,000 --> 00:04:08,000
So let's take a look at what that looks like up here.

36
00:04:08,000 --> 00:04:21,000
So I'd mentioned these four different analysis types for performing or just looking at the distribution of an array of numbers.

37
00:04:21,000 --> 00:04:26,000
You just pass that into the analyze function pretty easy.

38
00:04:26,000 --> 00:04:36,000
Now, if you are passing in an array of strings, it will perform a frequency analysis for you.

39
00:04:36,000 --> 00:04:45,000
And then now with location testing, this is where it gets a little tricky depending on how your data is shaped.

40
00:04:45,000 --> 00:04:54,000
If, let's say for instance, you have individual columns in a Pandas data frame that you pass in,

41
00:04:54,000 --> 00:05:07,000
you could pass these in as a list or as a dictionary to perform some kind of analysis between these groups.

42
00:05:07,000 --> 00:05:17,000
Or if you're working with stacked data and what I mean there is you have a separate column that identifies the groups,

43
00:05:17,000 --> 00:05:26,000
then you just pass in the column of that data frame and then specify the groups.

44
00:05:26,000 --> 00:05:37,000
And then lastly, if we want to perform a bivariate or correlation analysis, then we just pass both those arrays in.

45
00:05:37,000 --> 00:05:46,000
So I know this is kind of high level right now. I'm going to go into more detail and show you some examples here in a bit.

46
00:05:46,000 --> 00:05:57,000
So now just one simple example here is what I did is I create a NumPy array of normally distributed random variables

47
00:05:57,000 --> 00:06:07,000
and then just pass that into the analyze function. And what sign analysis gives me is a graph of the histogram.

48
00:06:07,000 --> 00:06:16,000
I get a box plot and then optionally I told it to give me a cumulative distribution function here as well.

49
00:06:16,000 --> 00:06:25,000
And for each graph I get, I also get summary stats and it also tells me whether my data is normally distributed or not.

50
00:06:25,000 --> 00:06:34,000
So there's a little bit of input up there, but everything else is what you get whenever you use the analyze function.

51
00:06:35,000 --> 00:06:44,000
Okay, so let's move on to problem two. So unless you have a background in statistics,

52
00:06:44,000 --> 00:06:52,000
it's often difficult to know which hypothesis test you should use in certain cases.

53
00:06:52,000 --> 00:07:02,000
I mean, even for statisticians, this sometimes gets tricky and we like to argue over this quite often.

54
00:07:02,000 --> 00:07:12,000
So sign analysis is going to try to take care of that by choosing the most appropriate test given the data that you've supplied.

55
00:07:12,000 --> 00:07:25,000
So as an example, let's take a look at what that looks like. So this is the decision tree for performing the location testing.

56
00:07:25,000 --> 00:07:34,000
First thing it's going to do is see, did you provide more than two groups of data analyze?

57
00:07:34,000 --> 00:07:40,000
And if so, it's going to perform a one-way ANOVA if that data is normally distributed.

58
00:07:40,000 --> 00:07:47,000
Otherwise, it's going to perform a non-parametric Kruskal-Wallis test.

59
00:07:47,000 --> 00:07:55,000
And then if you're using two groups and both are normally distributed, it will perform a T-test.

60
00:07:55,000 --> 00:07:59,000
Otherwise, it goes down to Mann-Whitney-New test.

61
00:07:59,000 --> 00:08:10,000
And then if you have less than 20 samples in that data, then you're down to the least sensitive test, which is the KS2 sample test.

62
00:08:10,000 --> 00:08:17,000
All right. So problem three, working with missing data is tricky.

63
00:08:17,000 --> 00:08:27,000
And so in this example here, I am using Matplotlib to graph two lists.

64
00:08:27,000 --> 00:08:37,000
Each list is just values ranging from one to five, with the exception that one of them has the value three missing in it.

65
00:08:37,000 --> 00:08:49,000
So how Matplotlib handles this is when it encounters that missing value, you just get missing segment in the line there.

66
00:08:49,000 --> 00:08:58,000
Now in the case of, now let's say we wanted to try to do a best line fit on that, and we have a missing value.

67
00:08:58,000 --> 00:09:05,000
I pass that to the NumPy polyfit function, and it just gives me a value error, right?

68
00:09:05,000 --> 00:09:10,000
So Psi analysis can pretty seamlessly take care of this.

69
00:09:10,000 --> 00:09:22,000
I pass in the exact same lists to the analyze function, and it knows that there is a missing value in one of the lists,

70
00:09:22,000 --> 00:09:29,000
and then drops the value at the same index in the corresponding array.

71
00:09:30,000 --> 00:09:41,000
So you can see here where it's pointing out the linear regression, n is equal to four there, because it did drop the corresponding value.

72
00:09:41,000 --> 00:09:51,000
All right. So I'm going to stop talking about some of the motivation and purpose there, and I want to show you some actual examples.

73
00:09:51,000 --> 00:10:04,000
So what I've done is I want to try to answer the question, which city has the best weather out of Austin, Denver, Las Vegas, New York, and Seattle?

74
00:10:04,000 --> 00:10:15,000
And what I've done is I've put together a notebook that if you want to follow along, the notebook you can find on my GitHub.

75
00:10:15,000 --> 00:10:30,000
So it's github.com slash, oops, sure, OK, yeah, that's it right there, slash PyTexas underscore 2019.

76
00:10:30,000 --> 00:10:38,000
And you can view it there or you can clone the repo and work with it in a Jupyter notebook if you prefer.

77
00:10:38,000 --> 00:10:46,000
Alternatively, you can also open the notebook in Google CoLab or Binder.

78
00:10:46,000 --> 00:10:51,000
Anybody here use Google CoLab? A few hands? Yeah, I think it's good stuff.

79
00:10:51,000 --> 00:10:56,000
What about Binder? Anybody use Binder? OK, OK, one, one, so OK.

80
00:10:56,000 --> 00:11:01,000
Yeah, definitely if you haven't heard of Binder, go check it out. It's good stuff.

81
00:11:01,000 --> 00:11:14,000
OK, so what I've done for this notebook is I pulled a NOAA GSOD data set from Google Cloud.

82
00:11:14,000 --> 00:11:24,000
It's stored as a BigQuery table and let's see, it's a public data set under samples dot GSOD.

83
00:11:24,000 --> 00:11:28,000
There's a couple of them out there, so I wanted to specify which one I'm using there.

84
00:11:28,000 --> 00:11:38,000
Also, this table is 16 gigs, so I've limited the analysis to just the years 2005 to 2009.

85
00:11:38,000 --> 00:11:45,000
And then also best weather, I mean, come on, that's a super subjective thing, right?

86
00:11:45,000 --> 00:11:57,000
So I want to try to quantify that a little bit by adding the constraint that best weather is going to be the city with the maximum number of good weather days

87
00:11:57,000 --> 00:12:04,000
minus bad weather days. So again, still, good weather, how do we define that?

88
00:12:04,000 --> 00:12:13,000
So I'm going to take a stab at it and say that it is the average days per year where the temperature is between 60 and 80 degrees Fahrenheit

89
00:12:13,000 --> 00:12:22,000
plus the average number of days per year where the dew point is between 40 and 60 degrees Fahrenheit divided by 2.

90
00:12:22,000 --> 00:12:29,000
All right, so we're going to codify that into a function in our notebook.

91
00:12:29,000 --> 00:12:39,000
All right, so first thing is there's no cities in this data set, which I'll show in a minute.

92
00:12:39,000 --> 00:12:47,000
Instead, it lists all the data by the W band weather station that collected the data.

93
00:12:47,000 --> 00:12:59,000
So first thing it did is write a function here that is going to set the city by the weather station number in the data set.

94
00:12:59,000 --> 00:13:10,000
All right. And next, it's always a good idea when doing EDA to at least do some sanity checks upfront.

95
00:13:10,000 --> 00:13:20,000
Here I'm looking at the data frame and I can see that I have 9,081 columns, sorry, rows.

96
00:13:20,000 --> 00:13:29,000
And then here I'm just listing the column values or sorry, column names.

97
00:13:29,000 --> 00:13:33,000
And then I want to look at the data types of these columns to see what I'm working with.

98
00:13:34,000 --> 00:13:42,000
And here I can see I have a column for year, month, and day that are integers.

99
00:13:42,000 --> 00:13:47,000
I have a mean temperature column, which that's going to be useful.

100
00:13:47,000 --> 00:13:52,000
I also have mean dew point, which I can use for determining good weather.

101
00:13:52,000 --> 00:13:57,000
A little bit further down, I see I have a maximum temperature column too.

102
00:13:57,000 --> 00:14:02,000
So I have two different temperature columns, so I might want to see which one of those to use.

103
00:14:02,000 --> 00:14:12,000
And then I have several Boolean columns for fog, rain, snow, hail, thunder, and tornadoes,

104
00:14:12,000 --> 00:14:17,000
which I'm going to use to build up my bad weather list.

105
00:14:18,000 --> 00:14:31,000
So first things first, I use the analyze function on the city column to make sure I have equal numbers of data per city.

106
00:14:31,000 --> 00:14:40,000
So what I can see up there is that each city makes up about 20% of the data, which is great.

107
00:14:41,000 --> 00:14:50,000
Now let's repeat this process for year and month to make sure that those are close to equal as well.

108
00:14:50,000 --> 00:14:58,000
So again here for the years 2005 through 2009 have roughly the same amount of data.

109
00:14:58,000 --> 00:15:01,000
So far so good.

110
00:15:01,000 --> 00:15:11,000
Here month, looking at the months, there's some variation there by number of days per month, which we would expect.

111
00:15:11,000 --> 00:15:14,000
But again, this is looking good so far.

112
00:15:14,000 --> 00:15:20,000
So now I'd mentioned there was two different temperature columns.

113
00:15:20,000 --> 00:15:25,000
So let's see which one we might want to use here.

114
00:15:25,000 --> 00:15:30,000
So one of them is called mean temp, the other is called max temperature.

115
00:15:30,000 --> 00:15:35,000
So max temperature would make me think those values are higher.

116
00:15:35,000 --> 00:15:42,000
So it's a good thing that I didn't make that assumption because looking at it here,

117
00:15:42,000 --> 00:15:50,000
I can actually see that max temperature has a lower mean by like 10 degrees on here.

118
00:15:50,000 --> 00:16:01,000
Now this is something that I might have missed had I not plopped this data into psi analysis and saw this real quickly.

119
00:16:01,000 --> 00:16:13,000
Now since max temperature looks like a liar and I have a mean dew point column,

120
00:16:13,000 --> 00:16:20,000
I'm going to stick with mean temp for the rest of the analysis here.

121
00:16:20,000 --> 00:16:23,000
All right.

122
00:16:23,000 --> 00:16:32,000
So now we're just looking at the general distribution of mean temp here.

123
00:16:32,000 --> 00:16:41,000
Then we can also now group mean temp by city and look at it that way as well.

124
00:16:41,000 --> 00:16:50,000
We can see that Austin and Las Vegas have higher average temperatures, which we would expect.

125
00:16:50,000 --> 00:17:03,000
And now we can repeat the process for dew point by looking at the overall distribution of dew point and then again by city here.

126
00:17:03,000 --> 00:17:14,000
Okay. So now there is actually a relationship between temperature and dew point or at least so I'm told according to Wikipedia.

127
00:17:14,000 --> 00:17:16,000
So let's look at that correlation.

128
00:17:16,000 --> 00:17:27,000
And what we see is there appears to be a pretty good correlation there, but our best fit line doesn't look great.

129
00:17:27,000 --> 00:17:33,000
And we have all these kind of low flyers on there.

130
00:17:33,000 --> 00:17:38,000
So we can drill down into this a little bit more and kind of see what's going on.

131
00:17:38,000 --> 00:17:48,000
I can repeat this exact same analysis, but here I'm passing in groups is equal to our city column.

132
00:17:48,000 --> 00:17:54,000
And it's because the colors are a little tough to see there.

133
00:17:54,000 --> 00:18:02,000
It's Las Vegas has a lot of those low flyer points on there.

134
00:18:02,000 --> 00:18:10,000
So what I might want to do here is because Las Vegas and Denver are both a little bit lower.

135
00:18:10,000 --> 00:18:22,000
I can compare both those individually to New York, which actually has the highest correlation coefficient here.

136
00:18:22,000 --> 00:18:25,000
Okay. So here the red is New York.

137
00:18:25,000 --> 00:18:27,000
The blue is Las Vegas.

138
00:18:27,000 --> 00:18:31,000
And we can see there's there's actually a pretty big difference there.

139
00:18:31,000 --> 00:18:41,000
I think this is attributed to differences in relative humidity since Las Vegas is in the desert after all.

140
00:18:41,000 --> 00:18:49,000
And does have a lower relative humidity throughout most of the year.

141
00:18:49,000 --> 00:18:52,000
And, you know, a similar thing for Denver here as well.

142
00:18:52,000 --> 00:18:55,000
I can kind of see and there it's not so much.

143
00:18:55,000 --> 00:19:00,000
I mean, Denver is definitely not the desert, but it is at higher altitude.

144
00:19:00,000 --> 00:19:06,000
So I think that is what kind of explains the data there.

145
00:19:06,000 --> 00:19:10,000
So now we're getting close to wrapping up.

146
00:19:10,000 --> 00:19:24,000
I create a function to define my bad weather, which is if any of the columns rain, fog, snow, hail, thunder, tornado are true, then I mark that as bad weather for that day.

147
00:19:24,000 --> 00:19:34,000
I also have functions for determining the temperate climate, which is between 60 and 80 degrees.

148
00:19:34,000 --> 00:19:45,000
And then also kind of the comfortable humidity level between 40 and 60 degrees and apply those to the data frame.

149
00:19:45,000 --> 00:19:52,000
And then I finally summarize all this by grouping by city and year.

150
00:19:52,000 --> 00:19:53,000
By city and year.

151
00:19:53,000 --> 00:20:01,000
And so since I'm looking at five years worth of data, what I do is I get the total count per city per year throughout all that.

152
00:20:01,000 --> 00:20:07,000
And then next is to average that across the five years by city.

153
00:20:07,000 --> 00:20:13,000
And then finally apply our best weather formula there.

154
00:20:13,000 --> 00:20:18,000
So anybody want to guess which city actually has the best weather?

155
00:20:18,000 --> 00:20:22,000
Just go ahead and yell it out.

156
00:20:22,000 --> 00:20:23,000
OK, I hear Vegas.

157
00:20:23,000 --> 00:20:28,000
Any other guesses?

158
00:20:28,000 --> 00:20:30,000
I heard Seattle, Austin.

159
00:20:30,000 --> 00:20:32,000
OK, so San Diego.

160
00:20:32,000 --> 00:20:37,000
Honestly, San Diego probably is true, but it's not in here.

161
00:20:37,000 --> 00:20:40,000
So that was kind of on purpose.

162
00:20:40,000 --> 00:20:48,000
OK, so the answer, according to our data here, is Seattle.

163
00:20:48,000 --> 00:20:51,000
I was shocked, too, actually.

164
00:20:51,000 --> 00:20:58,000
It turns out if I had used max temperature, then the answer is Austin.

165
00:20:58,000 --> 00:21:06,000
But here really what's doing it for Seattle is the dew point.

166
00:21:06,000 --> 00:21:26,000
And I mean, now Seattle does have the most bad weather days, but I guess it has so many days where the humidity is between 40 and 60 degrees Fahrenheit that it edges out Austin here by a little bit.

167
00:21:26,000 --> 00:21:28,000
So that's actually all I have.

168
00:21:28,000 --> 00:21:34,000
This is just the highlights of the notebook.

169
00:21:34,000 --> 00:21:42,000
Again, if you would like to play around with the data in the notebook, I encourage you to do that to learn more about how to use psi analysis.

170
00:21:42,000 --> 00:21:56,000
And thank you.

171
00:21:56,000 --> 00:21:58,000
So we have a little bit of time for questions.

172
00:21:58,000 --> 00:21:59,000
Does anyone have a question?

173
00:21:59,000 --> 00:22:09,000
Just raise your hand and I'll run out to you.

174
00:22:09,000 --> 00:22:13,000
Just out of curiosity, is this your first package that you're in?

175
00:22:13,000 --> 00:22:16,000
Yes, it is actually.

176
00:22:16,000 --> 00:22:22,000
Thank you.

177
00:22:22,000 --> 00:22:27,000
Just along those lines, approximately how long have you been working on this?

178
00:22:27,000 --> 00:22:31,000
Okay, so the question is how long I've been working on this.

179
00:22:31,000 --> 00:22:35,000
A while, actually.

180
00:22:35,000 --> 00:22:40,000
So it's I think I started in 2015.

181
00:22:40,000 --> 00:22:46,000
Back then it was just a single module of several functions.

182
00:22:46,000 --> 00:22:49,000
And it's gone through a few refactors already.

183
00:22:49,000 --> 00:22:53,000
So now it's actually object oriented.

184
00:22:53,000 --> 00:22:57,000
The code's still not as clean as I'd like it to be.

185
00:22:57,000 --> 00:23:06,000
So definitely if you want to check out my GitHub and look at it, you can see my trash there to quote the keynote.

186
00:23:06,000 --> 00:23:15,000
And so I've been working on it a little bit over the years here and there between work and video games and teaching.

187
00:23:15,000 --> 00:23:20,000
So yeah, it's still a work in progress.

188
00:23:20,000 --> 00:23:26,000
There's still more stuff that I want to add in there.

189
00:23:26,000 --> 00:23:35,000
And so it's pretty great, but I guess my main question is with that single entry point through the analyze function,

190
00:23:35,000 --> 00:23:41,000
there's no way to, how do you choose what you're looking for if you know what you're looking for?

191
00:23:41,000 --> 00:23:44,000
You know, I want this type of analysis versus another.

192
00:23:44,000 --> 00:23:48,000
It just depends on what you give it or is that being explained in the box or something?

193
00:23:48,000 --> 00:23:50,000
So that's a great question.

194
00:23:50,000 --> 00:23:56,000
And it's something that I've been thinking about a lot is behind the scenes,

195
00:23:56,000 --> 00:24:01,000
there is kind of an API for performing different analysis types.

196
00:24:01,000 --> 00:24:07,000
And the analyze function is basically just the logic for choosing which one to use.

197
00:24:07,000 --> 00:24:15,000
So I've been thinking about creating the docs for that so you can go and use the one you specifically want.

198
00:24:15,000 --> 00:24:21,000
But now kind of what I'm toying around with is actually creating preferences.

199
00:24:21,000 --> 00:24:32,000
And so you could optionally create a serialized preference file to disk that it can read from that.

200
00:24:32,000 --> 00:24:40,000
Or alternatively, you could pass in some arguments to kind of set your preferences for the rest of the session.

201
00:24:40,000 --> 00:24:46,000
So in that way, like say you don't want to use the Kruskal-Wallis test, you have something against that.

202
00:24:46,000 --> 00:24:52,000
Then you can set the preference for the test you want to use and it will just always use that one every time.

203
00:24:56,000 --> 00:24:58,000
You answered this question.

204
00:24:58,000 --> 00:24:59,000
Oh, great.

205
00:24:59,000 --> 00:25:01,000
Any other questions?

206
00:25:01,000 --> 00:25:15,000
Oh, that's a great question.

207
00:25:15,000 --> 00:25:19,000
So I'm currently squishing bugs.

208
00:25:19,000 --> 00:25:26,000
But also, so I showed that I can do some analysis of categorical data.

209
00:25:26,000 --> 00:25:29,000
That is the next big expansion.

210
00:25:29,000 --> 00:25:38,000
So one feature I've been working on for a while now is actually grouping, being able to do grouped categorical analysis.

211
00:25:38,000 --> 00:25:41,000
And then also some support in there for sampling.

212
00:25:41,000 --> 00:25:48,000
So that might enable analyzing polling data, for example.

213
00:25:48,000 --> 00:25:51,000
That's something that I've really been interested in.

214
00:25:51,000 --> 00:26:07,000
Also, the other two big features are going to be being able to create like a heat map or a matrix plot of multiple numeric pieces of data.

215
00:26:07,000 --> 00:26:09,000
And then also time series analysis.

216
00:26:09,000 --> 00:26:15,000
That's a big one that I'm pretty excited for and to get started on.

217
00:26:15,000 --> 00:26:20,000
But that I think is going to take me a while to get all the way through that.

218
00:26:20,000 --> 00:26:22,000
Any other questions?

219
00:26:22,000 --> 00:26:24,000
Okay, let's give a round of applause.

