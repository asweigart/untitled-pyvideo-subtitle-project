1
00:00:00,000 --> 00:00:03,000
Can everybody hear me?

2
00:00:03,000 --> 00:00:04,000
Okay.

3
00:00:04,000 --> 00:00:05,000
All right.

4
00:00:05,000 --> 00:00:10,000
So as Paul introduced me, I'm Pavan Agarwal, and I am a high school student from Houston,

5
00:00:10,000 --> 00:00:11,000
Texas.

6
00:00:11,000 --> 00:00:19,500
So recently, I had an awesome opportunity to work as a full stack developer in a local

7
00:00:19,500 --> 00:00:22,640
business near my house.

8
00:00:22,640 --> 00:00:27,080
Through this job, I was able to gain a lot of experience with serverless tech, non-serverless

9
00:00:27,080 --> 00:00:30,080
deployments, and Python, JavaScript in general.

10
00:00:30,080 --> 00:00:33,580
Also, if you guys want to follow me on Twitter, there's my handle.

11
00:00:33,580 --> 00:00:39,080
I have one follower right now, so if you guys want to be the second or third, please join

12
00:00:39,080 --> 00:00:40,080
me.

13
00:00:40,080 --> 00:00:41,080
Okay.

14
00:00:41,080 --> 00:00:44,760
So before I begin this talk, I want to give the disclaimer, I'm only going to be focusing

15
00:00:44,760 --> 00:00:49,440
on the style of serverless that involves deploying pure code to cloud providers.

16
00:00:49,440 --> 00:00:55,820
I know GCP recently came out with Cloud Run that allows you to do serverless Docker containers.

17
00:00:55,820 --> 00:00:56,880
This is not going to be that.

18
00:00:56,880 --> 00:00:59,280
This is just pure function code.

19
00:00:59,280 --> 00:01:00,280
Okay.

20
00:01:00,280 --> 00:01:04,240
So I know that we had a good talk about serverless programming before this, but I just want to

21
00:01:04,240 --> 00:01:08,360
give a quick overview about what serverless programming is.

22
00:01:08,360 --> 00:01:11,080
So for starters, it's actually not serverless.

23
00:01:11,080 --> 00:01:15,700
No, cloud providers didn't figure out how to bank some rocks together in the cloud to

24
00:01:15,700 --> 00:01:17,720
generate some magical output.

25
00:01:17,720 --> 00:01:22,280
Your cloud code is still running on normal, everyday servers in the cloud.

26
00:01:22,280 --> 00:01:25,660
In reality, it's just more of a fancy abstraction.

27
00:01:25,660 --> 00:01:28,520
I think this meme does a pretty good job of explaining serverless.

28
00:01:28,520 --> 00:01:34,300
It's just a bunch of abstractions layered over abstractions.

29
00:01:34,300 --> 00:01:39,560
So to start the abstraction process, we're getting an abstraction over the platform.

30
00:01:39,560 --> 00:01:42,900
You're not dealing with, oh, is this Python runtime secure?

31
00:01:42,900 --> 00:01:44,860
Is it completely patched?

32
00:01:44,860 --> 00:01:49,420
You're not dealing with the scaling of the abstraction either.

33
00:01:49,420 --> 00:01:54,500
Usually when you provide code to the serverless platform, they are implicitly giving you a

34
00:01:54,540 --> 00:02:00,860
contract saying that they will increase the number of instances running your code at any

35
00:02:00,860 --> 00:02:05,780
time in order to ensure that everybody has a good user experience.

36
00:02:05,780 --> 00:02:10,020
Also, they're abstracting the runtime and the state so you don't get to persist any

37
00:02:10,020 --> 00:02:14,140
data on the servers running your code.

38
00:02:14,140 --> 00:02:18,340
In essence, you're providing a piece of code to a cloud provider and expecting them to

39
00:02:19,340 --> 00:02:25,140
the scaling of it and serving of it in order to keep it updated, patched, and deployed

40
00:02:25,140 --> 00:02:27,700
for everybody.

41
00:02:27,700 --> 00:02:31,700
Most cloud providers already have really great serverless runtime platforms.

42
00:02:31,700 --> 00:02:35,820
AWS has the Lambda platform, which has been around for the longest.

43
00:02:35,820 --> 00:02:39,500
Azure and GCP have their own function runtime environment.

44
00:02:39,500 --> 00:02:43,380
One thing you'll notice when using these runtime environments is that you'll run into vendor

45
00:02:43,380 --> 00:02:45,300
lock-in issues.

46
00:02:45,380 --> 00:02:52,380
In order to avoid this, a lot of open-source projects exist that expose a common framework

47
00:02:52,580 --> 00:02:55,980
that you can type into to make your serverless applications.

48
00:02:55,980 --> 00:03:02,980
On the JavaScript side, you have serverless, or SLS, that supports Azure, AWS, GCP, along

49
00:03:02,980 --> 00:03:07,540
with a lot of other serverless runtimes, and it works pretty well for JavaScript-based

50
00:03:07,540 --> 00:03:08,900
services.

51
00:03:08,900 --> 00:03:14,100
With Python, you have Zappa, and if I'm being honest, the first time I saw Zappa, I instantly

52
00:03:14,100 --> 00:03:15,940
knew I was going to love it.

53
00:03:15,940 --> 00:03:21,340
Zappa allows any Python developer to turn their Flask, Django, or Bottle apps directly

54
00:03:21,340 --> 00:03:22,980
into a serverless app.

55
00:03:22,980 --> 00:03:27,380
Currently, it only supports AWS, but hopefully in the future, they're going to support more

56
00:03:27,380 --> 00:03:28,940
cloud providers.

57
00:03:28,940 --> 00:03:33,300
Okay, so that's enough understanding about what serverless is.

58
00:03:33,300 --> 00:03:35,900
Let's talk about a quick hypothetical scenario.

59
00:03:35,900 --> 00:03:39,980
So imagine one day you want to start a fun company with you and a friend.

60
00:03:40,020 --> 00:03:45,540
You decide to call the startup, Super, obviously no relation to the ride-sharing company.

61
00:03:45,540 --> 00:03:51,540
Super specializes in giving rides as a service in submarines all around the world.

62
00:03:51,540 --> 00:03:55,900
So in order to start the company off right and quickly, you code the application in Django

63
00:03:55,900 --> 00:03:59,020
and deploy it to an EC2 instance running in the cloud.

64
00:03:59,020 --> 00:04:03,900
An EC2 instance is basically just what a VM, AWS calls.

65
00:04:03,900 --> 00:04:07,540
A few months pass, and it turns out nobody's really using Super.

66
00:04:07,540 --> 00:04:12,300
I guess the market for rentable submarines just isn't that big.

67
00:04:12,300 --> 00:04:17,780
The $100 per month you've been paying to Amazon for the EC2 instance has just gone to waste.

68
00:04:17,780 --> 00:04:21,660
But you decide to keep this running just in case people start to use the service.

69
00:04:21,660 --> 00:04:25,420
A few weeks pass again, and you receive a really exciting email.

70
00:04:25,420 --> 00:04:29,340
Super's been accepted into Y Combinator, and it's being published about in all the hottest

71
00:04:29,340 --> 00:04:33,420
newspapers, tech blogs, it's on the top of Hacker News, and it's on the top of Reddit

72
00:04:33,420 --> 00:04:34,740
all at the same time.

73
00:04:34,740 --> 00:04:36,700
Super's just exploding.

74
00:04:36,820 --> 00:04:40,820
But you go back to check and see how your EC2 instance is doing to hold up against this

75
00:04:40,820 --> 00:04:41,820
traffic.

76
00:04:41,820 --> 00:04:43,260
Oh no, it's completely crashed.

77
00:04:43,260 --> 00:04:45,020
You don't have any traffic.

78
00:04:45,020 --> 00:04:49,540
All the people are just running into tons of errors across all your APIs.

79
00:04:49,540 --> 00:04:54,380
You try to scale up the instance to one that's $10,000 per month and has a terabyte of RAM,

80
00:04:54,380 --> 00:04:57,140
but it's just not enough to hold the fork down.

81
00:04:57,140 --> 00:05:00,740
You add in CloudFir for caching, but your API is just slowing the entire application

82
00:05:00,740 --> 00:05:01,740
down.

83
00:05:01,780 --> 00:05:05,380
You miss all the potential traffic, and Super is dissolved as a company.

84
00:05:05,380 --> 00:05:11,180
Okay, but let's try again Super with some serverless programming.

85
00:05:11,180 --> 00:05:16,420
So this time when you deploy Super with Django, you're choosing to deploy it with AWS Lambda

86
00:05:16,420 --> 00:05:18,260
and the Zappa framework.

87
00:05:18,260 --> 00:05:23,500
It still doesn't get any initial customers, but once there is that huge surge of traffic,

88
00:05:23,500 --> 00:05:28,420
it's still okay, because Lambda's taking care of the horizontal scaling for your application

89
00:05:28,540 --> 00:05:32,740
that everyone is still able to log into the application and use it.

90
00:05:32,740 --> 00:05:36,180
So everyone is so impressed with your subsharing site that you become a billionaire and buy

91
00:05:36,180 --> 00:05:40,180
a private island of the Caribbean where you never have to think about code again.

92
00:05:40,180 --> 00:05:43,000
Won't that be awesome?

93
00:05:43,000 --> 00:05:49,060
So the Super example really shows us what the pros of serverless deployment are.

94
00:05:49,060 --> 00:05:52,140
Serverless deployment comes with huge billing benefits.

95
00:05:52,140 --> 00:05:56,620
Not only did you not pay for much when nobody was using your site, you didn't have to scale

96
00:05:56,660 --> 00:06:01,300
it to a ridiculous EC2 instance just to handle some new traffic.

97
00:06:01,300 --> 00:06:06,980
Lambda did all that for you in the background while still minimizing your cost.

98
00:06:06,980 --> 00:06:11,620
In the months nobody used Super, the AWS bill was less than a few dollars, and once people

99
00:06:11,620 --> 00:06:16,820
actually start using it a bunch, the bill is still pretty manageable.

100
00:06:16,820 --> 00:06:21,660
The biggest advantage with serverless programming we're seeing is horizontal scalability.

101
00:06:21,700 --> 00:06:26,580
As we saw with Super, you don't need to put in any extra effort to get that beautiful

102
00:06:26,580 --> 00:06:29,380
horizontal automatic scaling.

103
00:06:29,380 --> 00:06:34,380
Furthermore, because a lot of the struct was abstracted away, you didn't need to worry

104
00:06:34,380 --> 00:06:37,060
about how the code is hosted.

105
00:06:37,060 --> 00:06:41,700
You don't have to worry about whether NGINX is correctly configured, if it's up to date,

106
00:06:41,700 --> 00:06:46,060
if there's any zero days available, because hopefully the cloud provider you've chosen

107
00:06:46,060 --> 00:06:51,500
is doing a good job at managing that for you.

108
00:06:52,340 --> 00:06:56,380
But serverless really isn't all benefits, and anybody who tells you that is just lying

109
00:06:56,380 --> 00:07:01,060
to you, every single abstraction is going to come with downsides.

110
00:07:01,060 --> 00:07:03,980
Specifically for serverless, you're going to lose a lot of the control you get over

111
00:07:03,980 --> 00:07:04,980
the stack.

112
00:07:04,980 --> 00:07:09,820
So, for example, if you have a magical patch to the Python runtime that is able to speed

113
00:07:09,820 --> 00:07:14,740
it up by like two times the speed, but it's not getting merged into upstream Python any

114
00:07:14,740 --> 00:07:19,020
time soon, you're not going to be able to use this for your serverless application because

115
00:07:19,020 --> 00:07:23,620
you're stuck with the runtime that your cloud provider is providing for you.

116
00:07:23,620 --> 00:07:30,100
Furthermore, the performance story that serverless offers isn't always perfect.

117
00:07:30,100 --> 00:07:35,620
The scaling algorithm that cloud providers use is usually proprietary to their own service,

118
00:07:35,620 --> 00:07:39,060
so if you don't like the strategy they're adopting for scaling, you don't really have

119
00:07:39,060 --> 00:07:42,940
much ability to change it or modify it to your needs.

120
00:07:43,340 --> 00:07:49,060
Also, the instances that are serving your code are also limited in the specs that they

121
00:07:49,060 --> 00:07:50,060
can have.

122
00:07:50,060 --> 00:07:55,900
In AWS, this is a limitation of about three gigs of RAM and 512 megs of temporary disk

123
00:07:55,900 --> 00:07:58,540
storage space.

124
00:07:58,540 --> 00:08:03,540
Specifically for AWS, their deployment packages are usually limited to half a gig, so if you

125
00:08:03,540 --> 00:08:07,140
were thinking about serving that new hot new machine learning algorithm that fits in a

126
00:08:07,140 --> 00:08:12,340
gig of storage, you're not going to be able to do that on a serverless platform.

127
00:08:12,340 --> 00:08:16,940
As a general side note, some of these limitations vary from provider to provider, so you're

128
00:08:16,940 --> 00:08:22,900
going to need to do your own research and determine whether that provider will be right

129
00:08:22,900 --> 00:08:27,140
for you based on their instance sizes.

130
00:08:27,140 --> 00:08:30,340
And now we have to confront the elephant in the room.

131
00:08:30,340 --> 00:08:33,380
Cold starts are really hard with serverless programming.

132
00:08:33,380 --> 00:08:38,900
Basically, whenever your application goes unused for a period of time determined by

133
00:08:38,900 --> 00:08:44,180
the cloud provider, your application that the instance that's serving your application

134
00:08:44,180 --> 00:08:48,180
will be shut down in order to conserve their cost.

135
00:08:48,180 --> 00:08:53,820
So upon the next invocation, the cloud provider will once again start your app from a cold

136
00:08:53,820 --> 00:08:58,860
state which is going to increase the latency of that start.

137
00:08:58,860 --> 00:09:03,700
This time usually depends on the bundle of your code and what language you're using to

138
00:09:03,700 --> 00:09:09,900
code it in, but I've seen it to be around two seconds per cold boot, and if end users

139
00:09:09,900 --> 00:09:16,780
are curing these APIs, that could lead to a pretty poor user experience.

140
00:09:16,780 --> 00:09:21,580
Now there are ways to avoid this by using scheduled tasks to keep your instances warm,

141
00:09:21,580 --> 00:09:28,180
but you have to pay an increased cost in your serverless work environments in order to avoid

142
00:09:28,180 --> 00:09:31,500
this cold start issue, but it's not exactly a huge hit.

143
00:09:31,500 --> 00:09:33,900
And we're going to discuss more about this in some future slides.

144
00:09:33,900 --> 00:09:36,420
All right, that's enough theory.

145
00:09:36,420 --> 00:09:40,740
Let's actually look at some cool implementations of a serverless function.

146
00:09:40,740 --> 00:09:45,380
For this demo, I chose to create a really simple API that allows users to crack three

147
00:09:45,380 --> 00:09:47,740
character long MD5 hashes.

148
00:09:47,740 --> 00:09:53,020
It's done with brute force, so we're not using any rainbow tables or pre-cached computations.

149
00:09:53,020 --> 00:09:57,740
In other words, we're assuming that the hashes are salted with a known salt.

150
00:09:57,820 --> 00:10:01,660
I'm not saying this code is efficient at all, and it should never be used in production

151
00:10:01,660 --> 00:10:07,060
because it's really slow, but it's an example that shows how powerful serverless is.

152
00:10:07,060 --> 00:10:13,700
I'm exposing the API over Flask, and I'm using Zappa for the Lambda deployment.

153
00:10:13,700 --> 00:10:18,580
So I'm going to first start this off by just starting my normal Flask app.

154
00:10:18,580 --> 00:10:19,580
It's really simple.

155
00:10:19,580 --> 00:10:24,180
It's just like 15 lines of code, and then I have a second compute script that allows

156
00:10:24,180 --> 00:10:27,480
me to test any API that I have exposed.

157
00:10:27,480 --> 00:10:32,880
So I pass in what endpoint that I want to hit with my API tester, and right now I'm

158
00:10:32,880 --> 00:10:36,560
just going to query right directly to my laptop.

159
00:10:36,560 --> 00:10:43,480
And so we'll see that once the query finishes, it takes about one second to break one hash,

160
00:10:43,480 --> 00:10:46,200
which is not bad.

161
00:10:46,200 --> 00:10:51,100
And now we're going to try running the same script, but instead run it 10 times.

162
00:10:51,100 --> 00:10:55,840
This is taking about six seconds on my laptop.

163
00:10:55,840 --> 00:11:01,200
Now for the next experiment, I'm going to hit the API that I had deployed in the cloud.

164
00:11:01,200 --> 00:11:03,880
Once again, as I mentioned before, it's the same exact code.

165
00:11:03,880 --> 00:11:09,240
It's just deployed using AWS Lambda and the Zappa framework.

166
00:11:09,240 --> 00:11:12,960
Once it finishes the computation, you'll see that this actually ran a little bit slower

167
00:11:12,960 --> 00:11:14,760
than on my laptop.

168
00:11:14,760 --> 00:11:20,360
That's partly because of the cold start cost I'm paying on the first execution of the API.

169
00:11:21,120 --> 00:11:26,920
AWS needs to boot the instance that's running this API in order to serve the request for

170
00:11:26,920 --> 00:11:29,200
my local script.

171
00:11:29,200 --> 00:11:33,520
Also the compute script uses randomized test cases, and I got kind of unlucky with my test

172
00:11:33,520 --> 00:11:38,040
cases, but we don't have to mention that part.

173
00:11:38,040 --> 00:11:41,440
So now that our function's warmed up, let's try something interesting.

174
00:11:41,440 --> 00:11:44,360
Let's try to break a thousand hashes.

175
00:11:44,360 --> 00:11:49,600
So I'm going to attack the API with 200 concurrent requests, and you're going to see that once

176
00:11:49,640 --> 00:11:53,720
the script finishes, it does this super fast.

177
00:11:53,720 --> 00:11:59,360
In total, it's going to take 21 seconds to break these thousand hashes, and this is with

178
00:11:59,360 --> 00:12:02,760
changing literally nothing with my code.

179
00:12:02,760 --> 00:12:08,480
AWS is just scaling out all these instances for me invisibly.

180
00:12:08,480 --> 00:12:15,040
This really shows the power of the horizontal scaling AWS Lambda's offering to the application

181
00:12:15,040 --> 00:12:16,040
I've deployed.

182
00:12:16,520 --> 00:12:21,000
What took six seconds to compute ten times on my laptop was computed a thousand times

183
00:12:21,000 --> 00:12:24,000
in the cloud in less than 20 seconds.

184
00:12:24,000 --> 00:12:27,560
As a comparison, if I tried to compute these thousand hashes on my computer, it'd take

185
00:12:27,560 --> 00:12:32,040
a total of ten minutes, and would you guys really enjoy watching me compute a thousand

186
00:12:32,040 --> 00:12:33,960
hashes for ten minutes in this demo?

187
00:12:33,960 --> 00:12:34,960
No.

188
00:12:34,960 --> 00:12:36,680
That's why we have to use serverless.

189
00:12:36,680 --> 00:12:42,520
So all in all, we went through a billion hash combinations in 20 seconds.

190
00:12:42,520 --> 00:12:45,500
Let's quickly discuss about what exactly made it faster.

191
00:12:45,500 --> 00:12:50,620
When I was testing the API locally running on my computer, I had my compute high script

192
00:12:50,620 --> 00:12:54,900
call the API running on the local Flask instance on my laptop.

193
00:12:54,900 --> 00:12:59,940
I wasn't pulling any concurrent responses because my Flask API does not serve concurrently.

194
00:12:59,940 --> 00:13:04,020
It's blocked by any other previous request.

195
00:13:04,020 --> 00:13:09,240
After I switched to calling the API in the cloud, it started spawning more and more instances

196
00:13:09,240 --> 00:13:11,900
to serve the API request.

197
00:13:12,200 --> 00:13:17,940
Specifically, I told my compute script to concurrently call the API 200 times.

198
00:13:17,940 --> 00:13:23,900
Then AWS invisibly scaled the number of instances running the script in hopes to serve the traffic

199
00:13:23,900 --> 00:13:26,660
I was driving from my computer.

200
00:13:26,660 --> 00:13:31,020
This is known as horizontal scaling, where we increase the number of instances running

201
00:13:31,020 --> 00:13:33,180
our code.

202
00:13:33,180 --> 00:13:36,860
From what I'm aware right now, Lambda doesn't attempt to do any vertical scaling, which

203
00:13:36,860 --> 00:13:41,860
is what we tried with scooper and increasing the amount of RAM we get for the application.

204
00:13:41,900 --> 00:13:46,420
It's only attempting a horizontal scale out strategy.

205
00:13:46,420 --> 00:13:48,780
So how much money did I just waste during that demo?

206
00:13:48,780 --> 00:13:49,780
It must be a ton, right?

207
00:13:49,780 --> 00:13:52,220
No, it's actually a super small amount.

208
00:13:52,220 --> 00:13:56,940
Conveniently, it just fits into the free tier of AWS, but if it didn't, it would be a whopping

209
00:13:56,940 --> 00:14:02,700
three cents to run that entire experiment to compute hashes a billion times.

210
00:14:02,700 --> 00:14:09,860
AWS specifically bills by the gigabyte seconds you use during the computation of your script.

211
00:14:09,860 --> 00:14:16,300
From some calculations, I used about 1.7 terabyte seconds, and that equates to three cents

212
00:14:16,300 --> 00:14:19,420
of cost.

213
00:14:19,420 --> 00:14:22,660
And the time I spent writing that out was actually very short.

214
00:14:22,660 --> 00:14:27,060
It only took about 10 minutes to write out the Flask API and 10 minutes to deploy with

215
00:14:27,060 --> 00:14:28,340
Zappa.

216
00:14:28,340 --> 00:14:34,220
If I had tried doing that with VM scale sets, trying to deploy in Kubernetes with automatic

217
00:14:34,220 --> 00:14:38,460
scale out, it would have taken way longer, but the power of serverless just allowed me

218
00:14:38,460 --> 00:14:40,980
to do that in less than 20 minutes.

219
00:14:40,980 --> 00:14:43,980
It was a super easy project.

220
00:14:43,980 --> 00:14:47,900
Now that we've seen a good implementation usage of serverless design, let's look at

221
00:14:47,900 --> 00:14:51,900
a scenario where serverless didn't really work that well.

222
00:14:51,900 --> 00:14:55,860
As I explained at the beginning of this presentation, during the summer, I got to work with the

223
00:14:55,860 --> 00:15:00,040
local business as a full-stack developer.

224
00:15:00,040 --> 00:15:04,020
During that time, I was talking with the business development team at the location about how

225
00:15:04,020 --> 00:15:07,940
the IT team could help support the business as it grows.

226
00:15:08,420 --> 00:15:13,420
The business was using a very legacy application programmed in PHP.

227
00:15:13,420 --> 00:15:15,140
I don't know what that is anymore.

228
00:15:15,140 --> 00:15:19,460
And it was slow to respond to any application.

229
00:15:19,460 --> 00:15:26,060
Furthermore, other more legacy systems were integrating with that business app and sending

230
00:15:26,060 --> 00:15:31,360
bursts to that API that were slowing down the application for everybody.

231
00:15:31,360 --> 00:15:37,340
Because of this, end users were having a very bad experience with the entire business app.

232
00:15:37,340 --> 00:15:42,740
So I sat down with a team of four developers, and we started rewriting this backend in

233
00:15:42,740 --> 00:15:46,940
a more modern stack with JavaScript and TypeScript.

234
00:15:46,940 --> 00:15:50,860
After writing the app, though, we realized serverless might be a good choice to use with

235
00:15:50,860 --> 00:15:54,100
the backend.

236
00:15:54,100 --> 00:16:00,740
We were motivated to adopt a serverless design because a lot of the API queries that were

237
00:16:00,740 --> 00:16:06,620
hitting the backend were event-driven, and they required horizontal scaling for the processing.

238
00:16:06,620 --> 00:16:12,380
The legacy systems would get an influx of data and start hitting the business app directly,

239
00:16:12,380 --> 00:16:16,940
and because of that slowdown, we thought that implementing this in the serverless design

240
00:16:16,940 --> 00:16:23,220
would allow invisible scaling in the backend to help alleviate the issues we were facing.

241
00:16:23,220 --> 00:16:29,860
Also, during the day, the business app sometimes goes unused, but then at other times, the

242
00:16:29,860 --> 00:16:36,000
users just flood onto the system, once again hammering the slow server running the backend.

243
00:16:36,000 --> 00:16:41,280
Going by this, we could theoretically reduce our hosting costs by a lot if we went with

244
00:16:41,280 --> 00:16:46,680
the consumption-style plan of serverless programming.

245
00:16:46,680 --> 00:16:53,080
So like any modern development shop, we started work with a JavaScript-based backend deployed

246
00:16:53,080 --> 00:16:58,320
to the one and only AWS, but this is where we ran into some issues.

247
00:16:58,320 --> 00:17:03,920
Once we had deployed to a serverless backend, users were having really bad cold starts on

248
00:17:03,920 --> 00:17:10,800
the UI, and because we had gone with the microservice style of deployment, we were

249
00:17:10,800 --> 00:17:15,640
not able to keep all the lambdas warm with the repeated invocations, as that would increase

250
00:17:15,640 --> 00:17:18,120
our AWS bill a lot.

251
00:17:18,120 --> 00:17:24,360
Also, every single time a user queried the serverless endpoint, those invocations were

252
00:17:24,360 --> 00:17:30,400
pretty slow because we weren't caching the connections to our SQL server running on-premises,

253
00:17:30,440 --> 00:17:36,080
so every time there was a request to the API, we would have to reconnect to the SQL server.

254
00:17:36,080 --> 00:17:41,240
Of course, there are ways to avoid this by spending some extra engineering time to ensure

255
00:17:41,240 --> 00:17:45,160
the safe management of all those connections, but being a small development shop, we didn't

256
00:17:45,160 --> 00:17:50,120
really have the time to go through this and actually adapt our code to the serverless

257
00:17:50,120 --> 00:17:52,200
environment.

258
00:17:52,200 --> 00:17:54,800
Another small issue we were running into was debugging.

259
00:17:54,800 --> 00:17:59,400
The debugging experience was just not as smooth as normal debugging experiences really

260
00:17:59,400 --> 00:18:00,400
are.

261
00:18:00,400 --> 00:18:05,880
Usually, local environment node-debugs are very easy to do, they're quick to launch,

262
00:18:05,880 --> 00:18:11,160
but once we coupled that with serverless technology, we'd have to use a lambda emulator, which

263
00:18:11,160 --> 00:18:15,560
was slowing down development workflow, and new things our developers just didn't know

264
00:18:15,560 --> 00:18:18,520
how to use right now.

265
00:18:18,520 --> 00:18:22,440
But we can't just consider the negatives of this serverless deployment.

266
00:18:22,440 --> 00:18:24,620
We should also consider the benefits.

267
00:18:24,620 --> 00:18:30,140
We were spending a lot less with this serverless style deployment instead of the VM we were

268
00:18:30,140 --> 00:18:32,800
using to host it before.

269
00:18:32,800 --> 00:18:38,420
Also the event-driven API that was ingesting data from legacy systems was scaling really

270
00:18:38,420 --> 00:18:44,260
well and we were experiencing no data loss, no retries, any of that stuff with the serverless

271
00:18:44,260 --> 00:18:45,260
backend.

272
00:18:45,260 --> 00:18:50,780
But these benefits, unfortunately, did not outweigh the issues that we were having.

273
00:18:50,780 --> 00:18:53,380
And this brings me to a really important point.

274
00:18:53,460 --> 00:18:57,700
Serverless isn't magic, and if you really think about it, it's not anything super new.

275
00:18:57,700 --> 00:19:02,380
It's just another abstraction in a world of abstractions that allows fast horizontal scaling

276
00:19:02,380 --> 00:19:04,500
and better billing models.

277
00:19:04,500 --> 00:19:09,420
Serverless is not going to solve all the problems from anybody, nor is it the perfect solution

278
00:19:09,420 --> 00:19:12,620
that everybody should just go and switch to right away.

279
00:19:12,620 --> 00:19:17,820
You'll need to really evaluate hot new tech individually in your own environment to determine

280
00:19:17,820 --> 00:19:22,700
if it's something that's going to benefit you and your development team, and if the

281
00:19:22,740 --> 00:19:26,700
benefits are actually going to outweigh the issues you might experience with this new

282
00:19:26,700 --> 00:19:29,140
technology.

283
00:19:29,140 --> 00:19:33,340
For our case, our solution ended up being to split out our backend into one that still

284
00:19:33,340 --> 00:19:39,780
processes our event-driven workflow in a serverless model and a user-driven backend to a slightly

285
00:19:39,780 --> 00:19:41,580
smaller VM in the cloud.

286
00:19:41,580 --> 00:19:46,540
So we were able to scale back the deployment resources that we were forcing for our user-driven

287
00:19:46,540 --> 00:19:52,180
backend, but still we were able to serve all the users pretty well.

288
00:19:52,180 --> 00:19:56,060
And so if you adopt new tech like serverless, you don't always have to go fully in.

289
00:19:56,060 --> 00:20:01,020
You can adopt some parts of it, and that's perfectly okay.

290
00:20:01,020 --> 00:20:05,860
If we had more time to engineer around all these limitations that we found with serverless

291
00:20:05,860 --> 00:20:10,900
deployment, it is entirely possible that we could have taken advantage of Lambdas for

292
00:20:10,900 --> 00:20:16,700
deployment, but this is something that we just don't have as a small development shop.

293
00:20:16,700 --> 00:20:22,500
Ironically, one of the benefits that serverless has is developers spending less time thinking

294
00:20:22,500 --> 00:20:24,820
about scaling operations.

295
00:20:24,820 --> 00:20:30,700
In some cases, that might be true, but for other cases, it just might not work out.

296
00:20:30,700 --> 00:20:35,700
If you choose to adopt serverless, you might end up spending more time engineering your

297
00:20:35,700 --> 00:20:41,580
solutions to work in a serverless-style environment, and that's going to take away from you doing

298
00:20:41,580 --> 00:20:45,140
what you do best, writing code.

299
00:20:45,140 --> 00:20:47,860
So here's a different scenario.

300
00:20:47,860 --> 00:20:53,560
In this scenario, we have a school club at my local high school that everybody loves

301
00:20:53,560 --> 00:20:59,140
being a part of, and one of the things that we need to do for that is have our members

302
00:20:59,140 --> 00:21:01,420
sign up for events.

303
00:21:01,420 --> 00:21:06,380
So for this, we used to use something like a Google form to have students register onto

304
00:21:06,380 --> 00:21:11,480
these events, but we found that to be pretty hard to match with our workflow with cancellations

305
00:21:11,480 --> 00:21:14,180
and other logistic issues.

306
00:21:14,220 --> 00:21:18,240
So we thought, okay, let's write some code to automate this.

307
00:21:18,240 --> 00:21:24,020
So we went ahead and wrote a little small website in Django that helps students register

308
00:21:24,020 --> 00:21:30,860
for these events, and we deployed it to a VM in DigitalOcean, and it worked pretty well.

309
00:21:30,860 --> 00:21:36,100
But then we started having a larger amount of members join our club, and we were getting

310
00:21:36,100 --> 00:21:38,820
way higher request count.

311
00:21:38,820 --> 00:21:43,220
When event signups go out to our members, I was seeing our request per second peak to

312
00:21:43,260 --> 00:21:45,540
like 100 per second.

313
00:21:45,540 --> 00:21:47,900
So we started scaling up our VM.

314
00:21:47,900 --> 00:21:52,280
We were paying like $30 per month at that point, and unfortunately, we don't have much

315
00:21:52,280 --> 00:21:53,280
money as students.

316
00:21:53,280 --> 00:21:55,420
We don't really have jobs.

317
00:21:55,420 --> 00:22:00,000
So instead, we thought, what else can we do to make this better?

318
00:22:00,000 --> 00:22:06,260
So we started considering a serverless model for our deployment rather than a purely server

319
00:22:06,260 --> 00:22:07,860
based approach.

320
00:22:07,900 --> 00:22:13,900
Once again, we adopted the Zappa framework in order to deploy this to AWS Lambda, and

321
00:22:13,900 --> 00:22:16,780
we found this to be pretty advantageous.

322
00:22:16,780 --> 00:22:22,220
So horizontal scaling worked very well for this scenario because cold starts were not

323
00:22:22,220 --> 00:22:24,900
as perceived as an issue from users.

324
00:22:24,900 --> 00:22:28,860
It only happened about two to three times every time we sent out a link to sign up to

325
00:22:28,860 --> 00:22:34,340
new events, and the cold start boot times we were observing were about one to one and

326
00:22:34,340 --> 00:22:36,140
a half seconds.

327
00:22:36,140 --> 00:22:38,400
And we also had really good cost savings.

328
00:22:38,400 --> 00:22:42,380
We were no longer paying for the idle time of the application when users aren't signing

329
00:22:42,380 --> 00:22:45,120
up for events and people aren't going on the website.

330
00:22:45,120 --> 00:22:51,180
So in total, after deploying this serverless website, the total bill we had to AWS was like

331
00:22:51,180 --> 00:22:53,100
$0.10 or something.

332
00:22:53,100 --> 00:22:55,180
It was really small.

333
00:22:55,180 --> 00:23:00,580
And also, the amount of time we spent writing code, figuring out deployment strategies,

334
00:23:01,540 --> 00:23:06,580
was so little because it was just one command from the terminal.

335
00:23:06,580 --> 00:23:09,460
So what are some takeaways from this presentation?

336
00:23:09,460 --> 00:23:12,980
Serverless is not a be-all end-all solution for everything.

337
00:23:12,980 --> 00:23:19,620
It's really good in the case you need fast horizontal scaling or event driven processing.

338
00:23:19,620 --> 00:23:25,060
Serverless programming also tends to encourage vendor lock-in, so whenever you try considering

339
00:23:25,060 --> 00:23:29,960
to move to a serverless model, modulize your code so that it can easily be adopted to another

340
00:23:29,960 --> 00:23:33,000
vendor's serverless design.

341
00:23:33,000 --> 00:23:37,800
In cases for backends where serverless is a good tool to use, but it's just not working

342
00:23:37,800 --> 00:23:42,560
for everything, it's okay to split it out into one backend that's serverlessly deployed

343
00:23:42,560 --> 00:23:44,480
and one that's normally deployed.

344
00:23:44,480 --> 00:23:48,800
This is only going to be possible if you modulize your code and write it in a way that you can

345
00:23:48,800 --> 00:23:54,560
still run it locally, but also deployed to the cloud in a serverless framework.

346
00:23:54,560 --> 00:23:58,800
Conclusion, serverless programming, all high, are there benefits?

347
00:23:58,840 --> 00:24:00,220
Of course there are benefits.

348
00:24:00,220 --> 00:24:05,040
We saw with the hash-cracking API that we calculated a billion hashes in the cloud in

349
00:24:05,040 --> 00:24:11,080
a few seconds, but these benefits don't come without significant drawbacks.

350
00:24:11,080 --> 00:24:15,160
These drawbacks include extra engineering time you're spending adapting your code to

351
00:24:15,160 --> 00:24:16,960
serverless design.

352
00:24:16,960 --> 00:24:18,320
It could be cold starts.

353
00:24:18,320 --> 00:24:25,920
It could be regulatory compliances where you're not able to use serverless design features.

354
00:24:26,000 --> 00:24:30,600
So ultimately, I hope that you'll take this talk and if you want to adopt serverless in

355
00:24:30,600 --> 00:24:36,000
your own environment, I wish that you put in the effort to determine if serverless deployment

356
00:24:36,000 --> 00:24:40,440
is going to be a beneficial technology for you in terms of engineering time, deployment

357
00:24:40,440 --> 00:24:42,680
cost and user experience.

358
00:24:42,680 --> 00:24:46,560
By spending the time, you're going to be able to verify that this is the right tool for

359
00:24:46,560 --> 00:24:52,360
you and not just some random presenter telling you about it at PyTexas.

360
00:24:52,400 --> 00:24:56,240
Spend the time, figure out if it's really good for you, and then hit me up on Twitter

361
00:24:56,240 --> 00:25:00,520
and let me know what you thought and if serverless was successful for you.

362
00:25:00,520 --> 00:25:13,520
So thanks for listening, I have my information available and I am open for internships if

363
00:25:13,520 --> 00:25:15,720
anybody would like to hire me, thank you.

