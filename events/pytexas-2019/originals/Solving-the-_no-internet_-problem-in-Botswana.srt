1
00:00:00,000 --> 00:00:07,200
All right for the next talk we have Eric and he's going to talk about solving the internet problem another thing that's here

2
00:00:08,520 --> 00:00:10,520
Okay

3
00:00:13,040 --> 00:00:16,740
Spoiler alert no, I've been working in public health overseas in

4
00:00:17,520 --> 00:00:18,800
Botswana

5
00:00:18,800 --> 00:00:26,120
Conducting clinical trial research in HIV AIDS using electronic as academics like to call it data systems in

6
00:00:27,080 --> 00:00:29,080
remote areas of Botswana and

7
00:00:29,640 --> 00:00:34,680
Doing that despite having not necessarily no internet, but no reliable internet

8
00:00:35,800 --> 00:00:37,920
Just a little bit about Botswana. It's a middle-income country

9
00:00:38,680 --> 00:00:40,680
landlocked in Southern Africa

10
00:00:41,080 --> 00:00:43,320
population now just over 2 million and

11
00:00:43,880 --> 00:00:48,040
It is landmass about the same size as Texas and although a bit drier

12
00:00:48,840 --> 00:00:51,400
In the drier parts of Texas that are kind of looks the same

13
00:00:53,440 --> 00:00:57,760
That's me at the last icon I was at in Namibia last year

14
00:00:58,000 --> 00:00:59,760
I'm the

15
00:00:59,760 --> 00:01:03,400
Former director of data operations and IT. So it's a pretty broad

16
00:01:04,720 --> 00:01:07,800
job of the Botswana Harvard AIDS Institute

17
00:01:08,600 --> 00:01:14,560
headed data management and IT teams there for over 25 NIH funded clinical trials and

18
00:01:15,880 --> 00:01:24,100
during that time we developed clinical trial data management systems laboratory information systems and interface some of the analytical

19
00:01:24,640 --> 00:01:26,640
laboratory equipment and

20
00:01:26,960 --> 00:01:32,400
Right now I'm finishing up a trial that oddly enough. I'm running from the cloud

21
00:01:33,280 --> 00:01:34,760
which is

22
00:01:34,760 --> 00:01:36,920
We're enrolling people in Botswana, Malawi

23
00:01:37,760 --> 00:01:43,760
Uganda South Africa and Zimbabwe and I'm not a medical person

24
00:01:44,360 --> 00:01:47,680
But I've been surrounded by them for for a lot of years now

25
00:01:49,760 --> 00:01:53,400
Clinical research in HIV AIDS, so this is an academic endeavor

26
00:01:54,040 --> 00:01:59,960
Our investigators and research teams were built from or made up of international and local

27
00:02:01,080 --> 00:02:02,600
people

28
00:02:02,600 --> 00:02:05,160
Our work is funded by competitive grants

29
00:02:06,160 --> 00:02:09,720
Almost exclusively from the National Institutes of Health here in the US

30
00:02:10,600 --> 00:02:14,400
All of our trials were conducted in Botswana. I worked for

31
00:02:15,120 --> 00:02:20,240
Max Essex at Harvard and being a virologist that he is all of our

32
00:02:21,240 --> 00:02:24,000
Trials were had a heavy laboratory component

33
00:02:24,520 --> 00:02:26,520
So in

34
00:02:26,640 --> 00:02:29,240
Included in collecting all the data. We also collected

35
00:02:30,080 --> 00:02:33,200
Probably a little bit over six hundred thousand blood specimens

36
00:02:34,160 --> 00:02:36,160
to go with that a

37
00:02:36,800 --> 00:02:38,800
little bit about HIV AIDS in Botswana

38
00:02:40,040 --> 00:02:42,040
They I'm sure you know

39
00:02:42,560 --> 00:02:48,160
Well, I would say that the real excuse me. The real problem is not no internet. The real problem is

40
00:02:49,160 --> 00:02:51,160
The HIV epidemic

41
00:02:51,800 --> 00:02:54,080
You probably know a lot about it

42
00:02:54,080 --> 00:02:58,360
You also probably know about the epidemic even starting here in the 80s

43
00:02:58,360 --> 00:03:03,360
but that was followed with a heterosexual epidemic in in Africa and

44
00:03:04,720 --> 00:03:09,640
In Botswana around 1995 even though it already had started ramping up

45
00:03:11,400 --> 00:03:15,480
It was very clear that there was a very serious epidemic underway

46
00:03:16,440 --> 00:03:23,160
And by 2000 estimates were that 15 to 20 percent of the population was infected which is

47
00:03:24,320 --> 00:03:25,400
amazing

48
00:03:25,400 --> 00:03:31,320
life expectancy in Botswana was around 70 years and had dropped or expected to drop to around 40 and

49
00:03:32,720 --> 00:03:35,640
there were projections from the McKinsey group that

50
00:03:37,000 --> 00:03:43,520
Population of Botswana which at that time was about 1.2 million would half within the next 10 to 15 years if nothing was done

51
00:03:45,520 --> 00:03:51,760
So in 2000 the president of Botswana stood in front of the UN and basically

52
00:03:52,640 --> 00:03:57,400
Stated that his population is looking at extinction if nothing is done

53
00:03:58,320 --> 00:04:00,320
We had already started

54
00:04:00,640 --> 00:04:02,200
in a

55
00:04:02,200 --> 00:04:04,200
trial funded by the NIH and

56
00:04:05,080 --> 00:04:07,080
Bristol Myer Squibb

57
00:04:07,160 --> 00:04:09,440
Bristol Myer Squibb where we were

58
00:04:10,120 --> 00:04:17,680
Looking at ways of rolling out already approved adult treatment and that sort of merged in 2002 into a

59
00:04:18,360 --> 00:04:21,640
National program which meant that the government agreed to then

60
00:04:22,200 --> 00:04:23,360
fund

61
00:04:23,360 --> 00:04:24,760
free

62
00:04:24,760 --> 00:04:26,000
treatment

63
00:04:26,000 --> 00:04:29,000
Testing monitoring whatever was necessary to get people

64
00:04:29,680 --> 00:04:35,200
On to the drugs were then available and that program which still continues today is

65
00:04:35,880 --> 00:04:41,600
The probably the most successful one in the world at the moment for combating HIV

66
00:04:43,800 --> 00:04:47,600
So now to shift over a little bit about clinical trial data management

67
00:04:48,720 --> 00:04:53,300
Yeah, someone said to me don't worry. I won't ever need that data, but we always do

68
00:04:55,040 --> 00:05:00,880
Before I get into the solutions of what we did for offline use to take a few minutes just to talk about clinical trial data

69
00:05:00,880 --> 00:05:02,000
management

70
00:05:02,000 --> 00:05:04,880
So that you can appreciate why given

71
00:05:06,080 --> 00:05:10,160
Limited resources and so on. Why don't we just keep it simple and and flip back to paper

72
00:05:11,080 --> 00:05:13,080
because paper works, but

73
00:05:13,880 --> 00:05:16,400
so my role in all of this was to

74
00:05:17,600 --> 00:05:24,040
Support the research trials with data management and like I said, we built these laboratory information systems already where we're receiving

75
00:05:25,720 --> 00:05:31,360
Resulting specimens storing the specimens interface and laboratory equipment and we built data management systems

76
00:05:31,360 --> 00:05:35,320
this is prior the data managed systems and filing paper and

77
00:05:36,080 --> 00:05:40,080
All kinds of stuff like that for the research clinics that we are currently working with

78
00:05:41,920 --> 00:05:48,760
But paper is very difficult to work with and if you're dealing with large trials you start to trip over it

79
00:05:48,760 --> 00:05:55,320
It's bulky. It's hard to change data can't be validated in real time need to transcribe information which has a

80
00:05:56,520 --> 00:05:58,400
Fixed sort of 10% error rate

81
00:05:58,400 --> 00:06:02,440
You've got to store the paper and then when you want to do quality assurance or you're trying to prepare for

82
00:06:03,400 --> 00:06:10,000
Some sort of interim or final analysis. You've got to drag those papers back out and make corrections send them back to the clinic

83
00:06:10,880 --> 00:06:18,360
Then get them back into your data center transcribe them again store them and this process just goes on forever and it turns out that

84
00:06:19,560 --> 00:06:25,920
What'll happen is you get to some point in your trial where you want to do analysis and you can take eight to twelve

85
00:06:25,920 --> 00:06:27,920
months to prepare a data set

86
00:06:28,480 --> 00:06:33,120
Very often these trials are underfunded you ask for $10 you get six

87
00:06:33,680 --> 00:06:38,480
Kind of things so no matter what you cannot afford to have these long lag times

88
00:06:39,200 --> 00:06:44,440
For data preparation and when you get to the end of a trial, you're probably out of money already anyway

89
00:06:44,440 --> 00:06:49,520
So having to say oh we need another 12 months of staffing to work on a data set

90
00:06:49,560 --> 00:06:53,240
It's not going to be something you want to do plus you've got a lot of pressure to publish

91
00:06:53,560 --> 00:07:00,280
To because that is in some ways the main deliverable so in 2010 I introduced to my team

92
00:07:01,280 --> 00:07:03,280
Python and Django I

93
00:07:03,360 --> 00:07:06,240
looked at other things, but I decided to do that and

94
00:07:06,840 --> 00:07:09,220
We then got underway to develop

95
00:07:10,960 --> 00:07:13,160
Basically data collection that we would do at point of care

96
00:07:14,160 --> 00:07:15,480
and

97
00:07:15,480 --> 00:07:17,880
our main metric was to say okay if

98
00:07:18,480 --> 00:07:21,560
We say you want to freeze a data set for analysis

99
00:07:21,560 --> 00:07:23,560
How long is it going to take from that moment?

100
00:07:23,640 --> 00:07:29,600
To the point where we can actually do a fine do an analysis and we got that down from that eight to twelve month thing

101
00:07:29,600 --> 00:07:30,600
down to

102
00:07:30,600 --> 00:07:32,600
In our last one just a couple weeks

103
00:07:34,200 --> 00:07:36,400
So we tried this system out

104
00:07:37,040 --> 00:07:39,200
on one trial

105
00:07:39,200 --> 00:07:45,440
We called them a Bonner trial. This was looking at trying to completely block transmission from a mother to a child

106
00:07:46,280 --> 00:07:48,560
Successful trial we were able to show do that

107
00:07:49,760 --> 00:07:51,600
but it was

108
00:07:51,600 --> 00:07:54,880
We deployed this within clinics our research clinic

109
00:07:54,880 --> 00:07:59,080
So it's a very comfortable turf and we had control of the environment

110
00:07:59,080 --> 00:08:04,000
We were able to install connections and data connections and so on but then we had another trial

111
00:08:04,000 --> 00:08:07,000
which was just a small one which we did in a neighboring village where we

112
00:08:09,480 --> 00:08:14,960
Where we had to go into people's households that actually is a picture of somebody's household

113
00:08:15,600 --> 00:08:17,120
and

114
00:08:17,120 --> 00:08:21,040
That was different because now it's not our turf. We're actually operating in somebody's house

115
00:08:21,440 --> 00:08:23,560
Kick us out and we can't sit there

116
00:08:24,240 --> 00:08:30,560
anyway, and there wasn't good data connection there and we the group tried to do this on paper and

117
00:08:30,920 --> 00:08:34,840
within a few weeks about six weeks the whole thing collapsed because the

118
00:08:35,560 --> 00:08:40,440
Handling the paper just became a nightmare. You're collecting longitudinal data

119
00:08:40,440 --> 00:08:43,720
You're collecting data from one household over a number of days

120
00:08:44,200 --> 00:08:48,360
And it just it just became a nightmare. So we had to stop the trial and

121
00:08:49,080 --> 00:08:51,680
Finish and kind of come up with another solution

122
00:08:52,960 --> 00:08:59,240
The solution was our first iteration of offline use we had learned something already from

123
00:09:00,360 --> 00:09:06,920
making from the mobana trial with how to do electronic data connection and data collection and

124
00:09:07,520 --> 00:09:11,200
Management and we tried some offline use then in 2012

125
00:09:11,720 --> 00:09:18,440
We got a grant for about the large grant about 80 million US which

126
00:09:20,120 --> 00:09:23,360
You don't see that much on the ground, but that was the total apparently

127
00:09:24,040 --> 00:09:29,280
And since it was so large State Department and CDC got involved as well

128
00:09:29,640 --> 00:09:36,200
The trout brought together a lot of what we as data managers or our data manager team already had experience in now

129
00:09:36,200 --> 00:09:39,040
We knew how to do real-time data collection real-time validation

130
00:09:39,760 --> 00:09:46,280
We knew how to do the sort of the informed consent process how to handle HIV testing data treatment data

131
00:09:46,560 --> 00:09:50,680
how to do blood collection chain of custody and all kinds of

132
00:09:51,200 --> 00:09:53,600
questionnaires for men women and so on

133
00:09:54,360 --> 00:09:59,760
Regulatory compliance how to protect personal information through encryption and things like that. We knew how to do all this but

134
00:10:00,280 --> 00:10:02,280
this trial was different in that

135
00:10:02,560 --> 00:10:09,240
It wanted we wanted to operate in 30 extremely remote villages with distances of

136
00:10:09,920 --> 00:10:17,440
I'll say kilometers, but you know 700 kilometers away and things like that from where we were we were sitting so this is a different

137
00:10:18,360 --> 00:10:20,360
This is different

138
00:10:21,280 --> 00:10:28,120
So what was the current state of Internet in Botswana at that time so in 2009 3g was introduced

139
00:10:28,760 --> 00:10:35,960
But they just focused on the major city centers and the amount of bandwidth that they could actually handle on those 3d connections

140
00:10:36,200 --> 00:10:42,120
Pretty poor. It's mostly about like Facebook Twitter that kind of thing not really trying to do anything

141
00:10:42,120 --> 00:10:46,520
And they're always really lopsided you can bring a lot of data in but you can't push data up

142
00:10:46,720 --> 00:10:50,320
So you would prefer something symmetrical but fixed line penetration

143
00:10:50,840 --> 00:10:54,120
It's only about 10% in the country. So that's also no good

144
00:10:54,760 --> 00:10:59,240
The cost of a gigabyte around $70 per gigabyte, so that's pretty heavy

145
00:10:59,840 --> 00:11:03,480
We weren't really sure what kind of data throughput we would have

146
00:11:04,920 --> 00:11:07,160
And so we also looked at satellite

147
00:11:07,720 --> 00:11:09,880
Like five dollars a megabyte or something like that

148
00:11:09,880 --> 00:11:15,200
But there's also the complexity of dealing with satellite because we were thinking in terms of trying to do something in real time

149
00:11:15,200 --> 00:11:20,120
Because that's what we were working with we had these systems that work really well, but they were online

150
00:11:20,400 --> 00:11:23,200
So we wanted to see we could work with real time

151
00:11:23,200 --> 00:11:24,720
I

152
00:11:24,720 --> 00:11:30,720
Put into the budget pretty big trying to work with this five bucks a megabyte satellite thing

153
00:11:30,720 --> 00:11:37,000
But when we started to have discussions and the CDC moved in they said oh by the way this internet budget

154
00:11:37,000 --> 00:11:40,080
Can we remove that because we need that money for something else? I

155
00:11:41,520 --> 00:11:48,400
Had a lot of arguments a lot of people to explain that you know, no matter what anybody tells you 3g is not going to support this

156
00:11:48,520 --> 00:11:53,400
Anyway time went on into 2013 and 30 days before we

157
00:11:54,800 --> 00:11:59,600
We started the trial we had already written our solution to operate sort of offline

158
00:12:00,400 --> 00:12:06,960
They decided oh my gosh because they had a component of the trial that they were responsible for they wanted internet

159
00:12:08,000 --> 00:12:14,800
So they suddenly came up with a lot of money and started helping us put up towers like this

160
00:12:15,200 --> 00:12:22,560
Towers like this and we would we had 30 villages. They were to to we're doing a

161
00:12:23,280 --> 00:12:29,380
Comparative trial so we had a control and an intervention sites and we would so there's 15 and 15

162
00:12:29,380 --> 00:12:32,240
So we would go two sites at a time and we built these

163
00:12:32,760 --> 00:12:38,120
Right before we got to the next pair these were being built and so we walked along the country like this

164
00:12:38,960 --> 00:12:41,440
But these connections as expensive as they were

165
00:12:42,240 --> 00:12:44,240
We

166
00:12:45,400 --> 00:12:50,440
Put a half a million dollars into building these or they did and and

167
00:12:51,240 --> 00:12:57,720
Recurring costs of like fifty thousand dollars, but we're looking at one megabit per second is all we could sustain

168
00:12:58,240 --> 00:13:02,600
Which is okay for SSH and things like that, but it was one megabit

169
00:13:03,440 --> 00:13:09,720
Hopefully a lot of most of the time during the day was unusable and at the only at night could it be usable?

170
00:13:09,720 --> 00:13:12,400
So it's still not a solution to real-time data collection

171
00:13:14,520 --> 00:13:16,520
So what did we do

172
00:13:16,760 --> 00:13:21,400
So all we really are doing we came up with an offline solution and it's pretty simple

173
00:13:21,920 --> 00:13:25,600
I'm not spoiler alert. There's no magic to not having internet

174
00:13:25,600 --> 00:13:30,080
You don't have internet you don't have internet or you don't have a data connection Wi-Fi. There's no solution to that

175
00:13:30,600 --> 00:13:34,880
But you know through asynchronous sort of approach you can you can come up with something

176
00:13:35,400 --> 00:13:38,120
so we're using Django systems and

177
00:13:38,800 --> 00:13:42,440
So we've got database if we can

178
00:13:43,760 --> 00:13:46,960
first thing we do is we would we wrote some a little bit to

179
00:13:47,600 --> 00:13:53,000
We added to each of our system a new model an outgoing transaction models a single model

180
00:13:53,000 --> 00:13:59,760
and then we would also have an incoming transaction model sort of taking a almost like mail inbox outbox and

181
00:14:01,520 --> 00:14:03,520
All of the

182
00:14:03,520 --> 00:14:05,200
models within

183
00:14:05,240 --> 00:14:06,960
Within the system for each of the

184
00:14:06,960 --> 00:14:12,160
Questionnaires and forms and so on would all serialize into us each time you would hit a save method

185
00:14:12,160 --> 00:14:14,840
They would sink serialized into an outgoing transaction

186
00:14:15,760 --> 00:14:21,840
That transaction at some point would be all those transactions at some point coming out of that outgoing transaction model

187
00:14:21,960 --> 00:14:23,960
Would be dumped into a file

188
00:14:25,080 --> 00:14:29,200
The file then when we had some availability of some sort would be

189
00:14:30,200 --> 00:14:35,080
SCP to cross to a remote machine which would and then

190
00:14:35,920 --> 00:14:38,160
Reverse it read the file back in

191
00:14:39,160 --> 00:14:45,080
Deserialize all the transactions and everything works and as long as we maintain the order of how they played out

192
00:14:45,840 --> 00:14:48,760
Originally and then just play them back. Everything is fine

193
00:14:52,000 --> 00:14:54,000
So what we had were

194
00:14:55,000 --> 00:15:02,240
In some cases we would have 30 or 40 research assistants fan out into a village each one has a Mac air

195
00:15:03,120 --> 00:15:06,600
They would go out for about four hour shifts because they're also collecting blood

196
00:15:06,600 --> 00:15:10,640
They have to come back with their blood to what we had was a community server

197
00:15:11,240 --> 00:15:13,880
Which is still not connected to any remote system

198
00:15:14,640 --> 00:15:21,000
They would connect to a local Wi-Fi. We had a little Wi-Fi unit right there and they would connect to a MacBook

199
00:15:21,560 --> 00:15:22,880
push in all

200
00:15:22,880 --> 00:15:25,120
move in all of their transactions and

201
00:15:26,640 --> 00:15:32,680
At some point maybe in the night or in a few days that community server would do the same thing and push all its

202
00:15:32,840 --> 00:15:34,840
transactions to a headquarter server

203
00:15:35,800 --> 00:15:36,880
so

204
00:15:36,880 --> 00:15:39,120
Let's look at some some code

205
00:15:40,120 --> 00:15:44,280
So we have an installable application Django collect offline

206
00:15:45,040 --> 00:15:47,720
Which since these are Django applications you would?

207
00:15:48,520 --> 00:15:55,900
Put in your install app you migrate you'll get those two tables outgoing transaction table in the incoming transaction table or model

208
00:15:56,760 --> 00:16:04,360
those models simply have one field for the the serialized JSON and then some metadata for the

209
00:16:04,720 --> 00:16:07,280
administration around that that transaction and

210
00:16:08,560 --> 00:16:10,400
the incoming

211
00:16:10,400 --> 00:16:12,400
Model is exactly the same

212
00:16:12,680 --> 00:16:14,680
Just on the other end

213
00:16:15,320 --> 00:16:16,840
So

214
00:16:16,840 --> 00:16:21,320
to prepare a model for offline use if you have a model like

215
00:16:22,000 --> 00:16:24,680
This one I think is a Django tutorial or something like that

216
00:16:25,560 --> 00:16:30,040
I just added a report date time as we're always using that and I have my own little

217
00:16:31,840 --> 00:16:33,200
Utility

218
00:16:33,200 --> 00:16:34,840
package which uses

219
00:16:34,840 --> 00:16:36,760
I'm always working with

220
00:16:36,760 --> 00:16:39,600
timezone aware dates and for that

221
00:16:40,480 --> 00:16:46,080
Anyway, then you have because you're going to serialize you have to have a unique

222
00:16:46,840 --> 00:16:50,240
Constraint on at least one field or combination of fields

223
00:16:50,920 --> 00:16:56,120
I was just going to say for the date time stuff. I use the arrow package would recommend that

224
00:16:58,320 --> 00:17:04,600
So now and this is pretty much you can follow this pretty well in the end well described in the

225
00:17:05,280 --> 00:17:11,120
Django documentation you're preparing a model for a serialization, so you have to add

226
00:17:12,640 --> 00:17:17,920
Well what we did was we would make sure we would switch from a integer primary key to a UUID

227
00:17:18,200 --> 00:17:20,200
So we wouldn't get any clashes

228
00:17:20,720 --> 00:17:21,880
and

229
00:17:21,880 --> 00:17:26,640
That's now has become part of Django, so you can use that but previously it wasn't

230
00:17:27,800 --> 00:17:29,800
add a custom managers

231
00:17:30,160 --> 00:17:34,840
Plus the natural key so you can do the full circle of serializing and deserializing

232
00:17:36,760 --> 00:17:39,560
So that would be a model and this will we can you can also

233
00:17:40,320 --> 00:17:45,000
Add foreign keys or in Django you have the sort of many to many

234
00:17:47,000 --> 00:17:49,160
So it'll support all that kind of stuff

235
00:17:50,200 --> 00:17:52,200
then the next thing is to

236
00:17:52,480 --> 00:17:56,640
add a offline models module in your app and

237
00:17:57,440 --> 00:18:03,760
That's just where you're going to register which models you want to include in this process

238
00:18:03,760 --> 00:18:05,760
You don't have to include every model

239
00:18:06,240 --> 00:18:14,360
You can either register them explicitly in that offline models py or you could be safer if you want the whole

240
00:18:15,200 --> 00:18:17,360
application and all its models to to

241
00:18:18,520 --> 00:18:21,920
to be a part of this, then you just register with the the app name and

242
00:18:22,600 --> 00:18:30,960
That module will be discovered on boot up just like if you're familiar with Django Django has an admin

243
00:18:33,560 --> 00:18:41,000
Has admin classes and when you boot up the system looks for an admins py and registers any classes in there

244
00:18:41,040 --> 00:18:44,840
This uses the same the same approach some of the same code

245
00:18:45,800 --> 00:18:48,920
so that as soon as you boot up, it'll look for that and it have a

246
00:18:49,280 --> 00:18:55,360
a global site offline global that will you can refer to later in registry and

247
00:18:56,640 --> 00:19:03,720
Then the last thing is some sort of post-save signal. So once every time you for any model that's registered if you save it

248
00:19:04,480 --> 00:19:10,520
It's going to inspect to see if that model by name is registered in the global

249
00:19:10,520 --> 00:19:18,560
And if not, it'll just raise an exception and pass or if it is then it's going to fire this to outgoing transaction method

250
00:19:19,600 --> 00:19:24,320
And it will then serialize it and stick it in that outgoing transaction table

251
00:19:27,560 --> 00:19:32,240
We also encrypt it for security purposes as you should

252
00:19:33,960 --> 00:19:36,160
Yeah, so that takes care of

253
00:19:36,880 --> 00:19:42,480
Everything that's happening on a client machine and everything that will happen in reverse on some remote server

254
00:19:44,080 --> 00:19:46,080
and

255
00:19:46,280 --> 00:19:53,440
The next thing then is to handle the sort of file events what's going to happen there. So that process is started by

256
00:19:54,280 --> 00:19:56,960
One of our research assistants with their machine

257
00:19:56,960 --> 00:19:59,120
They have a we had several different interfaces

258
00:19:59,120 --> 00:20:05,640
so but it's basically just click and then that will export all those outgoing transactions into a file and

259
00:20:06,720 --> 00:20:08,640
then we use a

260
00:20:08,640 --> 00:20:10,640
module called watchdog

261
00:20:10,720 --> 00:20:12,720
which will just monitor a

262
00:20:13,440 --> 00:20:17,840
Directory for any events we had used this for some of the laboratory equipment that would drop

263
00:20:18,640 --> 00:20:23,240
result files and just pick them up and pull them into a database so this worked well here as well and

264
00:20:24,200 --> 00:20:30,260
Any sort of event you just pass it an event handler. This is just some simple code coming from the

265
00:20:31,840 --> 00:20:37,400
From its own documentation, so we had different event handlers one was to

266
00:20:37,960 --> 00:20:44,000
Export the outgoing transactions to a file another one would be to transfer files to a

267
00:20:44,520 --> 00:20:48,680
Remote system so the files created and then it can move to the remote system

268
00:20:48,680 --> 00:20:53,360
And another one would be that as soon as a remote system file gets dropped into a directory

269
00:20:53,360 --> 00:20:57,200
It then pulls it up into the incoming transaction model

270
00:20:58,920 --> 00:21:00,920
So that takes care of

271
00:21:01,040 --> 00:21:03,440
database file get it across

272
00:21:04,440 --> 00:21:08,160
Into they get the file across and then pull the file up there

273
00:21:08,800 --> 00:21:15,200
So the watchdog is on a community server, or it'll be on the final node however many you want to have

274
00:21:15,200 --> 00:21:16,880
but we only ever had

275
00:21:16,880 --> 00:21:19,080
client community server and then

276
00:21:19,920 --> 00:21:26,080
headquarters server and then obviously on the last database or whatever not the last one, but

277
00:21:26,800 --> 00:21:33,240
The incoming there would be a post-save signal that would then deserialize all your models as I described earlier

278
00:21:34,240 --> 00:21:36,240
so

279
00:21:37,440 --> 00:21:44,640
That's kind of it, so I told you wasn't a little shocking solution, and so what does it look like in the field?

280
00:21:46,440 --> 00:21:47,640
So

281
00:21:47,640 --> 00:21:53,160
This is actually some people working in the field so you can see them with their little Mac air there

282
00:21:53,160 --> 00:22:00,160
they actually came with the table and the chair and the CD4 counter and all the

283
00:22:01,040 --> 00:22:02,360
equipment to

284
00:22:02,360 --> 00:22:08,640
Collect blood and cool box and everything they they would had everything with them with they were going in

285
00:22:09,200 --> 00:22:14,200
They have some discussion they collect the blood like that

286
00:22:14,960 --> 00:22:17,640
they pack it all up and walk away and

287
00:22:18,400 --> 00:22:26,240
This whole process with the data collection the blood collection everything like that and the testing the counseling and everything that could take

288
00:22:26,240 --> 00:22:28,240
up to an hour hour and a half

289
00:22:28,880 --> 00:22:35,560
And in some cases we would have families of like ten because old ladies would tell all their relatives to come

290
00:22:36,200 --> 00:22:38,200
because they're coming to

291
00:22:39,080 --> 00:22:41,080
participate in the research then

292
00:22:41,200 --> 00:22:46,080
We had trucks like this which were our data center and laboratories so when

293
00:22:48,960 --> 00:22:50,680
Sorry when he

294
00:22:50,680 --> 00:22:54,720
Finishes then he goes to this truck. This is parked next to our laboratory

295
00:22:54,720 --> 00:23:01,880
But this would be parked also in the field and we have a laboratory there for him to bring in his blood specimens

296
00:23:02,040 --> 00:23:04,320
They would then be processed spun down

297
00:23:04,840 --> 00:23:10,720
The data would be sync synchronized into our data center, which is just on the other end of the truck

298
00:23:11,600 --> 00:23:17,520
The lab technician would then see the data on the server that they're accessing scan in the physical

299
00:23:18,240 --> 00:23:21,040
The physical specimens to verify everything's okay

300
00:23:21,440 --> 00:23:28,080
Then the research assistant will go back out to the next household and we're basically done and then up here

301
00:23:28,080 --> 00:23:31,240
We had a little wireless bridge that would connect in some way to

302
00:23:31,800 --> 00:23:37,020
That tower that you saw earlier when the connection was available and the data would end up

303
00:23:38,000 --> 00:23:40,000
Where we wanted it

304
00:23:40,080 --> 00:23:42,080
So we did exactly that

305
00:23:42,800 --> 00:23:50,200
Across 30 villages for four years. We would collect serialized fine working internet transferred deserialized this whole process

306
00:23:51,080 --> 00:23:52,320
and

307
00:23:52,320 --> 00:23:54,320
We did our first

308
00:23:55,520 --> 00:24:00,800
Enrollment in October of 2013 and got our first publication out in 2016

309
00:24:01,240 --> 00:24:03,680
To show that Botswana actually

310
00:24:04,320 --> 00:24:06,320
Had the best

311
00:24:07,400 --> 00:24:09,400
Program available

312
00:24:09,520 --> 00:24:14,580
Except I think Switzerland might have beat out with their point one percent prevalence

313
00:24:16,240 --> 00:24:18,240
In the world's

314
00:24:19,160 --> 00:24:22,280
Yeah, and that's it that's what we did so, thank you

315
00:24:32,800 --> 00:24:34,800
You

