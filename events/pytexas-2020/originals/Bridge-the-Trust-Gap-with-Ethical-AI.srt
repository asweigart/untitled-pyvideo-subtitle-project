1
00:00:00,000 --> 00:00:21,540
Hi, and welcome to Bridging the Trust Gap with Ethical AI.

2
00:00:21,540 --> 00:00:24,880
My name is Gabby Leo, and I'm a data scientist at SenseCorp.

3
00:00:24,880 --> 00:00:29,360
I've worked with multiple Fortune 500 companies, sharing and implementing a lot of data-driven

4
00:00:29,600 --> 00:00:33,040
applications. I have a lot of experience building and deploying machine learning models, and I serve

5
00:00:33,040 --> 00:00:37,680
as a thought leader within my company, promoting ethical AI at every step along our clients' journey

6
00:00:37,680 --> 00:00:42,880
to adoption. I'm excited to be speaking with you all today on this topic, so let's go ahead and dive

7
00:00:42,880 --> 00:00:48,880
right in. So firstly, I'm going to start with a quick overview about the importance of ethical AI

8
00:00:48,880 --> 00:00:54,080
and why ethics need to be seen as an important practical consideration for anyone who is using

9
00:00:54,080 --> 00:00:59,040
and building machine learning systems. Then I will jump into three examples of failures

10
00:00:59,040 --> 00:01:04,880
surrounding ethical AI and move into seven tips around how to bridge the trust gap and build

11
00:01:04,880 --> 00:01:10,560
ethical AI. This will focus on both organizational and programmatic solutions, and then I'll be

12
00:01:10,560 --> 00:01:15,680
wrapping up the talk with a walkthrough and a live example of a Python library you could start using

13
00:01:15,680 --> 00:01:24,400
today to implement ethical AI in your models. So let's take a step back and look at why ethical

14
00:01:24,400 --> 00:01:30,480
AI matters. There are two aspects here that we really need to focus on. It matters to the business

15
00:01:30,480 --> 00:01:36,480
and it matters in our models. For the business, we are in the age of AI. Most organizations are

16
00:01:36,480 --> 00:01:41,680
racing to adopt AI, and with that comes the added responsibility of being accountable for what they

17
00:01:41,680 --> 00:01:46,480
do and how they behave. If they want to generate trust from their customers, partners, and the

18
00:01:46,480 --> 00:01:52,560
public, they need to have models that are ethical and that instill trust within their customers.

19
00:01:52,560 --> 00:01:56,240
Trust is a competitive advantage, so in order for your business to thrive,

20
00:01:56,240 --> 00:02:01,920
you need your customers to trust you. Now your models can either harm or propel you when it

21
00:02:01,920 --> 00:02:06,640
comes to creating trust with your customers. So if your AI model is built without an ethical

22
00:02:06,640 --> 00:02:12,080
framework in mind, it's not like just making a single mistake. If the training data is biased,

23
00:02:12,080 --> 00:02:16,960
the mistakes will be compounded and the algorithm will continue to learn from flawed data.

24
00:02:17,920 --> 00:02:23,040
So this in turn could lead to automated decisions and predictions affecting a person's chance of

25
00:02:23,040 --> 00:02:29,120
getting a loan or maybe even failing to diagnose an illness or a disease. If the model is responsible

26
00:02:29,120 --> 00:02:33,360
for an incorrect decision, the trust from the customer is broken and you will likely lose their

27
00:02:33,360 --> 00:02:38,960
business. So these two aspects, trust in the business and trust in our models, go hand in

28
00:02:38,960 --> 00:02:46,560
hand towards creating and sustaining a successful organization. And to further prove this point,

29
00:02:46,560 --> 00:02:51,600
I'd like to call out a quote from a Capgemini report that was assessing consumer and executive

30
00:02:51,600 --> 00:02:57,280
sentiment regarding AI and ethical implementation around the globe. So the global survey was

31
00:02:57,280 --> 00:03:02,400
conducted from April to May of this year and included 2,900 consumers from six countries

32
00:03:02,400 --> 00:03:09,040
and 884 executives in 10 countries. And the quote reads, the majority of consumers expect

33
00:03:09,040 --> 00:03:14,400
companies to be accountable for their AI systems, yet about half of the companies do not have a

34
00:03:14,400 --> 00:03:20,320
dedicated member overseeing ethical AI implementation. If the business knows that

35
00:03:20,320 --> 00:03:24,720
the customers are going to hold them accountable, then why isn't any action being taken on the

36
00:03:24,720 --> 00:03:29,440
organization side? And it's not like the executives in the business are unaware of the issues

37
00:03:29,440 --> 00:03:35,520
surrounding ethical AI. Actually, 65% of the executives surveyed said that they were aware

38
00:03:35,520 --> 00:03:41,200
of the issue of discriminatory bias within these AI systems and a number of respondents said that

39
00:03:41,200 --> 00:03:47,520
their company had been negatively impacted by AI systems. So for example, six in 10 organizations

40
00:03:47,520 --> 00:03:53,200
had attracted legal scrutiny and nearly one quarter had experienced consumer backlash within the last

41
00:03:53,200 --> 00:03:58,880
three years due to decisions that were reached by AI systems. But despite all of this information

42
00:03:58,880 --> 00:04:04,800
being out there, still only 53% of respondents said that they had a dedicated leader responsible

43
00:04:04,800 --> 00:04:10,720
for overseeing AI ethics. There's clearly a big misalignment between consumers' needs and how

44
00:04:10,720 --> 00:04:15,280
organizations are responding in today's marketplace, and that gap needs to be closed.

45
00:04:17,200 --> 00:04:22,240
And the issue doesn't only exist in regards to the business and its customers, it also exists

46
00:04:22,240 --> 00:04:27,920
within the organization itself. So from that same Capgemini report, it was discovered that while

47
00:04:27,920 --> 00:04:34,560
four in 10 IT and data professionals said they had detailed knowledge of how and why our systems

48
00:04:34,560 --> 00:04:39,920
produce the output that they do, only about one quarter of the marketing and sales executives

49
00:04:39,920 --> 00:04:46,160
agreed. And furthermore, about 51% of marketing and sales executives said that they realized their AI

50
00:04:46,160 --> 00:04:51,120
systems sometimes make decisions which are incompatible with our corporate values compared to

51
00:04:51,120 --> 00:04:56,640
only 40% of IT and data professionals responding the same way. So this also signifies that there's

52
00:04:56,640 --> 00:05:01,120
a lack of development of internal practices within an organization that is hindering the

53
00:05:01,120 --> 00:05:07,600
path to ethical AI. If within an organization employees are not on the same page about ethical AI,

54
00:05:07,600 --> 00:05:12,560
the chances are slim that they're going to be able to generate trust around AI with their customers.

55
00:05:12,560 --> 00:05:20,880
So the problem needs to be fixed from within. And it starts by including ethical AI at every step in

56
00:05:20,880 --> 00:05:25,440
the process. So normally ethics are thought of only during the data and modeling phases,

57
00:05:25,440 --> 00:05:30,640
but in actuality it starts at the very beginning when we are identifying the problem. You have to

58
00:05:30,640 --> 00:05:34,720
assess the impact that your model is going to have on your business and your customers before

59
00:05:34,720 --> 00:05:39,360
you even begin solutioning. It also helps to have diverse team members from different parts of the

60
00:05:39,360 --> 00:05:44,320
business included in these conversations instead of just AI developers to ensure that your

61
00:05:44,320 --> 00:05:48,240
organization's core values are in line with the proposed solution for the problem.

62
00:05:49,760 --> 00:05:54,000
Once you have identified the problem and have a solution in mind, you have to start collecting

63
00:05:54,000 --> 00:05:58,560
your data. Here you need to think about the sensitivity of the data being collected.

64
00:05:58,560 --> 00:06:03,440
You have to ask questions such as, does it make sense to anonymize some of this information?

65
00:06:03,440 --> 00:06:07,760
Who in the organization should have access to this data? How was the data collected?

66
00:06:07,760 --> 00:06:11,600
If it was from the web, are we being socially responsible with the information that we've

67
00:06:11,600 --> 00:06:16,720
scraped? Did we get data from survey respondents? And if so, was the population of the respondents

68
00:06:16,720 --> 00:06:21,440
representative of a diverse population? This is just an example of a few questions that

69
00:06:21,440 --> 00:06:28,240
need to be addressed in this stage. Now that we have our data, we start doing some exploratory

70
00:06:28,240 --> 00:06:32,560
analysis on it, right? So this is a major step where we need to look for potentially biased

71
00:06:32,560 --> 00:06:37,440
fields such as gender, race, or age, and we need to exclude them from the data set before

72
00:06:37,440 --> 00:06:42,640
we move into modeling. In the modeling phase, we have to make sure that we are creating an

73
00:06:42,640 --> 00:06:46,640
explainable model and that we are checking to make sure that our predictions aren't biased

74
00:06:46,640 --> 00:06:52,080
towards specific demographics. It isn't just enough to create a model that has 95% accuracy

75
00:06:52,080 --> 00:06:56,320
and call it a day. Just because a model is accurate doesn't mean it isn't spitting out

76
00:06:56,320 --> 00:07:00,400
biased predictions. So you have to create checks and audit your models before putting them into

77
00:07:00,400 --> 00:07:06,480
production. And it doesn't end there, right? Over time, models may drift or become stable.

78
00:07:06,480 --> 00:07:10,880
So you must make sure that you have statistical process controls running against your models,

79
00:07:10,880 --> 00:07:16,080
checking for this as well as retraining your models every now and then on a more current

80
00:07:16,080 --> 00:07:23,040
training set of data. Once ethical AI is included in the entire process, then you will start to see

81
00:07:23,040 --> 00:07:27,040
firstly within the business that all your employees are aligned and then externally

82
00:07:27,040 --> 00:07:29,920
that you are creating solutions in which your customers trust.

83
00:07:32,640 --> 00:07:36,720
We touched on identifying the impact that your model is going to have on the business

84
00:07:36,720 --> 00:07:41,440
and the customers, but I want to elaborate on why it's so important that we think about this

85
00:07:41,440 --> 00:07:47,120
before we start developing our models. Think about how AI is used in our day-to-day lives.

86
00:07:47,120 --> 00:07:51,920
We have models running on Amazon's Alexa that interpret what a user says and responds with an

87
00:07:51,920 --> 00:07:56,560
answer. In the grand scheme of things, if you think about the impact this tool has on people,

88
00:07:56,560 --> 00:08:01,840
it's pretty minimal. What happens if you ask Alexa what the weather outside is and she says

89
00:08:01,840 --> 00:08:07,600
80 degrees when it's actually 60? The impact is that you're going to be cold when you walk outside,

90
00:08:07,600 --> 00:08:12,960
so I would say this is a pretty low impact tool. Let's up the stakes a little more, though.

91
00:08:13,600 --> 00:08:17,600
We're all familiar with YouTube, and whenever you're watching videos, you get recommendations

92
00:08:17,600 --> 00:08:22,720
from an AI model on what videos to watch next, and that's related to your past preferences.

93
00:08:22,880 --> 00:08:27,120
What happens if a child is on there and has suddenly taken an interest with a conspiracy

94
00:08:27,120 --> 00:08:32,000
theory like the Illuminati? After that, the YouTube's recommendation engine starts recommending

95
00:08:32,000 --> 00:08:36,640
a plethora of conspiracy theories to this nine-year-old child. The impact of this tool

96
00:08:36,640 --> 00:08:40,960
is far greater than Alexa's because instead of just giving a wrong answer, it's spreading

97
00:08:40,960 --> 00:08:46,720
false information and fundamentally shaping a child's viewpoint. If we want to take it to

98
00:08:46,720 --> 00:08:51,680
the extreme, some tools such as models that are used to predict or decide someone's sentence in

99
00:08:51,680 --> 00:08:56,240
the court have a very high impact on someone's life because if the model produces an incorrect

100
00:08:56,240 --> 00:09:02,560
decision, that person's sentence could be 50 years instead of 10. The impact is very specific

101
00:09:02,560 --> 00:09:07,120
to the problem at hand that is trying to be solved, and if this is an afterthought instead

102
00:09:07,120 --> 00:09:12,560
of a forethought during your problem identification, your AI models will surely fail to be ethical.

103
00:09:13,040 --> 00:09:18,720
And here are three examples where it was clear that ethical AI was not a forethought. So let's

104
00:09:18,720 --> 00:09:24,080
take a look at the first one. In 2015, Amazon created a computer vision recruiting tool that

105
00:09:24,080 --> 00:09:29,280
was used to scan resumes. The idea was that it would automate this process for recruiters

106
00:09:29,280 --> 00:09:34,320
and give them a short list of the top resumes. Well, before the model made it into production,

107
00:09:34,320 --> 00:09:39,440
they discovered that it was biased against women. Since the training data over the past 10 years

108
00:09:39,920 --> 00:09:43,680
had been predominantly male resumes, coinciding with the historical trend of a male-dominated

109
00:09:43,680 --> 00:09:48,880
tech industry, the model taught itself to penalize any resume which had the word women in it such as

110
00:09:48,880 --> 00:09:55,680
women's college or a women's chess club and so on. Another example was in 2016 when ProPublica

111
00:09:55,680 --> 00:10:00,800
studied the outcomes of a software being used in the U.S. courts. The software being studied was

112
00:10:00,800 --> 00:10:05,040
a computer program that was used to spit out a score predicting the likelihood of a person who

113
00:10:05,520 --> 00:10:10,880
had been booked of committing a future crime. By studying what the algorithm predicted and comparing

114
00:10:10,880 --> 00:10:15,680
it to what actually ended up happening in the real world years later, they found that the algorithm

115
00:10:15,680 --> 00:10:21,760
was particularly likely to falsely flag black defendants as future criminals, wrongly labeling

116
00:10:21,760 --> 00:10:26,800
them this way at almost twice the rate as white defendants, and it found that white defendants

117
00:10:26,800 --> 00:10:33,280
were mislabeled as low-risk more often than black defendants. So the algorithm was able to

118
00:10:33,280 --> 00:10:40,320
falsely label them often than black defendants. Our last example was more recent in 2019, and it

119
00:10:40,320 --> 00:10:45,040
highlights the concept of explainability in our models and why it's necessary. A man went in to

120
00:10:45,040 --> 00:10:49,920
apply for credit at his bank. After his application got denied, he asked the reasonable question,

121
00:10:49,920 --> 00:10:55,600
why? The reason they gave him was because he was a Finnish-speaking man in a rural area. Most of

122
00:10:55,600 --> 00:11:00,800
the time, algorithms in the financial industry are making decisions autonomously. The person

123
00:11:00,880 --> 00:11:04,480
for the decision might not have the time or the skills to familiarize themselves with these

124
00:11:04,480 --> 00:11:10,080
models at hand, and therefore fewer than 1% of the time decisions are revised. If these people

125
00:11:10,080 --> 00:11:14,560
understood the models they were using, they could explain the outcomes of the model, and the customers

126
00:11:14,560 --> 00:11:19,360
wouldn't be left questioning the decision. It also highlights the need for human intervention at times

127
00:11:19,360 --> 00:11:25,920
when these models are being used to check that they're producing the right output. So maybe the

128
00:11:25,920 --> 00:11:30,400
data in the model wasn't trained on having an example of a Finnish-speaking man living in a

129
00:11:30,400 --> 00:11:35,840
rural area, and therefore it came to an incorrect decision. But unless we have that explainability

130
00:11:35,840 --> 00:11:40,320
of how our models came to a specific decision, we won't be able to pinpoint why the decision was

131
00:11:40,320 --> 00:11:48,400
incorrect. So now that we know that ethical AI is an issue that consumers in the marketplace care

132
00:11:48,400 --> 00:11:54,000
about, and that all organizations are not making an important focus area, how can you help your

133
00:11:54,000 --> 00:11:58,960
organization become aligned on an ethical AI strategy? I'm going to walk you through seven

134
00:11:58,960 --> 00:12:03,760
steps that will help you bridge the trust gap and create ethical AI not only within your organization

135
00:12:03,760 --> 00:12:10,960
but for the customers as well. Let's start with the first step, assessing the potential for ethical

136
00:12:10,960 --> 00:12:15,680
risk and setting up algorithm controls. We touched on both of these things when I talked about

137
00:12:15,680 --> 00:12:21,120
including ethical AI at every step in the process. The risk evaluation comes in the problem identification

138
00:12:21,120 --> 00:12:26,240
stage when we talk about the impact of our model. You need to perform these ethical sweeps at every

139
00:12:26,240 --> 00:12:30,880
step in the process to evaluate the solution's vulnerabilities. And a big part of this is in the

140
00:12:30,880 --> 00:12:37,840
modeling stage. You must evaluate various models designed for balancing between fitting bias and

141
00:12:37,840 --> 00:12:43,040
variance. And this also includes checking your predictions after the models are in production

142
00:12:43,040 --> 00:12:48,240
to make sure that your models aren't becoming stale or drifting. Secondly, you must set up a

143
00:12:48,240 --> 00:12:53,760
diverse development team. This also happens in the problem identification stage. And I don't just mean

144
00:12:53,760 --> 00:12:58,960
that you have diversity in your IT developers. Although that is a great step, it's not enough.

145
00:12:58,960 --> 00:13:03,680
You need to be including people on the business side, maybe in sales, marketing, even supply chain,

146
00:13:03,680 --> 00:13:08,240
dependent on what the specific problem is. And then you even need to think about including employees

147
00:13:08,240 --> 00:13:13,760
from legal or HR. Everyone will have a different viewpoint. And together, you can all catch some

148
00:13:13,760 --> 00:13:18,720
of those vulnerabilities you may not have even thought of with just your own IT team. If the

149
00:13:18,720 --> 00:13:22,960
diverse talent doesn't exist from within, this is where you would need to create an environment

150
00:13:22,960 --> 00:13:30,560
that allows you to hire or subcontract with diverse participants. The third step is to create a policy

151
00:13:30,560 --> 00:13:35,440
guide. If you want everyone in your organization to be on the same page about ethical AI,

152
00:13:35,440 --> 00:13:40,320
your organization should define their position related to AI technologies and communicate this

153
00:13:40,320 --> 00:13:45,520
through a policy statement or, at minimum, a set of guidelines. What happens if a user of an AI

154
00:13:45,520 --> 00:13:50,800
model within your organization realizes a model is making a flawed decision? What avenues do they

155
00:13:50,800 --> 00:13:55,520
have available to them to report this? That's just one example of an issue that could be solved with

156
00:13:55,520 --> 00:14:01,360
a policy guide. Making it known across the company that the organization has a stance on ethical AI

157
00:14:01,360 --> 00:14:05,280
isn't enough if avenues don't exist to ensure that that stance is being met.

158
00:14:06,960 --> 00:14:12,240
The fourth step is taking the time to educate AI teams. These conversations are not easy,

159
00:14:12,240 --> 00:14:16,240
but they have to be had. And it helps if it's done as a team or as a division.

160
00:14:16,320 --> 00:14:20,960
You could require employees to participate in an ethics and AI course, or you could have someone

161
00:14:20,960 --> 00:14:25,440
come in and conduct a workshop around it. It also helps if the team themselves are presenting

162
00:14:25,440 --> 00:14:29,920
internally on ethics topics, so they are not only talking the talk, but they're walking the walk.

163
00:14:30,480 --> 00:14:34,160
At my company, we have bi-weekly knowledge shares for our data science team.

164
00:14:34,160 --> 00:14:39,200
And multiple times we have discussed ethical AI in these sessions and its implications in our work.

165
00:14:39,200 --> 00:14:42,480
You could show an example of when you found a biased field in your data set,

166
00:14:42,480 --> 00:14:46,480
or maybe demonstrate how to use a new tool or technology that helps with the explainability

167
00:14:46,480 --> 00:14:54,720
of models. The fifth step is to track and participate in industry regulations.

168
00:14:54,720 --> 00:14:59,920
As an organization, you should be complying with all of the global regulations, such as GDPR,

169
00:14:59,920 --> 00:15:05,200
Ethics Guidelines for Trustworthy AI, and you can also support pledges that promote ethical AI,

170
00:15:05,200 --> 00:15:09,120
such as the Safe Face Pledge and the Lethal Autonomous Weapons Pledge.

171
00:15:10,560 --> 00:15:15,920
So those past five steps were all things that you could do as an organization to promote ethical AI

172
00:15:15,920 --> 00:15:21,120
in the workplace. But a lot of times we don't have the power to enact these changes. We can be a

173
00:15:21,120 --> 00:15:25,280
thought leader and promote these practices and spread the word, but at the end of the day,

174
00:15:25,280 --> 00:15:30,240
the organization needs to enact these practices. As a tech user, though, there are some things that

175
00:15:30,240 --> 00:15:35,760
we can do immediately to promote ethical AI, and that is where Step 6 and 7 come in.

176
00:15:35,760 --> 00:15:41,680
So Step 6 is around leveraging automated tools. You can leverage technology to evaluate AI systems

177
00:15:41,680 --> 00:15:47,200
with potential bias or flaws, to reduce the manual aspect of the task, and to ensure consistency in

178
00:15:47,200 --> 00:15:51,520
how the rules are applied. You can also use counterfactuals to increase the transparency

179
00:15:51,520 --> 00:15:57,520
of your models without revealing the underlying intellectual property. And lastly, Step 7 is

180
00:15:57,520 --> 00:16:02,240
minimizing the bias in the learning data, which we had talked about earlier when including ethical

181
00:16:02,240 --> 00:16:07,440
AI at every step in the process. Here we can create a bias hypothesis and then evaluate and

182
00:16:07,440 --> 00:16:14,160
test for bias in our exploratory data analysis. We can assess and recognize the data collection,

183
00:16:14,160 --> 00:16:18,880
sampling, and validity issues related to bias as well, so that moving forward we have a better

184
00:16:18,880 --> 00:16:21,920
understanding of how to remove the bias before it even gets into the model.

185
00:16:22,560 --> 00:16:27,040
But as you're all Python programmers, maybe even data scientists who are creating AI models,

186
00:16:27,040 --> 00:16:31,280
I wanted to give you an immediate solution that you could start implementing today that gives you

187
00:16:31,280 --> 00:16:37,280
control over producing ethical AI within your team. And that solution is Shapley Values. So

188
00:16:37,280 --> 00:16:42,000
SHAP is a Python library for modeling explainability, and it leverages the idea of

189
00:16:42,000 --> 00:16:47,280
Shapley Values for model feature influence scoring. There are other Python libraries out there that do

190
00:16:47,760 --> 00:16:52,240
super similar things, which include Lime or InterpretML. And some other automated tools you

191
00:16:52,240 --> 00:16:58,800
could use to minimize bias and increase explainability are the AI Explainability 360 and AI Fairness 360

192
00:16:58,800 --> 00:17:04,080
toolkits, which can be accessed through Python as well. They're all great tools, but today I want to

193
00:17:04,080 --> 00:17:09,120
focus on SHAP. I'm first going to walk you guys through an example of the calculation so you

194
00:17:09,120 --> 00:17:13,200
understand how the library works underneath the hood, and then I'll switch over to a Jupyter

195
00:17:13,200 --> 00:17:20,000
Notebook and run through an example of using the library to explain some predictions.

196
00:17:22,400 --> 00:17:28,880
So let's start with what SHAP stands for. It stands for Shapley Additive Explanations. And

197
00:17:28,880 --> 00:17:35,040
the definition of a Shapley Value is the average marginal contribution of a feature value across

198
00:17:35,040 --> 00:17:40,400
all possible coalitions. This uses a lot of jargon from game theory because it's a method

199
00:17:40,400 --> 00:17:44,800
from cooperative game theory that tells us how to fairly distribute the payout among features.

200
00:17:45,600 --> 00:17:51,120
Put simply, a prediction can be explained by assuming that each feature value of the instance

201
00:17:51,120 --> 00:17:57,520
or row is a player in a game where the prediction is the payout. So we're going to walk through

202
00:17:57,520 --> 00:18:02,320
this equation below with an example so that we can fully understand how the method works.

203
00:18:04,800 --> 00:18:10,320
So here's our use case. You're creating a machine learning model to predict condo prices. First,

204
00:18:10,320 --> 00:18:15,280
certain condo, it predicts that the condo is worth $300,000, and you need to explain this

205
00:18:15,280 --> 00:18:21,360
prediction. The apartment has a size of 50 meters squared. It's located on the second floor, has a

206
00:18:21,360 --> 00:18:28,960
park nearby, and cats are banned. If the average prediction for all condos is $310,000, we need to

207
00:18:28,960 --> 00:18:34,560
explain how each of these feature values mentioned above contribute to the prediction compared to

208
00:18:34,560 --> 00:18:43,040
the average prediction. So let's use our game theory jargon to frame this problem. So for our

209
00:18:43,040 --> 00:18:48,800
use case, the game that is being played is referring to a prediction task for a single

210
00:18:48,800 --> 00:18:54,720
instance of the data set, so known as a row. So we're going to compute Shapley values at a row

211
00:18:54,720 --> 00:19:01,360
level. The gain is the actual prediction for this instance minus the average prediction for all

212
00:19:01,360 --> 00:19:08,400
instances. So in our example, that would be $300,000 minus $310,000, so our gain here is

213
00:19:08,400 --> 00:19:15,280
negative $10,000. The players in our game are the feature values of the instance that collaborate

214
00:19:15,280 --> 00:19:20,800
to receive the gain or predict a certain value. So again, in our example, the players are having a

215
00:19:20,800 --> 00:19:27,680
park nearby, cats being banned, the area being equal to 50 meters squared, and the condo being

216
00:19:27,680 --> 00:19:32,880
on the second floor. So these are all working together to achieve a prediction of $300,000.

217
00:19:34,240 --> 00:19:39,920
So the overall goal of this game is to explain the difference between the actual prediction

218
00:19:39,920 --> 00:19:43,520
and the average prediction, which that difference is negative $10,000.

219
00:19:44,800 --> 00:19:49,760
So what do we mean by explain? This means a possible answer could be that park nearby

220
00:19:49,760 --> 00:19:56,560
contributed $30,000, the size being 50 meters squared contributed $10,000, the condo being on

221
00:19:56,560 --> 00:20:01,520
the second floor contributed nothing, and cats being banned contributed negative $50,000.

222
00:20:02,960 --> 00:20:07,520
So if you add all of those up, our final contribution is negative $10,000, which is

223
00:20:07,520 --> 00:20:14,640
equal to our gain. So here we were able to explain and distribute that payout among our players

224
00:20:14,640 --> 00:20:20,160
and explain why that condo prediction was $300,000 by looking at what each specific

225
00:20:20,160 --> 00:20:29,360
feature contributed to that gain. So those were made up payouts, but how do we actually calculate

226
00:20:29,360 --> 00:20:34,720
those values per each player or per each feature? And the answer is that we have to find the

227
00:20:34,720 --> 00:20:40,640
Shapley value of each feature. And if you remember the Shapley value definition, it's the average of

228
00:20:40,640 --> 00:20:45,760
all the marginal contributions to all possible coalitions. So how do we find that average?

229
00:20:46,480 --> 00:20:50,720
We'll first need to list out all of our coalitions. In our example, we're trying

230
00:20:50,720 --> 00:20:57,760
to find the Shapley value just for a cat being banned. We could have no features. We could have

231
00:20:57,760 --> 00:21:03,440
just park nearby. We could have park nearby and size of 50 squared meters. So you're starting

232
00:21:03,440 --> 00:21:08,400
to get the idea here. You can see the list of all the possible coalitions up there on the right

233
00:21:08,400 --> 00:21:12,960
for our specific example, which is the cat being banned, right? That's the feature we're

234
00:21:12,960 --> 00:21:18,240
trying to predict a Shapley value for. So to make it simple, we're just going to start with one

235
00:21:18,240 --> 00:21:23,360
coalition. So let's use the one that's circled in red. Park being nearby and the size of the

236
00:21:23,360 --> 00:21:29,840
apartment being 50 squared meters. If we want to simulate this coalition, we randomly sample from

237
00:21:29,840 --> 00:21:36,000
a donor row for the features that are not in the coalition, such as the floor feature. So the very

238
00:21:36,000 --> 00:21:40,400
first step is to randomly sample a donor row and use that information to fill in the gaps.

239
00:21:40,960 --> 00:21:45,920
Then we predict the condo price. So for this specific coalition, park nearby and 50 squared

240
00:21:45,920 --> 00:21:54,000
meters with the random values from the donor row, we get $310,000. Now what we do is remove cat banned

241
00:21:54,000 --> 00:21:59,040
and replace it with the value of cat from the donor row. So in this case, the random value is cat

242
00:21:59,040 --> 00:22:05,760
allowed. Now we predict the price of the condo and we get $320,000. So we see that that marginal

243
00:22:05,760 --> 00:22:14,560
contribution of a cat being banned was negative $10,000. Since we just sampled from one donor row,

244
00:22:16,560 --> 00:22:23,600
that's a very small sample set. So what we need to do is pick a new donor row and go through the

245
00:22:23,600 --> 00:22:29,120
exact same steps we just did, ending with finding the contribution. And now we would take the average

246
00:22:29,120 --> 00:22:35,440
of those two contributions. So the estimate that we get depends on the values of the randomly

247
00:22:35,440 --> 00:22:40,320
drawn donor row. So in order to get a better estimate for the Shapley values, we need to

248
00:22:40,320 --> 00:22:44,960
repeat this sampling step multiple times and average all of the contributions.

249
00:22:46,960 --> 00:22:52,720
Now remember, that was just for an example of one coalition for cat banned. But as you can see,

250
00:22:52,720 --> 00:22:58,160
there are multiple coalitions for just the feature cat banned. So we repeat this computation and

251
00:22:58,160 --> 00:23:03,920
process for all possible coalitions of cat banned. That means we predict the condo price with and

252
00:23:03,920 --> 00:23:08,560
without the feature value cat banned and take the difference to get the marginal contribution.

253
00:23:08,560 --> 00:23:13,760
And at the end, that Shapley value back to that definition is the average of all the marginal

254
00:23:13,760 --> 00:23:19,920
contributions to all possible coalitions. Now that definition makes sense in your head,

255
00:23:19,920 --> 00:23:25,440
but you're starting to imagine, whoa, this computation time is going to increase exponentially

256
00:23:25,440 --> 00:23:31,040
with the number of features, especially with that sampling that's going on. So to keep computation

257
00:23:31,040 --> 00:23:37,040
times manageable, you must compute contributions for only a few samples of the possible coalitions.

258
00:23:40,960 --> 00:23:46,560
So what is Shap, the Python library output actually looked like when implemented in Python?

259
00:23:46,560 --> 00:23:51,200
So it essentially can take a black box model with its feature inputs, base rate prediction,

260
00:23:51,200 --> 00:23:56,240
and output prediction for one row or instance. And then it will explain exactly why that prediction

261
00:23:56,240 --> 00:24:00,560
was made by showing the payouts from each feature or the features Shapley values.

262
00:24:00,560 --> 00:24:05,920
In this example, the base rate was 0.1 and the prediction that came out was 0.4. So how did we

263
00:24:05,920 --> 00:24:12,320
come to that prediction? Why did it have a gain of 0.3? So you can see that BMI being equal to 40 was

264
00:24:12,320 --> 00:24:17,360
responsible for 0.1 of that increase from the base rate. And so was BP being equal to 180.

265
00:24:18,240 --> 00:24:24,720
But sex being female bumped down that gain by negative 0.3 and then her age bumped it back

266
00:24:24,720 --> 00:24:29,840
up by 0.4. So if you add up all those values together, you see that the overall prediction

267
00:24:29,840 --> 00:24:35,040
increase from the base rate is 0.3. And you know exactly how much each of the features

268
00:24:35,040 --> 00:24:40,160
contributed to that increase. In this case, you could say that age was the most important.

269
00:24:43,680 --> 00:24:47,520
Shap has great documentation online. So you can dive deeper into this,

270
00:24:47,520 --> 00:24:52,240
but I just wanted to show you that it works on a variety of models. It has explainers specifically

271
00:24:52,240 --> 00:24:56,720
for trees, gradients, or deep learning models. And then it has a catch all explainer called

272
00:24:56,720 --> 00:25:01,440
kernel explainer that explains the output of any function. And it also has different types

273
00:25:01,440 --> 00:25:07,440
of plots that you can look at for the outputs. So I'm going to show you guys what this looks

274
00:25:07,440 --> 00:25:13,120
like for different models using the Titanic data set. So we're going to go ahead and hop over to

275
00:25:13,120 --> 00:25:20,080
my Jupyter notebook. So in here, all I have to do is import sklearn, my train test split,

276
00:25:20,080 --> 00:25:26,640
pandas, numpy, shap, and then time, because I was timing it earlier. I do some data cleansing here,

277
00:25:26,640 --> 00:25:32,720
right? I drop any rows that have null. It wasn't that many. And then I decide just to not even use

278
00:25:32,720 --> 00:25:40,400
cabin as a field. So now I have my cleaned up data set. And I do a little bit of feature

279
00:25:40,400 --> 00:25:45,680
transformation, right? So I pull out of their name, their title, because this has been shown to be

280
00:25:46,640 --> 00:25:51,600
a feature that has a lot of influence in this Titanic data set. So whether they were named a

281
00:25:51,600 --> 00:25:58,720
miss, a mister, a master, right, affected their chance of survival. And then I create a train,

282
00:25:59,440 --> 00:26:05,680
my Y train set, and my X train set. And here's just a little snippet of what my final training

283
00:26:05,680 --> 00:26:11,680
set looked like, right? So I have the P class, which is their ticket class. I have their sex,

284
00:26:11,680 --> 00:26:17,840
male or female. I have their age. I have whether how many siblings or spouses they had aboard

285
00:26:17,840 --> 00:26:24,240
Titanic and how many parents or children were aboard. I also have the amount they paid and,

286
00:26:24,240 --> 00:26:29,520
you know, money for their fare. I have what port they embarked from. And I have their prefix that

287
00:26:29,520 --> 00:26:36,080
I pulled out, right, miss or mister. And I also have whether they survived or not in my Y train.

288
00:26:36,080 --> 00:26:41,120
So here I'm just setting up my data again, doing a train test split. So now I have my training and

289
00:26:41,120 --> 00:26:48,240
my test sets. Now we get into the modeling. So I wanted to do KNN, a neural net, and then a random

290
00:26:48,240 --> 00:26:53,120
four. It's just to show you guys that Shapley will output different things for different models,

291
00:26:53,120 --> 00:26:59,280
because models score features different ways, right? So here we're doing, we're using sklearn.

292
00:26:59,280 --> 00:27:03,680
We're doing a k nearest neighbors model. That accuracy is going to be the same as the

293
00:27:03,840 --> 00:27:09,840
nearest neighbors model. That accuracy is 66%. So not too great on the test set, right?

294
00:27:11,200 --> 00:27:16,160
But what I want to show you is what Shapley would tell us. So we know that our model is not that

295
00:27:16,160 --> 00:27:21,200
great, but we also want to know exactly why it predicted something for a certain row. So remember

296
00:27:21,200 --> 00:27:27,200
that we're computing Shapley values for a specific instance or row in the data set. So I'm using the

297
00:27:27,200 --> 00:27:32,640
kernel explainer here. I'm calling the Shap library. I'm giving it the model that I want,

298
00:27:32,640 --> 00:27:38,160
right? The model that I initiated above. And I'm telling, use the X training set as that data set

299
00:27:38,160 --> 00:27:45,120
that you're going to sample from for those donor rows. Then I'm saying, grab the first row in the

300
00:27:45,120 --> 00:27:51,920
X test set and using that explainer that I just set above, produce the Shapley values for that

301
00:27:51,920 --> 00:27:58,000
specific row. And the way I want to see it is using the Shap force plot. So I pass it the expected

302
00:27:58,000 --> 00:28:03,840
value, right? That base rate, I pass it the Shapley values, and I pass it the actual values

303
00:28:03,840 --> 00:28:09,760
from the row. So this is what the force plot looks like. So you can see what the base value was,

304
00:28:09,760 --> 00:28:14,320
and then you can see what the output started at. If it's in red, it means that it made the prediction

305
00:28:14,320 --> 00:28:18,960
go higher. So here we were predicting if this person was going to survive or not. So this is

306
00:28:18,960 --> 00:28:23,120
the survival class that we're looking at. And so you can see here that, you know, their sex,

307
00:28:23,120 --> 00:28:28,640
they were female, they were 24, and they paid a high fare, right? So we predicted that they

308
00:28:28,640 --> 00:28:34,240
would have a 60% chance of survival. And that's looking at it from the force plot. If you want to

309
00:28:34,240 --> 00:28:38,560
look at it from the waterfall plot, that helps a little bit more in seeing exactly, you know,

310
00:28:41,280 --> 00:28:46,000
how important that specific feature was, because it increased it by 0.18. I mean, here you could

311
00:28:46,000 --> 00:28:49,280
kind of do the math yourself. You see that fair is the most important, but you don't know

312
00:28:49,280 --> 00:28:55,920
specifically by how much. So again, our base rate was starting at 0.37 something, and our prediction

313
00:28:55,920 --> 00:29:02,080
was 0.6, but you're seeing for each one in this waterfall plot exactly by how much it went up by.

314
00:29:05,360 --> 00:29:10,160
Here is another example. This is the 15th person in the data set, right? We're looking again,

315
00:29:10,160 --> 00:29:15,920
if this person would survive. They were a male, they were 32, and they paid a very low class. So

316
00:29:16,560 --> 00:29:22,080
the K&N model predicted that they had a 0% chance of survival. Again, we have that force plot and

317
00:29:22,080 --> 00:29:28,480
that waterfall plot. You can also explain all the predictions in a test set with SHAP. It takes a

318
00:29:28,480 --> 00:29:35,040
very long time to run, as you can imagine, but it basically averages all of the Shapley values per

319
00:29:35,040 --> 00:29:41,840
every single row that it calculates it for. And so what you get here is a plot of the most important

320
00:29:41,840 --> 00:29:46,800
features over the entire data set. So you see for class 0 and class 1, they're weighted the same,

321
00:29:46,800 --> 00:29:51,600
right? They have the same importance, but fair was the most important feature in this case for

322
00:29:51,600 --> 00:29:56,640
deciding if a person was going to survive or not for a K&N model, as well as age and then their

323
00:29:56,640 --> 00:30:04,080
sex and so on and so forth. You also have these really cool things called dependency plots. So

324
00:30:04,080 --> 00:30:10,480
you can show right here, we're showing the scale or the range of the fair, right? So it could go

325
00:30:10,480 --> 00:30:16,640
from anywhere from 0 to 500. And on the y-axis, you see what the Shapley value was for that specific

326
00:30:16,640 --> 00:30:21,840
instance, right? So it's just showing you that on a plot. But what it also shows you, I didn't specify

327
00:30:21,840 --> 00:30:27,120
what feature I wanted it to show me the dependence against. So it did an auto and it chooses the one

328
00:30:27,120 --> 00:30:32,960
that had the highest dependence. But in this case, it was mister. And you can see if it's one, that

329
00:30:32,960 --> 00:30:38,400
means that they were a mister, so they were a boy. If not, it's 0, so they were not. So you can see

330
00:30:38,400 --> 00:30:45,840
here, right? Most people that paid a low fair did not have a high chance at surviving, right?

331
00:30:45,840 --> 00:30:51,680
There were a lot of them down here that had negative Shapley value. So even if they had a

332
00:30:51,680 --> 00:30:59,440
low fair, right, that Shapley value was going to be low. You see a bit more women up here, right?

333
00:30:59,440 --> 00:31:05,520
It looks like most of the men are down here. And then you see as you get to the fair being more

334
00:31:05,520 --> 00:31:11,360
expensive, almost all those people starting at 100 had those higher Shapley values, which means

335
00:31:11,360 --> 00:31:15,760
that they had a higher chance of surviving. So we see here that fair plays a pretty important role

336
00:31:16,960 --> 00:31:24,160
and not so much here with the mister, the missus. Again, here we're looking at age. So we have that

337
00:31:24,160 --> 00:31:29,440
range of age, that range of age on the bottom. And then we have the Shapley values per every single

338
00:31:29,440 --> 00:31:34,080
prediction or per every single instance. And you have that dependence against the mister. So that

339
00:31:34,080 --> 00:31:42,960
was the KNN model. Now we want to look at a neural network. So our train was 84%. Our test was 79,

340
00:31:42,960 --> 00:31:49,520
a lot better than KNN. Let's see what it showed were the most important factors using this neural

341
00:31:49,520 --> 00:31:54,800
net model. So we see that the base value was 0.39 for that same person, right? Predicting that same

342
00:31:54,800 --> 00:31:59,680
class, whether they would survive or not. But this time, instead of a 60% survival rate, we said that

343
00:31:59,680 --> 00:32:05,280
they had a 98% survival rate and we have that force plot and we have that waterfall plot.

344
00:32:06,000 --> 00:32:10,080
And then in contrast, you see, we have both colors here. Now, when we did the KNN model,

345
00:32:10,640 --> 00:32:15,600
you only had one color. It was blue bringing it down, but we do see here that their siblings,

346
00:32:15,600 --> 00:32:22,160
right? Being zero increased this a little bit higher for them to survive, but still this person,

347
00:32:22,160 --> 00:32:29,040
instead of having 0% survival from the KNN model had 0.07% of survival from the neural network

348
00:32:29,040 --> 00:32:34,560
model. So it's good to see that different models have different Shapley values, right? Because they

349
00:32:34,560 --> 00:32:40,880
weigh things differently. And again, you can explain all the predictions in the test set.

350
00:32:40,880 --> 00:32:46,240
So if you remember from above, fair was the most important, but here Mr. is the most important. So

351
00:32:46,240 --> 00:32:50,560
whether they were male or not, if they had that prefix and then the class that they were in. So

352
00:32:50,560 --> 00:32:55,680
P class one was the highest ticket class, P class three was the lowest. So you see that those play

353
00:32:55,760 --> 00:33:00,560
a really important role in this specific model. So we're increasing that explainability, right? So

354
00:33:00,560 --> 00:33:07,840
for the overall predictions, we have this, but we also have per specific. If you needed to tell

355
00:33:07,840 --> 00:33:12,000
someone why that specific prediction was made, well, you have that explainability now.

356
00:33:13,440 --> 00:33:18,960
And we have these dependence plots here. You can see a really cool pattern. So when looking at age

357
00:33:18,960 --> 00:33:25,360
versus P class one, so being the highest class, you see that those between zero and 10. So those

358
00:33:25,360 --> 00:33:30,400
that were really young, they all had a really high Shapley values, right? So they had a higher

359
00:33:30,400 --> 00:33:35,200
chance for survival and it didn't matter whether they were in the first class or they weren't

360
00:33:35,200 --> 00:33:40,000
because they were young. They were going to be saved. Meanwhile, if you look down here, the 40

361
00:33:40,000 --> 00:33:43,920
between the seventies, the first thing you noticed is that most of those people were in the first

362
00:33:43,920 --> 00:33:49,280
class if they were older, right? And you also notice that it doesn't matter if they were in

363
00:33:49,280 --> 00:33:54,160
the first class or not, that didn't affect their chances of survival. It actually made it worse if

364
00:33:54,160 --> 00:33:58,880
they were older because they had a negative Shapley value associated to them. So those

365
00:33:58,880 --> 00:34:03,760
dependence plots are really cool as well. And then lastly, just to show the third difference,

366
00:34:03,760 --> 00:34:09,200
right? I did do a random forest. It overfit a little. The train was 92, but the test was 77.

367
00:34:10,320 --> 00:34:16,640
And again, we looked at the single predictions, right? So we looked at that first person. Here

368
00:34:16,640 --> 00:34:22,560
they had a 97% chance of survival. So one lower than the neural net one, and we have that waterfall

369
00:34:22,560 --> 00:34:30,240
plot and that dependence plot here. Sex seemed to be the most important. And then we have that

370
00:34:30,240 --> 00:34:35,200
other person, the 15th person in the data set, right? They had a 0.02 chance of survival. So

371
00:34:35,200 --> 00:34:39,520
lower than what the neural net said. And you see that Mr. played a really important role.

372
00:34:39,520 --> 00:34:45,520
Male, those are pretty much the same. And then explaining all the predictions in the test set.

373
00:34:45,520 --> 00:34:50,480
Here they thought sex, the sex was the most important. And this one also thought that the

374
00:34:50,480 --> 00:34:54,480
P class, whether they were in the first class or the third class was also pretty important.

375
00:34:54,480 --> 00:34:56,880
So this one was a little bit more similar to the neural net.

376
00:35:01,440 --> 00:35:06,560
Okay. So that is all I have for you guys today. Thank you so much for listening in. I hope that

377
00:35:06,560 --> 00:35:11,120
you guys will be able to implement this and use this in your models for explainability.

378
00:35:11,120 --> 00:35:16,960
For any questions, please email gleo at sense corp.com. And all my code is available at that

379
00:35:16,960 --> 00:35:21,920
link right there in my GitHub in a Jupyter Notebook. So you can go ahead and download it

380
00:35:21,920 --> 00:35:26,320
and use whatever you need. Again, thank you so much and enjoy the rest of your day.

