1
00:00:00,000 --> 00:00:08,140
I said most of you probably aren't as familiar with Kafka as you are with Python, so we'll

2
00:00:08,140 --> 00:00:13,580
start with just talking a little bit about Apache Kafka and what it is.

3
00:00:13,580 --> 00:00:16,200
There's a great book if you really want to learn about Kafka called Kafka the Definitive

4
00:00:16,200 --> 00:00:17,200
Guide by Gwen Shapira.

5
00:00:17,200 --> 00:00:18,200
It's in its second edition now.

6
00:00:18,200 --> 00:00:19,200
A lot of details there.

7
00:00:19,200 --> 00:00:20,200
I highly recommend that book.

8
00:00:20,200 --> 00:00:21,200
But we don't have time for all that.

9
00:00:21,200 --> 00:00:24,720
So we're going to use my version, which is Kafka the Diminutive Guide, which is a little

10
00:00:24,720 --> 00:00:25,720
bit shorter.

11
00:00:26,160 --> 00:00:29,760
So first we're going to talk about events, because Kafka at its core is an event streaming

12
00:00:29,760 --> 00:00:30,760
system.

13
00:00:30,760 --> 00:00:36,280
And so when we talk about an event, an event encompasses notification and state.

14
00:00:36,280 --> 00:00:40,640
So it tells us something that happened, and it tells us information about what happened.

15
00:00:40,640 --> 00:00:43,160
So in this case, we see we have order placed, we have details about the order.

16
00:00:43,160 --> 00:00:45,720
We have a temperature reading, and we have temperature.

17
00:00:45,720 --> 00:00:47,080
Now we may not need both of those, right?

18
00:00:47,080 --> 00:00:50,360
We might have a system that's just checking to make sure the temperature sensors are all

19
00:00:50,360 --> 00:00:51,360
online.

20
00:00:51,360 --> 00:00:54,120
And if that's the case, then we just know, oh, it was red, it was red, it's like a heartbeat.

21
00:00:54,120 --> 00:00:55,120
We know it's good.

22
00:00:55,200 --> 00:00:58,160
We also might want to trigger something based on a change in temperature.

23
00:00:58,160 --> 00:00:59,480
So we want to have that data as well.

24
00:00:59,480 --> 00:01:02,560
So an event encompasses both of those two things, notification and state.

25
00:01:02,560 --> 00:01:04,960
And that's really important.

26
00:01:04,960 --> 00:01:07,440
So this is an example of what an event might look like in Kafka.

27
00:01:07,440 --> 00:01:09,760
Now in Kafka, events are all just bytes.

28
00:01:09,760 --> 00:01:13,440
It's all binary in a proprietary format that's very efficient.

29
00:01:13,440 --> 00:01:17,160
But when we're serializing, deserializing, it's going to be some type of an object.

30
00:01:17,160 --> 00:01:18,160
Usually you'll see it.

31
00:01:18,160 --> 00:01:21,840
And the two key components are, or two main components, a key and a value.

32
00:01:21,840 --> 00:01:25,680
The key is usually a simple data type, like a long or a string.

33
00:01:25,680 --> 00:01:27,480
And the value is where you're going to have your payload.

34
00:01:27,480 --> 00:01:31,080
So it's going to be more like an object, like a JSON object or a protobuf or something like

35
00:01:31,080 --> 00:01:32,080
that.

36
00:01:34,840 --> 00:01:39,240
And those events are written to a log, which we call a topic in Kafka.

37
00:01:39,240 --> 00:01:42,720
And this is an append-only, immutable log of events.

38
00:01:42,720 --> 00:01:46,280
Events are written to, starting from the beginning, and you just keep it getting added.

39
00:01:46,280 --> 00:01:48,080
And you see that number there that represents an offset.

40
00:01:48,760 --> 00:01:52,400
Offset is a number that's auto-incrementing, and it only increases for each partition or

41
00:01:52,400 --> 00:01:55,600
each topic log.

42
00:01:55,600 --> 00:01:56,600
It never gets reused.

43
00:01:56,600 --> 00:02:01,600
And so that offset number stays with that event for its whole life.

44
00:02:01,600 --> 00:02:06,800
Now actually, since Kafka is a distributed system, most topics are made up of multiple

45
00:02:06,800 --> 00:02:10,960
partitions, which are, multiple logs called partitions, sorry.

46
00:02:10,960 --> 00:02:14,560
And this allows us to spread the workload out so that each of these partitions can reside

47
00:02:14,560 --> 00:02:16,840
on a different node in the Kafka cluster.

48
00:02:16,840 --> 00:02:17,920
The nodes are called brokers.

49
00:02:17,920 --> 00:02:22,160
So you have multiple brokers in your cluster, and the partitions will reside on different

50
00:02:22,160 --> 00:02:23,160
ones.

51
00:02:23,160 --> 00:02:26,920
And this allows us to scale.

52
00:02:26,920 --> 00:02:29,320
So events are written to these partitions as log.

53
00:02:29,320 --> 00:02:30,800
And right now, we're not using a key.

54
00:02:30,800 --> 00:02:31,800
We just have a value in here.

55
00:02:31,800 --> 00:02:35,520
And so these events are just going to be somewhat round robin distributed.

56
00:02:35,520 --> 00:02:38,360
Mostly the goal here is to keep the partitions evenly filled up.

57
00:02:38,360 --> 00:02:41,120
But as you'll see, we don't have here is any ordering.

58
00:02:41,120 --> 00:02:44,760
So you see our 100 series events are in different partitions.

59
00:02:44,760 --> 00:02:48,920
And that makes a difference, because when events are consumed or read from this topic,

60
00:02:48,920 --> 00:02:50,720
they're going to be read by partition.

61
00:02:50,720 --> 00:02:53,800
And we only have ordering guarantees at a partition level.

62
00:02:53,800 --> 00:02:56,200
So if order matters, you need to use a key.

63
00:02:56,200 --> 00:03:00,040
So when we do have a key, the key is used to determine what partition to write the event

64
00:03:00,040 --> 00:03:01,840
to.

65
00:03:01,840 --> 00:03:03,640
So we have this A1 here.

66
00:03:03,640 --> 00:03:05,360
There's an algorithm that figures out where that should go.

67
00:03:05,360 --> 00:03:09,240
But from then on, every event with that key will end up in that same partition.

68
00:03:09,240 --> 00:03:13,600
And this will keep our events in order in the partitions, and that means they're also

69
00:03:13,600 --> 00:03:14,600
consumed in order.

70
00:03:14,600 --> 00:03:16,680
So we don't have guaranteed ordering, which is important if you're working with things

71
00:03:16,680 --> 00:03:17,680
like financial transactions.

72
00:03:17,680 --> 00:03:21,320
We don't want to have a credit and debit mixed up.

73
00:03:21,320 --> 00:03:26,080
Now, this also does mean that we might not have quite as even of distribution.

74
00:03:26,080 --> 00:03:27,080
Oops, I went too far.

75
00:03:27,080 --> 00:03:28,080
Sorry about that.

76
00:03:28,080 --> 00:03:29,080
Oh, now I have to go through them again.

77
00:03:29,080 --> 00:03:30,540
You might have quite as even a distribution.

78
00:03:30,540 --> 00:03:33,040
So it is important to think about what your key is going to be.

79
00:03:33,040 --> 00:03:37,120
And this is the same thing with any distributed system that uses partitions or sharding.

80
00:03:37,120 --> 00:03:40,600
You can end up with a hot partition if your keys are out of whack.

81
00:03:40,600 --> 00:03:43,960
So if you have thousands of customers, but you have two customers that make up 98% of

82
00:03:43,960 --> 00:03:46,960
your business, then you don't want to use customer ID as a key.

83
00:03:46,960 --> 00:03:47,960
All right.

84
00:03:47,960 --> 00:03:52,200
So the thing that writes the data or these events to the topic is a producer.

85
00:03:52,200 --> 00:03:54,480
The producer is part of a client library.

86
00:03:54,480 --> 00:03:58,360
The standard ones that comes with Kafka is written in Java, but there's also a Python

87
00:03:58,360 --> 00:04:01,200
library, and there's libraries for Golang, .NET, lots of other languages, but there's

88
00:04:01,200 --> 00:04:04,000
a really good one for Python, which we'll be using today.

89
00:04:04,000 --> 00:04:07,200
And the producer has the smarts of knowing where to write the events to, which partition

90
00:04:07,200 --> 00:04:08,200
it goes to.

91
00:04:08,200 --> 00:04:13,800
And it just writes those events one at a time, again, keeps appending at the end.

92
00:04:13,800 --> 00:04:16,600
And then the consumer comes along and reads those.

93
00:04:16,600 --> 00:04:19,400
The consumer is not really the best name because it doesn't actually consume them.

94
00:04:19,400 --> 00:04:22,480
One of the nice things with Kafka topics is they're durable.

95
00:04:22,480 --> 00:04:25,240
Data stays in a Kafka topic until we don't want it there anymore.

96
00:04:25,240 --> 00:04:29,040
We can set a time period, like I think it defaults seven days, we can set it for a year,

97
00:04:29,040 --> 00:04:30,040
whatever we want.

98
00:04:30,040 --> 00:04:32,600
And at that time period, then older things will start getting deleted.

99
00:04:32,600 --> 00:04:34,040
Or we can set a size limit.

100
00:04:34,040 --> 00:04:35,040
Or we can make it just permanent.

101
00:04:35,240 --> 00:04:38,440
We can have permanent retention.

102
00:04:38,440 --> 00:04:41,440
So the consumer will read the data and do something with it.

103
00:04:41,440 --> 00:04:44,640
And then it stays there, and another consumer from a different application can also use

104
00:04:44,640 --> 00:04:47,560
it, which we'll see how that comes into play later.

105
00:04:47,560 --> 00:04:50,760
So the consumer comes along and reads, and these are completely decoupled, they're independent

106
00:04:50,760 --> 00:04:52,880
of each other, they work at their own pace.

107
00:04:52,880 --> 00:04:57,940
The consumer keeps track of the offset that is processed, and periodically it will commit

108
00:04:57,940 --> 00:04:58,940
that offset.

109
00:04:58,940 --> 00:05:01,200
This is, OK, this is how far I've gotten, and I know I've gotten this one completely

110
00:05:01,200 --> 00:05:02,200
done.

111
00:05:02,280 --> 00:05:06,480
Then if a consumer goes offline for some reason, the producer keeps on going, it's merry way,

112
00:05:06,480 --> 00:05:07,480
it doesn't care.

113
00:05:07,480 --> 00:05:09,960
There is a metric you can watch for that so you know that something's wrong if you need

114
00:05:09,960 --> 00:05:14,120
to intervene, but it won't stop the producer from working.

115
00:05:14,120 --> 00:05:16,880
And then when the consumer comes back online, it knows where it left off in that committed

116
00:05:16,880 --> 00:05:19,640
offset, it picks up where it left off, and it keeps on going.

117
00:05:19,640 --> 00:05:24,680
This is important because it means you never skip any events, and you never lose data.

118
00:05:27,120 --> 00:05:28,880
All right, so consumer groups.

119
00:05:28,880 --> 00:05:30,120
Consumers run in a consumer group.

120
00:05:30,120 --> 00:05:34,000
And consumer groups are a really great tool because they allow us to balance out the workload

121
00:05:34,000 --> 00:05:36,160
across these multiple partitions that we have.

122
00:05:36,160 --> 00:05:38,880
In this example here, we see we have four partitions, and we have two instances of our

123
00:05:38,880 --> 00:05:39,880
consumer application.

124
00:05:39,880 --> 00:05:43,520
This would be the same application, just two instances of it running, leaving Kubernetes

125
00:05:43,520 --> 00:05:46,720
or something like that, and they have a group ID that tells them they're part of the same

126
00:05:46,720 --> 00:05:47,720
group.

127
00:05:47,720 --> 00:05:51,680
And so Kafka will automatically balance the workload across those two instances.

128
00:05:51,680 --> 00:05:56,360
If we get busy, we want to pick up the pace, we can add two more, and now we have four

129
00:05:56,360 --> 00:05:59,680
instances running, and each of those is getting data from its own partition.

130
00:05:59,720 --> 00:06:03,280
So as far as we can scale with this topic, because we only have four partitions, you

131
00:06:03,280 --> 00:06:06,880
cannot split the data in one partition across multiple consumers, otherwise your ordering

132
00:06:06,880 --> 00:06:09,600
guarantees are going to be thrown off again.

133
00:06:09,600 --> 00:06:12,920
Now we can add a fifth one here, it'll just hit idle, which sometimes we'll do that, and

134
00:06:12,920 --> 00:06:15,800
it's a hot standby, so that can be done.

135
00:06:15,800 --> 00:06:18,560
And you can also have multiple groups, they're independent of each other, they can read at

136
00:06:18,560 --> 00:06:21,800
their own pace, they can read starting at different places, they don't affect each other

137
00:06:21,800 --> 00:06:22,800
in any way.

138
00:06:22,800 --> 00:06:27,760
Again, the data is durable and it's independent of the consumers that are reading it.

139
00:06:27,760 --> 00:06:28,760
That's consumer groups.

140
00:06:28,760 --> 00:06:29,760
That's it for Kafka.

141
00:06:29,760 --> 00:06:33,040
So now back to our event-driven microservices.

142
00:06:37,600 --> 00:06:38,600
Why is it going to be interesting?

143
00:06:40,600 --> 00:06:41,600
I really wish the screen worked right.

144
00:06:43,880 --> 00:06:49,480
Okay, so this is kind of the current state, and it's kind of a very simplified, but pretty

145
00:06:49,480 --> 00:06:52,280
common scenario that we'll see with microservices.

146
00:06:52,280 --> 00:06:58,520
We'll see we have a client talking to our web server, that's our blue service there,

147
00:06:58,560 --> 00:07:04,960
and then it's connecting with this green service over a request-response-based HTTP calls, maybe

148
00:07:04,960 --> 00:07:10,280
REST, maybe a GRPC, something like that, and then the blue service is talking to the red

149
00:07:10,280 --> 00:07:15,000
service, the pink service, the gray service, and then sometimes we have maybe an external

150
00:07:15,000 --> 00:07:17,080
system that we have to hit as well.

151
00:07:17,080 --> 00:07:21,000
So this is kind of a common pattern, pretty simple, pretty straightforward, might have

152
00:07:21,000 --> 00:07:24,320
a different variation on it where it's something like this with an orchestrator that fans out

153
00:07:24,360 --> 00:07:31,360
the different services, but still all request-response-based over HTTP or RPC, something like that.

154
00:07:31,720 --> 00:07:36,800
So now this works fine, and it's in use in a lot of places.

155
00:07:36,800 --> 00:07:40,160
I've built a lot of systems this way, but we have, I've run into a lot of problems.

156
00:07:40,160 --> 00:07:43,200
A lot of our systems, even sometimes before getting into production, they'll end up something

157
00:07:43,200 --> 00:07:45,920
more like this, or even worse, right?

158
00:07:45,920 --> 00:07:50,440
And you end up with really chatty services, you end up with circular references, it makes

159
00:07:50,440 --> 00:07:54,200
it very difficult to maintain or extend your system.

160
00:07:54,400 --> 00:07:58,560
And this is where I think event-driven architecture can really help.

161
00:07:58,560 --> 00:08:01,400
So let's look at another option.

162
00:08:01,400 --> 00:08:04,360
So with a similar type of system, we start off the same way, we're still using request-response

163
00:08:04,360 --> 00:08:09,000
from the client to our web server, which makes sense, but now our blue service is going to

164
00:08:09,000 --> 00:08:12,840
do its work and produce some data to a topic.

165
00:08:12,840 --> 00:08:16,240
And then our green service is going to consume the data from that topic and do whatever work

166
00:08:16,240 --> 00:08:17,240
it needs to do.

167
00:08:17,240 --> 00:08:19,320
And it's going to do that, whenever it sees data there, it's going to grab it and use

168
00:08:19,320 --> 00:08:23,080
it, it doesn't have to wait for a call, it's completely independent.

169
00:08:23,320 --> 00:08:27,400
The blue and green are decoupled, and it's going to write to its own topic when it's

170
00:08:27,400 --> 00:08:28,400
done doing its work.

171
00:08:28,400 --> 00:08:31,640
And it could be more than one, this is just a simple example, it could be writing to multiple

172
00:08:31,640 --> 00:08:34,440
topics depending on the different things that the service might do, although it's good to

173
00:08:34,440 --> 00:08:36,760
have services do one thing if you can.

174
00:08:36,760 --> 00:08:40,680
And then the red one consumes that, and on and on, and does the whole thing.

175
00:08:40,680 --> 00:08:47,080
And in the end, the blue service might consume from that gray topic to complete the loop.

176
00:08:47,080 --> 00:08:50,560
Now with this scenario, these services are all decoupled now, they don't know anything

177
00:08:50,560 --> 00:08:51,640
about each other.

178
00:08:51,640 --> 00:08:55,120
If the red service goes down, there's no cascading failure.

179
00:08:55,120 --> 00:08:57,880
The pink service will just sit there and wait until there's more data in that topic for

180
00:08:57,880 --> 00:09:02,760
it to do its work, and when the red service comes back online, the pink service will pick

181
00:09:02,760 --> 00:09:06,320
up the data and keep on going.

182
00:09:06,320 --> 00:09:09,440
You can have things like alerting and monitoring to see when those things like that happen,

183
00:09:09,440 --> 00:09:13,280
but it won't cause failures downstream.

184
00:09:13,280 --> 00:09:18,560
And also this is a little bit more efficient, because the binary data in the Kafka protocol

185
00:09:18,640 --> 00:09:23,600
is very efficient, and it's a lot faster communication between services than HTTP with text-based

186
00:09:23,600 --> 00:09:24,600
messaging.

187
00:09:24,600 --> 00:09:28,400
The other thing that's nice is extensibility.

188
00:09:28,400 --> 00:09:29,920
So is that extra service we still have?

189
00:09:29,920 --> 00:09:33,440
Now that was still, in this diagram, is still a request-response HTTP connection, because

190
00:09:33,440 --> 00:09:36,000
sometimes there's other services you interact with that don't speak events.

191
00:09:36,000 --> 00:09:39,920
And so I wanted to put that in there just to show that it's not either or.

192
00:09:39,920 --> 00:09:43,840
These techniques can be mixed and matched as needed when you're building an application.

193
00:09:43,840 --> 00:09:45,680
But the other thing is you can extend this.

194
00:09:45,680 --> 00:09:48,520
If say later on we decide something else and use the data in that green topic, and we want

195
00:09:48,560 --> 00:09:52,640
to do something else with that data, now we could have just maybe modified the red service

196
00:09:52,640 --> 00:09:55,000
to do this new thing that we want to do, but now we have the red service doing more than

197
00:09:55,000 --> 00:09:58,400
one thing, which isn't a good idea, and we have to open up that service and do some work

198
00:09:58,400 --> 00:10:00,200
when it's already in production.

199
00:10:00,200 --> 00:10:03,560
So instead we can add a whole other service, completely independent of the rest of the

200
00:10:03,560 --> 00:10:07,440
application, and consume from that same topic and do whatever we need to do.

201
00:10:07,440 --> 00:10:10,840
This allows you to do things like adding on even temporary services under a production

202
00:10:10,840 --> 00:10:15,440
application where you want to do some extra logging or monitoring or some analytics or

203
00:10:15,440 --> 00:10:18,080
something like that that isn't part of your main flow and you don't want to interfere

204
00:10:18,080 --> 00:10:19,960
with your main application.

205
00:10:19,960 --> 00:10:23,280
You can do that just by reading from those same topics.

206
00:10:23,280 --> 00:10:28,880
All right, so that's the new scenario.

207
00:10:28,880 --> 00:10:30,880
How do we get there?

208
00:10:30,880 --> 00:10:35,680
So the first thing to do in designing systems like this is to find our events.

209
00:10:35,680 --> 00:10:39,800
Now if we're designing a brand new system, which I envy you, that's always a fun place

210
00:10:39,800 --> 00:10:42,480
to be, I recommend something called event modeling.

211
00:10:42,480 --> 00:10:46,200
There's a video at that link there you can check out that talks about how that's used,

212
00:10:46,280 --> 00:10:53,880
it's basically a system for designing your entire applications using events at the core,

213
00:10:53,880 --> 00:10:55,080
from the UI all the way through.

214
00:10:55,080 --> 00:10:59,160
So it's really a great technique and it's a really cool tool that they show in that

215
00:10:59,160 --> 00:11:00,160
video as well.

216
00:11:00,160 --> 00:11:05,720
So you can check that out.

217
00:11:05,720 --> 00:11:09,560
And for existing systems though, if you already have something in the works, you can still

218
00:11:09,560 --> 00:11:13,920
go through your system and find the events, and here's some tips for finding them.

219
00:11:13,920 --> 00:11:15,640
Think about what causes a service to start.

220
00:11:15,720 --> 00:11:19,080
Is it an API call or something, something else triggers it to do something?

221
00:11:19,080 --> 00:11:22,400
Usually it's an API call.

222
00:11:22,400 --> 00:11:23,480
What info does that service need?

223
00:11:23,480 --> 00:11:26,320
So what parameters are passed in in that call?

224
00:11:26,320 --> 00:11:27,800
What action does a service perform?

225
00:11:27,800 --> 00:11:30,000
What kind of work is it doing?

226
00:11:30,000 --> 00:11:33,080
And then what information results from those actions?

227
00:11:33,080 --> 00:11:34,080
Those become your events.

228
00:11:34,080 --> 00:11:38,800
I'm sorry, I'm moving away from the microphone, it's just hard to twist and see.

229
00:11:38,800 --> 00:11:44,200
So those become the notifications and the clumps of state that will make up your events.

230
00:11:45,200 --> 00:11:51,640
The next thing is to define your topics, how you're going to store these events.

231
00:11:51,640 --> 00:11:56,480
One topic per event type is a good way to go, it's a common pattern.

232
00:11:56,480 --> 00:11:59,400
Sometimes you might want to use something more like a topic per domain.

233
00:11:59,400 --> 00:12:02,000
For example, if you're, say, working with a customer and you want everything that happens

234
00:12:02,000 --> 00:12:05,240
with that customer to show up in that same topic, especially so that they can be consumed

235
00:12:05,240 --> 00:12:06,240
in the same order.

236
00:12:06,240 --> 00:12:10,160
So you might do all the different types of events, but they're all related to that same

237
00:12:10,160 --> 00:12:13,360
domain, so they go into one topic.

238
00:12:13,360 --> 00:12:14,960
Or you can mix and match.

239
00:12:14,960 --> 00:12:15,960
Just think that through.

240
00:12:15,960 --> 00:12:18,720
It's an important step to go through to think of where the data is going to be.

241
00:12:18,720 --> 00:12:24,000
If you have your topic in one topic per event type, then you might end up having to read

242
00:12:24,000 --> 00:12:26,720
from multiple topics to do the work you're going to do, which is fine to do, and you

243
00:12:26,720 --> 00:12:34,760
can use tools like Fouse to do streaming and manipulate your data that way.

244
00:12:34,760 --> 00:12:36,560
And then don't forget schema.

245
00:12:36,560 --> 00:12:37,560
Schema is really important.

246
00:12:37,560 --> 00:12:41,960
It defines what the event payload is, especially if you're using multiple event types in a

247
00:12:42,120 --> 00:12:45,040
single topic, then you really want to have a schema to differentiate them, but you pretty

248
00:12:45,040 --> 00:12:47,400
much always do.

249
00:12:47,400 --> 00:12:50,160
Something really simple like the demo I'm going to show in a bit doesn't have one, but

250
00:12:50,160 --> 00:12:55,120
normally you're going to want to have that to show what the data in your event represents

251
00:12:55,120 --> 00:12:57,320
so that you can serialize and deserialize it.

252
00:12:57,320 --> 00:13:04,080
Now, if you're using schemas like JSON, Avro, or Protobuf, then Confluent has a schema registry,

253
00:13:04,080 --> 00:13:10,520
which is a tool that allows you to manage the schemas and evolve them and use them automatically.

254
00:13:11,360 --> 00:13:14,160
Schemas will change, so think about compatibility.

255
00:13:14,160 --> 00:13:18,680
It's important things to think through when you're doing that.

256
00:13:18,680 --> 00:13:20,960
One more thing I wanted to make clear.

257
00:13:20,960 --> 00:13:22,440
This doesn't mean no databases.

258
00:13:22,440 --> 00:13:26,040
A lot of people think it's like events-driven versus databases, and it's really not.

259
00:13:26,040 --> 00:13:28,920
We're talking about request-response versus event-driven.

260
00:13:28,920 --> 00:13:31,960
Each of those services in that diagram we earlier probably has their own database, right?

261
00:13:31,960 --> 00:13:35,680
I do recommend one database per service if you're doing microservices, if they need

262
00:13:35,680 --> 00:13:36,680
one.

263
00:13:36,680 --> 00:13:38,640
Maybe they don't need one, but if they do, it'll be there.

264
00:13:38,640 --> 00:13:40,840
This isn't between events and database.

265
00:13:40,840 --> 00:13:43,560
Some people think of Kafka versus databases, and there may be some of that in some other

266
00:13:43,560 --> 00:13:46,600
domains, but in microservices, that's not really an issue.

267
00:13:46,600 --> 00:13:47,600
I like databases.

268
00:13:47,600 --> 00:13:51,520
I like that picture my daughter drew outside to throw it in somewhere.

269
00:13:51,520 --> 00:13:52,520
All right, a demo.

270
00:13:52,520 --> 00:13:56,000
We're not going to build this, because it's actually built, because we don't have much

271
00:13:56,000 --> 00:14:01,000
time here, but we're going to take a look at a random pizza generator written with Python

272
00:14:01,000 --> 00:14:02,000
and Kafka.

273
00:14:02,000 --> 00:14:05,240
Okay, yeah, so here's my two endpoints.

274
00:14:05,240 --> 00:14:12,320
This is for placing an order for pizzas, and we're going to generate these pizzas randomly.

275
00:14:12,320 --> 00:14:20,120
So this is going to call out to our pizza service, which I believe is up here.

276
00:14:20,120 --> 00:14:22,320
Nope, wrong one.

277
00:14:22,320 --> 00:14:27,320
All right, so in our pizza service here, I'm really sorry, I expected to be able to see

278
00:14:27,320 --> 00:14:29,320
my screen better than this.

279
00:14:29,320 --> 00:14:32,720
To use Kafka in a Python application is really simple.

280
00:14:33,720 --> 00:14:37,440
We're going to import a producer and consumer from the Confluent Kafka package.

281
00:14:37,440 --> 00:14:40,600
There's other goodies in that package that you could use.

282
00:14:40,600 --> 00:14:42,840
There's specific types of producers and consumers.

283
00:14:42,840 --> 00:14:47,800
If you're using specific types of schemas, you can use different serializing producers

284
00:14:47,800 --> 00:14:51,600
and consumers, but we're just going to use the basic ones for this.

285
00:14:51,600 --> 00:14:54,320
And there's some configuration information we need.

286
00:14:54,320 --> 00:14:57,320
We just have a config.properties file, which basically just tells us where's our Kafka

287
00:14:57,320 --> 00:15:00,800
cluster at, so it has an endpoint and some credentials for logging in, so nothing interesting

288
00:15:00,880 --> 00:15:03,880
to see there, but that's important to have.

289
00:15:03,880 --> 00:15:07,320
And then we're going to create our producer here, just using the constructor and passing

290
00:15:07,320 --> 00:15:10,320
in that configuration file, and we're calling that our pizza producer.

291
00:15:10,320 --> 00:15:14,960
And then we've got a pizza alarm, which is a dictionary to hold our orders of pizzas

292
00:15:14,960 --> 00:15:18,040
while they're being cooked.

293
00:15:18,040 --> 00:15:21,360
And this completed pizza topic, because we saw that chain that we looked at before, a

294
00:15:21,360 --> 00:15:24,600
chain of services, the last one's going to write to a final topic, and this just tells

295
00:15:24,600 --> 00:15:29,920
us what topic that is so we can know what to read from to complete the loop.

296
00:15:30,040 --> 00:15:32,040
We could hard code that, but I just put it in there, because it could change.

297
00:15:32,040 --> 00:15:33,040
You never know.

298
00:15:33,040 --> 00:15:35,600
We might change the way we make our pizzas.

299
00:15:35,600 --> 00:15:39,840
So here's that method that's called when we hit that endpoint.

300
00:15:39,840 --> 00:15:42,000
So we're passing in a count.

301
00:15:42,000 --> 00:15:46,480
We're going to create a new pizza order, give me that count, we're going to stuff that into

302
00:15:46,480 --> 00:15:50,960
our pizza warmer just to hold it there while we're doing our work, and then we're going

303
00:15:50,960 --> 00:15:57,960
to loop through based on the count, we're going to create a new pizza.

304
00:15:58,000 --> 00:16:00,800
And then we're going to set the order ID of that pizza order, we're going to set that

305
00:16:00,800 --> 00:16:01,800
on the pizza.

306
00:16:01,800 --> 00:16:05,760
So now each pizza inside the order has a link there, so we can keep those in sync.

307
00:16:05,760 --> 00:16:07,520
And then here's the key part for Kafka.

308
00:16:07,520 --> 00:16:11,080
We're going to use our producer to just produce to the topic.

309
00:16:11,080 --> 00:16:13,840
We're going to give it the name of the topic, so this pizza object's going to land in the

310
00:16:13,840 --> 00:16:18,540
pizza topic, and the key is going to be our order ID.

311
00:16:18,540 --> 00:16:21,000
So all of the pizzas for that order are going to make sure they're going to be in the same

312
00:16:21,000 --> 00:16:22,000
partition.

313
00:16:22,000 --> 00:16:24,400
In our example, I'm using one partition, but if there were multiple, they would stay in

314
00:16:24,400 --> 00:16:26,920
that same partition based on that key.

315
00:16:27,440 --> 00:16:31,560
The value is going to be the pizza written out to JSON.

316
00:16:31,560 --> 00:16:34,080
And then when we've done all of them for that order, we're going to flush to make sure they're

317
00:16:34,080 --> 00:16:36,600
all written to the topic.

318
00:16:36,600 --> 00:16:45,080
And when we do this, this is going to trigger the next application, which I think is over

319
00:16:45,080 --> 00:16:46,080
here.

320
00:16:46,080 --> 00:16:51,640
So this one is our, I believe, is our sauce service.

321
00:16:51,640 --> 00:16:55,960
And so our sauce service has a pizza consumer, which is going to consume from the pizza topic

322
00:16:55,960 --> 00:16:59,600
that we just wrote to.

323
00:16:59,600 --> 00:17:02,920
And when it sees a new pizza landing there, it's going to be a blank pizza, and it's going

324
00:17:02,920 --> 00:17:03,920
to go to work on it.

325
00:17:03,920 --> 00:17:10,480
So it's going to find that pizza, and it's going to add sauce to it.

326
00:17:10,480 --> 00:17:17,160
And the add sauce function down here calculates some random sauce to put on there, and then

327
00:17:17,160 --> 00:17:21,160
calls the sauce producer, which is also defined up above, using the same thing, basically,

328
00:17:21,160 --> 00:17:22,160
right?

329
00:17:22,360 --> 00:17:26,680
So we're going to use the producer constructor to create that using the client config.

330
00:17:26,680 --> 00:17:31,360
So we're going to take our sauce producer, and we're going to produce to the pizza with

331
00:17:31,360 --> 00:17:36,560
sauce topic, using the same key again, and the newly enhanced pizza is going to land

332
00:17:36,560 --> 00:17:38,640
in that topic.

333
00:17:38,640 --> 00:17:41,200
And then the pattern just repeats, so we don't have to go through all of these.

334
00:17:41,200 --> 00:17:45,480
But basically, each of these services is going to have this couplet of a consumer to consume

335
00:17:45,480 --> 00:17:49,680
from the topic it's interested in, it's going to do its own work, and then it's going to

336
00:17:49,680 --> 00:17:53,120
produce to another topic.

337
00:17:53,120 --> 00:18:02,560
All the way until we got to the last one, which is our veggie one, we're going to add

338
00:18:02,560 --> 00:18:07,480
our veggies, and the veggie producer is going to produce to the pizza with veggies topic.

339
00:18:07,480 --> 00:18:08,480
That's the final one.

340
00:18:08,480 --> 00:18:11,280
At that point, we'd have a complete pizza.

341
00:18:11,280 --> 00:18:16,080
So we're going to run back here to our main app and fire this up.

342
00:18:16,600 --> 00:18:18,640
The other services are already running.

343
00:18:18,640 --> 00:18:21,640
Hopefully they still are, anyways.

344
00:18:21,640 --> 00:18:31,440
So we're going to run that, and then probably do some typos here since I can't see what

345
00:18:31,440 --> 00:18:42,280
I'm typing, but let's see, big crowd here, so maybe we should order like 10 pizzas.

346
00:18:42,280 --> 00:18:47,040
I don't know if it would take too long to run to me, that's just five.

347
00:18:47,040 --> 00:18:49,520
Oh, how'd that happen?

348
00:18:49,520 --> 00:18:51,520
Caps lock, that's how that happened.

349
00:18:51,520 --> 00:19:17,040
All right, so that ran off, and it created our order, and ran through our topics, and

350
00:19:17,040 --> 00:19:19,320
that's our order number so we can remember it.

351
00:19:19,800 --> 00:19:22,520
Let's listen for that number to be called, and you'll know your pizza's ready.

352
00:19:22,520 --> 00:19:27,120
Let's see, over here we could pop over real quickly and just take a peek.

353
00:19:27,120 --> 00:19:29,240
How are we doing in time?

354
00:19:29,240 --> 00:19:35,200
Got up there, it's not 10 minutes anymore, I know that.

355
00:19:35,200 --> 00:19:39,120
So over here, this is our Kafka cluster on Confluent Cloud, which is I think one of the

356
00:19:39,120 --> 00:19:40,120
best ways to use Kafka.

357
00:19:40,120 --> 00:19:45,000
It's really quick and easy to use, but here we can look and see, go right to our final

358
00:19:45,000 --> 00:19:51,520
topic, our pizza with veggies, we can see here that hopefully our pizzas are sitting

359
00:19:51,520 --> 00:20:03,480
in that topic, and there they are.

360
00:20:03,480 --> 00:20:17,680
So now, we should be able, like I said, using that order number, we should be able to pull

361
00:20:17,680 --> 00:20:21,840
up our pizzas now.

362
00:20:21,840 --> 00:20:22,840
And there they are.

363
00:20:22,840 --> 00:20:23,840
Those look good.

364
00:20:23,840 --> 00:20:24,840
All right, thank you.

365
00:20:24,840 --> 00:20:32,440
All right, so I did want to show what we did there for getting the order.

366
00:20:32,440 --> 00:20:36,200
So the get order was another endpoint that we had, and that leads to this one here.

367
00:20:36,200 --> 00:20:42,280
So for this, what we do now is we'll call this load orders, and that's going to do is

368
00:20:42,280 --> 00:20:43,280
fire off a consumer.

369
00:20:43,280 --> 00:20:45,880
This is going to be a short load consumer we're going to do here, because we're only

370
00:20:45,880 --> 00:20:47,280
doing it when the endpoint's called.

371
00:20:47,280 --> 00:20:51,280
So when we call the thing, we're going to create the consumer right then and there.

372
00:20:51,280 --> 00:20:53,360
Now, consumer isn't just created and used.

373
00:20:53,360 --> 00:20:56,280
A consumer, you have to create it, and then you subscribe to one or more topics.

374
00:20:56,280 --> 00:21:01,680
So you tell it what topics are you interested in, and we're interested in the completed

375
00:21:01,920 --> 00:21:05,040
pizza topic, which is our pizza with veggies.

376
00:21:05,040 --> 00:21:08,940
So then once we've subscribed to that topic, now we can pull that topic and just grab any

377
00:21:08,940 --> 00:21:09,940
data that's there.

378
00:21:09,940 --> 00:21:12,840
So we're just going to loop through, and I put in 60 for now, but we could just, whatever

379
00:21:12,840 --> 00:21:15,920
you want to do there, we just pull through that many times and grab any pizzas that are

380
00:21:15,920 --> 00:21:16,920
there ready to go.

381
00:21:16,920 --> 00:21:20,160
If there's none, we'll just skip that and wait, and then if there's an error, we'll

382
00:21:20,160 --> 00:21:21,160
say bummer.

383
00:21:21,160 --> 00:21:28,560
And if there's a pizza, then we'll go ahead and we'll add that pizza to our order.

384
00:21:28,560 --> 00:21:30,240
And at the end of all that, we'll close the thing.

385
00:21:30,240 --> 00:21:34,400
So that's loading the order for us.

386
00:21:34,400 --> 00:21:40,200
And then if we find that everything's good, then we can go ahead and return that as JSON.

387
00:21:40,200 --> 00:21:44,080
So in this situation, the other ones have a consumer that's running all the time, just

388
00:21:44,080 --> 00:21:47,560
waiting for another topic to link, so we don't know when it's going to happen.

389
00:21:47,560 --> 00:21:50,480
And then when it does land, it does its work and it produces to another topic.

390
00:21:50,480 --> 00:21:57,120
With this one, we have a consumer that we fire off when we need it to get our order.

391
00:21:57,120 --> 00:22:01,360
So now with this, we could have also done with threading, we could have maybe had a

392
00:22:01,360 --> 00:22:04,880
consumer just running all the time waiting for a completed pizza and veggie topic.

393
00:22:04,880 --> 00:22:08,200
We could have done it that way as well, but this seems simpler just to only use it when

394
00:22:08,200 --> 00:22:09,200
you need it.

395
00:22:09,200 --> 00:22:10,200
You could create the consumer then.

396
00:22:10,200 --> 00:22:11,280
These are very lightweight objects.

397
00:22:11,280 --> 00:22:13,520
You can create them and get rid of them as you need to.

398
00:22:13,520 --> 00:22:16,160
So you can see in there, we're doing that.

399
00:22:16,160 --> 00:22:21,840
We're creating the consumer, we're using it, and then we're being a good citizen and closing

400
00:22:21,840 --> 00:22:22,840
it.

401
00:22:22,840 --> 00:22:25,640
So it's easy to work with it that way, and we've got five minutes left.

402
00:22:25,640 --> 00:22:33,120
So that was the demo, and then I was going to ... Oh, I was going to show you this earlier.

403
00:22:33,120 --> 00:22:36,080
This is what we just built, and there's a GitHub repo if you want to look at the code

404
00:22:36,080 --> 00:22:37,080
for that.

405
00:22:37,080 --> 00:22:38,080
It's not great.

406
00:22:38,080 --> 00:22:40,600
In fact, if you want to look at the code and send me some pointers, I'd really appreciate

407
00:22:40,600 --> 00:22:41,600
that.

408
00:22:41,600 --> 00:22:42,600
Like I said, I'm a little bit new to Python.

409
00:22:42,600 --> 00:22:43,960
I'm loving it though.

410
00:22:43,960 --> 00:22:47,200
After doing Java for so many years, it's like a real breath of fresh air.

411
00:22:47,200 --> 00:22:52,160
But that's the application that we just looked at there.

412
00:22:53,160 --> 00:22:56,840
GitHub repo, in the readme, there's a bunch of other links to other resources that might

413
00:22:56,840 --> 00:22:57,840
be helpful to you.

414
00:22:57,840 --> 00:23:00,160
But some of them I'm going to give you right here.

415
00:23:00,160 --> 00:23:07,720
All right, so this is ... We have a site called Confluent Developer, but it's developer.confluent.io,

416
00:23:07,720 --> 00:23:11,320
and there's a ton of great resources there on that site.

417
00:23:11,320 --> 00:23:15,840
There's video courses, there's a quick start guide, there's a Python quick start guide

418
00:23:15,840 --> 00:23:18,160
you can check out, all kinds of really good things.

419
00:23:18,160 --> 00:23:20,960
There's also this book that you see on the left, Designing Event-Driven Systems.

420
00:23:20,960 --> 00:23:25,000
It's a free e-book, not real long, but it's a great read.

421
00:23:25,000 --> 00:23:29,560
Really goes into depth about how and why to design event-driven systems.

422
00:23:29,560 --> 00:23:34,040
It's language agnostic, but it just goes into the architecture part of it really good.

423
00:23:34,040 --> 00:23:38,240
And then there's the GitHub URL, again, just in case you missed it from the last slide.

424
00:23:38,240 --> 00:23:39,240
And then there's my contact info.

425
00:23:39,240 --> 00:23:42,760
If you have any questions at all about this stuff, feel free to reach out to me, email

426
00:23:42,760 --> 00:23:45,080
or Twitter are the best ways to reach me.

427
00:23:45,080 --> 00:23:48,280
And if you want to learn more about Kafka, make sure you're here tomorrow for my coworker

428
00:23:48,280 --> 00:23:49,280
Danica Fine.

429
00:23:49,600 --> 00:23:51,600
She's going to be doing an even more interesting presentation.

430
00:23:51,600 --> 00:23:54,600
Hopefully she won't have the technical snafus that I've had either.

431
00:23:54,600 --> 00:23:56,280
So I think that's all I had.

432
00:23:56,280 --> 00:23:58,280
If there's any questions now, feel free.

433
00:23:58,280 --> 00:24:05,280
Do, do, do, do, do, do, do, do, do, do, do, do.

434
00:24:05,280 --> 00:24:06,280
Okay, all right, well thank you very much.

