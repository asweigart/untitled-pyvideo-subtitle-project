1
00:00:00,000 --> 00:00:10,640
Okay, my name is Danica Fine, and we're going to talk about some Kafka-based data pipelines

2
00:00:10,640 --> 00:00:11,640
here.

3
00:00:11,640 --> 00:00:14,640
Just give me one second to make sure everything is up.

4
00:00:14,640 --> 00:00:19,080
All right, but before we get into that, I want to know how many of you picked up some

5
00:00:19,080 --> 00:00:24,120
crazy hobbies over the last year, or immersed yourself in things that you just way too deep,

6
00:00:24,120 --> 00:00:25,120
right?

7
00:00:25,120 --> 00:00:27,120
Okay, I feel like some of you are lying, all right, and I'm just going to move on.

8
00:00:27,720 --> 00:00:30,720
I'm going to pretend like everyone said yes, because it's probably true.

9
00:00:30,720 --> 00:00:32,720
You can't hear me?

10
00:00:32,720 --> 00:00:33,720
No?

11
00:00:33,720 --> 00:00:34,720
Oh, really close.

12
00:00:34,720 --> 00:00:35,720
Oh, man.

13
00:00:35,720 --> 00:00:36,720
Okay.

14
00:00:36,720 --> 00:00:37,720
Anyway.

15
00:00:37,720 --> 00:00:38,720
Well, okay, I'll ask that again.

16
00:00:38,720 --> 00:00:40,720
How many of you have weird hobbies from the last two years?

17
00:00:40,720 --> 00:00:41,720
Okay.

18
00:00:41,720 --> 00:00:43,720
Yeah, more of you answered.

19
00:00:43,720 --> 00:00:44,720
Great.

20
00:00:44,720 --> 00:00:48,120
How did this go, right?

21
00:00:48,120 --> 00:00:49,520
Things got a little weird.

22
00:00:49,520 --> 00:00:50,520
I have a hobby.

23
00:00:50,520 --> 00:00:51,520
It got really weird.

24
00:00:51,520 --> 00:00:54,800
I have a lot of houseplants, and that's not even all of them.

25
00:00:54,800 --> 00:00:55,840
Probably like 40 plants.

26
00:00:55,840 --> 00:00:56,840
It's a jungle.

27
00:00:57,560 --> 00:01:02,880
Everyone hates me sometimes, but yeah, this really got out of hand, especially when I

28
00:01:02,880 --> 00:01:05,880
considered that I was spending more time checking on my plants every day than I was actually

29
00:01:05,880 --> 00:01:12,160
working, doing things that I probably should be paid to do, feeding myself, you know, whatever,

30
00:01:12,160 --> 00:01:19,040
and I also realized that I'm just a bad plant mom sometimes, so this is a problem, right?

31
00:01:19,040 --> 00:01:22,160
I feel like I needed something to help me, right?

32
00:01:22,160 --> 00:01:26,000
And I'm an engineer, so I could build something to help me and probably spend way too much

33
00:01:26,000 --> 00:01:27,800
time doing that, you know?

34
00:01:27,800 --> 00:01:29,760
So I asked myself, is there a better way?

35
00:01:29,760 --> 00:01:32,160
Which really was a silly question, because of course there's a better way.

36
00:01:32,160 --> 00:01:35,080
I could buy some plant monitors, obviously.

37
00:01:35,080 --> 00:01:36,720
So that's the wrong question.

38
00:01:36,720 --> 00:01:39,480
The real question is, is there a more interesting way, right?

39
00:01:39,480 --> 00:01:42,960
And as an engineer, yes, of course there's a more interesting way.

40
00:01:42,960 --> 00:01:48,120
I have the technology and probably the willpower and way too much time, and I can also build

41
00:01:48,120 --> 00:01:50,560
a pretty cool plant monitoring system, right?

42
00:01:50,560 --> 00:01:55,400
And this was my excuse to get a Raspberry Pi and finally build out that hardware project

43
00:01:55,400 --> 00:01:57,080
that I wanted to build out, right?

44
00:01:57,080 --> 00:02:01,680
So yeah, this is my first hardware project, and I had way too much fun, and this is just

45
00:02:01,680 --> 00:02:04,680
the beginning, so there will be a lot more of these talks.

46
00:02:04,680 --> 00:02:07,800
I'm just going to keep going.

47
00:02:07,800 --> 00:02:12,080
All right, so I asked myself, how could I build a plant monitoring system?

48
00:02:12,080 --> 00:02:16,680
This is a super, super high level, like, yeah, scratch pad of the design.

49
00:02:16,680 --> 00:02:20,040
I needed something to monitor my plants and let me know when to water them.

50
00:02:20,040 --> 00:02:23,440
I didn't want to get into, you know, automating the entire system.

51
00:02:23,480 --> 00:02:27,120
I rent, so I feel like that would be a bad idea.

52
00:02:27,120 --> 00:02:30,360
So I just wanted something to monitor my plants at the very least.

53
00:02:30,360 --> 00:02:32,640
The system that I envisioned was pretty simple.

54
00:02:32,640 --> 00:02:37,880
I'd get some soil moisture sensors, put this in some plants, take those readings regularly,

55
00:02:37,880 --> 00:02:42,240
and then I would enrich those readings with some metadata, obviously, like, I don't know,

56
00:02:42,240 --> 00:02:45,600
how much water does that plant need, and what does that plant's name while I'm at it?

57
00:02:45,600 --> 00:02:49,560
So I'd add some of those bits in, throw it into a black box, and at the end I'd get some

58
00:02:49,560 --> 00:02:50,560
sort of alert, right?

59
00:02:50,720 --> 00:02:55,760
So this is a very, very high level view of what I wanted to do, and so from there I fleshed

60
00:02:55,760 --> 00:03:00,480
it out a little bit, zero indexed, just so you guys didn't get lost.

61
00:03:00,480 --> 00:03:05,000
So I'd start with that metadata, so like I said, like, what are the moisture levels that

62
00:03:05,000 --> 00:03:06,520
every plant needs?

63
00:03:06,520 --> 00:03:07,720
What are these plants?

64
00:03:07,720 --> 00:03:08,720
Where is it?

65
00:03:08,720 --> 00:03:09,720
I don't know.

66
00:03:09,720 --> 00:03:10,720
When did I get it?

67
00:03:10,720 --> 00:03:13,000
So I'd have that information, and I wanted to put it in Kafka.

68
00:03:13,000 --> 00:03:14,000
I work for Confluence.

69
00:03:14,000 --> 00:03:15,000
I really love Kafka.

70
00:03:15,000 --> 00:03:19,120
Okay, we'll get into that later, but I wanted to use Kafka as sort of the backbone of this

71
00:03:19,200 --> 00:03:20,200
whole project.

72
00:03:20,200 --> 00:03:22,400
Okay, so I have that metadata.

73
00:03:22,400 --> 00:03:26,680
I then, you know, set up my Raspberry Pi with my moisture sensors and capture these soil

74
00:03:26,680 --> 00:03:29,400
readings every so often.

75
00:03:29,400 --> 00:03:32,280
From there I'd use some sort of stream processing, right?

76
00:03:32,280 --> 00:03:36,800
So I'm getting these real-time readings from my plants, and I'd need to combine that with

77
00:03:36,800 --> 00:03:40,880
the metadata, get some answers, right, and start acting on it, and then at the end I

78
00:03:40,880 --> 00:03:45,400
could send some alerts to my phone, hopefully, that's what I wanted to do, and I learned

79
00:03:45,400 --> 00:03:48,840
that I could be using that with a Telegram bot, which is pretty cool.

80
00:03:48,840 --> 00:03:51,280
I don't know if you all use Telegram, but it's pretty awesome.

81
00:03:51,280 --> 00:03:55,680
So we'll get into that a little bit more, but knowing this high-level workflow, what

82
00:03:55,680 --> 00:03:58,400
I wanted to do with it, I could then buy some hardware.

83
00:03:58,400 --> 00:04:01,440
That's the best part.

84
00:04:01,440 --> 00:04:04,400
This Raspberry Pi, absolutely overkill, but this is the only thing I could buy at the

85
00:04:04,400 --> 00:04:05,400
time.

86
00:04:05,400 --> 00:04:09,800
If any of you got into hardware projects in the last two years, you know that there was

87
00:04:09,800 --> 00:04:15,280
a very, very limited inventory, so yes, overkill is the only one I could find.

88
00:04:15,280 --> 00:04:21,360
I also got this half-size breadboard, which came for free with what I ordered, which really

89
00:04:21,360 --> 00:04:23,400
worked out well, because I actually needed it.

90
00:04:23,400 --> 00:04:25,640
I had no idea going into that.

91
00:04:25,640 --> 00:04:31,400
I chose these capacitive moisture sensors, they're I2C.

92
00:04:31,400 --> 00:04:34,240
If we want to get into this later, feel free to come up to me, and I will explain why I

93
00:04:34,240 --> 00:04:38,920
chose these ones, but they're pretty affordable and relatively high quality for what they

94
00:04:38,920 --> 00:04:44,080
were compared to everything else, so I bought a couple of those, and beyond that, I opted

95
00:04:44,240 --> 00:04:48,840
to crimp and cut all of my own wires, mostly for fun, I guess.

96
00:04:48,840 --> 00:04:55,840
I had a couple days, so I purchased some JST connectors, dusted off the crimpers in the

97
00:04:55,840 --> 00:04:56,840
garage.

98
00:04:56,840 --> 00:04:57,840
Why did I have those?

99
00:04:57,840 --> 00:04:58,840
I don't know.

100
00:04:58,840 --> 00:05:03,240
Yeah, I ended up using a couple meters of wire, so that was pretty great.

101
00:05:03,240 --> 00:05:06,920
Also a key thing is the plants that I'm monitoring.

102
00:05:06,920 --> 00:05:11,320
These were the unwilling participants, they had no idea what was coming at them.

103
00:05:11,320 --> 00:05:15,040
I chose these particular plants in a very scientific way.

104
00:05:15,040 --> 00:05:20,080
They were really close together, and there was an outlet behind them, so made it a little

105
00:05:20,080 --> 00:05:22,360
easy for myself.

106
00:05:22,360 --> 00:05:26,300
With the hardware in place, and I decided the plants I was going to use, I could actually

107
00:05:26,300 --> 00:05:30,280
start putting this together, and I'm going to do some hand-waving here in the interest

108
00:05:30,280 --> 00:05:37,360
of time, so if you have more questions afterwards, this isn't a hardware conference, so maybe

109
00:05:37,400 --> 00:05:42,160
at my mechanical engineering talk, we'll get more into this, but yes, suffice it to say,

110
00:05:42,160 --> 00:05:47,280
there was a lot of crimping, a lot of soldering, got into that, that was cool.

111
00:05:47,280 --> 00:05:50,800
Maybe a little bit of tears, but what good project does it involve that?

112
00:05:50,800 --> 00:05:54,280
In the end, I ended up with something cool like this.

113
00:05:54,280 --> 00:05:58,000
Please don't judge my soldering.

114
00:05:58,000 --> 00:06:03,080
In the end, I had four moisture sensors connecting these plants.

115
00:06:03,080 --> 00:06:05,880
Four seems like a little number, but I'll talk about that more.

116
00:06:05,880 --> 00:06:09,880
These are all wired through the breadboard, in turn going into the Raspberry Pi, and all

117
00:06:09,880 --> 00:06:15,400
the sensors, like I said, are I2C, so they're all communicating via unique address and had

118
00:06:15,400 --> 00:06:20,640
to use the same input channel, so hence the breadboard, and I'm going to move on before

119
00:06:20,640 --> 00:06:24,000
you look too closely.

120
00:06:24,000 --> 00:06:34,000
So on the software side of things, Python 3.7, I also used PIP, because I had it, sorry,

121
00:06:34,000 --> 00:06:37,440
to install the confluent Kafka library.

122
00:06:37,440 --> 00:06:41,480
I know, I know, how dare I use the tools I have.

123
00:06:41,480 --> 00:06:45,720
And then it's also worth noting that I was leveraging confluent cloud for my Kafka cluster,

124
00:06:45,720 --> 00:06:51,120
which we'll get into, and I'll be doing the stream processing on there with ksqlDB, but

125
00:06:51,120 --> 00:06:56,080
on that note, what even is Kafka, okay?

126
00:06:56,080 --> 00:07:00,280
If you attended my colleague Dave Klein's talk yesterday, you got a pretty quick Kafka

127
00:07:00,880 --> 00:07:06,080
But I want to flesh it out a little bit more and make sure everyone's on the same page.

128
00:07:06,080 --> 00:07:09,240
So Kafka is a distributed event streaming platform.

129
00:07:09,240 --> 00:07:13,440
That's a pretty concise short sentence, but let's unpack it.

130
00:07:13,440 --> 00:07:15,840
There's a lot of words in there.

131
00:07:15,840 --> 00:07:18,880
First of all, what are events, all right?

132
00:07:18,880 --> 00:07:22,600
Events are pieces of information that indicate that something has happened.

133
00:07:22,600 --> 00:07:23,600
Pretty simple.

134
00:07:23,600 --> 00:07:27,800
Ideally, an event, when we're dealing it in the context of Kafka, is the smallest amount

135
00:07:27,840 --> 00:07:32,480
of data without excess that you can use to completely describe that something that has

136
00:07:32,480 --> 00:07:33,600
happened, all right?

137
00:07:33,600 --> 00:07:35,520
So we need some sort of when.

138
00:07:35,520 --> 00:07:41,320
We need a who and a what, you know, flesh this out and decide what happened then.

139
00:07:41,320 --> 00:07:44,160
Events are a pretty efficient way to describe things that are happening throughout your

140
00:07:44,160 --> 00:07:45,160
whole system.

141
00:07:45,160 --> 00:07:50,560
So Kafka's a really great way to re-architect your entire architecture, I'm just saying.

142
00:07:50,560 --> 00:07:53,400
And this is due to the fact that events are immutable, all right?

143
00:07:53,400 --> 00:08:00,920
If you're a purist and you like functional programming, I grew up on OCaml, so if you

144
00:08:00,920 --> 00:08:05,920
like functional programming, this is entirely intuitive, right?

145
00:08:05,920 --> 00:08:08,440
The fact that events are immutable.

146
00:08:08,440 --> 00:08:12,760
Events describe things that have happened and barring weird things like you getting

147
00:08:12,760 --> 00:08:16,760
a time machine, you can't really change that, right?

148
00:08:16,760 --> 00:08:20,360
So you understand what events are, but I still kind of haven't told you what Kafka is.

149
00:08:20,360 --> 00:08:23,280
So again, it's a distributed event streaming platform.

150
00:08:23,280 --> 00:08:26,680
So a Kafka cluster consists of multiple broker nodes.

151
00:08:26,680 --> 00:08:29,800
These can be bare metal, VMs, containers, however you want to run it, wherever you want

152
00:08:29,800 --> 00:08:30,800
to run it.

153
00:08:30,800 --> 00:08:34,960
And the primary unit of storage in Kafka is called a topic.

154
00:08:34,960 --> 00:08:39,340
These topics are further broken down into smaller components called partitions.

155
00:08:39,340 --> 00:08:44,900
These partitions are actually themselves immutable append-only logs where these individual events

156
00:08:44,900 --> 00:08:48,460
that we talked about live, they're stored there.

157
00:08:48,460 --> 00:08:53,860
We produce data into Kafka as key value pairs, and we use separate clients called Kafka

158
00:08:53,860 --> 00:08:55,580
producers to do that.

159
00:08:55,580 --> 00:08:59,300
We can have as many producers as we want writing to as many topics as we want across the Kafka

160
00:08:59,300 --> 00:09:01,260
cluster.

161
00:09:01,260 --> 00:09:04,180
Once the data's in Kafka, from there it gets a little more interesting.

162
00:09:04,180 --> 00:09:08,560
We can start reading data and consuming it from a Kafka topic using another separate

163
00:09:08,560 --> 00:09:11,220
client called a Kafka consumer.

164
00:09:11,220 --> 00:09:15,100
And again, we can have as many consumers as we want consuming from as many Kafka topics

165
00:09:15,100 --> 00:09:16,100
as we want.

166
00:09:16,100 --> 00:09:20,860
We can take it a step further and these consumers can sort of share the processing load of consuming

167
00:09:20,860 --> 00:09:25,260
from a single topic or they can consume independently from the beginning of the topic whenever they

168
00:09:25,260 --> 00:09:26,660
want.

169
00:09:26,660 --> 00:09:28,660
And that's a really important thing in Kafka.

170
00:09:28,660 --> 00:09:31,860
A Kafka topic is not a messaging queue.

171
00:09:31,860 --> 00:09:37,020
Unlike other sort of event platforms, when data is consumed from a Kafka topic, it doesn't

172
00:09:37,020 --> 00:09:41,380
disappear and it's a log, a topic's a log, right?

173
00:09:41,380 --> 00:09:46,380
And when you yourself read application logs, what happens when you read a line?

174
00:09:46,380 --> 00:09:47,380
Nothing.

175
00:09:47,380 --> 00:09:48,380
It stays there.

176
00:09:48,380 --> 00:09:49,380
All right.

177
00:09:49,380 --> 00:09:52,300
So anyone else can come through and read that log from start to finish and they're going

178
00:09:52,300 --> 00:09:54,500
to get exactly the same information that you got, right?

179
00:09:54,500 --> 00:09:56,860
So the same thing is true of a Kafka topic.

180
00:09:56,860 --> 00:10:02,100
These messages are free to be read by as many consumers as you want and the messages don't

181
00:10:02,100 --> 00:10:03,100
disappear.

182
00:10:03,100 --> 00:10:06,820
The consumers are fully decoupled from the producers.

183
00:10:06,820 --> 00:10:10,500
That's also important.

184
00:10:10,500 --> 00:10:11,500
It is awesome.

185
00:10:11,500 --> 00:10:12,500
Thank you.

186
00:10:12,500 --> 00:10:17,740
But where does Confluent come in?

187
00:10:17,740 --> 00:10:21,820
Confluent offers Kafka fully managed in the cloud.

188
00:10:21,820 --> 00:10:27,260
So it's pretty great for these sorts of projects where you want to use Kafka but you don't

189
00:10:27,260 --> 00:10:28,980
want to set up any of the infrastructure.

190
00:10:28,980 --> 00:10:33,540
I had enough trouble setting up my actual hardware, let alone running the infrastructure

191
00:10:33,540 --> 00:10:34,540
behind the scenes.

192
00:10:34,540 --> 00:10:38,940
And on top of that, Confluent has some great tools built in to help you integrate with

193
00:10:38,980 --> 00:10:40,780
external systems.

194
00:10:40,780 --> 00:10:45,260
So there's tools to bring data from sources into Kafka and the move data from Kafka

195
00:10:45,260 --> 00:10:46,540
out into things.

196
00:10:46,540 --> 00:10:51,460
And then there's also some great stream processing tools, which we'll get into later.

197
00:10:51,460 --> 00:10:55,340
But back to the project at hand.

198
00:10:55,340 --> 00:10:59,500
Before embarking on any sort of data-related journey, the first thing I tell anyone is

199
00:10:59,500 --> 00:11:01,220
to think about the data you're using, all right?

200
00:11:01,220 --> 00:11:05,060
You really need to understand the data sets that you're getting into.

201
00:11:05,100 --> 00:11:08,900
So I really recommend taking a step back, thinking about these.

202
00:11:08,900 --> 00:11:13,140
And if you are the producer of the data, if you're creating it, definitely create a schema.

203
00:11:13,140 --> 00:11:16,260
It's going to save you a lot of trouble later on.

204
00:11:16,260 --> 00:11:20,580
And as you build out a schema, you're going to start thinking more deeply about the information

205
00:11:20,580 --> 00:11:21,580
that you're working with.

206
00:11:21,580 --> 00:11:24,660
And you're going to understand your data set better and really get a feel for what you

207
00:11:24,660 --> 00:11:26,740
need for your project.

208
00:11:26,740 --> 00:11:31,900
So beyond that, schemas are just great for making sure that downstream systems get the

209
00:11:31,900 --> 00:11:33,500
information that they think they're going to get.

210
00:11:33,500 --> 00:11:37,780
So if you're working in the industry, I recommend using them.

211
00:11:37,780 --> 00:11:41,700
So in my case, I defined two Avro schemas.

212
00:11:41,700 --> 00:11:46,380
One for that houseplant sort of metadata, like the information on the plant itself.

213
00:11:46,380 --> 00:11:49,300
I gave all my plants names, so I figured that would be fun to be in there.

214
00:11:49,300 --> 00:11:53,940
I also have the threshold for moisture that my plants would be anticipating, the lower

215
00:11:53,940 --> 00:11:54,940
threshold.

216
00:11:54,940 --> 00:11:58,380
And also information on temperature and what have you.

217
00:11:58,380 --> 00:12:01,300
Then on the other side of things, the other schema, I have the readings information.

218
00:12:01,300 --> 00:12:05,380
So the time that we're actually taking this reading, the temperature that is at the current

219
00:12:05,380 --> 00:12:09,300
moisture level, everything that we're getting from those sensors.

220
00:12:09,300 --> 00:12:13,900
And both data sets here had a plant identification number to help me keep track of the plants

221
00:12:13,900 --> 00:12:15,900
and serve as a key later on.

222
00:12:15,900 --> 00:12:19,220
So I have defined my schemas.

223
00:12:19,220 --> 00:12:23,740
I understand what data I'm working with, so let's actually put it into Kafka.

224
00:12:23,740 --> 00:12:28,180
So I created a couple Python scripts using that Confluent Kafka library to make this

225
00:12:28,180 --> 00:12:29,180
happen.

226
00:12:29,180 --> 00:12:31,420
The programming for this happened in Python.

227
00:12:31,420 --> 00:12:36,980
So coming from a Java background, this is so satisfying.

228
00:12:36,980 --> 00:12:39,980
In the end, I was using four sensors, and I mentioned that earlier.

229
00:12:39,980 --> 00:12:44,100
So that seems like a very small number of sensors, given the number of houseplants I

230
00:12:44,100 --> 00:12:45,100
have.

231
00:12:45,100 --> 00:12:46,660
This is actually a hardware limitation.

232
00:12:46,660 --> 00:12:51,060
There's only so many I squared C addresses that I could give to the sensors.

233
00:12:51,060 --> 00:12:53,340
It had a couple switches on the back that I could flip.

234
00:12:53,340 --> 00:12:55,900
But in the end, I can only use four.

235
00:12:55,900 --> 00:12:58,620
To take it a step further, that might be another phase of the project.

236
00:12:58,620 --> 00:13:01,140
I can add some more.

237
00:13:01,140 --> 00:13:05,420
So I had these four plants that I was monitoring, and I had a set of a mapping between the IDs

238
00:13:05,420 --> 00:13:10,460
of those plants and the I squared C addresses, and basically just had a loop going every

239
00:13:10,460 --> 00:13:11,460
five seconds.

240
00:13:11,460 --> 00:13:17,700
I could grab that temperature and moisture value and serialize it with Avro, push it

241
00:13:17,700 --> 00:13:20,780
into the Kafka topic.

242
00:13:20,780 --> 00:13:25,900
From the plant metadata side of things, again, I only had four plants that I was currently

243
00:13:25,900 --> 00:13:27,820
monitoring, small amount of data.

244
00:13:27,820 --> 00:13:35,140
I also wrote an additional script to take that metadata and push it into Kafka, but

245
00:13:35,140 --> 00:13:40,020
kind of overkill because I only had four rows going in there for now.

246
00:13:40,020 --> 00:13:41,940
The data is in Kafka.

247
00:13:41,940 --> 00:13:45,180
How do we actually go about processing it?

248
00:13:45,180 --> 00:13:47,100
There are a couple ways to do this.

249
00:13:47,100 --> 00:13:50,180
I really love showing this visual because I think it gives a good idea of the options

250
00:13:50,180 --> 00:13:53,500
that you have as far as processing with Kafka.

251
00:13:53,500 --> 00:13:57,780
The OG way to do this is to use the consumer producer API there at the bottom, and you'll

252
00:13:57,780 --> 00:14:01,020
see here that there's a scale of ease of use and flexibility.

253
00:14:01,020 --> 00:14:05,460
At the very bottom, the lowest level option is just that consumer producer API.

254
00:14:05,460 --> 00:14:13,220
It's available in your favorite languages, Python, I have a link at the end for you.

255
00:14:13,220 --> 00:14:17,260
That said, it's a little more difficult to use because you are, like I said, that lowest

256
00:14:17,260 --> 00:14:18,420
level option.

257
00:14:18,420 --> 00:14:22,180
If you're using this for stream processing, you have to actually go through the process

258
00:14:22,380 --> 00:14:27,740
of consuming the data from Kafka, doing any sort of transformations in memory, managing

259
00:14:27,740 --> 00:14:30,900
your own state, and then pushing it back up to Kafka at the end.

260
00:14:30,900 --> 00:14:33,620
It's a little more hands-on.

261
00:14:33,620 --> 00:14:39,180
If you're a sane person, you're probably going to use one of the other two.

262
00:14:39,180 --> 00:14:41,940
Moving up in ease of use is Kafka Streams.

263
00:14:41,940 --> 00:14:48,980
You see here some of the API that's available to you with the filtering aggregations.

264
00:14:49,020 --> 00:14:53,380
What Kafka Streams does for you is that it takes that hassle of managing state off of

265
00:14:53,380 --> 00:14:54,380
your hands.

266
00:14:54,380 --> 00:15:00,420
Kafka Streams is a Scala Java-based library that offers all of these transformations out

267
00:15:00,420 --> 00:15:02,460
of the box for you.

268
00:15:02,460 --> 00:15:05,060
You get some cool things for free, like scalability.

269
00:15:05,060 --> 00:15:11,260
I said that you can consume information in parallel with vanilla consumers.

270
00:15:11,260 --> 00:15:14,700
You get the scalability for free because it's built on top of the consumer and producer

271
00:15:14,700 --> 00:15:15,700
API.

272
00:15:16,700 --> 00:15:22,020
I said that this is a JVM library, but there are a lot of Python-friendly options available

273
00:15:22,020 --> 00:15:23,020
to you, some wrappers.

274
00:15:23,020 --> 00:15:26,380
I think the most popular one is called Faust.

275
00:15:26,380 --> 00:15:30,980
We like those sort of references in Kafka.

276
00:15:30,980 --> 00:15:37,380
Then the easiest way to conduct stream processing with Kafka is with ksqlDB.

277
00:15:37,380 --> 00:15:42,380
This is something that is available to you as part of Confluent Cloud, so in addition

278
00:15:42,380 --> 00:15:48,900
to getting your fully managed Kafka cluster, you also have that built-in stream processing

279
00:15:48,900 --> 00:15:49,900
capability.

280
00:15:49,900 --> 00:15:55,300
The ksqlDB is super cool because it's just SQL syntax.

281
00:15:55,300 --> 00:16:01,300
You see a little bit of it up there, and you have access to all this powerful stream processing.

282
00:16:01,300 --> 00:16:07,460
You can spin up streaming applications pretty easily just by writing a little bit of SQL.

283
00:16:07,460 --> 00:16:09,100
That's what I choose to do here.

284
00:16:09,100 --> 00:16:10,860
I didn't want to actually run anything on my own.

285
00:16:10,860 --> 00:16:15,540
I like that I can leverage it through the cloud and, yeah, not manage anything on my

286
00:16:15,540 --> 00:16:18,540
own side of things.

287
00:16:18,540 --> 00:16:22,580
We'll jump into ksqlDB now.

288
00:16:22,580 --> 00:16:26,700
There are two main constructs in ksqlDB, streams and tables.

289
00:16:26,700 --> 00:16:33,860
Remember, it is a SQL-like language, so I'm going to go into that, but keep that in mind.

290
00:16:33,860 --> 00:16:37,620
Streams represent unbounded ongoing events.

291
00:16:38,300 --> 00:16:43,500
And tables, on the other hand, show the current state for a given key or whatever keys are

292
00:16:43,500 --> 00:16:45,820
currently in the underlying Kafka topic.

293
00:16:45,820 --> 00:16:50,380
I'm going to use both of these as we move forward to represent both the plant's metadata

294
00:16:50,380 --> 00:16:55,380
and also the readings that we're going to dive into.

295
00:16:55,380 --> 00:17:03,620
This is a sample of ksqlDB code, and we are using this to create the table that is backed

296
00:17:03,620 --> 00:17:07,180
by the metadata topic.

297
00:17:07,180 --> 00:17:11,500
I chose to use a table here because if I ever wanted to update the value for the underlying

298
00:17:11,500 --> 00:17:19,100
metadata, I'd want my stream processing application to leverage that most recent data.

299
00:17:19,100 --> 00:17:20,100
That's more important here.

300
00:17:20,100 --> 00:17:25,740
And, again, the syntax should feel very familiar as it's basically SQL.

301
00:17:25,740 --> 00:17:29,540
And also, the underlying topic is formatted as Avro.

302
00:17:29,540 --> 00:17:32,260
Remember, we created that Avro schema.

303
00:17:32,300 --> 00:17:38,820
So the really cool thing about defining it as Avro is that ksqlDB, as a consumer of that

304
00:17:38,820 --> 00:17:44,620
Kafka topic, can get all of that field and type value on its own.

305
00:17:44,620 --> 00:17:48,660
The only thing I have to define, because this is a table, I have to define the key, which

306
00:17:48,660 --> 00:17:50,700
value we're actually using as the key.

307
00:17:50,700 --> 00:17:55,500
Every other field is pulled in for free without me making some weird spelling mistakes.

308
00:17:55,500 --> 00:18:02,180
I think that's the biggest set of errors that I encounter when I use any programming, really.

309
00:18:02,180 --> 00:18:05,620
So to create this table, we just point to the Kafka topic, tell it what format we're

310
00:18:05,620 --> 00:18:11,420
pulling it in as, and define the key, and we can move on our very way.

311
00:18:11,420 --> 00:18:15,740
And this is what the ksqlDB editor looks like in the cloud.

312
00:18:15,740 --> 00:18:20,780
You just copy and paste any sort of SQL, you can write it here, run it, and it'll go ahead

313
00:18:20,780 --> 00:18:27,620
and create that table and your streaming application behind the scenes.

314
00:18:27,620 --> 00:18:31,740
From there, I could create the actual stream of data for my houseplant readings, the stream

315
00:18:31,740 --> 00:18:32,740
that's coming in.

316
00:18:32,740 --> 00:18:38,260
Again, it is Avro data, so we don't have to specify any of the fields or their types.

317
00:18:38,260 --> 00:18:42,540
I just point to the topic, tell it it's Avro, and again, just run this through the cloud

318
00:18:42,540 --> 00:18:45,540
and it will start bringing in that stream.

319
00:18:45,540 --> 00:18:51,740
And then from there, I needed to combine these two data sets, right, conduct some stream

320
00:18:51,740 --> 00:18:53,140
enrichment.

321
00:18:53,140 --> 00:18:59,180
So I had that ongoing stream of readings, and yeah, I can determine that some plant

322
00:18:59,380 --> 00:19:03,820
needs to be watered from that, but what I really need is the metadata information to

323
00:19:03,820 --> 00:19:05,860
actually make sense of it, right?

324
00:19:05,860 --> 00:19:10,540
So to enrich this data, I am conducting a stream to table join, and this should feel

325
00:19:10,540 --> 00:19:14,180
familiar just like a regular SQL inner join, okay?

326
00:19:14,180 --> 00:19:19,100
And I'm using an inner join because I want to make sure that the output of this query,

327
00:19:19,100 --> 00:19:22,700
the output of this stream processing, has all the information that I actually need.

328
00:19:22,700 --> 00:19:26,380
So I need that metadata as well as the readings information to actually make sense of the

329
00:19:26,380 --> 00:19:31,900
information that's going through this application now.

330
00:19:31,900 --> 00:19:36,580
So to do this, again, we point to the topic that we're going to feed this into.

331
00:19:36,580 --> 00:19:40,500
We tell it I want it to be Avro again, and then we just do a simple select and join.

332
00:19:40,500 --> 00:19:45,420
Very, very similar to SQL, but you'll notice that I have this extra bit at the bottom with

333
00:19:45,420 --> 00:19:52,340
the emit changes, and this is a phrase that indicates that we're creating a push query.

334
00:19:52,820 --> 00:19:57,500
What this means is that the result set, the result of this stream processing stage, will

335
00:19:57,500 --> 00:20:02,140
be an open-ended stream of data that's pushed to me as output, okay?

336
00:20:02,140 --> 00:20:03,680
It's going to be ongoing.

337
00:20:03,680 --> 00:20:07,100
Every time a new piece of information arrives on the underlying Kafka topic going into

338
00:20:07,100 --> 00:20:11,700
this, we will get some sort of result as long as the join succeeds.

339
00:20:11,700 --> 00:20:14,740
And this is contrasted with a bounded pull query.

340
00:20:14,740 --> 00:20:19,460
This is another option available to you as part of ksqlDB, where I would be pulling a

341
00:20:19,540 --> 00:20:24,260
finite result set that reflects the current state of the underlying stream, all right?

342
00:20:24,260 --> 00:20:29,260
So in order to get a constant flow of data, we want it to be a push query.

343
00:20:33,100 --> 00:20:36,660
And now that I had all the information, I had a stream of data with all the information

344
00:20:36,660 --> 00:20:40,580
that I actually needed, I have it enriched, it's all there, ready to go, I can start to

345
00:20:40,580 --> 00:20:42,580
make sense of it.

346
00:20:42,580 --> 00:20:46,660
So I had to revisit the overall goal of this project at this point.

347
00:20:46,660 --> 00:20:53,660
So whenever a given plant has enough low-moisture readings, meaning that it needs to be watered,

348
00:20:53,820 --> 00:20:57,780
I want to send an alert, I would like to receive some sort of notification.

349
00:20:57,780 --> 00:21:03,020
But how do I determine how to send that alert, and how often do I send them?

350
00:21:03,020 --> 00:21:06,020
I decided that I didn't want to inundate my phone with alerts every time I got a low

351
00:21:06,020 --> 00:21:10,020
reading, that would just be ridiculous, and we're taking them every five seconds, so that

352
00:21:10,020 --> 00:21:15,260
would be a little too much, especially when I'm traveling and maybe just running errands.

353
00:21:15,260 --> 00:21:19,740
I figured that receiving an alert a couple times every day would give me enough time

354
00:21:19,740 --> 00:21:25,740
to act on this and water my plants, so I opted for a 12-hour notification window.

355
00:21:25,740 --> 00:21:29,980
I also noticed that my sensors weren't perfect, if you've done any hardware projects you've

356
00:21:29,980 --> 00:21:34,660
probably learned this, that they're not actually giving you all the data exactly as you want

357
00:21:34,660 --> 00:21:36,060
it.

358
00:21:36,060 --> 00:21:40,820
So sometimes I get false low readings or false high readings, but the general trend over

359
00:21:40,820 --> 00:21:44,620
time would be that the moisture percentage would decrease.

360
00:21:44,620 --> 00:21:49,940
So within a given 12-hour period, I decided that it would be good enough that if I received

361
00:21:49,940 --> 00:21:54,900
an hour's worth of low readings, that's enough to indicate that, okay, this is a real amount

362
00:21:54,900 --> 00:21:58,780
of low readings, I should probably water the plant.

363
00:21:58,780 --> 00:22:02,980
So the readings, again, being taken every five seconds, that would mean I'd need 720

364
00:22:02,980 --> 00:22:07,820
readings, low readings, in order to send an alert.

365
00:22:07,820 --> 00:22:12,340
This is the query that I wrote to achieve this stream processing.

366
00:22:12,380 --> 00:22:17,940
I wanted to receive an alert every 12 hours, so I set up a tumbling window, a non-overlapping

367
00:22:17,940 --> 00:22:24,900
12-hour windows, and I specifically, you see here, count the rows where the moisture value

368
00:22:24,900 --> 00:22:30,380
on the readings is lower than the lower moisture threshold that's contained in the metadata

369
00:22:30,380 --> 00:22:34,740
topic, or that had been contained in the metadata topic.

370
00:22:34,740 --> 00:22:41,380
And I specifically want to output a result when the count of those rows reaches 720.

371
00:22:41,380 --> 00:22:44,340
And this might seem a little wonky, so let me explain a bit more.

372
00:22:44,340 --> 00:22:48,500
Whenever we're windowing and conducting an aggregate, the output is a table where each

373
00:22:48,500 --> 00:22:51,740
row is computed per key per window, okay?

374
00:22:51,740 --> 00:22:55,300
And this makes sense because a table provides the latest value per key.

375
00:22:55,300 --> 00:22:59,060
But the query is operating over the full space of records, right?

376
00:22:59,060 --> 00:23:03,100
Every time a record is produced to Kafka, it's going to flow through here.

377
00:23:03,100 --> 00:23:06,420
So there's an underlying state, right, in order to conduct this sort of count.

378
00:23:06,420 --> 00:23:07,420
There's a state.

379
00:23:07,420 --> 00:23:10,660
So every time a record goes through, if it satisfies that filter, it will be added as

380
00:23:10,660 --> 00:23:13,040
part of that aggregate.

381
00:23:13,040 --> 00:23:20,020
So the bottom line is that rows 1 through 719, as this state is updated, they're not

382
00:23:20,020 --> 00:23:21,020
going to output anything.

383
00:23:21,020 --> 00:23:24,460
As soon as we get to 720, it will output a row.

384
00:23:24,460 --> 00:23:29,620
So we're going to receive one alert per plant per 12-hour window, which is exactly what

385
00:23:29,620 --> 00:23:32,820
I wanted.

386
00:23:32,820 --> 00:23:34,820
From there, you know, I had these messages in Kafka.

387
00:23:34,820 --> 00:23:37,900
I knew, you know, when I should actually be alerting.

388
00:23:37,900 --> 00:23:41,820
All I needed to do was move the data out of Kafka and actually give it to me, right?

389
00:23:41,820 --> 00:23:43,700
So I set up a Telegram bot.

390
00:23:43,700 --> 00:23:44,700
Anyone can do this.

391
00:23:44,700 --> 00:23:51,180
It takes two minutes to set up a bot and just start playing around on Telegram.

392
00:23:51,180 --> 00:23:54,860
And what's convenient about it is that it has a chat API so that you can leverage the

393
00:23:54,860 --> 00:23:59,540
conversation you're having with your bot and directly use that endpoint to feed a message

394
00:23:59,540 --> 00:24:02,320
to your phone, which is pretty cool.

395
00:24:02,320 --> 00:24:04,420
To do this, I leverage a tool called Kafka Connect.

396
00:24:04,420 --> 00:24:10,100
This is an auxiliary Kafka tool that's available through Confluent Cloud to move data from

397
00:24:10,100 --> 00:24:12,100
Kafka out of the ecosystem.

398
00:24:12,100 --> 00:24:16,180
So I'm using the HTTP sync connector.

399
00:24:16,180 --> 00:24:19,940
You point to a specific endpoint, and it will push data that way.

400
00:24:19,940 --> 00:24:21,060
It's fully managed in the cloud.

401
00:24:21,060 --> 00:24:22,060
You don't actually do any coding.

402
00:24:22,060 --> 00:24:23,060
It's pretty great.

403
00:24:23,060 --> 00:24:29,880
And then to get my sync connector up and running, I said it was no code, but this is the sort

404
00:24:29,880 --> 00:24:34,640
of output configuration that is generated as part of that connector.

405
00:24:34,640 --> 00:24:36,160
So you see some basic information.

406
00:24:36,160 --> 00:24:39,880
It points to the topic that we're actually using, pulling that data from the format that

407
00:24:39,880 --> 00:24:40,880
we're using.

408
00:24:40,880 --> 00:24:44,920
You specify a key to actually access your cluster, an API key.

409
00:24:44,920 --> 00:24:49,880
This is the actual API URL that we're pushing it to with my chat ID.

410
00:24:49,880 --> 00:24:56,000
And I'm using RegEx to actually go in, fetch the underlying message, the relevant information,

411
00:24:56,000 --> 00:25:03,280
and I'm pushing that to the text body of the message that's going to the API.

412
00:25:03,280 --> 00:25:06,160
That's the alert that I get on my phone now.

413
00:25:06,160 --> 00:25:11,000
They all have names, so Bradley was dry.

414
00:25:11,000 --> 00:25:16,000
And yeah, like I said, this was my first hardware-based project.

415
00:25:16,000 --> 00:25:17,000
I'm now inspired.

416
00:25:17,000 --> 00:25:19,200
I'm going to make so many more of these.

417
00:25:19,200 --> 00:25:21,000
I'm really excited about Raspberry Pi now.

418
00:25:21,000 --> 00:25:23,160
I've already purchased a bunch more sensors.

419
00:25:23,160 --> 00:25:24,160
I'm ready.

420
00:25:24,360 --> 00:25:28,200
If you are looking to play around with this, this is such a rewarding project for me, because

421
00:25:28,200 --> 00:25:32,640
I didn't have to set up any infrastructure, and I got to leverage stream processing the

422
00:25:32,640 --> 00:25:34,000
whole way.

423
00:25:34,000 --> 00:25:39,320
So on the left here, we have our developer Python resources, everything about the Python

424
00:25:39,320 --> 00:25:45,680
client, and on the right, I have my GitHub repo with all of the scripts that I used and

425
00:25:45,680 --> 00:25:49,800
some of the configuration files to set up the project.

426
00:25:50,800 --> 00:25:54,800
And now I'll leave that up, but if anyone has any questions, I'm happy to answer them now.

427
00:26:08,800 --> 00:26:12,800
Yes, ksqlDB is running exclusively through Confluent Cloud.

428
00:26:12,800 --> 00:26:15,800
What's running on the Raspberry Pi, and this is why it was complete overkill for the 8-gig

429
00:26:15,800 --> 00:26:16,800
Raspberry Pi.

430
00:26:16,800 --> 00:26:20,800
It's running two Python scripts to grab those readings.

431
00:26:22,800 --> 00:26:23,800
Yeah.

432
00:26:23,800 --> 00:26:25,800
It's pretty simple.

433
00:26:25,800 --> 00:26:26,800
Yeah.

434
00:26:32,800 --> 00:26:33,800
I know.

435
00:26:33,800 --> 00:26:34,800
I know.

436
00:26:34,800 --> 00:26:35,800
It's so cool.

437
00:26:35,800 --> 00:26:38,800
That was going to be the most difficult part of the pipeline, I think, is moving it and

438
00:26:38,800 --> 00:26:40,800
actually giving it to my phone.

439
00:26:40,800 --> 00:26:44,800
But yeah, the Telegram API made it so easy, so simple.

440
00:26:45,800 --> 00:26:46,800
Yeah?

441
00:26:48,800 --> 00:26:49,800
I'm sorry?

442
00:26:51,800 --> 00:26:53,800
The plants, yeah, the plants are okay.

443
00:26:55,800 --> 00:26:56,800
They are all right.

444
00:26:56,800 --> 00:26:59,800
They haven't complained, thankfully.

445
00:26:59,800 --> 00:27:01,800
But yeah, nothing so dramatic as that first picture.

446
00:27:01,800 --> 00:27:03,800
That has not happened recently.

447
00:27:05,800 --> 00:27:06,800
But yeah, I am already planning.

448
00:27:06,800 --> 00:27:13,800
As soon as I can get my hands on a zero, a Pi zero, I'm going to set up a lot more plant

449
00:27:13,800 --> 00:27:15,800
stations to monitor my plants.

450
00:27:15,800 --> 00:27:18,800
So there will be way too many of these in my house by the end of it.

451
00:27:21,800 --> 00:27:22,800
Yeah?

452
00:27:25,800 --> 00:27:27,800
Yeah, automate the watering, okay?

453
00:27:28,800 --> 00:27:29,800
Again, I said I rent.

454
00:27:29,800 --> 00:27:36,800
None of my plants actually are near a watering source, and I cannot bring myself to set up

455
00:27:36,800 --> 00:27:38,800
a bunch of tubing or reservoirs.

456
00:27:38,800 --> 00:27:39,800
I don't know.

457
00:27:39,800 --> 00:27:41,800
It seems like way too complicated at that point.

458
00:27:43,800 --> 00:27:47,800
Yeah, I want it to be automated like that, but we'll see.

459
00:27:47,800 --> 00:27:51,800
The next step that I want to do is have little monitors next to the plants so they just have

460
00:27:51,800 --> 00:27:55,800
like Tamagotchi style, like they just put sad faces on when they need to be watered.

461
00:27:56,800 --> 00:28:00,800
And then that way when I'm gone, you know, my boyfriend can just water them, right?

462
00:28:02,800 --> 00:28:03,800
It won't happen.

463
00:28:04,800 --> 00:28:06,800
Maybe I'll just make them cry at that point.

464
00:28:06,800 --> 00:28:07,800
Yes?

465
00:28:10,800 --> 00:28:11,800
Yeah, that's true.

466
00:28:12,800 --> 00:28:16,800
Yeah, I've gone back to the drawing board as far as how I'm going to do this.

467
00:28:17,800 --> 00:28:21,800
My partner is a mechanical engineer, so I'm like, you're just going to figure this out.

468
00:28:22,800 --> 00:28:23,800
We'll build something.

469
00:28:26,800 --> 00:28:27,800
Anyone else?

470
00:28:30,800 --> 00:28:31,800
Cool.

471
00:28:31,800 --> 00:28:32,800
Thank you so much.

472
00:28:32,800 --> 00:28:34,800
Happy to be here.

