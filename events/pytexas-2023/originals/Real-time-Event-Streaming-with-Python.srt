1
00:00:00,000 --> 00:00:05,460
So the tech's not working for me either, so things aren't quite the way I've arranged

2
00:00:05,460 --> 00:00:09,400
it to be, so I might be a little awkward here, but we'll make it work.

3
00:00:09,400 --> 00:00:10,400
So yeah, I'm Dave Klein.

4
00:00:10,400 --> 00:00:15,160
I am with Quicks, which is a company that builds an event streaming platform for Python

5
00:00:15,160 --> 00:00:16,160
developers.

6
00:00:16,160 --> 00:00:23,120
I used to be with Confluent, who was created by the creators of Kafka.

7
00:00:23,120 --> 00:00:26,640
So I've been working with Kafka for quite a bit for the last four or five years, and

8
00:00:26,640 --> 00:00:29,200
I love it, and I've also been working with Python for the last couple years, and I love

9
00:00:29,200 --> 00:00:30,200
that too.

10
00:00:30,200 --> 00:00:35,520
For some of that, I used like 20 years or more of Java, and this has been such a relief.

11
00:00:35,520 --> 00:00:37,160
I feel like I retired already.

12
00:00:37,160 --> 00:00:39,840
I'm still working, but it doesn't feel like it anymore.

13
00:00:39,840 --> 00:00:42,800
And I also want to mention that it's great to be back here.

14
00:00:42,800 --> 00:00:45,680
Last year at PyTexas, that was my very first Python conference.

15
00:00:45,680 --> 00:00:48,360
I spoke at hundreds of conferences, but it was the very first Python conference, and

16
00:00:48,360 --> 00:00:49,760
it was amazing, and it still is.

17
00:00:49,760 --> 00:00:52,200
So thank you all for the organizers and attendees.

18
00:00:52,200 --> 00:00:55,760
Yeah, it's just a great community.

19
00:00:55,760 --> 00:00:58,720
All right, so let's get started.

20
00:00:58,720 --> 00:01:05,120
I hope I can keep track with what I'm doing, and nope, I can't.

21
00:01:05,120 --> 00:01:09,600
So I'm going to talk about this projector.

22
00:01:09,600 --> 00:01:11,880
It's really a great projector.

23
00:01:11,880 --> 00:01:12,880
It's gone now too.

24
00:01:12,880 --> 00:01:17,880
I can't talk about anything.

25
00:01:17,880 --> 00:01:21,360
All right.

26
00:01:21,360 --> 00:01:22,360
We're back.

27
00:01:22,360 --> 00:01:23,360
Okay.

28
00:01:23,360 --> 00:01:28,200
So event processing, or real-time processing, or stream processing, you'll hear lots of

29
00:01:28,440 --> 00:01:32,520
different names for it, but it's basically talking about getting data into our applications,

30
00:01:32,520 --> 00:01:36,360
dealing with it right away as it's originated, and that's why I like the term event.

31
00:01:36,360 --> 00:01:41,560
You'll hear the terms event, message, record when dealing with things like this, and those

32
00:01:41,560 --> 00:01:46,920
are all valid terms for different use cases or different connotations.

33
00:01:46,920 --> 00:01:50,800
I like event because of the fact that it's tied to something happening, and that's really

34
00:01:50,800 --> 00:01:52,400
where all data originates, really.

35
00:01:52,400 --> 00:01:56,080
Something happens that generates data, and so the event tells us something happened,

36
00:01:56,080 --> 00:01:58,080
and it gives us the data about what happened.

37
00:01:59,040 --> 00:02:03,320
As data is generated, that's what is really most useful to us in most cases.

38
00:02:03,320 --> 00:02:08,720
Now, traditionally, we get data, the data is generated, it's stored into a database,

39
00:02:08,720 --> 00:02:11,720
and then we periodically run queries against the database to get the data out and do something

40
00:02:11,720 --> 00:02:12,720
with it.

41
00:02:12,720 --> 00:02:17,800
So the idea here, though, is instead to get that data as it's generated and make use of

42
00:02:17,800 --> 00:02:22,000
it right away, doing the processing that we want to do on it, one event, one record at

43
00:02:22,000 --> 00:02:24,560
a time in a continuous stream.

44
00:02:24,560 --> 00:02:27,280
So you can see data becoming from different applications.

45
00:02:27,280 --> 00:02:30,840
It could be coming from IoT devices, it could be coming from mobile phones, it could be

46
00:02:30,840 --> 00:02:33,800
coming from all kinds of different things, sensors, all kinds of stuff that could be

47
00:02:33,800 --> 00:02:37,200
bringing data into our systems, and then we run it through some whatever processing we're

48
00:02:37,200 --> 00:02:40,880
going to be doing to it, and then output it, usually, to something else, some downstream

49
00:02:40,880 --> 00:02:44,720
process, a storage thing, analytics engine, something like that.

50
00:02:44,720 --> 00:02:47,360
So that's the common use case for these types of things.

51
00:02:47,360 --> 00:02:50,680
And speaking of use cases, I want to mention a couple, there's lots of different places

52
00:02:50,680 --> 00:02:54,840
where this is used, but some of the most common ones are things like recommendation engines,

53
00:02:54,840 --> 00:02:55,840
right?

54
00:02:55,840 --> 00:02:58,800
Recommendations are often done for, they've been around for a long time now, ever since

55
00:02:58,800 --> 00:03:03,120
Amazon came on the scene, really, but these recommendation engines often early on would

56
00:03:03,120 --> 00:03:07,720
be done based on pulling data periodically, sometimes daily, sometimes hourly, different

57
00:03:07,720 --> 00:03:11,360
time intervals, but there's always a bit of staleness to that data.

58
00:03:11,360 --> 00:03:14,920
Now most systems like Amazon, others like that, are doing this in real time.

59
00:03:14,920 --> 00:03:19,600
So as you're browsing around the site, your recommendations are changing by your current

60
00:03:19,600 --> 00:03:24,040
behavior or the current behavior of people that you know through social media, things

61
00:03:24,040 --> 00:03:25,040
like that.

62
00:03:25,360 --> 00:03:28,080
It's still a bit scary how much they know about us and stuff like that, but they're

63
00:03:28,080 --> 00:03:31,720
able to make use of that, hopefully to our benefit, using real time processing.

64
00:03:33,120 --> 00:03:37,800
An example I saw, though, that hits me personally is since I do a bit of traveling for work,

65
00:03:38,920 --> 00:03:42,640
you have like frequent flyer programs or hotel loyalty programs in general.

66
00:03:43,120 --> 00:03:47,000
And there's a hotel that I stay with, a chain that I stay with regularly.

67
00:03:47,160 --> 00:03:51,440
And if I stay at a hotel, it's usually about maybe a week and a half or so before the points

68
00:03:51,440 --> 00:03:54,000
for that stay show up in my account, which is no big deal.

69
00:03:54,000 --> 00:03:57,480
It's not that it doesn't really matter that much, but I just noticing that recently the

70
00:03:57,480 --> 00:04:02,080
airline that I fly on, I landed in an airport and I can pull up my phone, pull up the account

71
00:04:02,080 --> 00:04:04,040
and I see the miles in my account right there.

72
00:04:04,440 --> 00:04:07,000
So you can see one of these companies is using real time processing.

73
00:04:07,000 --> 00:04:11,480
The other one's using some type of old batch system and it makes it a very different customer

74
00:04:11,480 --> 00:04:16,160
experience. And again, those aren't, you know, life or death situations, but they do make

75
00:04:16,160 --> 00:04:20,360
a better experience. So it's one of those things where we may not see right now.

76
00:04:20,400 --> 00:04:22,800
We were talking earlier, but we may not see right now the use cases for this.

77
00:04:22,800 --> 00:04:27,480
But they start to appear more and more the more we learn how real time processing works.

78
00:04:28,200 --> 00:04:30,400
And also, I think it's easier to do just like machine learning.

79
00:04:30,400 --> 00:04:33,520
There was a time when that was a real niche thing only for a few use cases.

80
00:04:33,520 --> 00:04:37,160
And now it's used everywhere because it's gotten easier to use and we understand it better.

81
00:04:40,000 --> 00:04:44,800
So when talking about this, I'm coming from the Apache Kafka background, but that's not the

82
00:04:44,800 --> 00:04:48,560
only tool that's used for this type of for event processing or stream processing.

83
00:04:48,920 --> 00:04:54,760
There's also tools like Apache Pulsar, which is similar in structure, but based on some

84
00:04:54,760 --> 00:04:56,000
different underlying technologies.

85
00:04:56,880 --> 00:05:00,240
There's Red Panda, which is basically Kafka, but written in C++.

86
00:05:01,120 --> 00:05:03,160
And then Amazon Kinesis is another pretty popular one.

87
00:05:03,160 --> 00:05:04,120
And there's some others as well.

88
00:05:04,880 --> 00:05:10,040
But these are these are different tools for for events streaming or message brokers or

89
00:05:10,040 --> 00:05:12,440
things you might want to call them. Not to be confused with message queues.

90
00:05:12,440 --> 00:05:13,800
That's a really different animal altogether.

91
00:05:13,800 --> 00:05:14,920
So I wasn't bringing those into it.

92
00:05:14,920 --> 00:05:20,280
And then for the processing part of it, you know, I was going to say close it off by saying

93
00:05:20,280 --> 00:05:21,960
Kafka really is the de facto standard, though.

94
00:05:21,960 --> 00:05:23,080
So it's one I'm going to stick with.

95
00:05:23,080 --> 00:05:23,960
It's one I'm most familiar with.

96
00:05:23,960 --> 00:05:25,480
But it's also the one that's most commonly used.

97
00:05:25,480 --> 00:05:29,880
So if you're getting into this to start out, Kafka is a great place to start with to do

98
00:05:29,880 --> 00:05:33,640
the actual processing. There's there's there's whole frameworks that have been built up to

99
00:05:33,640 --> 00:05:39,120
do real time processing with data in these event streams like Apache Flink, which is a

100
00:05:39,120 --> 00:05:40,000
pretty popular one.

101
00:05:40,640 --> 00:05:42,640
This is an application that you have to install.

102
00:05:42,640 --> 00:05:47,200
You usually run it in a cluster with several nodes and you write applications in SQL or using

103
00:05:47,200 --> 00:05:52,520
Python. You can use Python and you write your applications and then run them against the

104
00:05:52,520 --> 00:05:53,480
ongoing stream of data.

105
00:05:54,720 --> 00:05:59,880
Another one is ksqlDB, which, again, uses SQL to work it works against Kafka data.

106
00:06:01,680 --> 00:06:04,720
Or if you want to include something in your applications, there's libraries you can use.

107
00:06:04,720 --> 00:06:07,520
The most popular one is a Java based library called Kafka Streams.

108
00:06:07,520 --> 00:06:10,280
It's a part of the actually part of the Apache Kafka project.

109
00:06:10,760 --> 00:06:15,680
And it allows you to just include some classes in your application and do real time stream

110
00:06:15,680 --> 00:06:18,920
processing in your own applications, your microservices or other applications.

111
00:06:18,920 --> 00:06:20,320
Whoa, this thing's pretty sensitive.

112
00:06:21,480 --> 00:06:26,000
But you can build those things into your application instead of having to have another set of

113
00:06:26,000 --> 00:06:28,000
applications running in their own cluster somewhere.

114
00:06:28,520 --> 00:06:32,120
So that gives you a lot of flexibility and saves saves a lot of money as far as the

115
00:06:32,120 --> 00:06:33,640
processor operationally.

116
00:06:34,840 --> 00:06:36,560
But it's Java and that's not great.

117
00:06:36,560 --> 00:06:40,640
So there are some and I would end on a talk about some other libraries that are for Python

118
00:06:40,640 --> 00:06:42,040
that do similar things to Kafka Streams.

119
00:06:42,440 --> 00:06:47,920
But the goal of this talk is to show that you can do some some simple stream processing

120
00:06:47,920 --> 00:06:49,960
things with just plain Python and Kafka.

121
00:06:51,720 --> 00:06:55,520
So back to Kafka. So to do before we get started, I'm going to do I did speak on Kafka

122
00:06:55,520 --> 00:06:57,880
last year, and so I actually reuse a few slides here.

123
00:06:57,880 --> 00:07:02,080
I'm going to do a really brief introduction to what Kafka is just for people that aren't

124
00:07:02,080 --> 00:07:05,040
familiar with it. So if you are familiar with Kafka, please bear with me.

125
00:07:05,040 --> 00:07:07,840
It shouldn't take too long, but I just want to make sure everyone understands what we're

126
00:07:07,840 --> 00:07:08,840
talking about.

127
00:07:10,600 --> 00:07:13,800
So at the core of Kafka is a log.

128
00:07:13,880 --> 00:07:16,560
Really, Kafka is a storage engine for the most part.

129
00:07:17,360 --> 00:07:20,880
The real magic happens with the client libraries, which are producers and consumers.

130
00:07:20,880 --> 00:07:21,880
We're going to talk about those in a bit.

131
00:07:22,160 --> 00:07:25,160
But underlying Kafka really is just a log of data.

132
00:07:26,680 --> 00:07:31,840
It's an immutable, append-only, ordered and durable log of events.

133
00:07:32,320 --> 00:07:33,840
And these are important characteristics of it.

134
00:07:33,880 --> 00:07:37,000
So it's immutable in that when something happens, it stays happened.

135
00:07:37,000 --> 00:07:38,880
You can't undo it. You can add something.

136
00:07:38,920 --> 00:07:39,720
It's kind of like with accounting.

137
00:07:39,720 --> 00:07:44,480
There's an essay like Why Accountants Don't Use Erasers, I think it's called, which talks

138
00:07:44,480 --> 00:07:46,520
about this whole idea that when events happen, they happen.

139
00:07:46,520 --> 00:07:49,840
You can't undo that. You can you can do another event that counters what the other one

140
00:07:49,840 --> 00:07:53,160
happened. You can adjust the entry type of thing, but you can't undo the thing.

141
00:07:53,200 --> 00:07:56,160
So when something lands in a Kafka topic, it stays there.

142
00:07:58,000 --> 00:07:59,760
It's append-only, so we don't insert anywhere.

143
00:07:59,800 --> 00:08:01,120
It always just keeps on adding on.

144
00:08:01,640 --> 00:08:06,400
And it's ordered in that what you're seeing here on the screen is a single partition

145
00:08:06,400 --> 00:08:11,160
topic. So Kafka topics can be broken out into multiple partitions, and they usually

146
00:08:11,160 --> 00:08:16,560
are, but they don't have to be. But for each partition, it's its own log and

147
00:08:16,560 --> 00:08:19,200
the data in that log will be processed in the order it was written.

148
00:08:19,600 --> 00:08:21,680
So, you know, just right to left, keep on going.

149
00:08:22,000 --> 00:08:25,800
And yeah, so that's ordered and it's also durable.

150
00:08:25,800 --> 00:08:30,880
And this is a real key feature and really makes it differentiate between cues, which

151
00:08:30,960 --> 00:08:33,960
usually lose the data as just as it's consumed or processed.

152
00:08:33,960 --> 00:08:37,840
Right. A Kafka topic keeps this data as long as we want it to.

153
00:08:37,920 --> 00:08:42,800
We can set the retention through configurations so the older data can drop off and we

154
00:08:42,800 --> 00:08:45,480
don't need it. But you can also just have it stored permanently, especially with

155
00:08:46,160 --> 00:08:51,080
mechanisms now like infinite storage where you can have older data moved up to S3 or

156
00:08:51,080 --> 00:08:54,960
something like that and, you know, cheaply store all of your data permanently.

157
00:08:55,680 --> 00:08:58,840
But you don't have you can set your attention to drop it off at a certain age period.

158
00:08:59,080 --> 00:09:00,880
But the data is there as long as we want it there.

159
00:09:00,880 --> 00:09:03,040
It's not it doesn't disappear once it's been read.

160
00:09:04,040 --> 00:09:08,000
That's the key thing there. And this comes in handy for things like replaying and

161
00:09:08,240 --> 00:09:13,440
your for like machine learning models, your training model, you repeat the pipeline

162
00:09:13,440 --> 00:09:15,800
with the same data over and over again. You have consistent results because you

163
00:09:15,800 --> 00:09:19,560
have the same data. That number you see at the bottom of that screen of the log

164
00:09:19,560 --> 00:09:24,840
there is an offset. And so you can reset to any offset and just replay from there

165
00:09:24,840 --> 00:09:28,600
forward. So it's really helpful for retrying things, for fixing bugs and

166
00:09:28,600 --> 00:09:32,560
replaying the data or doing what if scenarios, things like that.

167
00:09:35,320 --> 00:09:39,560
All right. So here's a random representation of what an event might look like in

168
00:09:39,560 --> 00:09:43,720
Kafka. It's in Kafka, it's actually just bytes.

169
00:09:43,720 --> 00:09:46,320
But in our applications, it's going to usually be some type of an object that can

170
00:09:46,320 --> 00:09:50,120
be serialized to bytes. So either maybe a protobuf or JSON or something like that.

171
00:09:50,120 --> 00:09:55,680
Here we have a JSON example. It's got the two main components of an event are a

172
00:09:55,680 --> 00:10:00,960
key and a topic or a key and a value. Sorry. So the key is normally a simple

173
00:10:00,960 --> 00:10:03,680
data type, a string or something like that. It might be a UID, it might be a

174
00:10:03,680 --> 00:10:08,600
customer ID, something meaningful in your domain. But it's usually a simple

175
00:10:08,600 --> 00:10:12,200
value and then the or simple type. And then the value is the more complex type

176
00:10:12,200 --> 00:10:15,800
that's going to have your fields in it. It's more like your payload. Now it's

177
00:10:15,800 --> 00:10:20,400
also, I don't mention it here, but also every event in a Kafka or every Kafka

178
00:10:20,400 --> 00:10:23,520
event will have a timestamp, which is also really important, especially if you're

179
00:10:23,520 --> 00:10:30,560
working with time series data or things like that. All right. So these events are

180
00:10:30,560 --> 00:10:35,000
written to the Kafka topics using a producer. Now producer is a class in the

181
00:10:35,000 --> 00:10:38,400
client library. Excuse me. And we're going to talk about a couple of different

182
00:10:38,400 --> 00:10:41,800
Python client libraries that are out there. They're all client libraries in

183
00:10:41,800 --> 00:10:45,360
whatever language you're going to have these classes. So producer is one of the

184
00:10:45,360 --> 00:10:51,280
main classes. It is one that's responsible for writing data to the Kafka topics.

185
00:10:51,280 --> 00:10:55,120
It'll also handle things like compression. It'll handle retries. Also, if you're

186
00:10:55,120 --> 00:10:57,720
working with multiple partitions, the producer is what figures out which

187
00:10:57,720 --> 00:11:00,960
partitions to send each event to. So these are all important things that the

188
00:11:00,960 --> 00:11:06,640
producer does. And the producer is just a class in this client library. So as it

189
00:11:06,640 --> 00:11:10,760
writes the events one by one, it keeps appending to the next open offset, then

190
00:11:10,760 --> 00:11:14,240
the consumer comes along. The consumer is another class in the client library and

191
00:11:14,240 --> 00:11:17,400
the consumer is what will read the data. It's kind of, I think, misnamed because

192
00:11:17,400 --> 00:11:20,680
it doesn't consume the data. We mentioned earlier the durability, right?

193
00:11:20,720 --> 00:11:24,960
The data stays there, but the consumer will read the data and then do whatever

194
00:11:24,960 --> 00:11:27,920
processing we want it to do. We handle the processing because it's in our

195
00:11:27,920 --> 00:11:31,160
application. It's just a class we're including in our application. So we

196
00:11:31,160 --> 00:11:33,960
decide what we do with the data. The consumer just reads it for us.

197
00:11:36,840 --> 00:11:39,960
And so the consumer reads from left to right, just following the same path, but

198
00:11:39,960 --> 00:11:43,080
they're completely independent. The producer doesn't know if someone's

199
00:11:43,080 --> 00:11:46,280
consuming the data or when, and the consumer doesn't know where the data came

200
00:11:46,280 --> 00:11:50,240
from. They're completely decoupled, which is really important and allows us to

201
00:11:50,240 --> 00:11:52,360
build much less, less coupled applications.

202
00:11:55,240 --> 00:12:00,320
And the consumer will also periodically record the offset number for the most

203
00:12:00,320 --> 00:12:03,720
recent event that it's completely processed. You can do this automatically

204
00:12:03,720 --> 00:12:06,840
through configuration or you can do it in your code when you know you've

205
00:12:06,840 --> 00:12:09,840
completed a certain one. And this is important for making sure that you don't

206
00:12:09,840 --> 00:12:12,160
skip anything and that you don't do, don't do duplicates.

207
00:12:14,280 --> 00:12:17,920
So if the consumer goes down, the producer will keep on going. He doesn't

208
00:12:17,960 --> 00:12:21,000
really know the difference. It'll keep on producing and eventually there is a

209
00:12:21,000 --> 00:12:25,160
metric you can watch for here called consumer lag. So the consumer gets too

210
00:12:25,160 --> 00:12:29,080
far behind in reading what's been, what's on the topic, then it'll send, you

211
00:12:29,080 --> 00:12:31,560
can set up alerts for things like that so you know something's wrong with your

212
00:12:31,560 --> 00:12:36,280
system. But the application won't fail because of it, which is, if you remember

213
00:12:36,280 --> 00:12:38,520
last year in my talk, we talked about vendor of microservices, and that's

214
00:12:38,520 --> 00:12:42,160
really a key thing there so that a failure in a consumer doesn't cause a

215
00:12:42,160 --> 00:12:43,640
cascading failure downstream.

216
00:12:43,640 --> 00:12:48,440
But when it starts back up, it'll use that committed offset to pick up at the

217
00:12:48,440 --> 00:12:54,600
very next spot and keep on going. So, all right, so that's all I had on the

218
00:12:54,600 --> 00:12:59,800
producer-consumer exit. Those are part of the Kafka client package and there are

219
00:12:59,800 --> 00:13:05,600
two, two popular ones that I know of in Python. There's the Confluent Kafka,

220
00:13:05,600 --> 00:13:09,720
which is the one that's produced by Confluent. It's based on, it's basically

221
00:13:09,760 --> 00:13:14,640
a Python wrapper around a C library called LibRD Kafka, which is pretty widely

222
00:13:14,640 --> 00:13:19,920
known or accepted to be the closest in both performance and features to the Java

223
00:13:19,920 --> 00:13:25,400
client that comes with Apache Kafka if you download it. So it's a pretty good

224
00:13:25,400 --> 00:13:30,480
option for that. It's well maintained. They keep on top of it. And it's not

225
00:13:30,480 --> 00:13:33,880
fully open source, but it is. So I got to talk with someone before at a conference

226
00:13:33,880 --> 00:13:38,240
from, he's from Wikimedia. They can't use this package because it's not fully

227
00:13:38,240 --> 00:13:40,840
open source. It's one of those licenses where you can use it for anything you

228
00:13:40,840 --> 00:13:43,800
want as long as you're not competing with them in the cloud. But because of

229
00:13:43,800 --> 00:13:47,720
that little caveat, companies that are fully open source only can't use it. So

230
00:13:47,720 --> 00:13:50,720
for that, there's also Kafka Python, which is just a community one that is

231
00:13:50,720 --> 00:13:55,600
fully open source. And also, I haven't tried to work with it very much, but what

232
00:13:55,600 --> 00:13:59,720
I've seen and heard from other people is it is a bit more Python-ic, since it's

233
00:13:59,720 --> 00:14:05,760
written purely in Python and it follows more Python idioms and things. But they

234
00:14:05,800 --> 00:14:12,800
both have the classes that we're going to talk about here. I keep getting confused.

235
00:14:12,800 --> 00:14:15,000
I'm seeing something different on my screen when I look. It's down here. Okay,

236
00:14:15,000 --> 00:14:20,720
so. So here's an example. I'm going to step back here so I can see what we're

237
00:14:20,720 --> 00:14:25,760
talking about a little bit. This is a real simple example of a Python producer

238
00:14:25,760 --> 00:14:30,160
writing data to a Kafka topic. So you see, we just import the producer class. And

239
00:14:30,160 --> 00:14:34,680
then I'm setting up a couple of data bits there. This is a key and a value. They

240
00:14:34,680 --> 00:14:36,840
could be anything, but I just want to set something there for the example.

241
00:14:36,840 --> 00:14:39,840
Normally, this data would be something that's generated from a process that

242
00:14:39,840 --> 00:14:41,720
you're already running. It's not something you're going to create just for

243
00:14:41,720 --> 00:14:45,520
the purpose of writing to Kafka, but this is just for my example. So in the

244
00:14:45,520 --> 00:14:50,120
constructor, there's only one. You pass the constructor a dictionary. And you'll

245
00:14:50,120 --> 00:14:55,200
see a lot of examples. I may be off on this one, but a lot of people use like

246
00:14:55,200 --> 00:14:57,840
YAML files and things like that for the configuration, and then they have a bit

247
00:14:57,840 --> 00:15:00,760
that reads that in and converts it to a dictionary and then passes it to the

248
00:15:00,760 --> 00:15:03,840
constructor. And I don't know why they don't just use dictionaries. I love

249
00:15:03,840 --> 00:15:08,920
dictionaries. But anyway, so it just takes a dictionary, and you could put a

250
00:15:08,920 --> 00:15:11,560
ton of stuff in that dictionary. There's a lot of configurations that you can set

251
00:15:11,560 --> 00:15:15,400
on Kafka producers and consumers. There's only one for the producer that's

252
00:15:15,400 --> 00:15:18,760
required, and that's the bootstrap servers. The bootstrap servers tells your

253
00:15:18,760 --> 00:15:22,760
producer class, where can I find a Kafka broker? Now, Kafka normally runs in a

254
00:15:22,760 --> 00:15:26,280
cluster. So you normally have multiple brokers, usually a minimum of three, but

255
00:15:26,280 --> 00:15:31,000
some companies will have hundreds. And you only need one here, because if the

256
00:15:31,080 --> 00:15:35,200
producer finds one broker, that call to the broker will return the

257
00:15:35,200 --> 00:15:38,560
metadata for all the brokers in the cluster. So it'll get everything it needs

258
00:15:38,560 --> 00:15:41,600
from there. That's one thing that Kafka's really good at is communicating

259
00:15:41,600 --> 00:15:44,440
between different brokers, different consumers, all the different components

260
00:15:44,440 --> 00:15:49,240
that are running. They keep each other in sync of things pretty automatically.

261
00:15:51,240 --> 00:15:54,320
And then, so then once you construct the producer, then you just simply call

262
00:15:54,320 --> 00:15:59,000
produce. There's callbacks you can give to it if you wanna make sure that

263
00:15:59,040 --> 00:16:02,720
this thing was sent properly and things like that. But the basic usage is call

264
00:16:02,720 --> 00:16:07,080
produce, give it a topic. Really all you have to give is a topic and a value.

265
00:16:07,080 --> 00:16:11,440
The key is optional. I included here as well, but so you pass in a topic,

266
00:16:11,440 --> 00:16:14,800
it's always the first argument. And then actually the second argument is value

267
00:16:14,800 --> 00:16:17,360
and then key, but I don't like that order, so I put him in with named arguments

268
00:16:17,360 --> 00:16:19,200
just so I can put him in the order that I'm more comfortable with.

269
00:16:19,200 --> 00:16:23,520
But, and that's all there is. So that's gonna send that data to the Kafka

270
00:16:23,520 --> 00:16:26,960
topic called my topic. So now this is normally something that would be done

271
00:16:27,000 --> 00:16:29,320
over and over again through some of the process that you're running in.

272
00:16:29,320 --> 00:16:37,880
I just lost it again, huh? It's cause my, I think it's cause my laptop

273
00:16:37,880 --> 00:16:46,400
went to sleep. We're back down here. Oh, yay. I guess I'll just touch this

274
00:16:46,400 --> 00:16:49,440
once more so I don't lose. You know, it shows I'm talking too much on a

275
00:16:49,440 --> 00:16:52,080
single slide. We talked about it earlier, right? I shouldn't be staying on one

276
00:16:52,080 --> 00:16:58,960
slide for too long. So that's getting data into Kafka. And now consuming data

277
00:16:58,960 --> 00:17:03,240
from Kafka is similar. You import the consumer class and the consumer also

278
00:17:03,240 --> 00:17:06,640
takes a dictionary in its constructor. That dictionary is gonna have the bootstrap

279
00:17:06,640 --> 00:17:10,680
servers again. That part's the same, but there's one other required property

280
00:17:10,680 --> 00:17:16,640
in the consumer and that's group ID. So we're actually, you don't always have

281
00:17:16,640 --> 00:17:20,240
to use a group. Let me explain briefly what consumer groups are. They're really

282
00:17:20,280 --> 00:17:23,800
a powerful feature of Kafka. A consumer group allows you so you can have multiple

283
00:17:23,800 --> 00:17:28,080
instances of the same consumer application running and they will automatically

284
00:17:28,080 --> 00:17:32,960
share the workload of the data that they're reading. They split by partitions.

285
00:17:32,960 --> 00:17:36,440
So if you have a Kafka topic and it's got four partitions, you can have four

286
00:17:36,440 --> 00:17:40,520
instances of your consumer class running, of your consumer application running,

287
00:17:40,520 --> 00:17:45,720
and each of those will read from one partition. So the data stays processed

288
00:17:45,720 --> 00:17:49,080
in order by partition like the guarantees are, but the workload is split

289
00:17:49,120 --> 00:17:53,480
across four now evenly. If one of those consumers dies, then it'll automatically

290
00:17:53,480 --> 00:17:58,040
rebalance and give that orphan partition to another consumer that's running.

291
00:17:58,040 --> 00:18:00,920
Or if you have two to start with, it gives them each two and then you get busy,

292
00:18:00,920 --> 00:18:04,840
you start two more up, it automatically spreads those out again. So the balancing

293
00:18:04,840 --> 00:18:07,960
up and down, so scaling in and out, is all handled for you automatically with a

294
00:18:07,960 --> 00:18:11,400
consumer group. So it's really a powerful feature, but to use it, you need to have

295
00:18:11,400 --> 00:18:14,480
a group ID. Now if you're not using a group, you're gonna have just one

296
00:18:14,480 --> 00:18:17,160
instance of your application, then you technically don't need a group ID,

297
00:18:17,160 --> 00:18:22,040
but this particular client requires it anyway. So those are the two things

298
00:18:22,040 --> 00:18:25,040
we put in our dictionary that we pass to the consumer's constructor.

299
00:18:25,600 --> 00:18:28,320
And then the next step we have for the consumer, now we notice with the producer,

300
00:18:28,320 --> 00:18:31,360
we tell it in our call to produce, we say, what topic to send it to?

301
00:18:31,840 --> 00:18:34,920
With the consumer, you don't do that. With the consumer, you subscribe to one

302
00:18:34,920 --> 00:18:38,480
or more topics, and then when you call poll, which we're gonna do down below,

303
00:18:38,800 --> 00:18:42,840
it will poll from the topics that it's subscribed to. So here we're just

304
00:18:42,840 --> 00:18:46,320
subscribing to a single topic called My Topic, so I put it in a list

305
00:18:46,320 --> 00:18:49,040
of one. It does have to have a list in that subscribe call.

306
00:18:50,360 --> 00:18:53,840
And then we just jump into a loop, and we're gonna call the consumer.poll.

307
00:18:54,240 --> 00:18:57,120
Now poll will read from the topics. If there's more than one topic in

308
00:18:57,120 --> 00:18:59,240
that subscribe, it will read from each of them, one after the other,

309
00:18:59,240 --> 00:19:00,640
kind of in round robin fashion,

310
00:19:02,000 --> 00:19:06,400
and with each call to poll. And then the value I'm passing in,

311
00:19:06,400 --> 00:19:10,680
there's a timeout that basically says... No, I did it again, talking too much.

312
00:19:10,800 --> 00:19:16,120
But I'll keep talking while I do this. So what it'll do is it'll keep...

313
00:19:17,760 --> 00:19:21,920
It'll try grabbing data from the broker, and it'll keep trying until it

314
00:19:21,920 --> 00:19:25,040
either gets data or until that time has hit. So after one second,

315
00:19:25,040 --> 00:19:27,160
in this case, if there's no data, it'll just return none.

316
00:19:27,800 --> 00:19:30,640
But if there is data, it'll return instantly as soon as the data's there.

317
00:19:30,640 --> 00:19:33,080
So it doesn't slow you down to having a value here, this just tells you

318
00:19:33,080 --> 00:19:36,680
if you wanna leave the connection open to give it a little time to

319
00:19:36,680 --> 00:19:38,880
get data, you can do that with that value.

320
00:19:40,840 --> 00:19:47,080
And then... Yeah, so the event can be none, or it can have a value,

321
00:19:47,080 --> 00:19:49,360
or it could also have an error. So I didn't include it in the slide here,

322
00:19:49,360 --> 00:19:51,560
but you can also check the event.error, and if there's an error,

323
00:19:51,560 --> 00:19:56,320
you could do error handling. Then you could just grab the event value,

324
00:19:56,320 --> 00:20:01,000
which I'm doing there. And I mentioned that in Kafka, events are all bytes,

325
00:20:01,000 --> 00:20:03,520
so the value's a byte, the key's a byte, everything's just bytes.

326
00:20:03,520 --> 00:20:08,200
So I'm decoding it here. The library actually includes serializers that you

327
00:20:08,240 --> 00:20:13,040
can use to serialize and deserialize most of the basic types, and you can also

328
00:20:13,040 --> 00:20:17,520
use serializers that work with things like protobuf or Avro or JSON schema,

329
00:20:17,520 --> 00:20:19,960
things like that, to do your serialization. Or you can write your own

330
00:20:19,960 --> 00:20:23,280
serializers. There's lots of ways to get the data from bytes back to something

331
00:20:23,280 --> 00:20:26,920
useful. In this case, I'm just using decode, and then printing it out.

332
00:20:26,920 --> 00:20:31,440
So simple example that shows you how to use a Kafka consumer in Python.

333
00:20:34,240 --> 00:20:36,360
So we'll have some slides later where we're gonna show some examples

334
00:20:36,360 --> 00:20:40,320
of some streaming operations, and the consumer, all this stuff will be assumed

335
00:20:40,320 --> 00:20:44,000
that it's happening. In fact, that's where we're gonna go to next.

336
00:20:46,000 --> 00:20:49,120
So stream processing. So the goal here of this talk is to show

337
00:20:49,960 --> 00:20:53,280
that stream processing isn't that hard. There are libraries and frameworks and

338
00:20:53,280 --> 00:20:55,280
all those things like that to help you with it, but you don't always need

339
00:20:55,280 --> 00:20:59,000
those. And especially getting started, you don't need them. So I thought

340
00:20:59,000 --> 00:21:01,440
this was important because I think that stream processing can be very helpful

341
00:21:01,440 --> 00:21:04,920
to a lot of organizations, a lot of projects. But people think,

342
00:21:04,960 --> 00:21:07,360
I get scared off from it, thinking they need to have all this heavyweight

343
00:21:07,360 --> 00:21:09,520
stuff to do it. And I just wanna show that you really don't.

344
00:21:09,520 --> 00:21:11,720
You can do quite a bit with just plain Python code.

345
00:21:12,760 --> 00:21:17,360
So operations and streaming are broken into two categories, stateless operations

346
00:21:17,360 --> 00:21:21,320
and stateful operations. So I'm gonna show a few stateless operations first,

347
00:21:22,320 --> 00:21:25,600
some examples of those. So the first one we're looking at is filter,

348
00:21:25,800 --> 00:21:29,000
which is a pretty common one, actually. And that's where you got a stream

349
00:21:29,000 --> 00:21:32,160
of data coming in, and you want some of that data, but you don't want

350
00:21:32,200 --> 00:21:35,720
all of it. So you want some events and not all of them. And so based

351
00:21:35,720 --> 00:21:38,600
on some data, some value in the event, something maybe in the value

352
00:21:38,600 --> 00:21:40,800
or maybe in the key, usually it's in the value,

353
00:21:41,640 --> 00:21:45,080
but somewhere you make a decision whether you want that event or not.

354
00:21:46,800 --> 00:21:49,880
And so basically, you're pulling on it, and for each one that you

355
00:21:49,880 --> 00:21:52,160
get, you check it against whatever your predicate is.

356
00:21:53,120 --> 00:21:56,480
So I mocked out a check predicate method. That could be anything in there.

357
00:21:56,480 --> 00:22:00,040
That could open up the value, it could check a certain field if it's

358
00:22:00,040 --> 00:22:02,920
greater than this or less than that. It can make whatever distinctions

359
00:22:02,920 --> 00:22:06,000
it needs to, and it turns either true or false. And if it's true,

360
00:22:06,000 --> 00:22:08,320
then that event goes on to the next step. If it's false, that event

361
00:22:08,320 --> 00:22:11,560
gets skipped. So simple enough, and then we produce it to an output topic.

362
00:22:11,760 --> 00:22:15,320
And this is a common theme, a common pattern you'll see in stream processing

363
00:22:15,320 --> 00:22:17,920
is that you have an input topic and you have an output topic.

364
00:22:19,680 --> 00:22:21,200
And then you do something in between there.

365
00:22:22,480 --> 00:22:23,440
So that's a filter.

366
00:22:25,280 --> 00:22:27,360
The next one is branch, and this is a really helpful one.

367
00:22:27,360 --> 00:22:30,440
This allows you to take one stream of events and break it into multiple

368
00:22:30,440 --> 00:22:33,840
streams of events. And again, this is a feature that's built into most

369
00:22:33,840 --> 00:22:37,360
of the streaming libraries like Kafka Streams. I used to work for an

370
00:22:37,360 --> 00:22:41,240
insurance company that had, they used to call it the fire hose topic.

371
00:22:41,240 --> 00:22:44,600
It was a single Kafka topic with, I think, a thousand partitions,

372
00:22:44,600 --> 00:22:48,240
and all events coming into their organization came through that topic.

373
00:22:48,240 --> 00:22:51,960
And then we had a Kafka Streams application that used branch to split

374
00:22:51,960 --> 00:22:54,640
those out into a whole bunch of different topics based on the data in them.

375
00:22:54,680 --> 00:22:58,080
So basically, it gave them a way to log and monitor all the data

376
00:22:58,080 --> 00:23:01,000
coming in before they send it off to the downstream applications,

377
00:23:01,000 --> 00:23:02,840
which are based on departments and things like that.

378
00:23:04,480 --> 00:23:06,920
But we can do the same thing, which is Python, we're just using either

379
00:23:06,920 --> 00:23:09,680
ifs, or you could do a switch or something like that.

380
00:23:09,680 --> 00:23:13,520
But if we check the different predicates, I've got two of them here.

381
00:23:13,920 --> 00:23:17,200
If it meets the first one, it goes into topic or output topic A.

382
00:23:17,520 --> 00:23:19,200
If it meets the second one, it goes into B.

383
00:23:20,320 --> 00:23:24,040
And these are ordered, so the first one it hits, that one's gone.

384
00:23:24,040 --> 00:23:24,880
It won't check the others.

385
00:23:26,320 --> 00:23:28,680
And then we have a default one, which you can have or skip.

386
00:23:28,680 --> 00:23:31,400
You can just ignore the ones that don't match your predicates at all.

387
00:23:32,200 --> 00:23:34,200
Or you could put all of them into a default topic.

388
00:23:34,520 --> 00:23:37,840
But this just allows us to take that one incoming stream and split it into multiples.

389
00:23:39,080 --> 00:23:41,080
And then again, put that to another output topic.

390
00:23:41,720 --> 00:23:44,760
Now, I'm showing you this going from input topic to an output topic,

391
00:23:44,760 --> 00:23:47,880
and that's pretty common to do another topic for each step.

392
00:23:48,200 --> 00:23:50,720
But it doesn't necessarily mean it's the only thing in that application.

393
00:23:51,200 --> 00:23:53,880
So you might have multiple steps like this in one single application.

394
00:23:54,200 --> 00:23:59,360
Where the output topic of one process becomes the input topic for the next process.

395
00:23:59,360 --> 00:24:05,080
And you connect these together with nodes and basically in a directed acyclic graph.

396
00:24:06,280 --> 00:24:08,680
What it turns into. So that's a branch.

397
00:24:10,680 --> 00:24:14,080
And then map.

398
00:24:14,080 --> 00:24:18,680
So the map is basically you're just going to do something over every event in the stream as it comes through, right?

399
00:24:18,680 --> 00:24:20,200
And so it can be whatever you want.

400
00:24:20,200 --> 00:24:23,600
So. And this I just have my func.

401
00:24:23,960 --> 00:24:29,920
And rather than even doing the whole poll thing, I just have rather than reading the event in the polling loop,

402
00:24:29,920 --> 00:24:36,600
I just go ahead and call my func in the poll in the polling loop and just call the pass the poll method result into that.

403
00:24:36,600 --> 00:24:40,960
So that my func can do can be some kind of any kind of transformation, really.

404
00:24:40,960 --> 00:24:46,120
So you might be taking something like JSON data and flattening out to CSV or something like that.

405
00:24:46,440 --> 00:24:51,400
So changing the shape is a common use in enhancing the data.

406
00:24:51,400 --> 00:24:57,400
So maybe if you got like a lookup table that's loaded into memory that you can quickly call and enhance the data with each event as it's coming through.

407
00:24:57,400 --> 00:24:59,440
That's another thing is commonly done with map.

408
00:24:59,440 --> 00:25:01,840
So this is another really popular feature.

409
00:25:01,840 --> 00:25:06,000
And it's again, streaming libraries all have things like that for you, but we can pretty easily do it just with Python.

410
00:25:11,520 --> 00:25:14,440
OK, now let's switch over to stateful operations.

411
00:25:14,440 --> 00:25:22,160
And these are a little bit more complex because these other ones, the each event that came in as we're processing each event that came in,

412
00:25:22,160 --> 00:25:25,400
we didn't care at all what happened to the event before it, right?

413
00:25:25,400 --> 00:25:28,240
It didn't matter. Each one is handled independently completely.

414
00:25:28,240 --> 00:25:36,160
Now we do care about what happened before, because now we're dealing with things that involve the past state, for example, counting.

415
00:25:36,160 --> 00:25:42,440
Oh, by the way, speaking of counting, I'm actually using this one in my demo app if I have time to show, which I might run out, I'm not sure.

416
00:25:42,440 --> 00:25:47,720
But the demo app has to do with Twitter, so you should right now be tweeting about PyTexas.

417
00:25:47,720 --> 00:25:53,560
If you tweet about PyTexas, just show up on the leaderboard in a minute, hopefully, if I get that to work with the screen and all that stuff.

418
00:25:53,560 --> 00:25:59,560
And somebody has to start going to beat Andy Knight, because right now he's the top.

419
00:25:59,560 --> 00:26:02,360
I know, you know, I'm going to do another version that pulls from Rastadon as well.

420
00:26:02,360 --> 00:26:04,640
I just didn't have time to get that together for this.

421
00:26:04,640 --> 00:26:12,400
Sorry. All right, so let's take a look at count real quick and I'll try to move through these ones more quickly.

422
00:26:12,400 --> 00:26:18,080
So the key difference here is that we need to have someplace to store state in between in between events, right?

423
00:26:18,080 --> 00:26:22,880
For account, we're going to be accumulating an account for each key, and so we need to have some place to store that.

424
00:26:22,880 --> 00:26:25,640
So dictionary to the rescue, right? It works great.

425
00:26:25,640 --> 00:26:31,000
So for each one that comes in, we grab the check, the key to see if it's in the counts dictionary.

426
00:26:31,000 --> 00:26:36,520
If it is, then we go ahead and grab that item in the dictionary and increment its value.

427
00:26:36,520 --> 00:26:40,960
If it's not, then we add it to the dictionary and give it a value of one and just keep on going through.

428
00:26:40,960 --> 00:26:43,160
And as we do each step, we also produce it to the output.

429
00:26:43,160 --> 00:26:50,880
So we produce that the output at every step so that the output topic will always have the latest the latest value for each key.

430
00:26:50,880 --> 00:26:58,360
And if you're if this is a little bit off topic, but Kafka has a really cool feature called compacted topics, which are topics that don't have every event ever.

431
00:26:58,360 --> 00:27:00,520
They just have the latest value for each key.

432
00:27:00,520 --> 00:27:03,280
They have some others, too, because there's a cleanup process takes time to get rid of them.

433
00:27:03,280 --> 00:27:09,320
But they basically they guarantee you to have the latest value for the current value for each key.

434
00:27:09,320 --> 00:27:13,120
And that's that's perfect for storing things like count results.

435
00:27:13,120 --> 00:27:21,720
So we produce it after each call and it just keeps accumulating count like like a count of Twitter of tweets by a specific user.

436
00:27:21,720 --> 00:27:25,240
So that's count. Sum is another common one.

437
00:27:25,240 --> 00:27:27,800
And again, we use a dictionary to hold the sums.

438
00:27:27,800 --> 00:27:29,160
So it's really some of the counting.

439
00:27:29,160 --> 00:27:33,200
So instead of increment by one, we're going to just add the value for each new event.

440
00:27:33,200 --> 00:27:36,160
We're going to add the value to the value before with the same key.

441
00:27:36,160 --> 00:27:37,920
So it's all tied to the key of the event.

442
00:27:37,920 --> 00:27:40,280
You could actually use a certain field in the value.

443
00:27:40,280 --> 00:27:42,120
It'd be a little more complicated, but you could do that as well.

444
00:27:42,120 --> 00:27:46,280
But the key is the most common way to aggregate these things.

445
00:27:46,280 --> 00:27:50,520
So that is basically the same as the count.

446
00:27:50,520 --> 00:27:52,840
And then aggregate, which, again, is similar.

447
00:27:52,840 --> 00:27:58,360
It's just it's going to we're going to do some type of a generic aggregation, something that's going to work on all of them together.

448
00:27:58,360 --> 00:28:05,120
So one thing is an aggregation might be used for that's different than counting or summing would be like compiling them into an array.

449
00:28:05,160 --> 00:28:11,200
So you want to take a bunch of individual events coming in and you want to create a bunch of arrays of those events or lists of these events.

450
00:28:11,200 --> 00:28:14,120
You could do that with it with aggregate.

451
00:28:14,120 --> 00:28:15,600
So we just do the events come in.

452
00:28:15,600 --> 00:28:18,680
We have an aggregate dictionary called Ags here.

453
00:28:18,680 --> 00:28:21,320
We just check to see if there's something we need to check to see.

454
00:28:21,320 --> 00:28:27,600
We just keep on passing the the value that's in the dictionary along with the event value.

455
00:28:27,600 --> 00:28:30,440
And if it's not there yet, then it'll just have a none and work on that.

456
00:28:30,440 --> 00:28:34,440
So all these ones here, I'm not doing things like error hands like that.

457
00:28:34,440 --> 00:28:38,440
This can be done inside the functions that get the data or they can be done in this loop as well.

458
00:28:38,440 --> 00:28:43,400
So it's easier to show here by assuming that that stuff's not being handled in the in the functions.

459
00:28:46,360 --> 00:28:55,680
So that's that one. And then join is trickier, but it's still doable.

460
00:28:55,680 --> 00:29:03,040
What we have here now is we have two different two different input topics.

461
00:29:03,040 --> 00:29:05,080
And we're going to have two different consumers.

462
00:29:05,080 --> 00:29:07,720
We're going to pull each consumer.

463
00:29:07,720 --> 00:29:11,160
You can actually do this with one consumer and subscribe to both topics, but it's a little more confusing.

464
00:29:11,160 --> 00:29:14,080
So I thought it's clear to show with just two two consumers.

465
00:29:14,080 --> 00:29:16,000
So for each consumer, we're going to sort of start with the first one.

466
00:29:16,000 --> 00:29:18,640
We pull that that topic.

467
00:29:18,640 --> 00:29:24,800
If we get we get an event and we check to see if the key to that event is in.

468
00:29:24,800 --> 00:29:26,240
If it's in the B dictionary.

469
00:29:26,240 --> 00:29:30,080
So if the key to the event is in the B dictionary, then we can make our join there.

470
00:29:30,080 --> 00:29:33,120
If it's not, we still we've added our we've added a event already.

471
00:29:33,120 --> 00:29:34,720
So if it's not there, then we just go on and skip.

472
00:29:34,720 --> 00:29:38,560
We actually check to see if it's in the B dictionary and if it's not in the C dictionary.

473
00:29:38,560 --> 00:29:41,280
The C dictionary is going to be our out the source of our output.

474
00:29:41,280 --> 00:29:44,960
We're basically taking two events that have a similar key or the same key.

475
00:29:44,960 --> 00:29:49,600
We're going to join those together. Maybe we'll create a new a new type of object that has only some values from each.

476
00:29:49,600 --> 00:29:55,920
You know, kind of because it was a SQL join and that new objects will go into the C topic.

477
00:29:55,920 --> 00:29:59,280
And so if it's not in a the joint doesn't happen there, but the A value is stored.

478
00:29:59,280 --> 00:30:01,200
So it's sitting there and then the B one comes.

479
00:30:01,200 --> 00:30:05,600
And if it's the B one looks in the A dictionary to see if it's there, if it's there, it makes the join.

480
00:30:05,600 --> 00:30:07,200
And also if it's not in the C dictionary.

481
00:30:07,200 --> 00:30:10,480
So as soon as it lands in the C dictionary, then that's not happening again.

482
00:30:10,480 --> 00:30:12,400
So you won't join it multiple times.

483
00:30:12,400 --> 00:30:15,240
Now, the one challenge with this is this works fine.

484
00:30:15,240 --> 00:30:18,960
I've tested it out, but your dictionary will grow forever.

485
00:30:18,960 --> 00:30:25,520
So you want to probably add something in there like a scheduler or something that can remove some excess things.

486
00:30:25,520 --> 00:30:26,920
But you can't even do joins.

487
00:30:26,920 --> 00:30:31,480
These are all things that are that are that are challenging to do without a library to help you.

488
00:30:31,480 --> 00:30:34,400
But they're they're possible. So I just want to show these things are possible.

489
00:30:34,400 --> 00:30:38,680
I'll give you some ideas and I will try to start wrapping it up now.

490
00:30:38,680 --> 00:30:40,560
Oh, yeah, that's the thing for the next sign, right?

491
00:30:40,560 --> 00:30:44,360
Two minutes already. It's time now.

492
00:30:44,360 --> 00:30:51,360
No, how did that happen? OK, let me real quickly.

493
00:30:51,360 --> 00:30:55,760
You know, I've got some code on a GitHub repo, which I can send anybody they want to see the codes.

494
00:30:55,760 --> 00:30:58,760
I only have time to go through it. I will only show just one bit of it.

495
00:30:58,760 --> 00:31:01,960
I can't do that because the slides, you know, skip that.

496
00:31:01,960 --> 00:31:04,480
I can't show the code because it doesn't show up on the screen.

497
00:31:04,480 --> 00:31:14,080
I'm hoping I can drag my browser over there, at least show you the latest stats here.

498
00:31:14,080 --> 00:31:21,040
Give me one second. Oh, no.

499
00:31:21,040 --> 00:31:24,120
Technology hates me today.

500
00:31:24,120 --> 00:31:36,000
The Internet's down, too, so. Let me just real quickly get on my hot spot.

501
00:31:36,000 --> 00:31:40,120
Maybe you can come here to one of those jokes for a second here.

502
00:31:40,120 --> 00:31:49,960
Oh, anybody got a joke? And don't say my talk, please.

503
00:31:50,040 --> 00:32:01,040
You were thinking that?

504
00:32:01,040 --> 00:32:06,760
Oh, that is good. I like that one. I'm going to borrow that one for later.

505
00:32:06,760 --> 00:32:12,160
Come on now. I know I have a hot spot. It was working earlier here.

506
00:32:12,160 --> 00:32:19,560
All right, here we go.

507
00:32:19,560 --> 00:32:34,240
Here we don't go.

508
00:32:34,240 --> 00:32:37,800
All right, let's see if we can pull this over.

509
00:32:37,880 --> 00:32:53,080
How do we pull it to the other screen?

510
00:32:53,080 --> 00:33:01,560
All right. So what do we got here? Andy and I still tough to be with 13.

511
00:33:01,560 --> 00:33:06,440
But there we go. So this was doing is it's been pulling from Twitter every few minutes,

512
00:33:06,440 --> 00:33:14,080
checking the tweets for the pie Texas string and then and then just using the account like we showed earlier.

513
00:33:14,080 --> 00:33:19,800
We're using the user ID of the tweeter as the key and we're just incrementing the count every time we see a tweet from that person.

514
00:33:19,800 --> 00:33:25,440
We also have a deep step because the thing I'm using to pull the tweets in just does everyone pulls in duplicates.

515
00:33:25,440 --> 00:33:27,720
So it's easy to add a deep step in there as well.

516
00:33:27,720 --> 00:33:29,640
So, again, you can add multiple steps to your processes.

517
00:33:29,640 --> 00:33:30,960
And that's what I was going to show you in the code of high time.

518
00:33:30,960 --> 00:33:36,680
But like I said, the codes on GitHub repo, which I can send anybody the link to if they want to look at it later.

519
00:33:36,680 --> 00:33:39,440
But this was all done with just Python with no libraries, Python and Kafka.

520
00:33:39,440 --> 00:33:42,000
They work great together and you can do a lot with them.

521
00:33:42,000 --> 00:33:45,800
There are libraries, though. So I just I know just one more second here.

522
00:33:45,800 --> 00:33:52,400
How do I get back to what I had? That's like that.

523
00:33:52,400 --> 00:33:54,280
OK, it's a challenge. A state is a challenge.

524
00:33:54,280 --> 00:33:56,800
It really is because we did dictionary. Those are just in memory.

525
00:33:56,800 --> 00:34:00,320
So there's a lot of these libraries have persistent state, which is important.

526
00:34:00,320 --> 00:34:04,320
And then scale. I was only working on a single topic or single partition topic for all my things.

527
00:34:04,320 --> 00:34:07,240
If you have multiple partitions, those dictionaries are harder to keep in sync.

528
00:34:07,240 --> 00:34:09,680
So there are things that these libraries will help you with.

529
00:34:09,680 --> 00:34:12,280
I said you don't always need them, but there are times when they do come in handy.

530
00:34:12,280 --> 00:34:16,520
So here's a few we can look at.

531
00:34:16,520 --> 00:34:19,280
There is Faust, which is originally started by Robin Hood.

532
00:34:19,280 --> 00:34:22,640
Then they kind of dropped it and the community picked it up and it's still going somewhat.

533
00:34:22,640 --> 00:34:25,680
ByteWax and Fluvio are two newer ones that are out there.

534
00:34:25,680 --> 00:34:29,960
I think they're both rest based, but those are also streaming libraries for Python.

535
00:34:29,960 --> 00:34:33,000
And you can find those things out on GitHub.

536
00:34:33,000 --> 00:34:36,760
And then quick plug for the one that my company puts out, the company I work for now, Quix.

537
00:34:36,760 --> 00:34:39,960
They have quick streams that just they just open sourced it about a month ago.

538
00:34:39,960 --> 00:34:43,720
So it's still pretty new, but it also handles persistent state and different things like that.

539
00:34:43,720 --> 00:34:47,120
Also works with time, really works well with time, time series data.

540
00:34:47,120 --> 00:34:49,320
And you can find information there about that.

541
00:34:49,320 --> 00:34:54,280
And that would be great if you would check that out, because then they might keep sending me things like this.

542
00:34:54,400 --> 00:34:56,760
But anyways, thank you for letting me go a little bit over time.

543
00:34:56,760 --> 00:34:57,760
And that was my talk.

544
00:34:57,760 --> 00:34:58,260
Thanks.

