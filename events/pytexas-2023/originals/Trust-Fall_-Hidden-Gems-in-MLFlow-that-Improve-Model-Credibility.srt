1
00:00:00,000 --> 00:00:13,640
Hopefully, hopefully we have the tech issues resolved for the rest of this presentation.

2
00:00:13,640 --> 00:00:14,920
Hello and welcome.

3
00:00:14,920 --> 00:00:19,520
I'm here to talk about building trust.

4
00:00:19,520 --> 00:00:28,120
So before we get into that, I just want to see, does anyone recognize this face?

5
00:00:28,840 --> 00:00:35,120
Okay, so it sounds like most of us do, but for a quick rundown for those who don't, this

6
00:00:35,120 --> 00:00:43,320
is former CEO Elizabeth Holmes, to the now defunct biotech startup Theranos, and they

7
00:00:43,320 --> 00:00:49,760
had a ton of positive media attention and garnered a lot of investor money.

8
00:00:49,760 --> 00:00:55,040
At one point, they were actually valued at $9 billion, which surprised me, because now

9
00:00:55,080 --> 00:01:02,600
they are well worth nothing, and she's, I think, going to jail.

10
00:01:02,600 --> 00:01:04,480
But why am I bringing her up?

11
00:01:04,480 --> 00:01:11,520
So this story is a great reminder as to how important it is to build trust early on in

12
00:01:11,520 --> 00:01:17,880
the metrics behind your projects, particularly science-driven or experimental projects, such

13
00:01:17,880 --> 00:01:20,720
as machine learning projects.

14
00:01:20,760 --> 00:01:26,680
And I feel like this reminder is what I wanted to start off today's talk with, because we're

15
00:01:26,680 --> 00:01:31,640
going to be talking about MLflow, but in reality, what we're really trying to get at is how

16
00:01:31,640 --> 00:01:36,120
to build trust through metrics.

17
00:01:36,120 --> 00:01:40,600
So I'm going to argue that building trust through metrics and in your process is just

18
00:01:40,600 --> 00:01:44,200
as important as building your actual product.

19
00:01:44,200 --> 00:01:49,200
So today's talk is really about how we can build that trust and what tools we can use

20
00:01:49,200 --> 00:01:50,200
to do that.

21
00:01:50,360 --> 00:01:56,960
And we're going to be focusing specifically on machine learning projects.

22
00:01:56,960 --> 00:01:59,240
So how do we build that trust?

23
00:01:59,240 --> 00:02:04,440
Well, the first thing that you would need is proper documentation, a paper trail that

24
00:02:04,440 --> 00:02:09,920
shows people how you started your project, what you ended up doing, what the scientific

25
00:02:09,920 --> 00:02:13,440
process was that you took, and where you ended up.

26
00:02:13,440 --> 00:02:18,960
How did you get those metrics that you're touting?

27
00:02:19,000 --> 00:02:21,840
The second one is reproducibility.

28
00:02:21,840 --> 00:02:26,800
So we know that maybe you could get really great results from an experiment, but could

29
00:02:26,800 --> 00:02:29,920
you get them again and again?

30
00:02:29,920 --> 00:02:34,240
Validation is extremely important, but honestly often overlooked and can be very difficult

31
00:02:34,240 --> 00:02:39,800
if you don't have the proper tools in place and organization for your project from day

32
00:02:39,800 --> 00:02:41,920
one.

33
00:02:41,920 --> 00:02:43,840
And the last one is transparency.

34
00:02:43,840 --> 00:02:47,360
So what was the data that you used and what was the journey that you took to get to your

35
00:02:47,360 --> 00:02:49,080
final model?

36
00:02:49,080 --> 00:02:51,960
And how are you going to further iterate and improve on it?

37
00:02:51,960 --> 00:02:54,960
We all know that that first version isn't going to be perfect.

38
00:02:54,960 --> 00:02:59,960
So what are you going to do next?

39
00:02:59,960 --> 00:03:06,400
So now that we know how to build trust, I wanted to focus on particularly ML products

40
00:03:06,400 --> 00:03:10,560
and how that makes things slightly more complicated.

41
00:03:10,560 --> 00:03:13,700
Within machine learning, you have three parts.

42
00:03:13,700 --> 00:03:18,740
You have your actual code that's running your training pipeline and evaluating.

43
00:03:18,740 --> 00:03:23,580
You have your input data, which is being fed into the model to train it.

44
00:03:23,580 --> 00:03:27,500
And then you also have your metrics, which is relaying the performance of that model

45
00:03:27,500 --> 00:03:29,340
on new data.

46
00:03:29,340 --> 00:03:33,740
So it becomes kind of tricky to figure out how to organize all these things to get the

47
00:03:33,740 --> 00:03:37,180
proper documentation in the end.

48
00:03:37,180 --> 00:03:43,540
We're going to be focusing on a tool called MLflow that helps with automating metric tracking.

49
00:03:43,540 --> 00:03:50,820
MLflow is an open source package with a Python API that allows you to kind of quickly gather

50
00:03:50,820 --> 00:03:57,180
metrics and put together documentation based on your experiment from day one all the way

51
00:03:57,180 --> 00:03:59,140
until production.

52
00:03:59,140 --> 00:04:05,100
And it automatically tracks relevant metadata that we'll learn about later on and has a

53
00:04:05,100 --> 00:04:10,220
built-in database that you can use to store all of this data.

54
00:04:10,260 --> 00:04:16,340
And my favorite part is it comes with a UI, which you can use to share the experiment

55
00:04:16,340 --> 00:04:17,820
results with your team.

56
00:04:17,820 --> 00:04:23,940
Today, we're going to be talking about not just the basics of MLflow or how to import

57
00:04:23,940 --> 00:04:30,060
it and install it, but actually the hidden gems that my team discovered while working

58
00:04:30,060 --> 00:04:34,020
on a project that we're going to call Project Flum.

59
00:04:34,020 --> 00:04:40,780
My team is a team of machine learning engineers here at Austin, and we work on a lot of different

60
00:04:40,780 --> 00:04:43,140
complex AI products.

61
00:04:43,140 --> 00:04:47,740
And this one was particularly interesting.

62
00:04:47,740 --> 00:04:54,380
It had us go through a bunch of images, and we had to somehow detect from radar data what

63
00:04:54,380 --> 00:04:56,220
was a formed object or not.

64
00:04:56,220 --> 00:05:00,820
And the data itself wasn't necessarily easy.

65
00:05:00,820 --> 00:05:08,460
So we used MLflow because we'd been using it on a few other projects, and it was going

66
00:05:08,460 --> 00:05:09,460
pretty well.

67
00:05:09,460 --> 00:05:14,620
We were going through the exploratory phase and just kind of making sure that we were

68
00:05:14,620 --> 00:05:22,780
logging all of our metrics using MLflow's custom logging, which we had used on prior

69
00:05:22,780 --> 00:05:23,780
projects.

70
00:05:23,780 --> 00:05:29,060
For those of you who haven't used MLflow, basically you can just write in your Python

71
00:05:29,060 --> 00:05:34,500
code a one-liner for each parameter that you want to log, or you can kind of codify

72
00:05:34,500 --> 00:05:36,020
it a little bit.

73
00:05:36,020 --> 00:05:40,900
But if you want to log something, you have to tell MLflow how to log it and what to call

74
00:05:40,900 --> 00:05:45,580
the actual parameter or metric in your system.

75
00:05:45,580 --> 00:05:49,420
We had a ton of developers on this project, and we noticed that using MLflow's custom

76
00:05:49,420 --> 00:05:56,260
logging was actually difficult as time went on because everyone was calling things differently,

77
00:05:56,540 --> 00:05:59,440
and there was really inconsistent logging.

78
00:05:59,440 --> 00:06:07,980
So the logging became kind of chaotic, and we really wanted to find a way to better leverage

79
00:06:07,980 --> 00:06:12,220
MLflow so that way we could have consistency throughout our logging and we didn't have

80
00:06:12,220 --> 00:06:19,240
to use as many lines to actually do the logging that we kind of wanted MLflow to do.

81
00:06:19,240 --> 00:06:23,460
And so that's when we're going to talk about hidden gem number one.

82
00:06:23,460 --> 00:06:28,420
This is when we discovered that MLflow actually has an auto-logging feature, which you can

83
00:06:28,420 --> 00:06:33,460
use in your Python code, and all you have to do is add this one script when you start

84
00:06:33,460 --> 00:06:38,700
your MLflow experiment, and it will start logging all of your parameters and your metrics

85
00:06:38,700 --> 00:06:41,060
for that experiment.

86
00:06:41,060 --> 00:06:43,340
So this definitely made things a lot easier.

87
00:06:43,340 --> 00:06:48,220
We had a lot less lines of code related to logging, which involved a lot less cleanup

88
00:06:48,220 --> 00:06:54,380
in the end, and we had more consistent and thorough logging when it came to our experiment

89
00:06:54,380 --> 00:06:55,860
runs.

90
00:06:55,860 --> 00:07:00,500
This way it didn't matter what branch somebody was on or what they had called a particular

91
00:07:00,500 --> 00:07:01,980
parameter.

92
00:07:01,980 --> 00:07:10,880
Everyone knew what was being referred to, and we had more automation around the logging.

93
00:07:10,880 --> 00:07:17,560
So everything was going pretty well, and we had gone through the experimentation that

94
00:07:17,600 --> 00:07:23,540
we wanted to to explore the data, and we were now building our training pipeline.

95
00:07:23,540 --> 00:07:31,500
So we were doing some fine-tuning of the hyperparameters, and we wanted to make sure that we had a

96
00:07:31,500 --> 00:07:36,720
really good model, even though the problem itself was pretty complex.

97
00:07:36,720 --> 00:07:42,680
We were doing a lot of exploring, which is pretty difficult, but eventually one of our

98
00:07:42,680 --> 00:07:49,680
developers came across a really good model performance, and we were super, super stoked.

99
00:07:49,680 --> 00:07:57,240
It took a long time to get there, and we were excited to try to validate these results,

100
00:07:57,240 --> 00:08:03,280
which we were hoping would be a lot easier than it turned out to be.

101
00:08:03,280 --> 00:08:12,520
While we did have the MLflow data and metrics, it kind of felt like searching for a needle

102
00:08:12,520 --> 00:08:17,600
in a haystack when it came to the training pipeline code that was used to actually generate

103
00:08:17,600 --> 00:08:18,600
those metrics.

104
00:08:18,600 --> 00:08:25,080
What we really wanted was a way for MLflow, which tracked our metrics, to also align our

105
00:08:25,080 --> 00:08:30,000
code that was used to generate those metrics.

106
00:08:30,000 --> 00:08:35,320
So that way, whenever somebody wanted to validate a particular experiment's results, they knew

107
00:08:35,320 --> 00:08:40,720
exactly where to look for the exact version of code, so that way they could try to reproduce

108
00:08:40,720 --> 00:08:45,720
those results and those metrics.

109
00:08:45,720 --> 00:08:52,000
And that's when we searched through MLflow's documentation and searched deep into the corners

110
00:08:52,000 --> 00:08:58,640
to find hidden gem number two, which is MLflow's system tags.

111
00:08:58,640 --> 00:09:07,160
So MLflow, as we talked about a little earlier, saves metadata in the background that it kind

112
00:09:07,160 --> 00:09:15,960
of puts aside and allows you to use, but it doesn't necessarily log it automatically.

113
00:09:15,960 --> 00:09:21,740
And one piece of metadata that we found particularly relevant for our use case is the git commit

114
00:09:21,740 --> 00:09:23,280
hash.

115
00:09:23,280 --> 00:09:31,280
So using this key, you can tell MLflow in your Python code to set this tag and use this

116
00:09:32,280 --> 00:09:38,420
the value of the commit hash that is related to the experiment run that you just did to

117
00:09:38,420 --> 00:09:40,280
generate the metrics.

118
00:09:40,280 --> 00:09:45,680
So that way, when it comes time to validation, all you need to do is hand your teammates

119
00:09:45,680 --> 00:09:52,160
the run ID or the run name, and they can go and pull down that commit and rerun your code.

120
00:09:52,160 --> 00:09:55,120
So this is kind of how it looks in the UI.

121
00:09:55,120 --> 00:10:01,140
You can see that now there's a new column that's called tags, and it has this area

122
00:10:01,140 --> 00:10:03,460
that's called git commit hash.

123
00:10:03,460 --> 00:10:07,260
And a score, I know.

124
00:10:07,260 --> 00:10:11,820
This felt like a pretty big win because we wasted a lot less time searching through our

125
00:10:11,820 --> 00:10:13,780
code.

126
00:10:13,780 --> 00:10:18,980
It ended up also helping us spend more time doing validation because we didn't have to

127
00:10:18,980 --> 00:10:23,340
spend days and days trying to piece together the training pipeline.

128
00:10:23,340 --> 00:10:30,720
And we had better alignment with our code and our metrics.

129
00:10:30,720 --> 00:10:34,320
So this was all super exciting.

130
00:10:34,320 --> 00:10:41,080
And we were really pumped to finally have a model that was really strong in performance

131
00:10:41,080 --> 00:10:46,240
and that kind of beat out all of the other models that we had planned for.

132
00:10:46,240 --> 00:10:50,840
And we were ready for deployment, which was really exciting.

133
00:10:50,840 --> 00:10:59,160
All we had to do was just clean up some things for our documentation, which doesn't sound

134
00:10:59,360 --> 00:11:07,160
too bad until you realize that you have thousands and thousands of MLflow runs and a lot of

135
00:11:07,160 --> 00:11:13,160
data to sort through to figure out exactly how to piece together the final big picture

136
00:11:13,160 --> 00:11:17,000
that you want to display to the client.

137
00:11:17,000 --> 00:11:22,200
It ended up being pretty hard to package up all this data, and we found ourselves really

138
00:11:22,200 --> 00:11:28,080
wishing that we had a better way to pick out important milestones within our machine learning

139
00:11:28,080 --> 00:11:30,080
project.

140
00:11:30,080 --> 00:11:40,160
So we kind of searched around, and that's when we came across hidden gem number three.

141
00:11:40,160 --> 00:11:46,520
So hidden gem number three is MLflow's model registry.

142
00:11:46,520 --> 00:11:51,960
And basically, MLflow's model registry is a way for you to kind of keep track of all

143
00:11:51,960 --> 00:11:57,580
of your different models that you generate over an experiment and figure out exactly

144
00:11:58,140 --> 00:12:05,180
which model runs were important enough to version into an actual model name and continue

145
00:12:05,180 --> 00:12:07,300
keeping the history of.

146
00:12:07,300 --> 00:12:12,780
I think of it kind of like a tag for a git commit hash, where when you have an important

147
00:12:12,780 --> 00:12:17,900
commit, you can tag it and say, like, this commit is really important.

148
00:12:17,900 --> 00:12:21,580
If you're looking at my repo and you want to see kind of the most important changes

149
00:12:21,580 --> 00:12:24,060
we've made, look at this tag.

150
00:12:24,060 --> 00:12:28,380
And later on, if you want, you can even use the tag to version the code.

151
00:12:28,380 --> 00:12:34,780
So in that way, we started using the model registry to flag the most important MLflow

152
00:12:34,780 --> 00:12:40,100
runs and figure out which ones were relevant to the end product that we were trying to

153
00:12:40,100 --> 00:12:43,020
display to the customer.

154
00:12:43,020 --> 00:12:49,660
And so we ended up having around four versions for Project Llama that we documented this

155
00:12:49,740 --> 00:12:53,860
way using the MLflow's model registry.

156
00:12:53,860 --> 00:13:00,860
And basically, you can see, you can also transition the lifecycle stage using their registry.

157
00:13:00,860 --> 00:13:07,540
Basically, all we needed to do was provide that one command in our Python script and

158
00:13:07,540 --> 00:13:12,740
provide it with the actual run ID that is associated with the model that we wanted to

159
00:13:12,740 --> 00:13:15,740
create and give that model a name.

160
00:13:15,740 --> 00:13:20,140
Every single time we had an important enough change where we wanted to version up that

161
00:13:20,140 --> 00:13:25,500
model, we would just need to give a new run ID with the same model name, and MLflow automatically

162
00:13:25,500 --> 00:13:29,300
bumped up the version, as you can see here.

163
00:13:29,300 --> 00:13:33,700
If you want, you can also transition your model between different stages using that

164
00:13:33,700 --> 00:13:35,940
same command.

165
00:13:35,940 --> 00:13:43,100
So we found that pretty helpful, and we felt safer about our documentation and better about

166
00:13:43,100 --> 00:13:46,980
reporting different milestones that we had achieved.

167
00:13:46,980 --> 00:13:53,900
It definitely also helped searchability when it came to communicating within our team the

168
00:13:53,900 --> 00:14:04,740
most important MLflow runs for later modifications and improvements.

169
00:14:04,740 --> 00:14:09,780
So with these hidden gems, we were able to save a ton of precious development time because

170
00:14:09,900 --> 00:14:16,380
we weren't spending time searching through our code, looking through our commit hashes,

171
00:14:16,380 --> 00:14:19,340
or trying to piece together a training pipeline.

172
00:14:19,340 --> 00:14:24,420
And we weren't trying to reconcile all the different logging.

173
00:14:24,420 --> 00:14:31,180
We also built confidence in our metrics because we had a much more robust validation loop.

174
00:14:31,180 --> 00:14:35,820
We knew exactly the code that was used to generate those metrics, and it was no longer

175
00:14:35,820 --> 00:14:43,140
a mystery how we could reproduce those metrics because of the git commit hash logging.

176
00:14:43,140 --> 00:14:47,780
And we overall instilled trust in our process.

177
00:14:47,780 --> 00:14:52,900
So we were able to kind of demonstrate to the client all of the different tactics that

178
00:14:52,900 --> 00:14:58,380
we used to get to the final result, and it didn't feel like a black box to them.

179
00:14:58,380 --> 00:15:04,300
And that way, we earned ourselves a lifelong client, and despite the painful journey, lifelong

180
00:15:04,380 --> 00:15:08,260
lessons with MLflow.

181
00:15:08,260 --> 00:15:14,780
So Project Llama was very clearly a rollercoaster where we learned a lot, but we also learned

182
00:15:14,780 --> 00:15:19,700
a few more things that I wanted to share with you all that are more helpful tips rather

183
00:15:19,700 --> 00:15:23,500
than hidden gems within MLflow.

184
00:15:23,500 --> 00:15:26,860
The first one involves another story within Project Llama.

185
00:15:27,780 --> 00:15:34,820
So Project Llama was, again, a ton of developers with lots of different ranges of experience.

186
00:15:34,820 --> 00:15:44,140
And the midway through the project, we accidentally deleted our MLflow database, which is definitely

187
00:15:44,140 --> 00:15:51,260
a step back in terms of documentation and timelines.

188
00:15:51,260 --> 00:15:58,100
We had to spend another week trying to reproduce our results, and it took us time to retrace

189
00:15:58,100 --> 00:16:03,720
our steps and figure out where we want to go next when it came to experimentation and

190
00:16:03,720 --> 00:16:06,380
model development.

191
00:16:06,380 --> 00:16:11,900
So for that reason, I highly recommend that you create a backup of your MLflow database

192
00:16:11,900 --> 00:16:16,340
from day one, because even though you don't think you'll need it, you never know when

193
00:16:16,340 --> 00:16:19,980
your database will accidentally be deleted.

194
00:16:19,980 --> 00:16:28,740
I suggest either creating a backup of your RDS instance or however you have it connected,

195
00:16:28,740 --> 00:16:34,940
or just logging the actual MLflow files to S3 and ensuring that it is synced with your

196
00:16:34,940 --> 00:16:36,300
repo.

197
00:16:36,300 --> 00:16:42,700
Anything will help besides not having a backup at all.

198
00:16:42,700 --> 00:16:46,060
And the second one is to commit frequently.

199
00:16:46,060 --> 00:16:52,820
In software development, this is a mantra that we all know about, but I feel like when

200
00:16:52,820 --> 00:16:59,380
it comes to machine learning and data science, it can be hard to remember due to the highly

201
00:16:59,380 --> 00:17:04,020
experimental nature of the process.

202
00:17:04,020 --> 00:17:10,100
Even though your training pipeline code may seem totally far away from the final product

203
00:17:11,100 --> 00:17:16,980
experimentation, I recommend that you take the time to make sure that you're committing

204
00:17:16,980 --> 00:17:17,980
every single time.

205
00:17:17,980 --> 00:17:23,820
You have a change important enough to kick off a whole new run of experiments.

206
00:17:23,820 --> 00:17:29,260
I think that that has helped our team, because that means that every single time we've committed

207
00:17:29,260 --> 00:17:35,240
right before kicking off a batch of experiments, we are sure that the commit hash contains

208
00:17:35,240 --> 00:17:42,840
all of the code that would be required to then validate your model performance.

209
00:17:42,840 --> 00:17:48,680
Our team has actually instilled a bash script that we have to warn us when there is a dirty

210
00:17:48,680 --> 00:17:52,360
working tree before kicking off a training pipeline.

211
00:17:52,360 --> 00:17:53,360
Yay!

212
00:17:53,360 --> 00:17:58,360
That's how I feel.

213
00:17:58,360 --> 00:18:02,480
And the third one is to version your data.

214
00:18:02,480 --> 00:18:07,960
This one is more related to the input data, but as we talked about earlier, there are

215
00:18:07,960 --> 00:18:10,600
three parts of a machine learning project.

216
00:18:10,600 --> 00:18:13,880
There is your code, your metrics, and your data.

217
00:18:13,880 --> 00:18:19,960
MLflow does a great job at tracking your metrics, and it also has a way for you to align your

218
00:18:19,960 --> 00:18:25,940
metrics with your code, but as far as I know, it doesn't have any data versioning yet.

219
00:18:25,940 --> 00:18:31,240
So I would recommend implementing some kind of tool like Data Version Control or Cookie

220
00:18:31,280 --> 00:18:36,200
Cutter Data Science or some kind of knowing culture within your team that helps you understand

221
00:18:36,200 --> 00:18:38,480
the different phases of your data.

222
00:18:38,480 --> 00:18:42,200
This will help with validation as well.

223
00:18:42,200 --> 00:18:47,160
So that is kind of it for the three hidden gems of MLflow.

224
00:18:47,160 --> 00:18:50,520
I wanted to point out a few important links.

225
00:18:50,520 --> 00:18:55,240
If you want to try out these hidden gems, the first link contains a link to a GitHub

226
00:18:55,280 --> 00:19:01,720
repo that contains the code to a containerized application demonstrating these capabilities

227
00:19:01,720 --> 00:19:03,040
in working code.

228
00:19:03,040 --> 00:19:07,000
So definitely feel free to test that out.

229
00:19:07,000 --> 00:19:14,280
So you don't have to go and discover how to implement some of these things because they

230
00:19:14,280 --> 00:19:18,680
can be kind of tricky looking through the documentation alone.

231
00:19:18,680 --> 00:19:21,800
The second one is a link to my contact.

232
00:19:21,800 --> 00:19:23,960
So it has my LinkedIn and my social.

233
00:19:23,960 --> 00:19:25,320
Feel free to add that.

234
00:19:25,320 --> 00:19:30,560
And the third one is my company's name and our website.

235
00:19:30,560 --> 00:19:35,520
I work with Kung Fu AI, which is a boutique AI consulting firm, and if you want to work

236
00:19:35,520 --> 00:19:42,120
with a team that likes to build trust within their metrics and their process, then talk

237
00:19:42,120 --> 00:19:43,720
to me about it after this.

